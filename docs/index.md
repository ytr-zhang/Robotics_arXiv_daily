---
layout: default
---

## Updated on 2026.02.25
> Usage instructions: [here](./docs/README.md#usage)

## Manipulation

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking**|Brian Sheil Team|[2602.21161](http://arxiv.org/abs/2602.21161)|null|
|**2026-02-24**|**HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning**|Song Guo Team|[2602.21157](http://arxiv.org/abs/2602.21157)|null|
|**2026-02-24**|**Matching Multiple Experts: On the Exploitability of Multi-Agent Imitation Learning**|Negar Mehr Team|[2602.21020](http://arxiv.org/abs/2602.21020)|null|
|**2026-02-24**|**IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation**|Huixu Dong Team|[2602.20715](http://arxiv.org/abs/2602.20715)|null|
|**2026-02-24**|**BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model**|Hua Chen Team|[2602.20566](http://arxiv.org/abs/2602.20566)|null|
|**2026-02-24**|**Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination**|David C Parkes Team|[2602.20517](http://arxiv.org/abs/2602.20517)|null|
|**2026-02-23**|**FACTO: Function-space Adaptive Constrained Trajectory Optimization for Robotic Manipulators**|Minghui Zheng Team|[2602.20225](http://arxiv.org/abs/2602.20225)|null|
|**2026-02-22**|**Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation**|Liqiang Nie Team|[2602.20200](http://arxiv.org/abs/2602.20200)|null|
|**2026-02-23**|**AdaWorldPolicy: World-Model-Driven Diffusion Policy with Online Adaptive Learning for Robotic Manipulation**|Dong Xu Team|[2602.20057](http://arxiv.org/abs/2602.20057)|**[link](https://AdaWorldPolicy.github.io)**|
|**2026-02-23**|**Beyond Mimicry: Toward Lifelong Adaptability in Imitation Learning**|Odinaldo Rodrigues Team|[2602.19930](http://arxiv.org/abs/2602.19930)|null|
|**2026-02-23**|**Hilbert-Augmented Reinforcement Learning for Scalable Multi-Robot Coverage and Exploration**|Aryya Gangopadhyay Team|[2602.19400](http://arxiv.org/abs/2602.19400)|null|
|**2026-02-22**|**Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization**|Dimitris N. Metaxas Team|[2602.19372](http://arxiv.org/abs/2602.19372)|null|
|**2026-02-22**|**The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption**|Matthias Scheutz Team|[2602.19260](http://arxiv.org/abs/2602.19260)|null|
|**2026-02-22**|**Human-to-Robot Interaction: Learning from Video Demonstration for Robot Imitation**|Xiem HoangVan Team|[2602.19184](http://arxiv.org/abs/2602.19184)|null|
|**2026-02-21**|**TactEx: An Explainable Multimodal Robotic Interaction Framework for Human-Like Touch and Hardness Estimation**|Dandan Zhang Team|[2602.18967](http://arxiv.org/abs/2602.18967)|null|
|**2026-02-21**|**TPRU: Advancing Temporal and Procedural Understanding in Large Multimodal Models**|Yuan Xie Team|[2602.18884](http://arxiv.org/abs/2602.18884)|**[link](https://github.com/Stephen-gzk/TPRU)**|
|**2026-02-21**|**Issues with Measuring Task Complexity via Random Policies in Robotic Tasks**|Aditya Gilra Team|[2602.18856](http://arxiv.org/abs/2602.18856)|null|
|**2026-02-21**|**HeRO: Hierarchical 3D Semantic Representation for Pose-aware Object Manipulation**|Shuaicheng Liu Team|[2602.18817](http://arxiv.org/abs/2602.18817)|null|
|**2026-02-21**|**RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning**|Jinwoo Shin Team|[2602.18742](http://arxiv.org/abs/2602.18742)|**[link](https://seungkukim.github.io/robocurate/)**|
|**2026-02-20**|**Enhancing Goal Inference via Correction Timing**|Tesca Fitzgerald Team|[2602.18603](http://arxiv.org/abs/2602.18603)|null|
|**2026-02-20**|**SimVLA: A Simple VLA Baseline for Robotic Manipulation**|Zhenguo Li Team|[2602.18224](http://arxiv.org/abs/2602.18224)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-20**|**Quasi-Periodic Gaussian Process Predictive Iterative Learning Control**|Michael Burke Team|[2602.18014](http://arxiv.org/abs/2602.18014)|null|
|**2026-02-20**|**Learning Optimal and Sample-Efficient Decision Policies with Guarantees**|Daqian Shao Team|[2602.17978](http://arxiv.org/abs/2602.17978)|null|
|**2026-02-20**|**Graph-Neural Multi-Agent Coordination for Distributed Access-Point Selection in Cell-Free Massive MIMO**|Raouf Boutaba Team|[2602.17954](http://arxiv.org/abs/2602.17954)|null|
|**2026-02-20**|**ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models**|Ang Li Team|[2602.17951](http://arxiv.org/abs/2602.17951)|null|
|**2026-02-19**|**MePoly: Max Entropy Polynomial Policy Optimization**|Maani Ghaffari Team|[2602.17832](http://arxiv.org/abs/2602.17832)|null|
|**2026-02-19**|**IRIS: Learning-Driven Task-Specific Cinema Robot Arm for Visuomotor Motion Control**|Ali Bereyhi Team|[2602.17537](http://arxiv.org/abs/2602.17537)|null|
|**2026-02-19**|**Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success**|Torsten Sattler Team|[2602.17101](http://arxiv.org/abs/2602.17101)|null|
|**2026-02-18**|**SparTa: Sparse Graphical Task Models from a Handful of Demonstrations**|Abhinav Valada Team|[2602.16911](http://arxiv.org/abs/2602.16911)|null|
|**2026-02-20**|**MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation**|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-18**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-18**|**VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**|Stella X. Yu Team|[2602.16511](http://arxiv.org/abs/2602.16511)|null|
|**2026-02-19**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|null|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**|Takamitsu Matsubara Team|[2602.15351](http://arxiv.org/abs/2602.15351)|null|
|**2026-02-18**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**|Chuang Gan Team|[2602.14968](http://arxiv.org/abs/2602.14968)|null|
|**2026-02-16**|**Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**|Weiming Zhi Team|[2602.14874](http://arxiv.org/abs/2602.14874)|null|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-16**|**RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**|Alireza Taheri Team|[2602.14438](http://arxiv.org/abs/2602.14438)|null|
|**2026-02-16**|**A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**|Masashi Hamaya Team|[2602.14434](http://arxiv.org/abs/2602.14434)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**GRAIL: Goal Recognition Alignment through Imitation Learning**|Reuth Mirsky Team|[2602.14252](http://arxiv.org/abs/2602.14252)|null|
|**2026-02-15**|**Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**|Hao Dong Team|[2602.14193](http://arxiv.org/abs/2602.14193)|**[link](https://pa3ff.github.io)**|
|**2026-02-15**|**RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**|Jian Tang Team|[2602.14032](http://arxiv.org/abs/2602.14032)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**|Gabriel de Oliveira Ramos Team|[2602.13865](http://arxiv.org/abs/2602.13865)|null|
|**2026-02-14**|**Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**|Shengbo Eben Li Team|[2602.13810](http://arxiv.org/abs/2602.13810)|null|
|**2026-02-14**|**Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**|Lei Sun Team|[2602.13806](http://arxiv.org/abs/2602.13806)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**|Yide Liu Team|[2602.13718](http://arxiv.org/abs/2602.13718)|null|
|**2026-02-14**|**Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**|Tao Yu Team|[2602.13689](http://arxiv.org/abs/2602.13689)|null|
|**2026-02-14**|**Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**|Peng Liu Team|[2602.13640](http://arxiv.org/abs/2602.13640)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-13**|**Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**|Wei-Chiu Ma Team|[2602.13197](http://arxiv.org/abs/2602.13197)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**How Swarms Differ: Challenges in Collective Behaviour Comparison**|Jonas Kuckling Team|[2602.13016](http://arxiv.org/abs/2602.13016)|null|
|**2026-02-13**|**SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**|Andreas Kugi Team|[2602.12794](http://arxiv.org/abs/2602.12794)|null|
|**2026-02-13**|**Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**|Abhinav Valada Team|[2602.12734](http://arxiv.org/abs/2602.12734)|null|
|**2026-02-13**|**$\mathcal{X}$ -KD: General Experiential Knowledge Distillation for Large Language Models**|Yuyu Yuan Team|[2602.12674](http://arxiv.org/abs/2602.12674)|null|
|**2026-02-13**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|null|
|**2026-02-13**|**Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**|Jun Ma Team|[2602.12633](http://arxiv.org/abs/2602.12633)|**[link](https://physics-constrained-real2sim.github.io)**|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**|Weidi Xie Team|[2602.12155](http://arxiv.org/abs/2602.12155)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**HoloBrain-0 Technical Report**|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**Accelerating Robotic Reinforcement Learning with Agent Guidance**|Yaodong Yang Team|[2602.11978](http://arxiv.org/abs/2602.11978)|null|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|null|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|null|
|**2026-02-12**|**EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**|Qin Jin Team|[2602.11464](http://arxiv.org/abs/2602.11464)|null|
|**2026-02-11**|**Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**|Christopher G. Atkeson Team|[2602.11393](http://arxiv.org/abs/2602.11393)|null|
|**2026-02-11**|**MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**|Ranjay Krishna Team|[2602.11337](http://arxiv.org/abs/2602.11337)|null|
|**2026-02-11**|**ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**|Mu Xu Team|[2602.11236](http://arxiv.org/abs/2602.11236)|**[link](https://amap-cvlab.github.io/ABot-Manipulation/)**|
|**2026-02-11**|**YOR: Your Own Mobile Manipulator for Generalizable Robotics**|Zichen Jeff Cui Team|[2602.11150](http://arxiv.org/abs/2602.11150)|null|
|**2026-02-11**|**OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**|Balaraman Ravindran Team|[2602.11018](http://arxiv.org/abs/2602.11018)|null|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-11**|**Semi-Supervised Cross-Domain Imitation Learning**|Ping-Chun Hsieh Team|[2602.10793](http://arxiv.org/abs/2602.10793)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**From Interaction to Demonstration Quality in Virtual Reality: Effects of Interaction Modality and Visual Representation on Everyday Tasks**|Anna-Lisa Vollmer Team|[2602.10618](http://arxiv.org/abs/2602.10618)|null|
|**2026-02-11**|**Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning**|Penny Sweetser Team|[2602.10594](http://arxiv.org/abs/2602.10594)|null|
|**2026-02-10**|**Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning**|Faizan M. Tariq Team|[2602.10285](http://arxiv.org/abs/2602.10285)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos**|Jiangmiao Pang Team|[2602.10105](http://arxiv.org/abs/2602.10105)|null|
|**2026-02-10**|**Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction**|Jiangmiao Pang Team|[2602.10101](http://arxiv.org/abs/2602.10101)|null|
|**2026-02-10**|**UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking**|Yao Mu Team|[2602.10093](http://arxiv.org/abs/2602.10093)|**[link](https://univtac.github.io/)**|
|**2026-02-11**|**RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments**|Laxmidhar Behera Team|[2602.10015](http://arxiv.org/abs/2602.10015)|null|
|**2026-02-10**|**Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper**|Yen-Ling Kuo Team|[2602.10013](http://arxiv.org/abs/2602.10013)|null|
|**2026-02-10**|**RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**|Jiangmiao Pang Team|[2602.09973](http://arxiv.org/abs/2602.09973)|null|
|**2026-02-10**|**Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation**|Laxmidhar Behera Team|[2602.09940](http://arxiv.org/abs/2602.09940)|null|
|**2026-02-10**|**TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback**|Weiming Zhi Team|[2602.09888](http://arxiv.org/abs/2602.09888)|null|
|**2026-02-10**|**MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation**|Xiangyu Yue Team|[2602.09878](http://arxiv.org/abs/2602.09878)|null|
|**2026-02-10**|**Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning**|Wei Li Team|[2602.09767](http://arxiv.org/abs/2602.09767)|null|
|**2026-02-10**|**VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model**|Hui Xiong Team|[2602.09638](http://arxiv.org/abs/2602.09638)|null|
|**2026-02-10**|**Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation**|Danica Kragic Team|[2602.09583](http://arxiv.org/abs/2602.09583)|null|
|**2026-02-09**|**SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes**|Russ Tedrake Team|[2602.09153](http://arxiv.org/abs/2602.09153)|**[link](https://scenesmith.github.io/)**|
|**2026-02-09**|**TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation**|Shanghang Zhang Team|[2602.09023](http://arxiv.org/abs/2602.09023)|null|
|**2026-02-09**|**$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies**|Yibo Yuan Team|[2602.09021](http://arxiv.org/abs/2602.09021)|null|
|**2026-02-09**|**Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models**|Nur Muhammad Mahi Shafiullah Team|[2602.09017](http://arxiv.org/abs/2602.09017)|null|
|**2026-02-09**|**Learning Potentials for Dynamic Matching and Application to Heart Transplantation**|Tuomas Sandholm Team|[2602.08878](http://arxiv.org/abs/2602.08878)|null|
|**2026-02-09**|**Mimic Intent, Not Just Trajectories**|Panpan Cai Team|[2602.08602](http://arxiv.org/abs/2602.08602)|null|
|**2026-02-09**|**GISA: A Benchmark for General Information-Seeking Assistant**|Zhicheng Dou Team|[2602.08543](http://arxiv.org/abs/2602.08543)|null|
|**2026-02-09**|**Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes**|Ayoung Kim Team|[2602.08266](http://arxiv.org/abs/2602.08266)|null|
|**2026-02-09**|**STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction**|Guohao Dai Team|[2602.08245](http://arxiv.org/abs/2602.08245)|null|
|**2026-02-07**|**Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation**|Jianfei Yang Team|[2602.07388](http://arxiv.org/abs/2602.07388)|null|
|**2026-02-07**|**Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions**|Zhuo Zou Team|[2602.07341](http://arxiv.org/abs/2602.07341)|null|
|**2026-02-07**|**RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving**|Ganesh Krishnasamy Team|[2602.07339](http://arxiv.org/abs/2602.07339)|null|
|**2026-02-07**|**Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing**|Seokhwan Jeong Team|[2602.07326](http://arxiv.org/abs/2602.07326)|null|
|**2026-02-06**|**MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation**|Wei Gao Team|[2602.07082](http://arxiv.org/abs/2602.07082)|null|
|**2026-02-06**|**SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization**|Majid Khadiv Team|[2602.06864](http://arxiv.org/abs/2602.06864)|null|
|**2026-02-06**|**RAPID: Reconfigurable, Adaptive Platform for Iterative Design**|Jia Liu Team|[2602.06653](http://arxiv.org/abs/2602.06653)|null|
|**2026-02-06**|**Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique**|Toshiaki Tsuji Team|[2602.06620](http://arxiv.org/abs/2602.06620)|null|
|**2026-02-06**|**The Law of Task-Achieving Body Motion: Axiomatizing Success of Robot Manipulation Actions**|Michael Beetz Team|[2602.06572](http://arxiv.org/abs/2602.06572)|null|
|**2026-02-06**|**Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation**|Heng Tao Shen Team|[2602.06512](http://arxiv.org/abs/2602.06512)|null|
|**2026-02-06**|**World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy**|Mike Zheng Shou Team|[2602.06508](http://arxiv.org/abs/2602.06508)|null|
|**2026-02-06**|**Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation**|Weiying Xie Team|[2602.06356](http://arxiv.org/abs/2602.06356)|null|
|**2026-02-06**|**A High-Fidelity Robotic Manipulator Teleoperation Framework for Human-Centered Augmented Reality Evaluation**|Tian Guo Team|[2602.06273](http://arxiv.org/abs/2602.06273)|null|
|**2026-02-05**|**TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**|Shigeki Sugano Team|[2602.05468](http://arxiv.org/abs/2602.05468)|null|
|**2026-02-05**|**MobileManiBench: Simplifying Model Verification for Mobile Manipulation**|Baining Guo Team|[2602.05233](http://arxiv.org/abs/2602.05233)|null|
|**2026-02-04**|**VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**|Dongdong Chen Team|[2602.05049](http://arxiv.org/abs/2602.05049)|**[link](https://vista-vla.github.io/)**|
|**2026-02-04**|**CoWTracker: Tracking by Warping instead of Correlation**|Andrea Vedaldi Team|[2602.04877](http://arxiv.org/abs/2602.04877)|null|
|**2026-02-04**|**A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction**|Riddhiman Laha Team|[2602.04522](http://arxiv.org/abs/2602.04522)|null|
|**2026-02-04**|**Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation**|Wenzhao Lian Team|[2602.04243](http://arxiv.org/abs/2602.04243)|null|
|**2026-02-04**|**GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning**|Hongliang Ren Team|[2602.04231](http://arxiv.org/abs/2602.04231)|null|
|**2026-02-04**|**Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**|Badong Chen Team|[2602.04228](http://arxiv.org/abs/2602.04228)|null|
|**2026-02-04**|**OAT: Ordered Action Tokenization**|Yilun Du Team|[2602.04215](http://arxiv.org/abs/2602.04215)|null|
|**2026-02-04**|**InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons**|Reid Simmons Team|[2602.04213](http://arxiv.org/abs/2602.04213)|null|
|**2026-02-04**|**A brief review of evolutionary game dynamics in the reinforcement learning paradigm**|Li Chen Team|[2602.04150](http://arxiv.org/abs/2602.04150)|null|
|**2026-02-03**|**Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study**|Farshid Alambeigi Team|[2602.04076](http://arxiv.org/abs/2602.04076)|null|
|**2026-02-03**|**VLS: Steering Pretrained Robot Policies via Vision-Language Models**|Ranjay Krishna Team|[2602.03973](http://arxiv.org/abs/2602.03973)|**[link](https://vision-language-steering.github.io/webpage/)**|
|**2026-02-03**|**MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**|Jungwoo Lee Team|[2602.03668](http://arxiv.org/abs/2602.03668)|null|
|**2026-02-03**|**Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives**|Sho Sakaino Team|[2602.03188](http://arxiv.org/abs/2602.03188)|null|
|**2026-02-02**|**Language Movement Primitives: Grounding Language Models in Robot Motion**|Simon Stepputtis Team|[2602.02839](http://arxiv.org/abs/2602.02839)|null|
|**2026-02-02**|**On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning**|Sébastien Lachapelle Team|[2602.02762](http://arxiv.org/abs/2602.02762)|null|
|**2026-02-02**|**HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**|Ping Tan Team|[2602.02473](http://arxiv.org/abs/2602.02473)|null|
|**2026-02-02**|**TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**|Jiaqi Ma Team|[2602.02459](http://arxiv.org/abs/2602.02459)|null|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|**[link](https://world-gymnast.github.io/)**|
|**2026-02-02**|**Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning**|Alan Ritter Team|[2602.02405](http://arxiv.org/abs/2602.02405)|null|
|**2026-02-02**|**SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation**|Jiangmiao Pang Team|[2602.02402](http://arxiv.org/abs/2602.02402)|**[link](https://city-super.github.io/SoMA/)**|
|**2026-02-02**|**PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning**|Alexander Schperberg Team|[2602.02396](http://arxiv.org/abs/2602.02396)|null|
|**2026-02-02**|**Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy**|Qiang Nie Team|[2602.01939](http://arxiv.org/abs/2602.01939)|null|
|**2026-02-02**|**ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning**|Sifa Zheng Team|[2602.01916](http://arxiv.org/abs/2602.01916)|null|
|**2026-02-02**|**BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models**|Matteo Matteucci Team|[2602.01870](http://arxiv.org/abs/2602.01870)|null|
|**2026-02-03**|**RFS: Reinforcement learning with Residual flow steering for dexterous manipulation**|Abhishek Gupta Team|[2602.01789](http://arxiv.org/abs/2602.01789)|null|
|**2026-02-02**|**AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**|Yu She Team|[2602.01662](http://arxiv.org/abs/2602.01662)|null|
|**2026-02-02**|**A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**|Shreyas Kousik Team|[2602.01632](http://arxiv.org/abs/2602.01632)|**[link](https://sew-mimic.com/)**|
|**2026-02-01**|**Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning**|Weitong Zhang Team|[2602.01357](http://arxiv.org/abs/2602.01357)|null|
|**2026-02-01**|**Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**|Shanghang Zhang Team|[2602.01166](http://arxiv.org/abs/2602.01166)|null|
|**2026-02-01**|**Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**|Matteo Matteucci Team|[2602.01158](http://arxiv.org/abs/2602.01158)|null|
|**2026-02-01**|**UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors**|Shan Luo Team|[2602.01153](http://arxiv.org/abs/2602.01153)|null|
|**2026-02-01**|**KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV**|Ziyang Wang Team|[2602.01115](http://arxiv.org/abs/2602.01115)|null|
|**2026-02-01**|**StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**|Lu Fang Team|[2602.01100](http://arxiv.org/abs/2602.01100)|null|
|**2026-02-01**|**Estimating Force Interactions of Deformable Linear Objects from their Shapes**|Quang-Cuong Pham Team|[2602.01085](http://arxiv.org/abs/2602.01085)|null|
|**2026-02-01**|**A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**|Jose Barreiros Team|[2602.01067](http://arxiv.org/abs/2602.01067)|null|
|**2026-01-30**|**Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**|Liu Hong Team|[2601.23087](http://arxiv.org/abs/2601.23087)|null|
|**2026-01-30**|**Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**|Guang Chen Team|[2601.22988](http://arxiv.org/abs/2601.22988)|null|
|**2026-01-30**|**Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation**|Wuyue Zhao Team|[2601.22965](http://arxiv.org/abs/2601.22965)|null|
|**2026-01-29**|**PoSafeNet: Safe Learning with Poset-Structured Neural Nets**|Daniela Rus Team|[2601.22356](http://arxiv.org/abs/2601.22356)|null|
|**2026-01-29**|**Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data**|Bowen Weng Team|[2601.22242](http://arxiv.org/abs/2601.22242)|null|
|**2026-01-29**|**Causal Imitation Learning Under Measurement Error and Distribution Shift**|AmirEmad Ghassami Team|[2601.22206](http://arxiv.org/abs/2601.22206)|null|
|**2026-01-29**|**mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning**|Pieter Abbeel Team|[2601.22074](http://arxiv.org/abs/2601.22074)|**[link](https://github.com/mujocolab/mjlab)**|
|**2026-01-30**|**PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**|Jie Mei Team|[2601.22018](http://arxiv.org/abs/2601.22018)|null|
|**2026-01-29**|**Causal World Modeling for Robot Control**|Yinghao Xu Team|[2601.21998](http://arxiv.org/abs/2601.21998)|**[link](https://technology.robbyant.com/lingbot-va)**|
|**2026-01-29**|**MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**|Stefanie Speidel Team|[2601.21971](http://arxiv.org/abs/2601.21971)|null|
|**2026-01-29**|**Information Filtering via Variational Regularization for Robot Manipulation**|Jie Me Team|[2601.21926](http://arxiv.org/abs/2601.21926)|null|
|**2026-01-29**|**When does predictive inverse dynamics outperform behavior cloning?**|Sergio Valcarcel Macua Team|[2601.21718](http://arxiv.org/abs/2601.21718)|null|
|**2026-01-29**|**Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation**|Liming Chen Team|[2601.21416](http://arxiv.org/abs/2601.21416)|null|
|**2026-01-29**|**Towards Space-Based Environmentally-Adaptive Grasping**|Aleksandr Artemov Team|[2601.21394](http://arxiv.org/abs/2601.21394)|null|
|**2026-01-29**|**Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies**|Harold Soh Team|[2601.21251](http://arxiv.org/abs/2601.21251)|null|
|**2026-01-28**|**Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy**|Christos Bergeles Team|[2601.20776](http://arxiv.org/abs/2601.20776)|null|
|**2026-01-28**|**Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands**|Nicolás Navarro-Guerrero Team|[2601.20555](http://arxiv.org/abs/2601.20555)|null|
|**2026-01-28**|**Advancing Open-source World Models**|Hao Ouyang Team|[2601.20540](http://arxiv.org/abs/2601.20540)|**[link](https://technology.robbyant.com/lingbot-world)**|
|**2026-01-28**|**STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation**|Liming Chen Team|[2601.20381](http://arxiv.org/abs/2601.20381)|null|
|**2026-01-28**|**Demonstration-Free Robotic Control via LLM Agents**|Tiffany J. Hwu Team|[2601.20334](http://arxiv.org/abs/2601.20334)|null|
|**2026-01-28**|**Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation**|Ziyuan Jiao Team|[2601.20321](http://arxiv.org/abs/2601.20321)|null|
|**2026-01-28**|**TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement**|Yaonan Wang Team|[2601.20208](http://arxiv.org/abs/2601.20208)|**[link](https://github.com/Dikay1/TRACER)**|
|**2026-01-27**|**Real-Time Robot Execution with Masked Action Chunking**|Gaowen Liu Team|[2601.20130](http://arxiv.org/abs/2601.20130)|**[link](https://remac-async.github.io/)**|
|**2026-01-27**|**In-Context Reinforcement Learning From Suboptimal Historical Data**|Vahid Tarokh Team|[2601.20116](http://arxiv.org/abs/2601.20116)|null|
|**2026-01-27**|**Just in time Informed Trees: Manipulability-Aware Asymptotically Optimized Motion Planning**|Luis Figueredo Team|[2601.19972](http://arxiv.org/abs/2601.19972)|null|
|**2026-01-27**|**NeuroAI and Beyond**|Terrence Sejnowski Team|[2601.19955](http://arxiv.org/abs/2601.19955)|null|
|**2026-01-27**|**AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation**|Lei Zhu Team|[2601.19634](http://arxiv.org/abs/2601.19634)|null|
|**2026-01-27**|**ALRM: Agentic LLM for Robotic Manipulation**|Hakim Hacid Team|[2601.19510](http://arxiv.org/abs/2601.19510)|null|
|**2026-01-27**|**Task-Centric Policy Optimization from Misaligned Motion Priors**|Shentao Qin Team|[2601.19411](http://arxiv.org/abs/2601.19411)|null|
|**2026-01-27**|**Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation**|Heng Tao Shen Team|[2601.19406](http://arxiv.org/abs/2601.19406)|null|
|**2026-01-26**|**DeFM: Learning Foundation Representations from Depth for Robotics**|Marco Hutter Team|[2601.18923](http://arxiv.org/abs/2601.18923)|null|
|**2026-01-26**|**Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge**|Luc Van Gool Team|[2601.18733](http://arxiv.org/abs/2601.18733)|**[link](https://mars-eai.github.io/MARS-Challenge-Webpage/)**|
|**2026-01-26**|**Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods**|Hong Liu Team|[2601.18723](http://arxiv.org/abs/2601.18723)|null|
|**2026-01-26**|**A Pragmatic VLA Foundation Model**|Kecheng Zheng Team|[2601.18692](http://arxiv.org/abs/2601.18692)|**[link](https://technology.robbyant.com/lingbot-vla/)**|
|**2026-01-26**|**ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection**|Hao-Shu Fang Team|[2601.18629](http://arxiv.org/abs/2601.18629)|null|
|**2026-01-25**|**Masked Depth Modeling for Spatial Perception**|Nan Xue Team|[2601.17895](http://arxiv.org/abs/2601.17895)|null|
|**2026-01-25**|**Less Is More: Scalable Visual Navigation from Limited Data**|Marco Hutter Team|[2601.17815](http://arxiv.org/abs/2601.17815)|null|
|**2026-01-24**|**Towards Generalisable Imitation Learning Through Conditioned Transition Estimation and Online Behaviour Alignment**|Odinaldo Rodrigues Team|[2601.17563](http://arxiv.org/abs/2601.17563)|null|
|**2026-01-24**|**MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions**|Tongtong Feng Team|[2601.17507](http://arxiv.org/abs/2601.17507)|null|
|**2026-01-24**|**EquiForm: Noise-Robust SE(3)-Equivariant Policy Learning from 3D Point Clouds**|Yu She Team|[2601.17486](http://arxiv.org/abs/2601.17486)|**[link](https://ZhangZhiyuanZhang.github.io/equiform-website/)**|
|**2026-01-24**|**Scaling Rough Terrain Locomotion with Automatic Curriculum Reinforcement Learning**|Marco Hutter Team|[2601.17428](http://arxiv.org/abs/2601.17428)|null|
|**2026-01-24**|**EMPM: Embodied MPM for Modeling and Simulation of Deformable Objects**|Chenfanfu Jiang Team|[2601.17251](http://arxiv.org/abs/2601.17251)|null|
|**2026-01-23**|**Advancing Improvisation in Human-Robot Construction Collaboration: Taxonomy and Research Roadmap**|Carol C. Menassa Team|[2601.17219](http://arxiv.org/abs/2601.17219)|null|
|**2026-01-23**|**ConceptACT: Episode-Level Concepts for Sample-Efficient Robotic Imitation Learning**|Friedhelm Schwenker Team|[2601.17135](http://arxiv.org/abs/2601.17135)|null|
|**2026-01-23**|**Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators**|Daniele Nardi Team|[2601.16866](http://arxiv.org/abs/2601.16866)|null|
|**2026-01-23**|**Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation**|Álvaro Jesús López-López Team|[2601.16677](http://arxiv.org/abs/2601.16677)|null|
|**2026-01-23**|**ReWeaver: Towards Simulation-Ready and Topology-Accurate Garment Reconstruction**|Xiangru Huang Team|[2601.16672](http://arxiv.org/abs/2601.16672)|null|
|**2026-01-23**|**ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance**|Wei-Shi Zheng Team|[2601.16667](http://arxiv.org/abs/2601.16667)|null|
|**2026-01-22**|**Scalable Screw-Theoretic Synthesis for PDE-Based Dynamic Modeling of Multibody Flexible Manipulators**|J. Mattila Team|[2601.16242](http://arxiv.org/abs/2601.16242)|null|
|**2026-01-22**|**DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models**|Jingqun Tang Team|[2601.16065](http://arxiv.org/abs/2601.16065)|null|
|**2026-01-22**|**Off-Policy Actor-Critic with Sigmoid-Bounded Entropy for Real-World Robot Learning**|Shu Zhang Team|[2601.15761](http://arxiv.org/abs/2601.15761)|null|
|**2026-01-21**|**CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation**|Arash Ajoudani Team|[2601.15541](http://arxiv.org/abs/2601.15541)|null|
|**2026-01-22**|**BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries**|Kai Chen Team|[2601.15197](http://arxiv.org/abs/2601.15197)|null|
|**2026-01-21**|**UniCon: A Unified System for Efficient Robot Learning Transfers**|Weinan Zhang Team|[2601.14617](http://arxiv.org/abs/2601.14617)|null|
|**2026-01-20**|**Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects**|Pietro Falco Team|[2601.13979](http://arxiv.org/abs/2601.13979)|null|
|**2026-01-20**|**A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint**|Yongchun Fang Team|[2601.13639](http://arxiv.org/abs/2601.13639)|null|
|**2026-01-19**|**Static Is Not Enough: A Comparative Study of VR and SpaceMouse in Static and Dynamic Teleoperation Tasks**|Kim Baraka Team|[2601.13042](http://arxiv.org/abs/2601.13042)|null|
|**2026-01-19**|**Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization**|Zongqing Lu Team|[2601.12993](http://arxiv.org/abs/2601.12993)|null|
|**2026-01-19**|**Imitation learning-based spacecraft rendezvous and docking method with Expert Demonstration**|Mingxuan Jiang Team|[2601.12952](http://arxiv.org/abs/2601.12952)|null|
|**2026-01-19**|**ForeDiffusion: Foresight-Conditioned Diffusion Policy via Future View Construction for Robot Manipulation**|F. Richard Yu Team|[2601.12925](http://arxiv.org/abs/2601.12925)|null|
|**2026-01-19**|**Dynamic Hand Gesture Recognition for Robot Manipulator Tasks**|Laxmidhar Behera Team|[2601.12918](http://arxiv.org/abs/2601.12918)|null|
|**2026-01-19**|**Contact-Aware Neural Dynamics**|Sha Yi Team|[2601.12796](http://arxiv.org/abs/2601.12796)|null|
|**2026-01-18**|**ReWorld: Multi-Dimensional Reward Modeling for Embodied World Models**|Xin Jin Team|[2601.12428](http://arxiv.org/abs/2601.12428)|null|
|**2026-01-18**|**Learning Diverse Skills for Behavior Models with Mixture of Experts**|Ziyang Meng Team|[2601.12397](http://arxiv.org/abs/2601.12397)|null|
|**2026-01-17**|**BiKC+: Bimanual Hierarchical Imitation with Keypose-Conditioned Coordination-Aware Consistency Policies**|Jia PanI Team|[2601.12116](http://arxiv.org/abs/2601.12116)|null|
|**2026-01-16**|**Risk-Aware Human-in-the-Loop Framework with Adaptive Intrusion Response for Autonomous Vehicles**|Jin-Hee Cho Team|[2601.11781](http://arxiv.org/abs/2601.11781)|null|
|**2026-01-16**|**Generative Scenario Rollouts for End-to-End Autonomous Driving**|Hong Cai Team|[2601.11475](http://arxiv.org/abs/2601.11475)|null|
|**2026-01-16**|**The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents**|Yong-Lu Li Team|[2601.11421](http://arxiv.org/abs/2601.11421)|null|
|**2026-01-16**|**The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning**|Sebastian Trimpe Team|[2601.11394](http://arxiv.org/abs/2601.11394)|null|
|**2026-01-16**|**X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning**|Huazhe Xu Team|[2601.11269](http://arxiv.org/abs/2601.11269)|null|
|**2026-01-16**|**Skill-Aware Diffusion for Generalizable Robotic Manipulation**|Wei Zhang Team|[2601.11266](http://arxiv.org/abs/2601.11266)|null|
|**2026-01-15**|**Future Optical Flow Prediction Improves Robot Control & Video Generation**|Juan Carlos Niebles Team|[2601.10781](http://arxiv.org/abs/2601.10781)|**[link](https://fofpred.github.io)**|
|**2026-01-15**|**Combinatorial Optimization Augmented Machine Learning**|Axel Parmentier Team|[2601.10583](http://arxiv.org/abs/2601.10583)|null|
|**2026-01-14**|**SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping**|Jiachen Li Team|[2601.09920](http://arxiv.org/abs/2601.09920)|null|
|**2026-01-14**|**Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets**|Zsolt Kira Team|[2601.09605](http://arxiv.org/abs/2601.09605)|null|
|**2026-01-14**|**Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations**|Wei-Shi Zheng Team|[2601.09518](http://arxiv.org/abs/2601.09518)|null|
|**2026-01-14**|**A Canonical Internal Model for Disturbance Rejection for a Class of Nonlinear Systems Subject to Trigonometric-Polynomial Disturbances**|Jie Huang Team|[2601.09471](http://arxiv.org/abs/2601.09471)|null|
|**2026-01-14**|**Data Scaling for Navigation in Unknown Environments**|Joni-Kristian Kämäräinen Team|[2601.09444](http://arxiv.org/abs/2601.09444)|null|
|**2026-01-13**|**Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation**|Miao Li Team|[2601.09031](http://arxiv.org/abs/2601.09031)|null|
|**2026-01-13**|**Learning from Demonstrations via Capability-Aware Goal Sampling**|He Zhu Team|[2601.08731](http://arxiv.org/abs/2601.08731)|null|
|**2026-01-13**|**VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**|Junzhi Yu Team|[2601.08665](http://arxiv.org/abs/2601.08665)|**[link](https://wsakobe.github.io/VLingNav-web/)**|
|**2026-01-13**|**ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation**|Yanwei Fu Team|[2601.08325](http://arxiv.org/abs/2601.08325)|null|
|**2026-01-12**|**Video Generation Models in Robotics -- Applications, Research Challenges, Future Directions**|Anirudha Majumdar Team|[2601.07823](http://arxiv.org/abs/2601.07823)|null|
|**2026-01-12**|**LOONG: Online Time-Optimal Autonomous Flight for MAVs in Cluttered Environments**|Shuo Li Team|[2601.07434](http://arxiv.org/abs/2601.07434)|null|
|**2026-01-12**|**LRAS: Advanced Legal Reasoning with Agentic Search**|Yike Guo Team|[2601.07296](http://arxiv.org/abs/2601.07296)|null|
|**2026-01-11**|**PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation**|Ismini Lourentzou Team|[2601.07060](http://arxiv.org/abs/2601.07060)|null|
|**2026-01-13**|**On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning**|Cheng Han Team|[2601.06748](http://arxiv.org/abs/2601.06748)|null|
|**2026-01-13**|**Robotic Tele-Operation for Upper Aerodigestive Tract Microsurgery: System Design and Validation**|Leonardo S. Mattos Team|[2601.06617](http://arxiv.org/abs/2601.06617)|null|
|**2026-01-10**|**CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method**|Heewon Kim Team|[2601.06451](http://arxiv.org/abs/2601.06451)|null|
|**2026-01-09**|**Intelligent Singularity Avoidance in UR10 Robotic Arm Path Planning Using Hybrid Fuzzy Logic and Reinforcement Learning**|Jyh-Horng Wu Team|[2601.05836](http://arxiv.org/abs/2601.05836)|null|
|**2026-01-09**|**SceneFoundry: Generating Interactive Infinite 3D Worlds**|YuanFu Yang Team|[2601.05810](http://arxiv.org/abs/2601.05810)|null|
|**2026-01-09**|**EvoQRE: Modeling Bounded Rationality in Safety-Critical Traffic Simulation via Evolutionary Quantal Response Equilibrium**|Trung-Kiet Huynh Team|[2601.05653](http://arxiv.org/abs/2601.05653)|null|
|**2026-01-09**|**TOSC: Task-Oriented Shape Completion for Open-World Dexterous Grasp Generation from Partial Point Clouds**|Zhiping Cai Team|[2601.05499](http://arxiv.org/abs/2601.05499)|null|
|**2026-01-09**|**Assembling Solar Panels by Dual Robot Arms Towards Full Autonomous Lunar Base Construction**|Kazuya Yoshida Team|[2601.05491](http://arxiv.org/abs/2601.05491)|null|
|**2026-01-08**|**Imitation Learning for Combinatorial Optimisation under Uncertainty**|Louis-Martin Rousseau Team|[2601.05383](http://arxiv.org/abs/2601.05383)|null|
|**2026-01-08**|**Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models**|Yuchen Cui Team|[2601.05336](http://arxiv.org/abs/2601.05336)|null|
|**2026-01-08**|**LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model**|Shanghang Zhang Team|[2601.05248](http://arxiv.org/abs/2601.05248)|null|
|**2026-01-08**|**RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation**|Jiangmiao Pang Team|[2601.05241](http://arxiv.org/abs/2601.05241)|null|
|**2026-01-08**|**Plenoptic Video Generation**|Chen-Hsuan Lin Team|[2601.05239](http://arxiv.org/abs/2601.05239)|**[link](https://research.nvidia.com/labs/dir/plenopticdreamer/)**|
|**2026-01-08**|**UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation**|Peng Zhou Team|[2601.04629](http://arxiv.org/abs/2601.04629)|null|
|**2026-01-08**|**Multiagent Reinforcement Learning with Neighbor Action Estimation**|Aoxiang Liu Team|[2601.04511](http://arxiv.org/abs/2601.04511)|null|
|**2026-01-07**|**Choreographing a World of Dynamic Objects**|Jiajun Wu Team|[2601.04194](http://arxiv.org/abs/2601.04194)|null|
|**2026-01-07**|**Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test**|Jian Tang Team|[2601.04137](http://arxiv.org/abs/2601.04137)|null|
|**2026-01-07**|**PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation**|Li Fei-Fei Team|[2601.03782](http://arxiv.org/abs/2601.03782)|null|
|**2026-01-07**|**Accounting for Optimal Control in the Sizing of Isolated Hybrid Renewable Energy Systems Using Imitation Learning**|Brage Rugstad Knudsen Team|[2601.03679](http://arxiv.org/abs/2601.03679)|null|
|**2026-01-07**|**Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions**|Ping Jian Team|[2601.03590](http://arxiv.org/abs/2601.03590)|null|
|**2026-01-06**|**A High-Fidelity Digital Twin for Robotic Manipulation Based on 3D Gaussian Splatting**|Chengxu Zhou Team|[2601.03200](http://arxiv.org/abs/2601.03200)|null|
|**2026-01-06**|**SOP: A Scalable Online Post-Training System for Vision-Language-Action Models**|Jianlan Luo Team|[2601.03044](http://arxiv.org/abs/2601.03044)|null|
|**2026-01-05**|**InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation**|Yuchen Zhu Team|[2601.02456](http://arxiv.org/abs/2601.02456)|**[link](https://internrobotics.github.io/internvla-a1.github.io/)**|
|**2026-01-05**|**Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot**|Maoqing Yao Team|[2601.02078](http://arxiv.org/abs/2601.02078)|null|
|**2026-01-05**|**Learning Diffusion Policy from Primitive Skills for Robot Manipulation**|Dong Xu Team|[2601.01948](http://arxiv.org/abs/2601.01948)|null|
|**2026-01-04**|**STEMNIST: Spiking Tactile Extended MNIST Neuromorphic Dataset**|Arindam Basu Team|[2601.01658](http://arxiv.org/abs/2601.01658)|null|
|**2026-01-04**|**Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation**|Shanghang Zhang Team|[2601.01618](http://arxiv.org/abs/2601.01618)|null|
|**2026-01-04**|**Online Estimation and Manipulation of Articulated Objects**|Sethu Vijayakumar Team|[2601.01438](http://arxiv.org/abs/2601.01438)|null|
|**2026-01-02**|**Value Vision-Language-Action Planning & Search**|Cyrus Neary Team|[2601.00969](http://arxiv.org/abs/2601.00969)|null|
|**2026-01-08**|**RoboReward: General-Purpose Vision-Language Reward Models for Robotics**|Chelsea Finn Team|[2601.00675](http://arxiv.org/abs/2601.00675)|null|
|**2026-01-01**|**Imitation from Observations with Trajectory-Level Generative Embeddings**|Weitong Zhang Team|[2601.00452](http://arxiv.org/abs/2601.00452)|null|
|**2026-01-05**|**Compositional Diffusion with Guided Search for Long-Horizon Planning**|Danfei Xu Team|[2601.00126](http://arxiv.org/abs/2601.00126)|null|
|**2025-12-31**|**Coordinated Humanoid Manipulation with Choice Policies**|Jitendra Malik Team|[2512.25072](http://arxiv.org/abs/2512.25072)|**[link](https://choice-policy.github.io/)**|
|**2025-12-31**|**Dream2Flow: Bridging Video Generation and Open-World Manipulation with 3D Object Flow**|Ruohan Zhang Team|[2512.24766](http://arxiv.org/abs/2512.24766)|**[link](https://dream2flow.github.io/)**|
|**2026-01-06**|**RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence**|Jian Tang Team|[2512.24653](http://arxiv.org/abs/2512.24653)|null|
|**2025-12-31**|**Resolving State Ambiguity in Robot Manipulation via Adaptive Working Memory Recoding**|Wenchao Ding Team|[2512.24638](http://arxiv.org/abs/2512.24638)|null|
|**2025-12-30**|**Subsecond 3D Mesh Generation for Robot Manipulation**|Daniel Rakita Team|[2512.24428](http://arxiv.org/abs/2512.24428)|null|
|**2025-12-30**|**Real-world Reinforcement Learning from Suboptimal Interventions**|Jian Tang Team|[2512.24288](http://arxiv.org/abs/2512.24288)|null|
|**2025-12-30**|**Local Path Optimization in The Latent Space Using Learned Distance Gradient**|Jifeng Guo Team|[2512.24272](http://arxiv.org/abs/2512.24272)|null|
|**2025-12-30**|**GR-Dexter Technical Report**|Hang Li Team|[2512.24210](http://arxiv.org/abs/2512.24210)|null|
|**2026-01-01**|**Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training**|Jianlan Luo Team|[2512.24125](http://arxiv.org/abs/2512.24125)|null|
|**2025-12-29**|**Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation**|Shanghang Zhang Team|[2512.23703](http://arxiv.org/abs/2512.23703)|null|
|**2025-12-29**|**Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces**|Dongheui Lee Team|[2512.23616](http://arxiv.org/abs/2512.23616)|null|
|**2025-12-29**|**Act2Goal: From World Model To General Goal-conditioned Policy**|Jianlan Luo Team|[2512.23541](http://arxiv.org/abs/2512.23541)|null|
|**2025-12-29**|**Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery**|Mehdi Heydari Shahna Team|[2512.23505](http://arxiv.org/abs/2512.23505)|null|
|**2025-12-29**|**Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants**|Po-Chiang Lin Team|[2512.23312](http://arxiv.org/abs/2512.23312)|null|
|**2025-12-30**|**SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling**|Daguang Xu Team|[2512.23162](http://arxiv.org/abs/2512.23162)|null|
|**2025-12-28**|**A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms**|Jiacai Liu Team|[2512.23097](http://arxiv.org/abs/2512.23097)|null|
|**2025-12-28**|**Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives**|Badong Chen Team|[2512.22983](http://arxiv.org/abs/2512.22983)|null|
|**2025-12-28**|**ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning**|Hao Zhang Team|[2512.22854](http://arxiv.org/abs/2512.22854)|null|
|**2025-12-28**|**TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning**|Laxmidhar Behera Team|[2512.22824](http://arxiv.org/abs/2512.22824)|null|
|**2025-12-27**|**ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation**|Zhiyu Li Team|[2512.22575](http://arxiv.org/abs/2512.22575)|null|
|**2025-12-27**|**Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding**|Ngan Le Team|[2512.22519](http://arxiv.org/abs/2512.22519)|**[link](https://uark-aicv.github.io/OBEYED_VLA)**|
|**2025-12-26**|**StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision**|He Wang Team|[2512.21970](http://arxiv.org/abs/2512.21970)|null|
|**2025-12-26**|**Flexible Multitask Learning with Factorized Diffusion Policy**|Yilun Du Team|[2512.21898](http://arxiv.org/abs/2512.21898)|null|
|**2025-12-26**|**Optimal Trajectory Planning for Orbital Robot Rendezvous and Docking**|Kazuya Yoshida Team|[2512.21882](http://arxiv.org/abs/2512.21882)|null|
|**2025-12-25**|**Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations**|Dongbin Zhao Team|[2512.21586](http://arxiv.org/abs/2512.21586)|null|
|**2025-12-26**|**RoboCade: Gamifying Robot Data Collection**|Dorsa Sadigh Team|[2512.21235](http://arxiv.org/abs/2512.21235)|null|
|**2025-12-24**|**Language-Guided Grasp Detection with Coarse-to-Fine Learning for Robotic Manipulation**|Hu Cao Team|[2512.21065](http://arxiv.org/abs/2512.21065)|null|
|**2025-12-24**|**Tracing Energy Flow: Learning Tactile-based Grasping Force Control to Prevent Slippage in Dynamic Object Interaction**|Takamitsu Matsubara Team|[2512.21043](http://arxiv.org/abs/2512.21043)|null|
|**2025-12-24**|**Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task**|Tetsuya Ogata Team|[2512.20876](http://arxiv.org/abs/2512.20876)|null|
|**2025-12-23**|**YCB-Handovers Dataset: Analyzing Object Weight Impact on Human Handovers to Adapt Robotic Handover Motion**|Christian Smith Team|[2512.20847](http://arxiv.org/abs/2512.20847)|null|
|**2025-12-23**|**LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving**|Kashyap Chitta Team|[2512.20563](http://arxiv.org/abs/2512.20563)|null|
|**2025-12-23**|**Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation**|Lianyang Ma Team|[2512.20188](http://arxiv.org/abs/2512.20188)|null|
|**2025-12-23**|**LoLA: Long Horizon Latent Action Learning for General Robot Manipulation**|Baining Guo Team|[2512.20166](http://arxiv.org/abs/2512.20166)|null|
|**2025-12-22**|**Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations**|Ping Tan Team|[2512.19583](http://arxiv.org/abs/2512.19583)|null|
|**2025-12-22**|**REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation**|Vladimir Petrik Team|[2512.19562](http://arxiv.org/abs/2512.19562)|null|
|**2025-12-22**|**A Gauss-Newton-Induced Structure-Exploiting Algorithm for Differentiable Optimal Control**|Hong Chen Team|[2512.19447](http://arxiv.org/abs/2512.19447)|null|
|**2025-12-22**|**Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface**|Hao Dong Team|[2512.19402](http://arxiv.org/abs/2512.19402)|null|
|**2025-12-22**|**TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation**|Hao Dong Team|[2512.19390](http://arxiv.org/abs/2512.19390)|null|
|**2025-12-22**|**OMP: One-step Meanflow Policy with Directional Alignment**|Yutong Ban Team|[2512.19347](http://arxiv.org/abs/2512.19347)|null|
|**2025-12-22**|**Are All Data Necessary? Efficient Data Pruning for Large-scale Autonomous Driving Dataset via Trajectory Entropy Maximization**|Diange Yang Team|[2512.19270](http://arxiv.org/abs/2512.19270)|null|
|**2025-12-22**|**Translating Flow to Policy via Hindsight Online Imitation**|Yang Gao Team|[2512.19269](http://arxiv.org/abs/2512.19269)|null|
|**2025-12-22**|**DTCCL: Disengagement-Triggered Contrastive Continual Learning for Autonomous Bus Planners**|Diange Yang Team|[2512.18988](http://arxiv.org/abs/2512.18988)|null|
|**2025-12-21**|**Offline Reinforcement Learning for End-to-End Autonomous Driving**|Takaki Yamamoto Team|[2512.18662](http://arxiv.org/abs/2512.18662)|null|
|**2025-12-21**|**ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning**|Dan Negrut Team|[2512.18619](http://arxiv.org/abs/2512.18619)|null|
|**2025-12-21**|**SD2AIL: Adversarial Imitation Learning from Synthetic Demonstrations via Diffusion Models**|Xin Xu Team|[2512.18583](http://arxiv.org/abs/2512.18583)|null|
|**2025-12-20**|**STORM: Search-Guided Generative World Models for Robotic Manipulation**|Keze Wang Team|[2512.18477](http://arxiv.org/abs/2512.18477)|null|
|**2025-12-20**|**AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation**|Yakun Huang Team|[2512.18396](http://arxiv.org/abs/2512.18396)|null|
|**2025-12-20**|**Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation**|Jingya Wang Team|[2512.18368](http://arxiv.org/abs/2512.18368)|null|
|**2025-12-19**|**SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning**|Axel Krieger Team|[2512.18068](http://arxiv.org/abs/2512.18068)|null|
|**2025-12-19**|**Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation**|Eric Sax Team|[2512.18028](http://arxiv.org/abs/2512.18028)|null|
|**2025-12-19**|**Robotic VLA Benefits from Joint Learning with Motion Image Diffusion**|Juan Carlos Niebles Team|[2512.18007](http://arxiv.org/abs/2512.18007)|**[link](https://vla-motion.github.io/)**|
|**2025-12-19**|**Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy**|Alberto Speranzon Team|[2512.17899](http://arxiv.org/abs/2512.17899)|null|
|**2025-12-19**|**AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning**|Karl Schmeckpeper Team|[2512.17853](http://arxiv.org/abs/2512.17853)|null|
|**2025-12-19**|**Kinematics-Aware Diffusion Policy with Consistent 3D Observation and Action Space for Whole-Arm Robotic Manipulation**|Xiang Li Team|[2512.17568](http://arxiv.org/abs/2512.17568)|**[link](https://kinematics-aware-diffusion-policy.github.io)**|
|**2025-12-19**|**Learning-Based Safety-Aware Task Scheduling for Efficient Human-Robot Collaboration**|P. Rocco Team|[2512.17560](http://arxiv.org/abs/2512.17560)|null|
|**2025-12-19**|**TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data**|Dongbin Zhao Team|[2512.17370](http://arxiv.org/abs/2512.17370)|null|
|**2025-12-19**|**Mitty: Diffusion-based Human-to-Robot Video Generation**|Mike Zheng Shou Team|[2512.17253](http://arxiv.org/abs/2512.17253)|null|
|**2025-12-19**|**Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots**|Gang Zhang Team|[2512.17183](http://arxiv.org/abs/2512.17183)|null|
|**2025-12-18**|**Lang2Manip: A Tool for LLM-Based Symbolic-to-Geometric Planning for Manipulation**|Irfan Hussain Team|[2512.17062](http://arxiv.org/abs/2512.17062)|null|
|**2025-12-18**|**Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning**|Sergey Levine Team|[2512.16911](http://arxiv.org/abs/2512.16911)|null|
|**2025-12-18**|**PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies**|Karl Pertsch Team|[2512.16881](http://arxiv.org/abs/2512.16881)|**[link](https://polaris-evals.github.io/)**|
|**2025-12-18**|**ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning**|Caelan Garrett Team|[2512.16861](http://arxiv.org/abs/2512.16861)|null|
|**2025-12-18**|**OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction**|Paul Pu Liang Team|[2512.16842](http://arxiv.org/abs/2512.16842)|**[link](https://opentouch-tactile.github.io/)**|
|**2025-12-18**|**GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation**|Li Jiang Team|[2512.16811](http://arxiv.org/abs/2512.16811)|null|
|**2025-12-18**|**VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation**|Liang Wang Team|[2512.16724](http://arxiv.org/abs/2512.16724)|null|
|**2025-12-18**|**Single-View Shape Completion for Robotic Grasping in Clutter**|Todor Stoyanov Team|[2512.16449](http://arxiv.org/abs/2512.16449)|null|
|**2025-12-18**|**ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation**|Yang Gao Team|[2512.16302](http://arxiv.org/abs/2512.16302)|null|
|**2025-12-17**|**CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion**|Abhinav Valada Team|[2512.16023](http://arxiv.org/abs/2512.16023)|null|
|**2025-12-17**|**Large Video Planner Enables Generalizable Robot Control**|Yilun Du Team|[2512.15840](http://arxiv.org/abs/2512.15840)|null|
|**2025-12-17**|**In Pursuit of Pixel Supervision for Visual Pre-training**|Hu Xu Team|[2512.15715](http://arxiv.org/abs/2512.15715)|**[link](https://github.com/facebookresearch/pixio)**|
|**2025-12-19**|**mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs**|Elvis Nava Team|[2512.15692](http://arxiv.org/abs/2512.15692)|null|
|**2025-12-17**|**ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision**|Jie Mei Team|[2512.15020](http://arxiv.org/abs/2512.15020)|null|
|**2025-12-16**|**Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections**|Jeff Da Team|[2512.14895](http://arxiv.org/abs/2512.14895)|null|
|**2025-12-16**|**EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models**|Mike Zheng Shou Team|[2512.14666](http://arxiv.org/abs/2512.14666)|null|
|**2025-12-16**|**A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data**|Shuo Gao Team|[2512.14329](http://arxiv.org/abs/2512.14329)|null|
|**2025-12-16**|**DRAW2ACT: Turning Depth-Encoded Trajectories into Robotic Demonstration Videos**|Gitta Kutyniok Team|[2512.14217](http://arxiv.org/abs/2512.14217)|null|
|**2025-12-17**|**Context Representation via Action-Free Transformer encoder-decoder for Meta Reinforcement Learning**|Homayoun Najjaran Team|[2512.14057](http://arxiv.org/abs/2512.14057)|null|
|**2025-12-15**|**NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks**|Mingyu Cai Team|[2512.13670](http://arxiv.org/abs/2512.13670)|null|
|**2025-12-16**|**MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning**|Xiang Bai Team|[2512.13636](http://arxiv.org/abs/2512.13636)|**[link](https://xiaomi-mlab.github.io/MindDrive/)**|
|**2025-12-15**|**Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving**|Monu Surana Team|[2512.13262](http://arxiv.org/abs/2512.13262)|null|
|**2025-12-15**|**Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation**|Zhong Cao Team|[2512.13094](http://arxiv.org/abs/2512.13094)|null|
|**2025-12-15**|**PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations**|Wenjun Zeng Team|[2512.13093](http://arxiv.org/abs/2512.13093)|null|
|**2025-12-15**|**Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos**|Zongqing Lu Team|[2512.13080](http://arxiv.org/abs/2512.13080)|null|
|**2025-12-12**|**CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction**|Stan Birchfield Team|[2512.11988](http://arxiv.org/abs/2512.11988)|**[link](https://nvlabs.github.io/CARI4D/)**|
|**2025-12-12**|**A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach**|Haoran Wang Team|[2512.11944](http://arxiv.org/abs/2512.11944)|null|
|**2025-12-11**|**Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control**|Ibrahim Sheikh Mohamed Team|[2512.11921](http://arxiv.org/abs/2512.11921)|null|
|**2025-12-10**|**Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning**|Aske Plaat Team|[2512.11902](http://arxiv.org/abs/2512.11902)|null|
|**2025-12-12**|**AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis**|Vitor Guizilini Team|[2512.11797](http://arxiv.org/abs/2512.11797)|**[link](https://jay-ye.github.io/AnchorDream/)**|
|**2025-12-12**|**UniBYD: A Unified Framework for Learning Robotic Manipulation Across Embodiments Beyond Imitation of Human Demonstrations**|Jinqiao Wang Team|[2512.11609](http://arxiv.org/abs/2512.11609)|null|
|**2025-12-12**|**Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing**|Daqiang Guo Team|[2512.11275](http://arxiv.org/abs/2512.11275)|null|
|**2025-12-11**|**Learning Category-level Last-meter Navigation from RGB Demonstrations of a Single-instance**|Karthik Desingh Team|[2512.11173](http://arxiv.org/abs/2512.11173)|null|
|**2025-12-12**|**Iterative Compositional Data Generation for Robot Control**|Eric Eaton Team|[2512.10891](http://arxiv.org/abs/2512.10891)|null|
|**2025-12-11**|**XDen-1K: A Density Field Dataset of Real-World Objects**|Jingyi Yu Team|[2512.10668](http://arxiv.org/abs/2512.10668)|null|
|**2025-12-10**|**Fast Functionally Redundant Inverse Kinematics for Robotic Toolpath Optimisation in Manufacturing Tasks**|Tirthankar Bandyopadhyay Team|[2512.10116](http://arxiv.org/abs/2512.10116)|**[link](https://ssl.linklings.net/conferences/acra/acra2025_proceedings/views/includes/files/pap149s2.pdf)**|
|**2025-12-10**|**HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models**|Donglin Wang Team|[2512.09928](http://arxiv.org/abs/2512.09928)|**[link](https://hifvla.github.io)**|
|**2025-12-10**|**Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation**|Yixin Zhu Team|[2512.09851](http://arxiv.org/abs/2512.09851)|null|
|**2025-12-10**|**ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics**|Paolo Roberto Massenio Team|[2512.09510](http://arxiv.org/abs/2512.09510)|null|
|**2025-12-10**|**Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing**|Yasuhisa Hasegawa Team|[2512.09462](http://arxiv.org/abs/2512.09462)|null|
|**2025-12-10**|**H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos**|Mike Zheng Shou Team|[2512.09406](http://arxiv.org/abs/2512.09406)|null|
|**2025-12-10**|**One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation**|Kui Jia Team|[2512.09297](http://arxiv.org/abs/2512.09297)|null|
|**2025-12-10**|**UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects**|Shifeng Huang Team|[2512.09283](http://arxiv.org/abs/2512.09283)|null|
|**2025-12-09**|**Masked Generative Policy for Robotic Control**|Paul Henderson Team|[2512.09101](http://arxiv.org/abs/2512.09101)|null|
|**2025-12-09**|**Bridging Scale Discrepancies in Robotic Control via Language-Based Action Representations**|Ting Liu Team|[2512.08548](http://arxiv.org/abs/2512.08548)|null|
|**2025-12-09**|**Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks**|Kalathur Chenchu Kishore Kumar Team|[2512.08545](http://arxiv.org/abs/2512.08545)|null|
|**2025-12-09**|**Learning Robot Manipulation from Audio World Models**|Michael Gienger Team|[2512.08405](http://arxiv.org/abs/2512.08405)|null|
|**2025-12-09**|**Embodied Tree of Thoughts: Deliberate Manipulation Planning with Embodied World Model**|Rui Chen Team|[2512.08188](http://arxiv.org/abs/2512.08188)|**[link](https://embodied-tree-of-thoughts.github.io)**|
|**2025-12-11**|**An Introduction to Deep Reinforcement and Imitation Learning**|Pedro Santana Team|[2512.08052](http://arxiv.org/abs/2512.08052)|null|
|**2025-12-08**|**DiffusionDriveV2: Reinforcement Learning-Constrained Truncated Diffusion Modeling in End-to-End Autonomous Driving**|Xinggang Wang Team|[2512.07745](http://arxiv.org/abs/2512.07745)|null|
|**2025-12-08**|**Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks**|Shayegan Omidshafiei Team|[2512.07697](http://arxiv.org/abs/2512.07697)|null|
|**2025-12-08**|**See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations**|Yufeng Yue Team|[2512.07582](http://arxiv.org/abs/2512.07582)|null|
|**2025-12-08**|**Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation**|Chang Xu Team|[2512.07472](http://arxiv.org/abs/2512.07472)|null|
|**2025-12-08**|**ESPADA: Execution Speedup via Semantics Aware Demonstration Data Downsampling for Imitation Learning**|Byoung-Tak Zhang Team|[2512.07371](http://arxiv.org/abs/2512.07371)|**[link](https://project-espada.github.io/espada/)**|
|**2025-12-08**|**Benchmarking Humanoid Imitation Learning with Motion Difficulty**|Yipeng Qin Team|[2512.07248](http://arxiv.org/abs/2512.07248)|null|
|**2025-12-09**|**VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation**|Sungho Kim Team|[2512.07215](http://arxiv.org/abs/2512.07215)|null|
|**2025-12-08**|**Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation**|Ye Shi Team|[2512.07212](http://arxiv.org/abs/2512.07212)|null|
|**2025-12-07**|**A Hetero-Associative Sequential Memory Model Utilizing Neuromorphic Signals: Validated on a Mobile Manipulator**|Gordon Cheng Team|[2512.07032](http://arxiv.org/abs/2512.07032)|null|
|**2025-12-07**|**VideoVLA: Video Generators Can Be Generalizable Robot Manipulators**|Baining Guo Team|[2512.06963](http://arxiv.org/abs/2512.06963)|**[link](https://videovla-nips2025.github.io)**|
|**2025-12-07**|**MIND-V: Hierarchical Video Generation for Long-Horizon Robotic Manipulation with RL-based Physical Alignment**|Xiu Li Team|[2512.06628](http://arxiv.org/abs/2512.06628)|null|
|**2025-12-04**|**Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction**|Alexander E. Siemenn Team|[2512.06038](http://arxiv.org/abs/2512.06038)|null|
|**2025-12-05**|**SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models**|Yilun Du Team|[2512.05955](http://arxiv.org/abs/2512.05955)|null|
|**2025-12-05**|**Correspondence-Oriented Imitation Learning: Flexible Visuomotor Control with 3D Conditioning**|Kuan Fang Team|[2512.05953](http://arxiv.org/abs/2512.05953)|null|
|**2025-12-05**|**World Models That Know When They Don't Know: Controllable Video Generation with Calibrated Uncertainty**|Anirudha Majumdar Team|[2512.05927](http://arxiv.org/abs/2512.05927)|null|
|**2025-12-05**|**Real-time Remote Tracking and Autonomous Planning for Whale Rendezvous using Robots**|Stephanie Gil Team|[2512.05808](http://arxiv.org/abs/2512.05808)|null|
|**2025-12-05**|**3D Path Planning for Robot-assisted Vertebroplasty from Arbitrary Bi-plane X-ray via Differentiable Rendering**|Mathias Unberath Team|[2512.05803](http://arxiv.org/abs/2512.05803)|null|
|**2025-12-05**|**Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth**|Soodeh Bakhshandeh Team|[2512.05783](http://arxiv.org/abs/2512.05783)|null|
|**2025-12-05**|**An Integrated System for WEEE Sorting Employing X-ray Imaging, AI-based Object Detection and Segmentation, and Delta Robot Manipulation**|Panagiotis Chatzakos Team|[2512.05599](http://arxiv.org/abs/2512.05599)|null|
|**2025-12-05**|**A Comprehensive Framework for Automated Quality Control in the Automotive Industry**|Panagiotis Chatzakos Team|[2512.05579](http://arxiv.org/abs/2512.05579)|null|
|**2025-12-05**|**MIND: Multi-rationale INtegrated Discriminative Reasoning Framework for Multi-modal Large Models**|Xiangyu Yue Team|[2512.05530](http://arxiv.org/abs/2512.05530)|null|
|**2025-12-05**|**Distributed scalable coupled policy algorithm for networked multi-agent reinforcement learning**|Wei Ren Team|[2512.05447](http://arxiv.org/abs/2512.05447)|null|
|**2025-12-05**|**State-Conditional Adversarial Learning: An Off-Policy Visual Domain Transfer Method for End-to-End Imitation Learning**|Shengfan Cao Team|[2512.05335](http://arxiv.org/abs/2512.05335)|null|
|**2025-12-08**|**Disturbance Compensation for Safe Kinematic Control of Robotic Systems with Closed Architecture**|Qin Lin Team|[2512.05292](http://arxiv.org/abs/2512.05292)|null|
|**2025-12-04**|**Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective**|Yaniv Romano Team|[2512.05267](http://arxiv.org/abs/2512.05267)|null|
|**2025-12-04**|**Age-Inclusive 3D Human Mesh Recovery for Action-Preserving Data Anonymization**|Petros Maragos Team|[2512.05259](http://arxiv.org/abs/2512.05259)|null|
|**2025-12-04**|**IE2Video: Adapting Pretrained Diffusion Models for Event-Based Video Reconstruction**|Yihui Ren Team|[2512.05240](http://arxiv.org/abs/2512.05240)|null|
|**2025-12-04**|**STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models**|Benjamin Busam Team|[2512.05107](http://arxiv.org/abs/2512.05107)|null|
|**2025-12-04**|**From Generated Human Videos to Physically Plausible Robot Trajectories**|Roei Herzig Team|[2512.05094](http://arxiv.org/abs/2512.05094)|**[link](https://genmimic.github.io)**|
|**2025-12-04**|**Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints**|Michael Posa Team|[2512.05079](http://arxiv.org/abs/2512.05079)|**[link](https://contactgen3d.github.io/)**|
|**2025-12-04**|**Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction**|Xipeng Qiu Team|[2512.04987](http://arxiv.org/abs/2512.04987)|null|
|**2025-12-04**|**Hybrid-Diffusion Models: Combining Open-loop Routines with Visuomotor Diffusion Policies**|Danica Kragic Team|[2512.04960](http://arxiv.org/abs/2512.04960)|null|
|**2025-12-04**|**FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization**|Hang Zhao Team|[2512.04952](http://arxiv.org/abs/2512.04952)|null|
|**2025-12-04**|**Hoi! -- A Multimodal Dataset for Force-Grounded, Cross-View Articulated Manipulation**|Zuria Bauer Team|[2512.04884](http://arxiv.org/abs/2512.04884)|null|
|**2025-12-04**|**MOVE: A Simple Motion-Based Data Collection Paradigm for Spatial Generalization in Robotic Manipulation**|Gao Huang Team|[2512.04813](http://arxiv.org/abs/2512.04813)|null|
|**2025-12-04**|**Bridging Simulation and Reality: Cross-Domain Transfer with Semantic 2D Gaussian Splatting**|Xuguang Lan Team|[2512.04731](http://arxiv.org/abs/2512.04731)|null|
|**2025-12-04**|**TRINITY: An Evolved LLM Coordinator**|Yujin Tang Team|[2512.04695](http://arxiv.org/abs/2512.04695)|null|
|**2025-12-04**|**Gauss-Newton accelerated MPPI Control**|Johannes Reuter Team|[2512.04579](http://arxiv.org/abs/2512.04579)|null|
|**2025-12-04**|**GTM: Simulating the World of Tools for AI Agents**|Jiyan He Team|[2512.04535](http://arxiv.org/abs/2512.04535)|null|
|**2025-12-04**|**MARL Warehouse Robots**|Salmon Riaz Team|[2512.04463](http://arxiv.org/abs/2512.04463)|**[link](https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/)**|
|**2025-12-04**|**Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops**|Minghui Zheng Team|[2512.04446](http://arxiv.org/abs/2512.04446)|null|
|**2025-12-04**|**Bridging Probabilistic Inference and Behavior Trees: An Interactive Framework for Adaptive Multi-Robot Cooperation**|Changju Wu Team|[2512.04404](http://arxiv.org/abs/2512.04404)|null|
|**2025-12-04**|**Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation**|Hesheng Wang Team|[2512.04399](http://arxiv.org/abs/2512.04399)|null|
|**2025-12-03**|**ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models**|Jianwei Zhang Team|[2512.04308](http://arxiv.org/abs/2512.04308)|**[link](https://sites.google.com/view/responsible-robotbench)**|
|**2025-12-03**|**Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies**|Jaerock Kwon Team|[2512.04279](http://arxiv.org/abs/2512.04279)|null|
|**2025-12-03**|**SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL**|Jonathan Tremblay Team|[2512.04069](http://arxiv.org/abs/2512.04069)|null|
|**2025-12-03**|**When to Say "Hi" - Learn to Open a Conversation with an in-the-wild Dataset**|Anja Richert Team|[2512.03991](http://arxiv.org/abs/2512.03991)|null|
|**2025-12-03**|**Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning**|Justin Carpentier Team|[2512.03973](http://arxiv.org/abs/2512.03973)|null|
|**2025-12-03**|**Classification of User Satisfaction in HRI with Social Signals in the Wild**|Anja Richert Team|[2512.03945](http://arxiv.org/abs/2512.03945)|null|
|**2025-12-03**|**Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware**|Carl Glen Henshaw Team|[2512.03911](http://arxiv.org/abs/2512.03911)|null|
|**2025-12-03**|**Comparison of neural network training strategies for the simulation of dynamical systems**|Andreas Körner Team|[2512.03851](http://arxiv.org/abs/2512.03851)|null|
|**2025-12-03**|**Cross-embodied Co-design for Dexterous Hands**|Xiaolong Wang Team|[2512.03743](http://arxiv.org/abs/2512.03743)|null|
|**2025-12-03**|**Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control**|Carl Glen Henshaw Team|[2512.03736](http://arxiv.org/abs/2512.03736)|null|
|**2025-12-03**|**Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing**|Carl Glen Henshaw Team|[2512.03729](http://arxiv.org/abs/2512.03729)|null|
|**2025-12-03**|**PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention**|Mingming Gong Team|[2512.03724](http://arxiv.org/abs/2512.03724)|null|
|**2025-12-03**|**ContactRL: Safe Reinforcement Learning based Motion Planning for Contact based Human Robot Collaboration**|Emma Li Team|[2512.03707](http://arxiv.org/abs/2512.03707)|null|
|**2025-12-03**|**A Novel Approach to Tomato Harvesting Using a Hybrid Gripper with Semantic Segmentation and Keypoint Detection**|Bishakh Bhattacharya Team|[2512.03684](http://arxiv.org/abs/2512.03684)|null|
|**2025-12-03**|**RoboScape-R: Unified Reward-Observation World Models for Generalizable Robotics Training via RL**|Yong Li Team|[2512.03556](http://arxiv.org/abs/2512.03556)|null|
|**2025-12-03**|**AdaPower: Specializing World Foundation Models for Predictive Manipulation**|Kai Xu Team|[2512.03538](http://arxiv.org/abs/2512.03538)|null|
|**2025-12-03**|**PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers**|Minghui Zheng Team|[2512.03444](http://arxiv.org/abs/2512.03444)|null|
|**2025-12-03**|**Multimodal Reinforcement Learning with Agentic Verifier for AI Agents**|Jianfeng Gao Team|[2512.03438](http://arxiv.org/abs/2512.03438)|null|
|**2025-12-03**|**World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations**|Daniel Fernando Tello Gamarra Team|[2512.03429](http://arxiv.org/abs/2512.03429)|null|
|**2025-12-03**|**What Is The Best 3D Scene Representation for Robotics? From Geometric to Foundation Models**|Weidong Chen Team|[2512.03422](http://arxiv.org/abs/2512.03422)|null|
|**2025-12-03**|**GOMP: Grasped Object Manifold Projection for Multimodal Imitation Learning of Manipulation**|Nima Fazeli Team|[2512.03347](http://arxiv.org/abs/2512.03347)|null|
|**2025-12-02**|**KALIKO: Kalman-Implicit Koopman Operator Learning For Prediction of Nonlinear Dynamical Systems**|Aaron D. Ames Team|[2512.03256](http://arxiv.org/abs/2512.03256)|null|
|**2025-12-02**|**Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling**|Shanghang Zhang Team|[2512.03044](http://arxiv.org/abs/2512.03044)|null|
|**2025-12-02**|**SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control**|Xue Bin Peng Team|[2512.03028](http://arxiv.org/abs/2512.03028)|null|
|**2025-12-02**|**Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger**|Nilanjan Chakraborty Team|[2512.02951](http://arxiv.org/abs/2512.02951)|null|
|**2025-12-02**|**SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots**|Dzmitry Tsetserukou Team|[2512.02851](http://arxiv.org/abs/2512.02851)|null|
|**2025-12-02**|**Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms**|Hongrui Yu Team|[2512.02810](http://arxiv.org/abs/2512.02810)|null|
|**2025-12-02**|**Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols**|Yong-Lu Li Team|[2512.02787](http://arxiv.org/abs/2512.02787)|null|
|**2025-12-02**|**RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning**|Haoqian Wang Team|[2512.02729](http://arxiv.org/abs/2512.02729)|null|
|**2025-12-02**|**SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction**|Yong Zhao Team|[2512.02609](http://arxiv.org/abs/2512.02609)|null|
|**2025-12-02**|**From Imitation to Discrimination: Toward A Generalized Curriculum Advantage Mechanism Enhancing Cross-Domain Reasoning Tasks**|Guoquan Zhang Team|[2512.02580](http://arxiv.org/abs/2512.02580)|null|
|**2025-12-02**|**In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs**|Kayvon Fatahalian Team|[2512.02543](http://arxiv.org/abs/2512.02543)|null|
|**2025-12-02**|**Synthetic Error Injection Fails to Elicit Self-Correction In Language Models**|Stuart Russell Team|[2512.02389](http://arxiv.org/abs/2512.02389)|null|
|**2025-12-01**|**EfficientFlow: Efficient Equivariant Flow Policy Learning for Embodied AI**|Xiangyu Xu Team|[2512.02020](http://arxiv.org/abs/2512.02020)|**[link](https://efficientflow.github.io/)**|
|**2025-12-01**|**ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation**|Shanghang Zhang Team|[2512.02013](http://arxiv.org/abs/2512.02013)|null|
|**2025-12-01**|**Learning Sim-to-Real Humanoid Locomotion in 15 Minutes**|Pieter Abbeel Team|[2512.01996](http://arxiv.org/abs/2512.01996)|**[link](https://younggyo.me/fastsac-humanoid)**|
|**2025-12-02**|**Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models**|Cordelia Schmid Team|[2512.01946](http://arxiv.org/abs/2512.01946)|**[link](https://www.di.ens.fr/willow/research/guardian/.)**|
|**2025-12-01**|**Real-World Robot Control by Deep Active Inference With a Temporally Hierarchical World Model**|Shingo Murata Team|[2512.01924](http://arxiv.org/abs/2512.01924)|null|
|**2025-12-01**|**SARL: Spatially-Aware Self-Supervised Representation Learning for Visuo-Tactile Perception**|Dandan Zhang Team|[2512.01908](http://arxiv.org/abs/2512.01908)|null|
|**2025-12-01**|**Register Any Point: Scaling 3D Point Cloud Registration by Flow Matching**|Cyrill Stachniss Team|[2512.01850](http://arxiv.org/abs/2512.01850)|null|
|**2025-12-02**|**GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation**|Yonghui Wu Team|[2512.01801](http://arxiv.org/abs/2512.01801)|null|
|**2025-12-01**|**IGen: Scalable Data Generation for Robot Learning from Open-World Images**|Zhi Wang Team|[2512.01773](http://arxiv.org/abs/2512.01773)|null|
|**2025-12-01**|**DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models**|Zongqing Lu Team|[2512.01715](http://arxiv.org/abs/2512.01715)|null|
|**2025-12-01**|**SPARK: Sim-ready Part-level Articulated Reconstruction with VLM Knowledge**|Chenfanfu Jiang Team|[2512.01629](http://arxiv.org/abs/2512.01629)|null|
|**2025-12-01**|**A Cross-Embodiment Gripper Benchmark for Rigid-Object Manipulation in Aerial and Industrial Robotics**|Ivan Virgala Team|[2512.01598](http://arxiv.org/abs/2512.01598)|null|
|**2025-12-01**|**On robotic manipulators with time-dependent inertial parameters: From physical consistency to boundedness of the mass matrix**|Johann Reger Team|[2512.01482](http://arxiv.org/abs/2512.01482)|null|
|**2025-12-01**|**$\mathbf{M^3A}$ Policy: Mutable Material Manipulation Augmentation Policy through Photometric Re-rendering**|Jianfei Yang Team|[2512.01446](http://arxiv.org/abs/2512.01446)|null|
|**2025-12-01**|**PointNet4D: A Lightweight 4D Point Cloud Video Backbone for Online and Offline Perception in Robotic Applications**|Jiayang Ao Team|[2512.01383](http://arxiv.org/abs/2512.01383)|null|
|**2025-12-01**|**Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1**|Songhwai Oh Team|[2512.01358](http://arxiv.org/abs/2512.01358)|null|
|**2025-12-01**|**Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning**|Donglin Wang Team|[2512.01336](http://arxiv.org/abs/2512.01336)|null|
|**2025-12-01**|**TabletopGen: Instance-Level Interactive 3D Tabletop Scene Generation from Text or Single Image**|Hu Su Team|[2512.01204](http://arxiv.org/abs/2512.01204)|**[link](https://d-robotics-ai-lab.github.io/TabletopGen.project/)**|
|**2025-12-01**|**Real-World Reinforcement Learning of Active Perception Behaviors**|Dinesh Jayaraman Team|[2512.01188](http://arxiv.org/abs/2512.01188)|null|
|**2025-11-30**|**Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer**|Yuke Zhu Team|[2512.01061](http://arxiv.org/abs/2512.01061)|**[link](https://doorman-humanoid.github.io/)**|
|**2025-11-28**|**From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products**|Jürgen Fleischer Team|[2511.23407](http://arxiv.org/abs/2511.23407)|null|
|**2025-11-28**|**SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot**|Dzmitry Tsetserukou Team|[2511.23300](http://arxiv.org/abs/2511.23300)|null|
|**2025-11-28**|**Synthetic Industrial Object Detection: GenAI vs. Feature-Based Methods**|Jörg Krüger Team|[2511.23241](http://arxiv.org/abs/2511.23241)|null|
|**2025-11-28**|**Obstruction reasoning for robotic grasping**|Fabio Poiesi Team|[2511.23186](http://arxiv.org/abs/2511.23186)|null|
|**2025-11-28**|**LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models**|Jianlong Fu Team|[2511.23034](http://arxiv.org/abs/2511.23034)|**[link](https://mm-robot.github.io/distill_latent_action/)**|
|**2025-11-28**|**Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary**|Jingya Wang Team|[2511.22963](http://arxiv.org/abs/2511.22963)|**[link](https://humanoidlla.github.io/)**|
|**2025-11-28**|**DM $^3$ T: Harmonizing Modalities via Diffusion for Multi-Object Tracking**|Zhenbo Li Team|[2511.22896](http://arxiv.org/abs/2511.22896)|null|
|**2025-11-27**|**Switching control of underactuated multi-channel systems with input constraints for cooperative manipulation**|H. Jin Kim Team|[2511.22810](http://arxiv.org/abs/2511.22810)|null|
|**2025-11-27**|**Distracted Robot: How Visual Clutter Undermine Robotic Manipulation**|Xuan Zhao Team|[2511.22780](http://arxiv.org/abs/2511.22780)|null|
|**2025-11-27**|**Improving Robotic Manipulation Robustness via NICE Scene Surgery**|Amir Rasouli Team|[2511.22777](http://arxiv.org/abs/2511.22777)|null|
|**2025-11-27**|**CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance**|Amir Rasouli Team|[2511.22773](http://arxiv.org/abs/2511.22773)|null|
|**2025-11-27**|**Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion**|Wael Suleiman Team|[2511.22744](http://arxiv.org/abs/2511.22744)|**[link](https://anonymous.4open.science/r/multiview-parkour-6FB8)**|
|**2025-11-27**|**Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation**|Mingyu Cai Team|[2511.22685](http://arxiv.org/abs/2511.22685)|null|
|**2025-11-27**|**Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention**|Meibao Yao Team|[2511.22555](http://arxiv.org/abs/2511.22555)|null|
|**2025-11-27**|**RealD $^2$ iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion**|Jianhua Sun Team|[2511.22505](http://arxiv.org/abs/2511.22505)|null|
|**2025-11-27**|**Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion**|Jitendra Malik Team|[2511.22445](http://arxiv.org/abs/2511.22445)|null|
|**2025-11-27**|**Exposing Vulnerabilities in RL: A Novel Stealthy Backdoor Attack through Reward Poisoning**|Junfeng Wu Team|[2511.22415](http://arxiv.org/abs/2511.22415)|null|
|**2025-11-27**|**BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands**|Jonghyun Choi Team|[2511.22364](http://arxiv.org/abs/2511.22364)|null|
|**2025-11-27**|**Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning**|Zichun Wang Team|[2511.22338](http://arxiv.org/abs/2511.22338)|null|
|**2025-11-27**|**3D Affordance Keypoint Detection for Robotic Manipulation**|Marcelo H Ang Team|[2511.22195](http://arxiv.org/abs/2511.22195)|null|
|**2025-11-26**|**TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos**|Furong Huang Team|[2511.21690](http://arxiv.org/abs/2511.21690)|null|
|**2025-11-26**|**VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation**|Shaoshuai Shi Team|[2511.21557](http://arxiv.org/abs/2511.21557)|null|
|**2025-11-26**|**$\mathcal{E}_0$ : Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion**|Guangrun Wang Team|[2511.21542](http://arxiv.org/abs/2511.21542)|null|
|**2025-11-26**|**Hybrid Control for Robotic Nut Tightening Task**|Dmitri Kovalenko Team|[2511.21366](http://arxiv.org/abs/2511.21366)|null|
|**2025-11-26**|**Neural NMPC through Signed Distance Field Encoding for Collision Avoidance**|Kostas Alexis Team|[2511.21312](http://arxiv.org/abs/2511.21312)|null|
|**2025-11-26**|**Sampling-Based Optimization with Parallelized Physics Simulator for Bimanual Manipulation**|Arun Kumar Singh Team|[2511.21264](http://arxiv.org/abs/2511.21264)|null|
|**2025-11-26**|**When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models**|Xudong Jiang Team|[2511.21192](http://arxiv.org/abs/2511.21192)|null|
|**2025-11-26**|**Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation**|Qijun Chen Team|[2511.21169](http://arxiv.org/abs/2511.21169)|null|
|**2025-11-26**|**MarketGen: A Scalable Simulation Platform with Auto-Generated Embodied Supermarket Environments**|Zhaoxiang Zhang Team|[2511.21161](http://arxiv.org/abs/2511.21161)|**[link](https://xuhu0529.github.io/MarketGen)**|
|**2025-11-26**|**Maglev-Pentabot: Magnetic Levitation System for Non-Contact Manipulation using Deep Reinforcement Learning**|Zongfu Yu Team|[2511.21149](http://arxiv.org/abs/2511.21149)|null|
|**2025-11-26**|**SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation**|Yu Zhang Team|[2511.21135](http://arxiv.org/abs/2511.21135)|null|
|**2025-11-26**|**Dual-Agent Reinforcement Learning for Adaptive and Cost-Aware Visual-Inertial Odometry**|Guangbin Dou Team|[2511.21083](http://arxiv.org/abs/2511.21083)|null|
|**2025-11-26**|**AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios**|Qing-Long Han Team|[2511.21053](http://arxiv.org/abs/2511.21053)|null|
|**2025-11-26**|**Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning**|Hao Su Team|[2511.21011](http://arxiv.org/abs/2511.21011)|null|
|**2025-11-25**|**Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy**|Eric Vanden-Eijnden Team|[2511.20906](http://arxiv.org/abs/2511.20906)|null|
|**2025-11-25**|**ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation**|Xiaolong Wang Team|[2511.20887](http://arxiv.org/abs/2511.20887)|null|
|**2025-11-25**|**MODEST: Multi-Optics Depth-of-Field Stereo Dataset**|Dante Lok Team|[2511.20853](http://arxiv.org/abs/2511.20853)|null|
|**2025-11-25**|**NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities**|Alex Hodges Team|[2511.20848](http://arxiv.org/abs/2511.20848)|null|
|**2025-11-25**|**OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping**|Odest Chadwicke Jenkins Team|[2511.20841](http://arxiv.org/abs/2511.20841)|null|
|**2025-11-25**|**Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning**|Mac Schwager Team|[2511.20811](http://arxiv.org/abs/2511.20811)|null|
|**2025-11-25**|**Reinforcing Action Policies by Prophesying**|Li Zhang Team|[2511.20633](http://arxiv.org/abs/2511.20633)|**[link](https://LogosRoboticsGroup.github.io/ProphRL)**|
|**2025-11-25**|**Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning**|Abdalla Swikir Team|[2511.20593](http://arxiv.org/abs/2511.20593)|null|
|**2025-11-25**|**Gated Uncertainty-Aware Runtime Dual Invariants for Neural Signal-Controlled Robotics**|Oiwi Parker Jones Team|[2511.20570](http://arxiv.org/abs/2511.20570)|null|
|**2025-11-25**|**Metric, inertially aligned monocular state estimation via kinetodynamic priors**|Laurent Kneip Team|[2511.20496](http://arxiv.org/abs/2511.20496)|null|
|**2025-11-25**|**Improved adaptive wind driven optimization algorithm for real-time path planning**|Le-le Mao Team|[2511.20394](http://arxiv.org/abs/2511.20394)|null|
|**2025-11-25**|**How Robot Kinematics Influence Human Performance in Virtual Robot-to-Human Handover Tasks**|Joost C. Dessing Team|[2511.20299](http://arxiv.org/abs/2511.20299)|null|
|**2025-11-25**|**HAFO: Humanoid Force-Adaptive Control for Intense External Force Interaction Environments**|Bin He Team|[2511.20275](http://arxiv.org/abs/2511.20275)|null|
|**2025-11-25**|**CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents**|Yunsung Lee Team|[2511.20216](http://arxiv.org/abs/2511.20216)|null|
|**2025-11-25**|**WPT: World-to-Policy Transfer via Online World Model Distillation**|Xu Yan Team|[2511.20095](http://arxiv.org/abs/2511.20095)|null|
|**2025-11-25**|**Energy Costs and Neural Complexity Evolution in Changing Environments**|Geoff Nitschke Team|[2511.20018](http://arxiv.org/abs/2511.20018)|null|
|**2025-11-25**|**ShapeForce: Low-Cost Soft Robotic Wrist for Contact-Rich Manipulation**|Lin Shao Team|[2511.19955](http://arxiv.org/abs/2511.19955)|null|
|**2025-11-25**|**Collaborate sim and real: Robot Bin Packing Learning in Real-world and Physical Engine**|Tian He Team|[2511.19932](http://arxiv.org/abs/2511.19932)|null|
|**2025-11-25**|**GigaWorld-0: World Models as Data Engine to Empower Embodied AI**|Zheng Zhu Team|[2511.19861](http://arxiv.org/abs/2511.19861)|**[link](https://gigaworld0.github.io/)**|
|**2025-11-25**|**Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation**|Sanglu Lu Team|[2511.19859](http://arxiv.org/abs/2511.19859)|null|
|**2025-11-24**|**Multi-Hypotheses Ego-Tracking for Resilient Navigation**|Roberto Galeazzi Team|[2511.19770](http://arxiv.org/abs/2511.19770)|null|
|**2025-11-24**|**Robot-Powered Data Flywheels: Deploying Robots in the Wild for Continual Data Collection and Foundation Model Adaptation**|Dorsa Sadigh Team|[2511.19647](http://arxiv.org/abs/2511.19647)|null|
|**2025-11-24**|**Mixture of Horizons in Action Chunking**|Mingyu Ding Team|[2511.19433](http://arxiv.org/abs/2511.19433)|null|
|**2025-11-24**|**Real-Time Object Tracking with On-Device Deep Learning for Adaptive Beamforming in Dynamic Acoustic Environments**|Maximo Cobos Team|[2511.19396](http://arxiv.org/abs/2511.19396)|null|
|**2025-11-24**|**Rethinking Intermediate Representation for VLM-based Robot Manipulation**|Chi-Wing Fu Team|[2511.19315](http://arxiv.org/abs/2511.19315)|null|
|**2025-11-24**|**Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap**|Tian Qiu Team|[2511.19201](http://arxiv.org/abs/2511.19201)|null|
|**2025-11-24**|**Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework**|Andreas Mueller Team|[2511.19094](http://arxiv.org/abs/2511.19094)|null|
|**2025-11-24**|**ReEXplore: Improving MLLMs for Embodied Exploration with Contextualized Retrospective Experience Replay**|Volker Tresp Team|[2511.19033](http://arxiv.org/abs/2511.19033)|null|
|**2025-11-24**|**Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors**|Yuchen Zhou Team|[2511.19031](http://arxiv.org/abs/2511.19031)|null|
|**2025-11-24**|**AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention**|Xiaoyuan Yu Team|[2511.18960](http://arxiv.org/abs/2511.18960)|null|
|**2025-11-24**|**Learning to Compress Graphs via Dual Agents for Consistent Topological Robustness Evaluation**|Tao Jia Team|[2511.18958](http://arxiv.org/abs/2511.18958)|null|
|**2025-11-24**|**Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation**|Wenjing Qian Team|[2511.18950](http://arxiv.org/abs/2511.18950)|null|
|**2025-11-24**|**Accelerating Reinforcement Learning via Error-Related Human Brain Signals**|Hyo-Jeong Jang Team|[2511.18878](http://arxiv.org/abs/2511.18878)|null|
|**2025-11-24**|**AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion**|Mingguo Zhao Team|[2511.18857](http://arxiv.org/abs/2511.18857)|null|
|**2025-11-24**|**Any4D: Open-Prompt 4D Generation from Natural Language and Images**|Qiao Sun Team|[2511.18746](http://arxiv.org/abs/2511.18746)|null|
|**2025-11-23**|**AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations**|Erdem Biyik Team|[2511.18617](http://arxiv.org/abs/2511.18617)|**[link](http://autofocus-il.github.io/)**|
|**2025-11-23**|**Object-centric Task Representation and Transfer using Diffused Orientation Fields**|Sylvain Calinon Team|[2511.18563](http://arxiv.org/abs/2511.18563)|null|
|**2025-11-23**|**SafeFall: Learning Protective Control for Humanoid Robots**|Siyuan Huang Team|[2511.18509](http://arxiv.org/abs/2511.18509)|null|
|**2025-11-23**|**ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints**|Yinghui Xu Team|[2511.18450](http://arxiv.org/abs/2511.18450)|null|
|**2025-11-23**|**Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video**|Takehisa Yairi Team|[2511.18322](http://arxiv.org/abs/2511.18322)|null|
|**2025-11-23**|**MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing**|Jia-Yeu Lin Team|[2511.18299](http://arxiv.org/abs/2511.18299)|null|
|**2025-11-23**|**Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters**|Matthew McCrink Team|[2511.18243](http://arxiv.org/abs/2511.18243)|null|
|**2025-11-21**|**RynnVLA-002: A Unified Vision-Language-Action and World Model**|Hao Chen Team|[2511.17502](http://arxiv.org/abs/2511.17502)|null|
|**2025-11-21**|**RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation**|Guocai Yao Team|[2511.17441](http://arxiv.org/abs/2511.17441)|null|
|**2025-11-21**|**SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding**|Danda Pani Paudel Team|[2511.17411](http://arxiv.org/abs/2511.17411)|null|
|**2025-11-21**|**Feasibility of Embodied Dynamics Based Bayesian Learning for Continuous Pursuit Motion Control of Assistive Mobile Robots in the Built Environment**|Vineet R. Kamat Team|[2511.17401](http://arxiv.org/abs/2511.17401)|null|
|**2025-11-21**|**Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data**|Hongyang Li Team|[2511.17373](http://arxiv.org/abs/2511.17373)|null|
|**2025-11-21**|**METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model**|Shanghang Zhang Team|[2511.17366](http://arxiv.org/abs/2511.17366)|null|
|**2025-11-21**|**A ROS2 Interface for Universal Robots Collaborative Manipulators Based on ur_rtde**|Jacopo Aleotti Team|[2511.17237](http://arxiv.org/abs/2511.17237)|null|
|**2025-11-21**|**VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation**|Gim Hee Lee Team|[2511.17199](http://arxiv.org/abs/2511.17199)|null|
|**2025-11-21**|**H-GAR: A Hierarchical Interaction Framework via Goal-Driven Observation-Action Refinement for Robotic Manipulation**|Zitong Yu Team|[2511.17079](http://arxiv.org/abs/2511.17079)|**[link](https://github.com/JiuTian-VL/H-GAR)**|
|**2025-11-21**|**MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints**|Hong Zhang Team|[2511.17013](http://arxiv.org/abs/2511.17013)|null|
|**2025-11-21**|**Stable Offline Hand-Eye Calibration for any Robot with Just One Mark**|Yu-Gang Jiang Team|[2511.17001](http://arxiv.org/abs/2511.17001)|null|
|**2025-11-20**|**Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations**|Homanga Bharadhwaj Team|[2511.16661](http://arxiv.org/abs/2511.16661)|null|
|**2025-11-20**|**InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy**|Jiangmiao Pang Team|[2511.16651](http://arxiv.org/abs/2511.16651)|null|
|**2025-11-20**|**Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies**|Aviv Tamar Team|[2511.16596](http://arxiv.org/abs/2511.16596)|null|
|**2025-11-20**|**Green Resilience of Cyber-Physical Systems: Doctoral Dissertation**|Diaeddin Rimawi Team|[2511.16593](http://arxiv.org/abs/2511.16593)|null|
|**2025-11-21**|**VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference**|Bo Zhao Team|[2511.16449](http://arxiv.org/abs/2511.16449)|null|
|**2025-11-20**|**Graph Neural Networks for Surgical Scene Segmentation**|Danail Stoyanov Team|[2511.16430](http://arxiv.org/abs/2511.16430)|null|
|**2025-11-20**|**LAOF: Robust Latent Action Learning with Optical Flow Constraints**|Wei Li Team|[2511.16407](http://arxiv.org/abs/2511.16407)|**[link](https://github.com/XizoB/LAOF)**|
|**2025-11-20**|**Homogeneous Proportional-Integral-Derivative Controller in Mobile Robotic Manipulators**|Andrey Polyakov Team|[2511.16406](http://arxiv.org/abs/2511.16406)|null|
|**2025-11-20**|**Robot Metacognition: Decision Making with Confidence for Tool Invention**|Pablo Lanillos Team|[2511.16390](http://arxiv.org/abs/2511.16390)|null|
|**2025-11-20**|**Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning**|Mohammad Yaqub Team|[2511.16333](http://arxiv.org/abs/2511.16333)|null|
|**2025-11-20**|**Safe and Optimal Variable Impedance Control via Certified Reinforcement Learning**|Ravi Prakash Team|[2511.16330](http://arxiv.org/abs/2511.16330)|null|
|**2025-11-20**|**InEKFormer: A Hybrid State Estimator for Humanoid Robots**|Frank Kirchner Team|[2511.16306](http://arxiv.org/abs/2511.16306)|null|
|**2025-11-20**|**DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks**|Anna Valente Team|[2511.16223](http://arxiv.org/abs/2511.16223)|null|
|**2025-11-20**|**When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models**|Yaochu Jin Team|[2511.16203](http://arxiv.org/abs/2511.16203)|null|
|**2025-11-20**|**Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight**|Zhijie Deng Team|[2511.16175](http://arxiv.org/abs/2511.16175)|null|
|**2025-11-20**|**EvoVLA: Self-Evolving Vision-Language-Action Model**|Hao Tang Team|[2511.16166](http://arxiv.org/abs/2511.16166)|null|
|**2025-11-20**|**MagBotSim: Physics-Based Simulation and Reinforcement Learning Environments for Magnetic Robotics**|Klaus Neumann Team|[2511.16158](http://arxiv.org/abs/2511.16158)|null|
|**2025-11-20**|**Real-Time 3D Object Detection with Inference-Aligned Learning**|Nan Xue Team|[2511.16140](http://arxiv.org/abs/2511.16140)|null|
|**2025-11-20**|**Bi-AQUA: Bilateral Control-Based Imitation Learning for Underwater Robot Arms via Lighting-Aware Action Chunking with Transformers**|Yuki Uranishi Team|[2511.16050](http://arxiv.org/abs/2511.16050)|null|
|**2025-11-20**|**PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization**|Meng Guo Team|[2511.15995](http://arxiv.org/abs/2511.15995)|null|
|**2025-11-19**|**Optimus-Q: Utilizing Federated Learning in Adaptive Robots for Intelligent Nuclear Power Plant Operations through Quantum Cryptography**|Sajedul Talukder Team|[2511.15614](http://arxiv.org/abs/2511.15614)|null|
|**2025-11-19**|**SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models**|Xipeng Qiu Team|[2511.15605](http://arxiv.org/abs/2511.15605)|null|
|**2025-11-19**|**Learning from Mistakes: Loss-Aware Memory Enhanced Continual Learning for LiDAR Place Recognition**|Tiantian Feng Team|[2511.15597](http://arxiv.org/abs/2511.15597)|null|
|**2025-11-19**|**NMPC-based Motion Planning with Adaptive Weighting for Dynamic Object Interception**|Steven Liu Team|[2511.15532](http://arxiv.org/abs/2511.15532)|null|
|**2025-11-19**|**Decentralized Gaussian Process Classification and an Application in Subsea Robotics**|James McMahon Team|[2511.15529](http://arxiv.org/abs/2511.15529)|null|
|**2025-11-19**|**Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies**|François Ferland Team|[2511.15520](http://arxiv.org/abs/2511.15520)|null|
|**2025-11-19**|**IPR-1: Interactive Physical Reasoner**|Yong-Lu Li Team|[2511.15407](http://arxiv.org/abs/2511.15407)|null|
|**2025-11-19**|**Platform-Agnostic Reinforcement Learning Framework for Safe Exploration of Cluttered Environments with Graph Attention**|George Nikolakopoulos Team|[2511.15358](http://arxiv.org/abs/2511.15358)|null|
|**2025-11-19**|**Adversarial Attack on Black-Box Multi-Agent by Adaptive Perturbation**|Fanjiang Xu Team|[2511.15292](http://arxiv.org/abs/2511.15292)|null|
|**2025-11-19**|**Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments**|Moharram Challenger Team|[2511.15284](http://arxiv.org/abs/2511.15284)|null|
|**2025-11-19**|**Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception**|Wenzhao Lian Team|[2511.15279](http://arxiv.org/abs/2511.15279)|null|
|**2025-11-19**|**Behavior Trees vs Executable Ontologies: a Comparative Analysis of Robot Control Paradigms**|Alexander Boldachev Team|[2511.15274](http://arxiv.org/abs/2511.15274)|null|
|**2025-11-19**|**Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy**|Tadashi Kozuno Team|[2511.15239](http://arxiv.org/abs/2511.15239)|null|
|**2025-11-19**|**Efficient Transformer-Integrated Deep Neural Architectures for Robust EEG Decoding of Complex Visual Imagery**|Byoung-Hee Kwon Team|[2511.15218](http://arxiv.org/abs/2511.15218)|null|
|**2025-11-19**|**VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation**|Yuke Zhu Team|[2511.15200](http://arxiv.org/abs/2511.15200)|**[link](https://viral-humanoid.github.io/)**|
|**2025-11-19**|**Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization**|Zhenzhou Shao Team|[2511.15194](http://arxiv.org/abs/2511.15194)|null|
|**2025-11-19**|**Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation**|Yong Huang Team|[2511.15167](http://arxiv.org/abs/2511.15167)|null|
|**2025-11-19**|**An Alignment-Based Approach to Learning Motions from Demonstrations**|Julie A Shah Team|[2511.14988](http://arxiv.org/abs/2511.14988)|null|
|**2025-11-18**|**Automated laboratory x-ray diffractometer and fluorescence spectrometer for high-throughput materials characterization**|Todd C. Hufnagel Team|[2511.14905](http://arxiv.org/abs/2511.14905)|**[link](https://doi.org/10.34863/e8bx-pk70)**|
|**2025-11-19**|**$π^{*}_{0.6}$ : a VLA That Learns From Experience**|Zhiyuan Zhou Team|[2511.14759](http://arxiv.org/abs/2511.14759)|null|
|**2025-11-18**|**HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation**|Xiaolong Wang Team|[2511.14756](http://arxiv.org/abs/2511.14756)|null|
|**2025-11-18**|**Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language**|Andreea Bobu Team|[2511.14565](http://arxiv.org/abs/2511.14565)|null|
|**2025-11-18**|**A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning**|Shengwen Yu Team|[2511.14533](http://arxiv.org/abs/2511.14533)|null|
|**2025-11-18**|**Achieving Safe Control Online through Integration of Harmonic Control Lyapunov-Barrier Functions with Unsafe Object-Centric Action Policies**|Matthias Scheutz Team|[2511.14434](http://arxiv.org/abs/2511.14434)|null|
|**2025-11-18**|**Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning**|Georgia Chalvatzaki Team|[2511.14427](http://arxiv.org/abs/2511.14427)|null|
|**2025-11-18**|**Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning**|Hongpeng Wang Team|[2511.14396](http://arxiv.org/abs/2511.14396)|**[link](https://qhemu.github.io/CCoL/)**|
|**2025-11-18**|**MA-SLAM: Active SLAM in Large-Scale Unknown Environment using Map Aware Deep Reinforcement Learning**|Yi Jiang Team|[2511.14330](http://arxiv.org/abs/2511.14330)|null|
|**2025-11-18**|**NeuralBoneReg: A Novel Self-Supervised Method for Robust and Accurate Multi-Modal Bone Surface Registration**|Philipp Fürnstahl Team|[2511.14286](http://arxiv.org/abs/2511.14286)|null|
|**2025-11-18**|**Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion**|Fei Chen Team|[2511.14178](http://arxiv.org/abs/2511.14178)|null|
|**2025-11-18**|**RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action**|Jiayu Chen Team|[2511.14161](http://arxiv.org/abs/2511.14161)|null|
|**2025-11-18**|**AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models**|Biqing Qi Team|[2511.14148](http://arxiv.org/abs/2511.14148)|null|
|**2025-11-17**|**From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands**|Xiaolong Wang Team|[2511.13710](http://arxiv.org/abs/2511.13710)|**[link](https://jianglongye.com/power-to-precision)**|
|**2025-11-17**|**OpenRoboCare: A Multimodal Multi-Task Expert Demonstration Dataset for Robot Caregiving**|Tapomayukh Bhattacharjee Team|[2511.13707](http://arxiv.org/abs/2511.13707)|null|
|**2025-11-17**|**PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image**|Ziwei Liu Team|[2511.13648](http://arxiv.org/abs/2511.13648)|**[link](https://physx-anything.github.io/)**|
|**2025-11-17**|**Contact-Safe Reinforcement Learning with ProMP Reparameterization and Energy Awareness**|Luis Figueredo Team|[2511.13459](http://arxiv.org/abs/2511.13459)|null|
|**2025-11-17**|**ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning**|Ruizhen Hu Team|[2511.13327](http://arxiv.org/abs/2511.13327)|null|
|**2025-11-17**|**EL3DD: Extended Latent 3D Diffusion for Language Conditioned Multitask Manipulation**|Sven Behnke Team|[2511.13312](http://arxiv.org/abs/2511.13312)|null|
|**2025-11-17**|**Robust Control Design Using a Hybrid-Gain Finite-Time Sliding-Mode Controller**|Fernando A. C. C. Fontes Team|[2511.13260](http://arxiv.org/abs/2511.13260)|null|
|**2025-11-17**|**Difficulty-Aware Label-Guided Denoising for Monocular 3D Object Detection**|Dongbo Min Team|[2511.13195](http://arxiv.org/abs/2511.13195)|null|
|**2025-11-17**|**Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers**|Itzik Klein Team|[2511.13071](http://arxiv.org/abs/2511.13071)|null|
|**2025-11-17**|**Learning Branching Policies for MILPs with Proximal Policy Optimization**|Amal El Fallah Seghrouchni Team|[2511.12986](http://arxiv.org/abs/2511.12986)|null|
|**2025-11-17**|**ArtiWorld: LLM-Driven Articulation of 3D Objects in Scenes**|Feng Zheng Team|[2511.12977](http://arxiv.org/abs/2511.12977)|null|
|**2025-11-17**|**DiffuDepGrasp: Diffusion-based Depth Noise Modeling Empowers Sim2Real Robotic Grasping**|Dongbin Zhao Team|[2511.12912](http://arxiv.org/abs/2511.12912)|null|
|**2025-11-17**|**Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views**|Hesheng Wang Team|[2511.12878](http://arxiv.org/abs/2511.12878)|null|
|**2025-11-17**|**Structured Imitation Learning of Interactive Policies through Inverse Games**|Todd Murphey Team|[2511.12848](http://arxiv.org/abs/2511.12848)|**[link](https://sites.google.com/view/gai-hri/)**|
|**2025-11-17**|**Mapping fNIRS Signals to Agent Performance: Toward Reinforcement Learning from Neural Feedback**|Jivko SInapov Team|[2511.12844](http://arxiv.org/abs/2511.12844)|null|
|**2025-11-16**|**Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation**|Hongyang R. Zhang Team|[2511.12779](http://arxiv.org/abs/2511.12779)|null|
|**2025-11-16**|**Task-Aware Morphology Optimization of Planar Manipulators via Reinforcement Learning**|Sohom Chakrabarty Team|[2511.12650](http://arxiv.org/abs/2511.12650)|null|
|**2025-11-16**|**Botany Meets Robotics in Alpine Scree Monitoring**|Manolo Garabini Team|[2511.12526](http://arxiv.org/abs/2511.12526)|null|
|**2025-11-16**|**RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation**|Long Chen Team|[2511.12436](http://arxiv.org/abs/2511.12436)|null|
|**2025-11-16**|**VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving**|David Hyunchul Shim Team|[2511.12405](http://arxiv.org/abs/2511.12405)|null|
|**2025-11-14**|**Volumetric Ergodic Control**|Todd Murphey Team|[2511.11533](http://arxiv.org/abs/2511.11533)|null|
|**2025-11-14**|**Terrain Costmap Generation via Scaled Preference Conditioning**|Joydeep Biswas Team|[2511.11529](http://arxiv.org/abs/2511.11529)|null|
|**2025-11-14**|**Scalable Policy Evaluation with Video World Models**|Lin Yen-Chen Team|[2511.11520](http://arxiv.org/abs/2511.11520)|null|
|**2025-11-14**|**Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities**|Jingyuan Chen Team|[2511.11512](http://arxiv.org/abs/2511.11512)|null|
|**2025-11-14**|**Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective**|Ngan Le Team|[2511.11478](http://arxiv.org/abs/2511.11478)|null|
|**2025-11-14**|**Simulating an Autonomous System in CARLA using ROS 2**|Mohamed Al-Musleh Team|[2511.11310](http://arxiv.org/abs/2511.11310)|null|
|**2025-11-14**|**Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation**|Xi Zheng Team|[2511.11298](http://arxiv.org/abs/2511.11298)|null|
|**2025-11-14**|**Sashimi-Bot: Autonomous Tri-manual Advanced Manipulation and Cutting of Deformable Objects**|Ekrem Misimi Team|[2511.11223](http://arxiv.org/abs/2511.11223)|null|
|**2025-11-14**|**Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning**|Xiaoyu Ren Team|[2511.11218](http://arxiv.org/abs/2511.11218)|null|
|**2025-11-14**|**One-to-N Backdoor Attack in 3D Point Cloud via Spherical Trigger**|Chongxia Wang Team|[2511.11210](http://arxiv.org/abs/2511.11210)|null|
|**2025-11-14**|**Viper-F1: Fast and Fine-Grained Multimodal Understanding with Cross-Modal State-Space Modulation**|Debesh Jha Team|[2511.11177](http://arxiv.org/abs/2511.11177)|null|
|**2025-11-14**|**Phys-Liquid: A Physics-Informed Dataset for Estimating 3D Geometry and Volume of Transparent Deformable Liquids**|Tian Xia Team|[2511.11077](http://arxiv.org/abs/2511.11077)|**[link](https://github.com/dualtransparency/Phys-Liquid-AAAI)**|
|**2025-11-14**|**AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation**|Lin Shao Team|[2511.11052](http://arxiv.org/abs/2511.11052)|null|
|**2025-11-14**|**Autonomous Vehicle Path Planning by Searching With Differentiable Simulation**|Luc Van Gool Team|[2511.11043](http://arxiv.org/abs/2511.11043)|null|
|**2025-11-14**|**Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment**|Yi Sun Team|[2511.10987](http://arxiv.org/abs/2511.10987)|null|
|**2025-11-14**|**Collaborative Multi-Robot Non-Prehensile Manipulation via Flow-Matching Co-Generation**|Jiaoyang Li Team|[2511.10874](http://arxiv.org/abs/2511.10874)|null|
|**2025-11-14**|**WetExplorer: Automating Wetland Greenhouse-Gas Surveys with an Autonomous Mobile Robot**|Xuping Zhang Team|[2511.10864](http://arxiv.org/abs/2511.10864)|null|
|**2025-11-13**|**SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?**|Chandan K. Reddy Team|[2511.10833](http://arxiv.org/abs/2511.10833)|null|
|**2025-11-13**|**Expert Consensus-based Video-Based Assessment Tool for Workflow Analysis in Minimally Invasive Colorectal Surgery: Development and Validation of ColoWorkflow**|Nicolas Padoy Team|[2511.10766](http://arxiv.org/abs/2511.10766)|null|
|**2025-11-13**|**Attentive Feature Aggregation or: How Policies Learn to Stop Worrying about Robustness and Attend to Task-Relevant Visual Cues**|Chris Xiaoxuan Lu Team|[2511.10762](http://arxiv.org/abs/2511.10762)|null|
|**2025-11-13**|**Robot Crash Course: Learning Soft and Stylized Falling**|Moritz Bächer Team|[2511.10635](http://arxiv.org/abs/2511.10635)|null|
|**2025-11-13**|**OmniVGGT: Omni-Modality Driven Visual Geometry Grounded**|Ziwei Liu Team|[2511.10560](http://arxiv.org/abs/2511.10560)|**[link](https://livioni.github.io/OmniVGGT-offcial/)**|
|**2025-11-13**|**SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation**|Liqiang Nie Team|[2511.10518](http://arxiv.org/abs/2511.10518)|**[link](https://github.com/JiuTian-VL/SemanticVLA)**|
|**2025-11-13**|**RoboBenchMart: Benchmarking Robots in Retail Environment**|Vlad Shakhuro Team|[2511.10276](http://arxiv.org/abs/2511.10276)|null|
|**2025-11-13**|**Learning a Thousand Tasks in a Day**|Edward Johns Team|[2511.10110](http://arxiv.org/abs/2511.10110)|**[link](https://www.science.org/doi/10.1126/scirobotics.adv7594.)**|
|**2025-11-13**|**Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning**|Xiaocong Li Team|[2511.10087](http://arxiv.org/abs/2511.10087)|null|
|**2025-11-13**|**Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks**|Yinghua Liu Team|[2511.10079](http://arxiv.org/abs/2511.10079)|null|
|**2025-11-13**|**Efficient Verification and Falsification of ReLU Neural Barrier Certificates**|Bai Xue Team|[2511.10015](http://arxiv.org/abs/2511.10015)|null|
|**2025-11-13**|**Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation**|Changbo Wang Team|[2511.09958](http://arxiv.org/abs/2511.09958)|null|
|**2025-11-13**|**A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation**|Hanwen Wang Team|[2511.09932](http://arxiv.org/abs/2511.09932)|null|
|**2025-11-13**|**Harnessing Bounded-Support Evolution Strategies for Policy Refinement**|Fabio Ramos Team|[2511.09923](http://arxiv.org/abs/2511.09923)|null|
|**2025-11-13**|**Evolving Rules: Imitation and Best Response Learning in Cournot Oligopoly**|Boyu Zhang Team|[2511.09839](http://arxiv.org/abs/2511.09839)|null|
|**2025-11-13**|**Provably Safe Stein Variational Clarity-Aware Informative Planning**|Dimitra Panagou Team|[2511.09836](http://arxiv.org/abs/2511.09836)|**[link](https://usahai18.github.io/stein_clarity/))**|
|**2025-11-12**|**A Robust Task-Level Control Architecture for Learned Dynamical Systems**|Naira Hovakimyan Team|[2511.09790](http://arxiv.org/abs/2511.09790)|null|
|**2025-11-12**|**Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy**|Peter R. Wurman Team|[2511.09737](http://arxiv.org/abs/2511.09737)|**[link](https://github.com/bramgrooten/sparc)**|
|**2025-11-12**|**Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard**|Katerina Pastra Team|[2511.09727](http://arxiv.org/abs/2511.09727)|null|
|**2025-11-12**|**SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning**|Haibo Hu Team|[2511.09681](http://arxiv.org/abs/2511.09681)|null|
|**2025-11-12**|**Statistically Consistent Approximate Model Predictive Control**|Melanie N. Zeilinger Team|[2511.09661](http://arxiv.org/abs/2511.09661)|null|
|**2025-11-12**|**IFG: Internet-Scale Guidance for Functional Grasping Generation**|Deepak Pathak Team|[2511.09558](http://arxiv.org/abs/2511.09558)|**[link](https://ifgrasping.github.io/)**|
|**2025-11-12**|**SpatialActor: Exploring Disentangled Spatial Representations for Robust Robotic Manipulation**|Gao Huang Team|[2511.09555](http://arxiv.org/abs/2511.09555)|**[link](https://shihao1895.github.io/SpatialActor)**|
|**2025-11-10**|**Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields**|Pieter Abbeel Team|[2511.07418](http://arxiv.org/abs/2511.07418)|**[link](https://github.com/zhaohengyin/lightning-grasp)**|
|**2025-11-10**|**Robot Learning from a Physical World Model**|Yue Wang Team|[2511.07416](http://arxiv.org/abs/2511.07416)|**[link](https://pointscoder.github.io/PhysWorld_Web/)**|
|**2025-11-10**|**Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization**|Shalabh Bhatnagar Team|[2511.07288](http://arxiv.org/abs/2511.07288)|null|
|**2025-11-10**|**SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation**|Ngan Le Team|[2511.06754](http://arxiv.org/abs/2511.06754)|null|
|**2025-11-10**|**Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning**|Nam Pham Hai Team|[2511.06745](http://arxiv.org/abs/2511.06745)|null|
|**2025-11-10**|**Rapidly Learning Soft Robot Control via Implicit Time-Stepping**|Dezhong Tong Team|[2511.06667](http://arxiv.org/abs/2511.06667)|**[link](https://github.com/QuantuMope/dismech-rl)**|
|**2025-11-09**|**Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator**|Ruigang Yang Team|[2511.06434](http://arxiv.org/abs/2511.06434)|null|
|**2025-11-09**|**ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval**|Jeff Ichnowski Team|[2511.06202](http://arxiv.org/abs/2511.06202)|null|
|**2025-11-08**|**Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds**|Jun Liu Team|[2511.05996](http://arxiv.org/abs/2511.05996)|null|
|**2025-11-08**|**Gentle Manipulation Policy Learning via Demonstrations from VLM Planned Atomic Skills**|Renjing Xu Team|[2511.05855](http://arxiv.org/abs/2511.05855)|null|
|**2025-11-08**|**VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models**|Aniket Bera Team|[2511.05791](http://arxiv.org/abs/2511.05791)|null|
|**2025-11-07**|**VLM-driven Skill Selection for Robotic Assembly Tasks**|Chang-Hyun Kim Team|[2511.05680](http://arxiv.org/abs/2511.05680)|null|
|**2025-11-07**|**EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation**|Samuel Dickerson Team|[2511.05397](http://arxiv.org/abs/2511.05397)|null|
|**2025-11-07**|**ETHOS: A Robotic Encountered-Type Haptic Display for Social Interaction in Virtual Reality**|Matthew K. X. J. Pan Team|[2511.05379](http://arxiv.org/abs/2511.05379)|null|
|**2025-11-07**|**Force-Safe Environment Maps and Real-Time Detection for Soft Robot Manipulators**|Andrew P. Sabelhaus Team|[2511.05307](http://arxiv.org/abs/2511.05307)|null|
|**2025-11-07**|**Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning**|Gerhard Neumann Team|[2511.05234](http://arxiv.org/abs/2511.05234)|null|
|**2025-11-07**|**Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation**|Feifei Feng Team|[2511.05199](http://arxiv.org/abs/2511.05199)|null|
|**2025-11-07**|**Follow-Me in Micro-Mobility with End-to-End Imitation Learning**|Jorge Peña Queralta Team|[2511.05158](http://arxiv.org/abs/2511.05158)|null|
|**2025-11-07**|**TAPOM: Task-Space Topology-Guided Motion Planning for Manipulating Elongated Object in Cluttered Environments**|Yijiang Huang Team|[2511.05052](http://arxiv.org/abs/2511.05052)|null|
|**2025-11-07**|**MoE-DP: An MoE-Enhanced Diffusion Policy for Robust Long-Horizon Robotic Manipulation with Skill Decomposition and Failure Recovery**|Huazhe Xu Team|[2511.05007](http://arxiv.org/abs/2511.05007)|null|
|**2025-11-06**|**Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning**|Gavriel State Team|[2511.04831](http://arxiv.org/abs/2511.04831)|**[link](https://github.com/isaac-sim/IsaacLab)**|
|**2025-11-06**|**Unified Multimodal Diffusion Forcing for Forceful Manipulation**|Dmitry Berenson Team|[2511.04812](http://arxiv.org/abs/2511.04812)|**[link](https://unified-df.github.io)**|
|**2025-11-06**|**ReGen: Generative Robot Simulation via Inverse Design**|Daniela Rus Team|[2511.04769](http://arxiv.org/abs/2511.04769)|null|
|**2025-11-06**|**X-Diffusion: Training Diffusion Policies on Cross-Embodiment Human Demonstrations**|Kushal Kedia Team|[2511.04671](http://arxiv.org/abs/2511.04671)|null|
|**2025-11-06**|**Real-to-Sim Robot Policy Evaluation with Gaussian Splatting Simulation of Soft-Body Interactions**|Yunzhu Li Team|[2511.04665](http://arxiv.org/abs/2511.04665)|**[link](https://real2sim-eval.github.io/)**|
|**2025-11-06**|**ForeRobo: Unlocking Infinite Simulation Data for 3D Goal-driven Robotic Manipulation**|Chunsheng Liu Team|[2511.04381](http://arxiv.org/abs/2511.04381)|null|
|**2025-11-06**|**GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies**|Cédric Buche Team|[2511.04357](http://arxiv.org/abs/2511.04357)|null|
|**2025-11-06**|**Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots**|Mingguo Zhao Team|[2511.03996](http://arxiv.org/abs/2511.03996)|**[link](https://humanoid-kick.github.io)**|
|**2025-11-05**|**Investigating Robot Control Policy Learning for Autonomous X-ray-guided Spine Procedures**|Mathias Unberath Team|[2511.03882](http://arxiv.org/abs/2511.03882)|null|
|**2025-11-05**|**Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning**|Georgios Chalkiadakis Team|[2511.03616](http://arxiv.org/abs/2511.03616)|null|
|**2025-11-05**|**Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances**|Georgios Chalkiadakis Team|[2511.03565](http://arxiv.org/abs/2511.03565)|null|
|**2025-11-05**|**Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control**|Sheng Yi Team|[2511.03481](http://arxiv.org/abs/2511.03481)|null|
|**2025-11-05**|**Learning-based Cooperative Robotic Paper Wrapping: A Unified Control Policy with Residual Force Control**|Kensuke Harada Team|[2511.03181](http://arxiv.org/abs/2511.03181)|null|
|**2025-11-05**|**Learning Natural and Robust Hexapod Locomotion over Complex Terrains via Motion Priors based on Deep Reinforcement Learning**|Feng Gao Team|[2511.03167](http://arxiv.org/abs/2511.03167)|null|
|**2025-11-05**|**ISC-Perception: A Hybrid Computer Vision Dataset for Object Detection in Novel Steel Assembly**|Debra F. Laefer Team|[2511.03098](http://arxiv.org/abs/2511.03098)|null|
|**2025-11-04**|**3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors**|Gregory Reardon Team|[2511.03078](http://arxiv.org/abs/2511.03078)|null|
|**2025-11-04**|**Audience Amplified: Virtual Audiences in Asynchronously Performed AR Theater**|Tobias Höllerer Team|[2511.02807](http://arxiv.org/abs/2511.02807)|null|
|**2025-11-04**|**Dexterous Robotic Piano Playing at Scale**|Dieter Büchler Team|[2511.02504](http://arxiv.org/abs/2511.02504)|null|
|**2025-11-04**|**LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation**|Changhyun Choi Team|[2511.02239](http://arxiv.org/abs/2511.02239)|**[link](https://vla2026.github.io/LACY/)**|
|**2025-10-31**|**A Step Toward World Models: A Survey on Robotic Manipulation**|Heng Tao Shen Team|[2511.02097](http://arxiv.org/abs/2511.02097)|null|
|**2025-11-03**|**TRACE: Textual Reasoning for Affordance Coordinate Extraction**|Matthew S. Brown Team|[2511.01999](http://arxiv.org/abs/2511.01999)|null|
|**2025-11-01**|**iFlyBot-VLA Technical Report**|Jia Pan Team|[2511.01914](http://arxiv.org/abs/2511.01914)|null|
|**2025-11-03**|**SE(3)-PoseFlow: Estimating 6D Pose Distributions for Uncertainty-Aware Robotic Manipulation**|Georgia Chalvatzaki Team|[2511.01501](http://arxiv.org/abs/2511.01501)|null|
|**2025-11-03**|**RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models**|Donglin Wang Team|[2511.01331](http://arxiv.org/abs/2511.01331)|null|
|**2025-11-03**|**Improving Needle Penetration via Precise Rotational Insertion Using Iterative Learning Control**|Tsu-Chin Tsao Team|[2511.01256](http://arxiv.org/abs/2511.01256)|null|
|**2025-11-03**|**Embodiment Transfer Learning for Vision-Language-Action Models**|Yaxin Peng Team|[2511.01224](http://arxiv.org/abs/2511.01224)|null|
|**2025-11-02**|**Deployable Vision-driven UAV River Navigation via Human-in-the-loop Preference Alignment**|Nina Mahmoudian Team|[2511.01083](http://arxiv.org/abs/2511.01083)|null|
|**2025-11-02**|**GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies**|Ruimao Zhang Team|[2511.00998](http://arxiv.org/abs/2511.00998)|**[link](https://ziyeeee.github.io/gaudp.io/)**|
|**2025-11-01**|**Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy**|Zhongliang Jiang Team|[2511.00555](http://arxiv.org/abs/2511.00555)|null|
|**2025-10-31**|**EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations**|Philipp Wu Team|[2511.00153](http://arxiv.org/abs/2511.00153)|null|
|**2025-10-31**|**Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping**|Xiaonan Huang Team|[2510.27666](http://arxiv.org/abs/2510.27666)|null|
|**2025-10-31**|**Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs**|Shinkyu Park Team|[2510.27558](http://arxiv.org/abs/2510.27558)|null|
|**2025-10-31**|**When AI Trading Agents Compete: Adverse Selection of Meta-Orders by Reinforcement Learning-Based Market Making**|Nick Firoozye Team|[2510.27334](http://arxiv.org/abs/2510.27334)|null|
|**2025-10-31**|**Learning Generalizable Visuomotor Policy through Dynamics-Alignment**|Jungwoo Lee Team|[2510.27114](http://arxiv.org/abs/2510.27114)|null|
|**2025-10-30**|**Hybrid Consistency Policy: Decoupling Multi-Modal Diversity and Real-Time Efficiency in Robotic Manipulation**|Qiaojun Yu Team|[2510.26670](http://arxiv.org/abs/2510.26670)|null|
|**2025-10-31**|**An Impulse Control Approach to Market Making in a Hawkes LOB Market**|Philip Treleaven Team|[2510.26438](http://arxiv.org/abs/2510.26438)|null|
|**2025-10-30**|**Human-in-the-loop Online Rejection Sampling for Robotic Manipulation**|Yansong Tang Team|[2510.26406](http://arxiv.org/abs/2510.26406)|null|
|**2025-10-30**|**Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving**|Yandan Luo Team|[2510.26292](http://arxiv.org/abs/2510.26292)|null|
|**2025-10-30**|**Learning to Manage Investment Portfolios beyond Simple Utility Functions**|J. Doyne Farmer Team|[2510.26165](http://arxiv.org/abs/2510.26165)|null|
|**2025-10-28**|**A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation**|Kyung-Joong Kim Team|[2510.25725](http://arxiv.org/abs/2510.25725)|null|
|**2025-10-29**|**Sim-to-Real Gentle Manipulation of Deformable and Fragile Objects with Stress-Guided Reinforcement Learning**|Florian T. Pokorny Team|[2510.25405](http://arxiv.org/abs/2510.25405)|null|
|**2025-10-29**|**SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation**|Dan Guo Team|[2510.25268](http://arxiv.org/abs/2510.25268)|null|
|**2025-10-29**|**Time-Optimal Transport of Loosely Placed Liquid Filled Cups along Prescribed Paths**|Andreas Mueller Team|[2510.25255](http://arxiv.org/abs/2510.25255)|null|
|**2025-10-29**|**Hybrid Vision Servoing with Depp Alignment and GRU-Based Occlusion Recovery**|Jongseong Brad Choi Team|[2510.25233](http://arxiv.org/abs/2510.25233)|null|
|**2025-10-29**|**Learning Spatial-Aware Manipulation Ordering**|Jian Pu Team|[2510.25138](http://arxiv.org/abs/2510.25138)|null|
|**2025-10-29**|**NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies**|Jinghui Lu Team|[2510.25122](http://arxiv.org/abs/2510.25122)|null|
|**2025-10-28**|**Fare: Failure Resilience in Learned Visual Navigation Control**|David Hsu Team|[2510.24680](http://arxiv.org/abs/2510.24680)|null|
|**2025-10-28**|**Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning**|Arnold W. Schumann Team|[2510.24650](http://arxiv.org/abs/2510.24650)|null|
|**2025-10-28**|**DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation**|Gang Hua Team|[2510.24261](http://arxiv.org/abs/2510.24261)|null|
|**2025-10-28**|**Manipulate as Human: Learning Task-oriented Manipulation Skills by Adversarial Motion Priors**|Yue Gao Team|[2510.24257](http://arxiv.org/abs/2510.24257)|null|
|**2025-10-28**|**Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames**|Aviv Tamar Team|[2510.24194](http://arxiv.org/abs/2510.24194)|null|
|**2025-10-28**|**PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI**|Philip Dames Team|[2510.24109](http://arxiv.org/abs/2510.24109)|null|
|**2025-10-28**|**ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring**|Jose M. Alvarez Team|[2510.24108](http://arxiv.org/abs/2510.24108)|null|
|**2025-10-28**|**Learning Parameterized Skills from Demonstrations**|George Konidaris Team|[2510.24095](http://arxiv.org/abs/2510.24095)|null|
|**2025-10-28**|**Language-Conditioned Representations and Mixture-of-Experts Policy for Robust Multi-Task Robotic Manipulation**|Jiashuo Bai Team|[2510.24055](http://arxiv.org/abs/2510.24055)|null|
|**2025-10-27**|**Adaptive Keyframe Selection for Scalable 3D Scene Reconstruction in Dynamic Environments**|Giuseppe Loianno Team|[2510.23928](http://arxiv.org/abs/2510.23928)|null|
|**2025-10-29**|**RoboOmni: Proactive Robot Manipulation in Omni-modal Context**|Xipeng Qiu Team|[2510.23763](http://arxiv.org/abs/2510.23763)|null|
|**2025-10-27**|**RobotArena $\infty$ : Scalable Robot Benchmarking via Real-to-Sim Translation**|Katerina Fragkiadaki Team|[2510.23571](http://arxiv.org/abs/2510.23571)|**[link](https://robotarenainf.github.io)**|
|**2025-10-27**|**Optimal Dimensioning of Elastic-Link Manipulators regarding Lifetime Estimation**|Andreas Mueller Team|[2510.23234](http://arxiv.org/abs/2510.23234)|null|
|**2025-10-27**|**Workspace Registration and Collision Detection for Industrial Robotics Applications**|Andreas Mueller Team|[2510.23227](http://arxiv.org/abs/2510.23227)|null|
|**2025-10-27**|**Finding 3D Scene Analogies with Multimodal Foundation Models**|Young Min Kim Team|[2510.23184](http://arxiv.org/abs/2510.23184)|null|
|**2025-10-27**|**ManiDP: Manipulability-Aware Diffusion Policy for Posture-Dependent Bimanual Manipulation**|Fei Chen Team|[2510.23016](http://arxiv.org/abs/2510.23016)|null|
|**2025-10-26**|**Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning**|Guoquan Huang Team|[2510.22789](http://arxiv.org/abs/2510.22789)|null|
|**2025-10-26**|**Edge Collaborative Gaussian Splatting with Integrated Rendering and Communication**|Chengzhong Xu Team|[2510.22718](http://arxiv.org/abs/2510.22718)|null|
|**2025-10-26**|**FastVLM: Self-Speculative Decoding for Fast Vision-Language Model Inference**|Manjesh Kumar Hanawal Team|[2510.22641](http://arxiv.org/abs/2510.22641)|null|
|**2025-10-25**|**A Novel Multi-Timescale Stability-Preserving Hierarchical Reinforcement Learning Controller Framework for Adaptive Control in High-Dimensional Dynamical Systems**|Benyamin Safizadeh Team|[2510.22420](http://arxiv.org/abs/2510.22420)|null|
|**2025-10-25**|**ACG: Action Coherence Guidance for Flow-based VLA models**|Jaegul Choo Team|[2510.22201](http://arxiv.org/abs/2510.22201)|null|
|**2025-10-25**|**RaycastGrasp: Eye-Gaze Interaction with Wearable Devices for Robotic Manipulation**|Yang Ye Team|[2510.22113](http://arxiv.org/abs/2510.22113)|null|
|**2025-10-24**|**Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising**|Yinchuan Li Team|[2510.21991](http://arxiv.org/abs/2510.21991)|null|
|**2025-10-27**|**On Uncertainty Calibration for Equivariant Functions**|Robin Walters Team|[2510.21691](http://arxiv.org/abs/2510.21691)|**[link](https://github.com/EdwardBerman/EquiUQ)**|
|**2025-10-24**|**Enhancing Tactile-based Reinforcement Learning for Robotic Control**|Sethu Vijayakumar Team|[2510.21609](http://arxiv.org/abs/2510.21609)|null|
|**2025-10-24**|**Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos**|Baining Guo Team|[2510.21571](http://arxiv.org/abs/2510.21571)|**[link](https://microsoft.github.io/VITRA/)**|
|**2025-10-24**|**Learning Neural Control Barrier Functions from Expert Demonstrations using Inverse Constraint Learning**|Hussein Sibai Team|[2510.21560](http://arxiv.org/abs/2510.21560)|null|
|**2025-10-24**|**Generalizable Hierarchical Skill Learning via Object-Centric Representation**|Robert Platt Team|[2510.21121](http://arxiv.org/abs/2510.21121)|null|
|**2025-10-23**|**BioDet: Boosting Industrial Object Detection with Image Preprocessing Strategies**|Benjamin Busam Team|[2510.21000](http://arxiv.org/abs/2510.21000)|null|
|**2025-10-23**|**SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing**|Axel Krieger Team|[2510.20965](http://arxiv.org/abs/2510.20965)|null|
|**2025-10-23**|**GSWorld: Closed-Loop Photo-Realistic Simulation Suite for Robotic Manipulation**|Xiaolong Wang Team|[2510.20813](http://arxiv.org/abs/2510.20813)|null|
|**2025-10-23**|**FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation**|Yao Mu Team|[2510.20774](http://arxiv.org/abs/2510.20774)|**[link](https://fieldgen.github.io/)**|
|**2025-10-23**|**A Parameter-Linear Formulation of the Optimal Path Following Problem for Robotic Manipulator**|Andreas Mueller Team|[2510.20496](http://arxiv.org/abs/2510.20496)|null|
|**2025-10-23**|**Dual Control Reference Generation for Optimal Pick-and-Place Execution under Payload Uncertainty**|Tom Lefebvre Team|[2510.20483](http://arxiv.org/abs/2510.20483)|null|
|**2025-10-23**|**PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning**|Gerhard Neumann Team|[2510.20406](http://arxiv.org/abs/2510.20406)|null|
|**2025-10-23**|**NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control**|Nathan F. Lepora Team|[2510.20390](http://arxiv.org/abs/2510.20390)|null|
|**2025-10-23**|**MemER: Scaling Up Memory for Robot Control via Experience Retrieval**|Chelsea Finn Team|[2510.20328](http://arxiv.org/abs/2510.20328)|**[link](https://jen-pan.github.io/memer/)**|
|**2025-10-22**|**Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning**|Bart De Schutter Team|[2510.20040](http://arxiv.org/abs/2510.20040)|null|
|**2025-10-22**|**Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets**|Xuanmeng Zhang Team|[2510.19944](http://arxiv.org/abs/2510.19944)|**[link](https://seed.bytedance.com/seed3d)**|
|**2025-10-25**|**Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning**|Abhishek Gupta Team|[2510.19495](http://arxiv.org/abs/2510.19495)|null|
|**2025-10-22**|**Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes**|Baining Guo Team|[2510.19400](http://arxiv.org/abs/2510.19400)|**[link](https://github.com/microsoft/MV-RoboBench)**|
|**2025-10-22**|**Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets**|Bernadette Bucher Team|[2510.19373](http://arxiv.org/abs/2510.19373)|null|
|**2025-10-22**|**Imitation Learning Policy based on Multi-Step Consistent Integration Shortcut Model**|Jie Zhao Team|[2510.19356](http://arxiv.org/abs/2510.19356)|null|
|**2025-10-22**|**Unified Reinforcement and Imitation Learning for Vision-Language Models**|Yueh-Hua Wu Team|[2510.19307](http://arxiv.org/abs/2510.19307)|**[link](https://byungkwanlee.github.io/RIL-page)**|
|**2025-10-22**|**TARMAC: A Taxonomy for Robot Manipulation in Chemistry**|Jihong Zhu Team|[2510.19289](http://arxiv.org/abs/2510.19289)|null|
|**2025-10-21**|**A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model**|Homayoun Najjaran Team|[2510.19128](http://arxiv.org/abs/2510.19128)|null|
|**2025-10-21**|**Efficient Model-Based Reinforcement Learning for Robot Control via Online Learning**|Marco Hutter Team|[2510.18518](http://arxiv.org/abs/2510.18518)|null|
|**2025-10-23**|**MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning**|Heng Yang Team|[2510.18337](http://arxiv.org/abs/2510.18337)|null|
|**2025-10-21**|**MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation**|Li Fei-Fei Team|[2510.18316](http://arxiv.org/abs/2510.18316)|null|
|**2025-10-20**|**Quality Over Quantity: Curating Contact-Based Robot Datasets Improves Learning**|Ian Abraham Team|[2510.18137](http://arxiv.org/abs/2510.18137)|null|
|**2025-10-20**|**R2BC: Multi-Agent Imitation Learning from Single-Agent Demonstrations**|Daniel S. Brown Team|[2510.18085](http://arxiv.org/abs/2510.18085)|null|
|**2025-10-20**|**SPACeR: Self-Play Anchoring with Centralized Reference Models**|Wei Zhan Team|[2510.18060](http://arxiv.org/abs/2510.18060)|**[link](https://spacer-ai.github.io/)**|
|**2025-10-20**|**RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation**|Ziwei Wang Team|[2510.17640](http://arxiv.org/abs/2510.17640)|null|
|**2025-10-20**|**Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm**|Xiaoji Niu Team|[2510.17604](http://arxiv.org/abs/2510.17604)|null|
|**2025-10-20**|**Plasma Shape Control via Zero-shot Generative Reinforcement Learning**|Wulyu Zhong Team|[2510.17531](http://arxiv.org/abs/2510.17531)|null|
|**2025-10-20**|**A Generalization of Input-Output Linearization via Dynamic Switching Between Melds of Output Functions**|Antonio Franchi Team|[2510.17448](http://arxiv.org/abs/2510.17448)|null|
|**2025-10-22**|**OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation**|Arash Ajoudani Team|[2510.17150](http://arxiv.org/abs/2510.17150)|**[link](https://sites.google.com/view/omni-vic})**|
|**2025-10-20**|**Decentralized Real-Time Planning for Multi-UAV Cooperative Manipulation via Imitation Learning**|Sihao Sun Team|[2510.17143](http://arxiv.org/abs/2510.17143)|null|
|**2025-10-20**|**Learning to Design Soft Hands using Reward Models**|Sha Yi Team|[2510.17086](http://arxiv.org/abs/2510.17086)|null|
|**2025-10-19**|**End-to-end Listen, Look, Speak and Act**|Chao Zhang Team|[2510.16756](http://arxiv.org/abs/2510.16756)|null|
|**2025-10-18**|**MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation**|Ufuk Topcu Team|[2510.16617](http://arxiv.org/abs/2510.16617)|null|
|**2025-10-18**|**Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making**|Jean-Michel Loubes Team|[2510.16462](http://arxiv.org/abs/2510.16462)|null|
|**2025-10-18**|**Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach**|Chengzhong Xu Team|[2510.16424](http://arxiv.org/abs/2510.16424)|null|
|**2025-10-17**|**DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly**|Minghui Zheng Team|[2510.16231](http://arxiv.org/abs/2510.16231)|null|
|**2025-10-17**|**DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation**|Yiwen Lu Team|[2510.15786](http://arxiv.org/abs/2510.15786)|null|
|**2025-10-22**|**VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation**|Bin He Team|[2510.15530](http://arxiv.org/abs/2510.15530)|null|
|**2025-10-17**|**Exploring Conditions for Diffusion models in Robotic Control**|Taekyung Kim Team|[2510.15510](http://arxiv.org/abs/2510.15510)|**[link](https://orca-rc.github.io/)**|
|**2025-10-17**|**Perfect Prediction or Plenty of Proposals? What Matters Most in Planning for Autonomous Driving**|Joschka Boedecker Team|[2510.15505](http://arxiv.org/abs/2510.15505)|null|
|**2025-10-17**|**Learning to Answer from Correct Demonstrations**|Nathan Srebro Team|[2510.15464](http://arxiv.org/abs/2510.15464)|null|
|**2025-10-17**|**GaussGym: An open-source real-to-sim framework for learning locomotion from pixels**|Pieter Abbeel Team|[2510.15352](http://arxiv.org/abs/2510.15352)|null|
|**2025-10-16**|**RM-RL: Role-Model Reinforcement Learning for Precise Robot Manipulation**|Jianfei Yang Team|[2510.15189](http://arxiv.org/abs/2510.15189)|null|
|**2025-10-18**|**VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tuning**|Yunzhu Li Team|[2510.14930](http://arxiv.org/abs/2510.14930)|**[link](https://binghao-huang.github.io/vt_refine/)**|
|**2025-10-16**|**SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time**|Javier Alonso-Mora Team|[2510.14851](http://arxiv.org/abs/2510.14851)|**[link](https://autonomousrobots.nl/paper_websites/sadcher_MRTA/)**|
|**2025-10-16**|**RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning**|Huazhe Xu Team|[2510.14830](http://arxiv.org/abs/2510.14830)|**[link](https://lei-kun.github.io/RL-100/)**|
|**2025-10-16**|**Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation**|Shan An Team|[2510.14771](http://arxiv.org/abs/2510.14771)|null|
|**2025-10-16**|**Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models**|Wilm Decré Team|[2510.14615](http://arxiv.org/abs/2510.14615)|null|
|**2025-10-16**|**Restoring Noisy Demonstration for Imitation Learning With Diffusion Models**|Shao-Hua Sun Team|[2510.14467](http://arxiv.org/abs/2510.14467)|null|
|**2025-10-16**|**Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning**|Yao Mu Team|[2510.14300](http://arxiv.org/abs/2510.14300)|null|
|**2025-10-15**|**ViTacGen: Robotic Pushing with Vision-to-Touch Generation**|Shan Luo Team|[2510.14117](http://arxiv.org/abs/2510.14117)|null|
|**2025-10-15**|**Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning**|Bram Vanderborght Team|[2510.14065](http://arxiv.org/abs/2510.14065)|null|
|**2025-10-17**|**CausalVerse: Benchmarking Causal Representation Learning with Configurable High-Fidelity Simulations**|Kun Zhang Team|[2510.14049](http://arxiv.org/abs/2510.14049)|null|
|**2025-10-15**|**LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models**|Xipeng Qiu Team|[2510.13626](http://arxiv.org/abs/2510.13626)|null|
|**2025-10-15**|**Efficient Force and Stiffness Prediction in Robotic Produce Handling with a Piezoresistive Pressure Sensor**|Xiaobo Tan Team|[2510.13616](http://arxiv.org/abs/2510.13616)|**[link](https://drive.google.com/drive/folders/1jol-_z6gaUfjpL1Qi7EG420usTbVSodv?usp=sharing)**|
|**2025-10-15**|**Active Tactile Exploration for Rigid Body Pose and Shape Estimation**|Michael Posa Team|[2510.13595](http://arxiv.org/abs/2510.13595)|null|
|**2025-10-15**|**Tactile-Conditioned Diffusion Policy for Force-Aware Robotic Manipulation**|Jan Peters Team|[2510.13324](http://arxiv.org/abs/2510.13324)|null|
|**2025-10-15**|**Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models**|Jingfeng Zhang Team|[2510.13237](http://arxiv.org/abs/2510.13237)|null|
|**2025-10-15**|**Beyond Static LLM Policies: Imitation-Enhanced Reinforcement Learning for Recommendation**|Sen Wang Team|[2510.13229](http://arxiv.org/abs/2510.13229)|null|
|**2025-10-15**|**VLA-0: Building State-of-the-Art VLAs with Zero Modification**|Fabio Ramos Team|[2510.13054](http://arxiv.org/abs/2510.13054)|null|
|**2025-10-14**|**Development of a Linear Guide-Rail Testbed for Physically Emulating ISAM Operations**|Christopher Petersen Team|[2510.13005](http://arxiv.org/abs/2510.13005)|null|
|**2025-10-14**|**Actron3D: Learning Actionable Neural Functions from Videos for Transferable Robotic Manipulation**|Stefan Leutenegger Team|[2510.12971](http://arxiv.org/abs/2510.12971)|null|
|**2025-10-14**|**Learning to Grasp Anything by Playing with Random Toys**|Roei Herzig Team|[2510.12866](http://arxiv.org/abs/2510.12866)|null|
|**2025-10-14**|**CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving**|Jiangtao Gong Team|[2510.12560](http://arxiv.org/abs/2510.12560)|null|
|**2025-10-14**|**Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge**|Bram Vanderborght Team|[2510.12509](http://arxiv.org/abs/2510.12509)|null|
|**2025-10-14**|**Fast Visuomotor Policy for Robotic Manipulation**|Wenqiang Zhang Team|[2510.12483](http://arxiv.org/abs/2510.12483)|null|
|**2025-10-14**|**Robot Learning: A Tutorial**|Michel Aractingi Team|[2510.12403](http://arxiv.org/abs/2510.12403)|null|
|**2025-10-14**|**Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking**|Eunhyeok Park Team|[2510.12392](http://arxiv.org/abs/2510.12392)|null|
|**2025-10-14**|**Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications**|Sungjoon Choi Team|[2510.12215](http://arxiv.org/abs/2510.12215)|**[link](https://chanwookim971024.github.io/PioneeR/)**|
|**2025-10-13**|**Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation**|Mac Schwager Team|[2510.11689](http://arxiv.org/abs/2510.11689)|null|
|**2025-10-14**|**ManiAgent: An Agentic Framework for General Robotic Manipulation**|Xudong Liu Team|[2510.11660](http://arxiv.org/abs/2510.11660)|null|
|**2025-10-13**|**HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data**|Yanchao Yang Team|[2510.11321](http://arxiv.org/abs/2510.11321)|null|
|**2025-10-13**|**FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks**|Alessandro Suglia Team|[2510.11307](http://arxiv.org/abs/2510.11307)|null|
|**2025-10-13**|**DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation**|Zongqing Lu Team|[2510.11258](http://arxiv.org/abs/2510.11258)|null|
|**2025-10-13**|**Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling**|Jingjing Liu Team|[2510.11083](http://arxiv.org/abs/2510.11083)|null|
|**2025-10-13**|**Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey**|Badong Chen Team|[2510.10903](http://arxiv.org/abs/2510.10903)|null|
|**2025-10-12**|**High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting**|Hua Zou Team|[2510.10637](http://arxiv.org/abs/2510.10637)|null|
|**2025-10-12**|**Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control**|Jeethu Sreenivas Amuthan Team|[2510.10516](http://arxiv.org/abs/2510.10516)|null|
|**2025-10-12**|**Data-driven simulator of multi-animal behavior with unknown dynamics via offline and online reinforcement learning**|Yoshinobu Kawahara Team|[2510.10451](http://arxiv.org/abs/2510.10451)|null|
|**2025-10-11**|**X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model**|Xianyuan Zhan Team|[2510.10274](http://arxiv.org/abs/2510.10274)|null|
|**2025-10-11**|**A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots**|Tetsuya Ogata Team|[2510.10221](http://arxiv.org/abs/2510.10221)|null|
|**2025-10-11**|**UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction**|Tetsuya Ogata Team|[2510.10217](http://arxiv.org/abs/2510.10217)|null|
|**2025-10-15**|**Ctrl-World: A Controllable Generative World Model for Robot Manipulation**|Chelsea Finn Team|[2510.10125](http://arxiv.org/abs/2510.10125)|null|
|**2025-10-10**|**VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation**|Caifeng Shan Team|[2510.09607](http://arxiv.org/abs/2510.09607)|**[link](https://ltbai.github.io/VITA-VLA/)**|
|**2025-10-13**|**Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards**|Alireza Ramezani Team|[2510.09543](http://arxiv.org/abs/2510.09543)|null|
|**2025-10-10**|**Autonomous Soft Robotic Guidewire Navigation via Imitation Learning**|Axel Krieger Team|[2510.09497](http://arxiv.org/abs/2510.09497)|null|
|**2025-10-13**|**Near-Optimal Second-Order Guarantees for Model-Based Adversarial Imitation Learning**|Weitong Zhang Team|[2510.09487](http://arxiv.org/abs/2510.09487)|null|
|**2025-10-13**|**Failure Prediction at Runtime for Generative Robot Policies**|Angela P. Schoellig Team|[2510.09459](http://arxiv.org/abs/2510.09459)|**[link](https://tum-lsy.github.io/fiper_website.)**|
|**2025-10-10**|**Rate optimal learning of equilibria from data**|Giorgia Ramponi Team|[2510.09325](http://arxiv.org/abs/2510.09325)|null|
|**2025-10-10**|**Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench Feedback Teleoperation System**|Pai Zheng Team|[2510.09229](http://arxiv.org/abs/2510.09229)|null|
|**2025-10-10**|**FM-IRL: Flow-Matching for Reward Modeling and Policy Regularization in Reinforcement Learning**|Ivor Tsang Team|[2510.09222](http://arxiv.org/abs/2510.09222)|null|
|**2025-10-10**|**When a Robot is More Capable than a Human: Learning from Constrained Demonstrators**|Erdem Bıyık Team|[2510.09096](http://arxiv.org/abs/2510.09096)|null|
|**2025-10-10**|**iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation**|Ziwei Wang Team|[2510.09036](http://arxiv.org/abs/2510.09036)|null|
|**2025-10-09**|**Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation**|Yue Wang Team|[2510.08807](http://arxiv.org/abs/2510.08807)|null|
|**2025-10-09**|**Geometry-aware Policy Imitation**|Sylvain Calinon Team|[2510.08787](http://arxiv.org/abs/2510.08787)|null|
|**2025-10-09**|**Point and Go: Intuitive Reference Frame Reallocation in Mode Switching for Assistive Robotics**|M. Jagersand Team|[2510.08753](http://arxiv.org/abs/2510.08753)|null|
|**2025-10-09**|**Agent Learning via Early Experience**|Yifan Wu Team|[2510.08558](http://arxiv.org/abs/2510.08558)|null|
|**2025-10-09**|**R2RGEN: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation**|Jiwen Lu Team|[2510.08547](http://arxiv.org/abs/2510.08547)|**[link](https://r2rgen.github.io/)**|
|**2025-10-09**|**Unlocking 3D Affordance Segmentation with 2D Semantic Knowledge**|Wei Shen Team|[2510.08316](http://arxiv.org/abs/2510.08316)|null|
|**2025-10-09**|**FastUMI-100K: Advancing Data-driven Robotic Manipulation with a Large-scale UMI-style Dataset**|Xuelong Li Team|[2510.08022](http://arxiv.org/abs/2510.08022)|null|
|**2025-10-09**|**DM1: MeanFlow with Dispersive Regularization for 1-Step Robotic Manipulation**|Weibing Li Team|[2510.07865](http://arxiv.org/abs/2510.07865)|**[link](https://guowei-zou.github.io/dm1/)**|
|**2025-10-09**|**Trajectory Conditioned Cross-embodiment Skill Transfer**|Bin Zhao Team|[2510.07773](http://arxiv.org/abs/2510.07773)|null|
|**2025-10-11**|**Differentiable Particle Optimization for Fast Sequential Manipulation**|Zachary Kingston Team|[2510.07674](http://arxiv.org/abs/2510.07674)|null|
|**2025-10-08**|**WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation**|Shanghang Zhang Team|[2510.07313](http://arxiv.org/abs/2510.07313)|null|
|**2025-10-09**|**TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics**|Shanghang Zhang Team|[2510.07181](http://arxiv.org/abs/2510.07181)|null|
|**2025-10-08**|**DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning**|Chen Lv Team|[2510.06913](http://arxiv.org/abs/2510.06913)|null|
|**2025-10-07**|**Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels**|Weiran Yao Team|[2510.06499](http://arxiv.org/abs/2510.06499)|null|
|**2025-10-07**|**EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model**|Zhaoxiang Zhang Team|[2510.06207](http://arxiv.org/abs/2510.06207)|**[link](https://anonymous.4open.science/w/Embodied-Coder/)**|
|**2025-10-07**|**Differentiable Model Predictive Control on the GPU**|Thomas Lew Team|[2510.06179](http://arxiv.org/abs/2510.06179)|null|
|**2025-10-07**|**Towards Autonomous Tape Handling for Robotic Wound Redressing**|Michael Yip Team|[2510.06127](http://arxiv.org/abs/2510.06127)|null|
|**2025-10-07**|**Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion**|Robin Chhabra Team|[2510.05957](http://arxiv.org/abs/2510.05957)|null|
|**2025-10-07**|**VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation**|Badong Chen Team|[2510.05827](http://arxiv.org/abs/2510.05827)|null|
|**2025-10-07**|**DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation**|Kuk-Jin Yoon Team|[2510.05662](http://arxiv.org/abs/2510.05662)|**[link](https://sites.google.com/view/DeLTa25/)**|
|**2025-10-07**|**Teaching Machines to Speak Using Articulatory Control**|Gopala Anumanchipalli Team|[2510.05619](http://arxiv.org/abs/2510.05619)|null|
|**2025-10-07**|**Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation**|Farrokh Janabi-Sharifi Team|[2510.05536](http://arxiv.org/abs/2510.05536)|null|
|**2025-10-06**|**VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing**|Masayoshi Tomizuka Team|[2510.05213](http://arxiv.org/abs/2510.05213)|null|
|**2025-10-06**|**Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration**|Jun Tani Team|[2510.05013](http://arxiv.org/abs/2510.05013)|null|
|**2025-10-06**|**Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization**|Arianna Traviglia Team|[2510.04781](http://arxiv.org/abs/2510.04781)|null|
|**2025-10-06**|**MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation**|Wenjie Song Team|[2510.04592](http://arxiv.org/abs/2510.04592)|null|
|**2025-10-05**|**Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators**|Anirudha Majumdar Team|[2510.04354](http://arxiv.org/abs/2510.04354)|null|
|**2025-10-05**|**RAP: 3D Rasterization Augmented End-to-End Planning**|Alexandre Alahi Team|[2510.04333](http://arxiv.org/abs/2510.04333)|null|
|**2025-10-04**|**NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation**|Chunhua Shen Team|[2510.03895](http://arxiv.org/abs/2510.03895)|null|
|**2025-10-04**|**EmbodiSwap for Zero-Shot Robot Imitation Learning**|Yiannis Aloimonos Team|[2510.03706](http://arxiv.org/abs/2510.03706)|**[link](https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing)**|
|**2025-10-04**|**Dissecting Larval Zebrafish Hunting using Deep Reinforcement Learning Trained RNN Agents**|Kanaka Rajan Team|[2510.03699](http://arxiv.org/abs/2510.03699)|null|
|**2025-10-04**|**Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning**|Majid Khadiv Team|[2510.03599](http://arxiv.org/abs/2510.03599)|null|
|**2025-10-03**|**Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching**|Xiao Liang Team|[2510.03460](http://arxiv.org/abs/2510.03460)|null|
|**2025-10-03**|**Mask2IV: Interaction-Centric Video Generation via Mask Trajectories**|Laura Sevilla-Lara Team|[2510.03135](http://arxiv.org/abs/2510.03135)|**[link](https://reagan1311.github.io/mask2iv)**|
|**2025-10-03**|**Learning Stability Certificate for Robotics in Real-World Environments**|Zhe Shen Team|[2510.03123](http://arxiv.org/abs/2510.03123)|null|
|**2025-10-06**|**Distributional Inverse Reinforcement Learning**|Anqi Wu Team|[2510.03013](http://arxiv.org/abs/2510.03013)|null|
|**2025-10-03**|**Action Deviation-Aware Inference for Low-Latency Wireless Robots**|Seong-Lyun Kim Team|[2510.02851](http://arxiv.org/abs/2510.02851)|null|
|**2025-10-03**|**Flow with the Force Field: Learning 3D Compliant Flow Matching Policies from Force and Demonstration-Guided Simulation Data**|Nadia Figueroa Team|[2510.02738](http://arxiv.org/abs/2510.02738)|null|
|**2025-10-02**|**A Recipe for Efficient Sim-to-Real Transfer in Manipulation with Online Imitation-Pretrained World Models**|Hao Su Team|[2510.02538](http://arxiv.org/abs/2510.02538)|null|
|**2025-10-02**|**U-LAG: Uncertainty-Aware, Lag-Adaptive Goal Retargeting for Robotic Manipulation**|Anujith Muraleedharan Team|[2510.02526](http://arxiv.org/abs/2510.02526)|null|
|**2025-10-02**|**Beyond Imitation: Recovering Dense Rewards from Demonstrations**|Gholamreza Haffari Team|[2510.02493](http://arxiv.org/abs/2510.02493)|null|
|**2025-10-02**|**ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation**|Cewu Lu Team|[2510.02298](http://arxiv.org/abs/2510.02298)|null|
|**2025-10-02**|**Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning**|Matthew R. Walter Team|[2510.02268](http://arxiv.org/abs/2510.02268)|null|
|**2025-10-02**|**GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning**|Bogdan Mazoure Team|[2510.02180](http://arxiv.org/abs/2510.02180)|null|
|**2025-10-02**|**Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions**|Shihua Li Team|[2510.02081](http://arxiv.org/abs/2510.02081)|null|
|**2025-10-02**|**Contrastive Representation Regularization for Vision-Language-Action Models**|Jinwoo Shin Team|[2510.01711](http://arxiv.org/abs/2510.01711)|null|
|**2025-10-02**|**Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation**|Nadia Figueroa Team|[2510.01661](http://arxiv.org/abs/2510.01661)|**[link](https://sites.google.com/view/symskill))**|
|**2025-10-02**|**FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models**|Bihan Wen Team|[2510.01642](http://arxiv.org/abs/2510.01642)|**[link](https://jimntu.github.io/FailSafe/)**|
|**2025-10-02**|**MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model**|Lili Wei Team|[2510.01635](http://arxiv.org/abs/2510.01635)|null|
|**2025-10-02**|**ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations**|Yi Xu Team|[2510.01607](http://arxiv.org/abs/2510.01607)|**[link](https://activeumi.github.io)**|
|**2025-10-02**|**MiniBEE: A New Form Factor for Compact Bimanual Dexterity**|Matei Ciocarlie Team|[2510.01603](http://arxiv.org/abs/2510.01603)|null|
|**2025-10-02**|**Predictive Preference Learning from Human Interventions**|Bolei Zhou Team|[2510.01545](http://arxiv.org/abs/2510.01545)|**[link](https://metadriverse.github.io/ppl)**|
|**2025-10-02**|**Information Seeking for Robust Decision Making under Partial Observability**|Tsung-Wei Ke Team|[2510.01531](http://arxiv.org/abs/2510.01531)|**[link](https://infoseekerllm.github.io)**|
|**2025-10-01**|**Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments**|Ahmed H. Qureshi Team|[2510.01519](http://arxiv.org/abs/2510.01519)|null|
|**2025-10-01**|**Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets**|Ali Baheri Team|[2510.01479](http://arxiv.org/abs/2510.01479)|null|
|**2025-10-01**|**AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation**|Pratap Tokekar Team|[2510.01433](http://arxiv.org/abs/2510.01433)|null|
|**2025-10-01**|**How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?**|Russ Tedrake Team|[2510.01404](http://arxiv.org/abs/2510.01404)|**[link](https://diffusion-learns-kinematic.github.io)**|
|**2025-10-01**|**Temporal Score Rescaling for Temperature Sampling in Diffusion and Flow Models**|Shubham Tulsiani Team|[2510.01184](http://arxiv.org/abs/2510.01184)|null|
|**2025-10-01**|**Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning**|D. Tsetserukou Team|[2510.01023](http://arxiv.org/abs/2510.01023)|null|
|**2025-10-01**|**On Discovering Algorithms for Adversarial Imitation Learning**|Pradeep Varakantham Team|[2510.00922](http://arxiv.org/abs/2510.00922)|null|
|**2025-10-01**|**TubeDAgger: Reducing the Number of Expert Interventions with Stochastic Reach-Tubes**|Sophie A. Neubauer Team|[2510.00906](http://arxiv.org/abs/2510.00906)|null|
|**2025-09-30**|**MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation**|Shanghang Zhang Team|[2509.26642](http://arxiv.org/abs/2509.26642)|null|
|**2025-09-30**|**Learning from Hallucinating Critical Points for Navigation in Dynamic Environments**|Xuesu Xiao Team|[2509.26513](http://arxiv.org/abs/2509.26513)|null|
|**2025-09-30**|**Anomaly detection for generic failure monitoring in robotic assembly, screwing and manipulation**|Kevin Haninger Team|[2509.26308](http://arxiv.org/abs/2509.26308)|null|
|**2025-09-30**|**Noise-Guided Transport for Imitation Learning**|Alexandros Kalousis Team|[2509.26294](http://arxiv.org/abs/2509.26294)|null|
|**2025-09-30**|**Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation**|Hao Chen Team|[2509.25852](http://arxiv.org/abs/2509.25852)|null|
|**2025-10-01**|**Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies**|Li Cheng Team|[2509.25822](http://arxiv.org/abs/2509.25822)|null|
|**2025-09-30**|**Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding**|Jiaojiao Fan Team|[2509.25794](http://arxiv.org/abs/2509.25794)|null|
|**2025-09-30**|**SAC Flow: Sample-Efficient Reinforcement Learning of Flow-Based Policies via Velocity-Reparameterized Sequential Modeling**|Wenbo Ding Team|[2509.25756](http://arxiv.org/abs/2509.25756)|null|
|**2025-09-30**|**Best of Sim and Real: Decoupled Visuomotor Manipulation via Learning Control in Simulation and Perception in Real**|Yang Gao Team|[2509.25747](http://arxiv.org/abs/2509.25747)|null|
|**2025-09-29**|**Boolean Satisfiability via Imitation Learning**|Xiangyu Xu Team|[2509.25411](http://arxiv.org/abs/2509.25411)|null|
|**2025-09-29**|**Parallel Heuristic Search as Inference for Actor-Critic Reinforcement Learning Models**|Maxim Likhachev Team|[2509.25402](http://arxiv.org/abs/2509.25402)|null|
|**2025-09-29**|**SARM: Stage-Aware Reward Modeling for Long Horizon Robot Manipulation**|Philipp Wu Team|[2509.25358](http://arxiv.org/abs/2509.25358)|null|
|**2025-09-29**|**SRMP: Search-Based Robot Motion Planning Library**|Maxim Likhachev Team|[2509.25352](http://arxiv.org/abs/2509.25352)|null|
|**2025-10-01**|**Curriculum Imitation Learning of Distributed Multi-Robot Policies**|Eduardo Montijano Team|[2509.25097](http://arxiv.org/abs/2509.25097)|null|
|**2025-09-29**|**Annotation-Free One-Shot Imitation Learning for Multi-Step Manipulation Tasks**|Ruchi Choudhary Team|[2509.24972](http://arxiv.org/abs/2509.24972)|null|
|**2025-09-29**|**MSG: Multi-Stream Generative Policies for Sample-Efficient Robotic Manipulation**|Abhinav Valada Team|[2509.24956](http://arxiv.org/abs/2509.24956)|null|
|**2025-09-29**|**World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training**|Qing Zhang Team|[2509.24948](http://arxiv.org/abs/2509.24948)|null|
|**2025-09-29**|**From Code to Action: Hierarchical Learning of Diffusion-VLM Policies**|Daniel Dijkman Team|[2509.24917](http://arxiv.org/abs/2509.24917)|null|
|**2025-09-29**|**Quantifying Generalisation in Imitation Learning**|Odinaldo Rodrigues Team|[2509.24784](http://arxiv.org/abs/2509.24784)|null|
|**2025-09-29**|**IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks**|Ville Kyrki Team|[2509.24768](http://arxiv.org/abs/2509.24768)|null|
|**2025-09-29**|**Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering**|Daniele Pucci Team|[2509.24697](http://arxiv.org/abs/2509.24697)|null|
|**2025-09-29**|**CEDex: Cross-Embodiment Dexterous Grasp Generation at Scale from Human-like Contact Representations**|Shan Luo Team|[2509.24661](http://arxiv.org/abs/2509.24661)|null|
|**2025-09-29**|**U-DiT Policy: U-shaped Diffusion Transformers for Robotic Manipulation**|Zhongxue Gan Team|[2509.24579](http://arxiv.org/abs/2509.24579)|null|
|**2025-09-29**|**Unlocking the Potential of Soft Actor-Critic for Imitation Learning**|Frank Kirchner Team|[2509.24539](http://arxiv.org/abs/2509.24539)|null|
|**2025-09-29**|**Learning to Sample: Reinforcement Learning-Guided Sampling for Autonomous Vehicle Motion Planning**|Johannes Betz Team|[2509.24313](http://arxiv.org/abs/2509.24313)|null|
|**2025-09-29**|**FreeAction: Training-Free Techniques for Enhanced Fidelity of Trajectory-to-Video Generation**|Minsu Cho Team|[2509.24241](http://arxiv.org/abs/2509.24241)|null|
|**2025-09-29**|**ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning**|Yang You Team|[2509.24219](http://arxiv.org/abs/2509.24219)|null|
|**2025-09-29**|**Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models**|Sethu Vijayakumar Team|[2509.24163](http://arxiv.org/abs/2509.24163)|null|
|**2025-09-29**|**Memory Transfer Planning: LLM-driven Context-Aware Code Adaptation for Robot Manipulation**|Yang You Team|[2509.24160](http://arxiv.org/abs/2509.24160)|null|
|**2025-09-28**|**Mash, Spread, Slice! Learning to Manipulate Object States via Visual Spatial Progress**|Kristen Grauman Team|[2509.24129](http://arxiv.org/abs/2509.24129)|null|
|**2025-09-28**|**DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation**|Yuanpei Chen Team|[2509.23829](http://arxiv.org/abs/2509.23829)|null|
|**2025-09-28**|**Control Your Robot: A Unified System for Robot Control and Policy Deployment**|Bingshan Hu Team|[2509.23823](http://arxiv.org/abs/2509.23823)|**[link](https://github.com/Tian-Nian/control_your_robot)**|
|**2025-09-30**|**Sequence Pathfinder for Multi-Agent Pickup and Delivery in the Warehouse**|Ying Wen Team|[2509.23778](http://arxiv.org/abs/2509.23778)|null|
|**2025-09-26**|**Pixel Motion Diffusion is What We Need for Robot Control**|Michael S. Ryoo Team|[2509.22652](http://arxiv.org/abs/2509.22652)|null|
|**2025-09-26**|**VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search**|Ziwei Wang Team|[2509.22643](http://arxiv.org/abs/2509.22643)|null|
|**2025-09-26**|**Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning**|Xing Sun Team|[2509.22601](http://arxiv.org/abs/2509.22601)|null|
|**2025-09-26**|**EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation**|Liang Wang Team|[2509.22578](http://arxiv.org/abs/2509.22578)|null|
|**2025-09-26**|**Learning to Ball: Composing Policies for Long-Horizon Basketball Moves**|C. Karen Liu Team|[2509.22442](http://arxiv.org/abs/2509.22442)|**[link](http://pei-xu.github.io/basketball.)**|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|Guan Huang Team|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-26**|**ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation**|Yang Yu Team|[2509.22402](http://arxiv.org/abs/2509.22402)|null|
|**2025-09-26**|**RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation**|Shuchao Pang Team|[2509.22356](http://arxiv.org/abs/2509.22356)|null|
|**2025-09-26**|**DemoGrasp: Universal Dexterous Grasping from a Single Demonstration**|Zongqing Lu Team|[2509.22149](http://arxiv.org/abs/2509.22149)|null|
|**2025-09-26**|**Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation**|Chang Xu Team|[2509.22093](http://arxiv.org/abs/2509.22093)|null|
|**2025-09-26**|**Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error**|Christos Tzamos Team|[2509.22023](http://arxiv.org/abs/2509.22023)|null|
|**2025-09-26**|**WAVE: Worm Gear-based Adaptive Variable Elasticity for Decoupling Actuators from External Forces**|Kazutoshi Tanaka Team|[2509.21878](http://arxiv.org/abs/2509.21878)|null|
|**2025-09-26**|**Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors**|Qinchuan Li Team|[2509.21810](http://arxiv.org/abs/2509.21810)|null|
|**2025-09-26**|**The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions**|Matthew Pan Team|[2509.21776](http://arxiv.org/abs/2509.21776)|**[link](https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/)**|
|**2025-09-25**|**Generating Stable Placements via Physics-guided Diffusion Models**|Jonathan Kelly Team|[2509.21664](http://arxiv.org/abs/2509.21664)|null|
|**2025-09-25**|**Inverse Reinforcement Learning Using Just Classification and a Few Regressions**|Aurélien Bibaut Team|[2509.21172](http://arxiv.org/abs/2509.21172)|null|
|**2025-09-25**|**ImaginationPolicy: Towards Generalizable, Precise and Reliable End-to-End Policy for Robotic Manipulation**|Kui Jia Team|[2509.20841](http://arxiv.org/abs/2509.20841)|**[link](https://sites.google.com/view/imaginationpolicy)**|
|**2025-09-25**|**Joint Flow Trajectory Optimization For Feasible Robot Motion Generation from Video Demonstrations**|Weiming Zhi Team|[2509.20703](http://arxiv.org/abs/2509.20703)|null|
|**2025-09-24**|**Large Pre-Trained Models for Bimanual Manipulation in 3D**|David Meger Team|[2509.20579](http://arxiv.org/abs/2509.20579)|null|
|**2025-09-24**|**Selective Progress-Aware Querying for Human-in-the-Loop Reinforcement Learning**|Anamika J H Team|[2509.20541](http://arxiv.org/abs/2509.20541)|null|
|**2025-09-26**|**mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies**|Shiwei Sheng Team|[2509.20297](http://arxiv.org/abs/2509.20297)|null|
|**2025-09-24**|**Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving**|Xianpeng Lang Team|[2509.20109](http://arxiv.org/abs/2509.20109)|null|
|**2025-09-24**|**LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs**|Amir Barati Farimani Team|[2509.20070](http://arxiv.org/abs/2509.20070)|null|
|**2025-09-25**|**Generalist Robot Manipulation beyond Action Labeled Data**|Danda Pani Paudel Team|[2509.19958](http://arxiv.org/abs/2509.19958)|null|
|**2025-09-24**|**SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process**|JingYuan Wang Team|[2509.19853](http://arxiv.org/abs/2509.19853)|null|
|**2025-09-24**|**TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies**|Animesh Garg Team|[2509.19712](http://arxiv.org/abs/2509.19712)|null|
|**2025-09-24**|**RoboSSM: Scalable In-context Imitation Learning via State-Space Models**|Peter Stone Team|[2509.19658](http://arxiv.org/abs/2509.19658)|null|
|**2025-09-23**|**EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data**|Danfei Xu Team|[2509.19626](http://arxiv.org/abs/2509.19626)|null|
|**2025-09-23**|**From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting**|Sylvia L. Herbert Team|[2509.19597](http://arxiv.org/abs/2509.19597)|null|
|**2025-09-23**|**Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action**|Liam Paull Team|[2509.19571](http://arxiv.org/abs/2509.19571)|**[link](https://montrealrobotics.ca/agentic-scene-policies.github.io/)**|
|**2025-09-23**|**Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation**|Chi-Guhn Lee Team|[2509.19524](http://arxiv.org/abs/2509.19524)|null|
|**2025-09-23**|**Self-evolved Imitation Learning in Simulated World**|Zhihe Lu Team|[2509.19460](http://arxiv.org/abs/2509.19460)|null|
|**2025-09-23**|**ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation**|Daniel Seita Team|[2509.19454](http://arxiv.org/abs/2509.19454)|null|
|**2025-09-23**|**SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration**|Cewu Lu Team|[2509.19292](http://arxiv.org/abs/2509.19292)|null|
|**2025-09-23**|**Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces**|Arash Ajoudani Team|[2509.19261](http://arxiv.org/abs/2509.19261)|null|
|**2025-09-23**|**FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation**|Jianwei Zhang Team|[2509.19102](http://arxiv.org/abs/2509.19102)|**[link](https://sites.google.com/view/funcanon)**|
|**2025-09-23**|**World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation**|Dongbin Zhao Team|[2509.19080](http://arxiv.org/abs/2509.19080)|null|
|**2025-09-23**|**ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation**|Kyoobin Lee Team|[2509.19047](http://arxiv.org/abs/2509.19047)|null|
|**2025-09-23**|**Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations**|Wen Yao Team|[2509.18953](http://arxiv.org/abs/2509.18953)|null|
|**2025-09-23**|**Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation**|Thanpimon Buamanee Team|[2509.18865](http://arxiv.org/abs/2509.18865)|null|
|**2025-09-23**|**DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation**|Jiajun Wu Team|[2509.18830](http://arxiv.org/abs/2509.18830)|null|
|**2025-09-23**|**VGGT-DP: Generalizable Robot Control via Vision Foundation Models**|Zhi Wang Team|[2509.18778](http://arxiv.org/abs/2509.18778)|null|
|**2025-09-23**|**MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning**|Fares Abu-Dakka Team|[2509.18757](http://arxiv.org/abs/2509.18757)|**[link](https://mv-umi.github.io)**|
|**2025-09-23**|**Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation**|Sanket Gujar Team|[2509.18734](http://arxiv.org/abs/2509.18734)|null|
|**2025-09-23**|**3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space**|Kyoobin Lee Team|[2509.18676](http://arxiv.org/abs/2509.18676)|null|
|**2025-09-24**|**Do You Need Proprioceptive States in Visuomotor Policies?**|Yang Gao Team|[2509.18644](http://arxiv.org/abs/2509.18644)|**[link](https://statefreepolicy.github.io)**|
|**2025-09-23**|**Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training**|Danfei Xu Team|[2509.18631](http://arxiv.org/abs/2509.18631)|null|
|**2025-09-23**|**SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones**|Mac Schwager Team|[2509.18610](http://arxiv.org/abs/2509.18610)|null|
|**2025-09-23**|**Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills**|Alois Knoll Team|[2509.18597](http://arxiv.org/abs/2509.18597)|null|
|**2025-09-23**|**A scaling law for large-deformation contact in soft materials**|Huajian Gao Team|[2509.18581](http://arxiv.org/abs/2509.18581)|null|
|**2025-09-22**|**Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task**|Luka Peternel Team|[2509.18463](http://arxiv.org/abs/2509.18463)|null|
|**2025-09-22**|**Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands**|Daniel Seita Team|[2509.18455](http://arxiv.org/abs/2509.18455)|null|
|**2025-09-22**|**PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction**|Tapomayukh Bhattacharjee Team|[2509.18447](http://arxiv.org/abs/2509.18447)|null|
|**2025-09-22**|**ByteWrist: A Parallel Robotic Wrist Enabling Flexible and Anthropomorphic Motion for Confined Spaces**|Zeyu Ren Team|[2509.18084](http://arxiv.org/abs/2509.18084)|**[link](https://bytewrist.github.io/)**|
|**2025-09-22**|**Prepare Before You Act: Learning From Humans to Rearrange Initial States**|Dylan P. Losey Team|[2509.18043](http://arxiv.org/abs/2509.18043)|null|
|**2025-09-22**|**FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance**|Ruixun Zhang Team|[2509.17964](http://arxiv.org/abs/2509.17964)|null|
|**2025-09-22**|**ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion**|Joydeep Biswas Team|[2509.17941](http://arxiv.org/abs/2509.17941)|**[link](https://amrl.cs.utexas.edu/ComposableNav/)**|
|**2025-09-22**|**DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving**|Zhaoxiang Zhang Team|[2509.17940](http://arxiv.org/abs/2509.17940)|null|
|**2025-09-23**|**RoboSeek: You Need to Interact with Your Objects**|Yatong Han Team|[2509.17783](http://arxiv.org/abs/2509.17783)|null|
|**2025-09-22**|**MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies**|Yang Gao Team|[2509.17759](http://arxiv.org/abs/2509.17759)|null|
|**2025-09-22**|**EigenSafe: A Spectral Framework for Learning-Based Stochastic Safety Filtering**|H. Jin Kim Team|[2509.17750](http://arxiv.org/abs/2509.17750)|null|
|**2025-09-22**|**DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning**|Zidong Chen Team|[2509.17684](http://arxiv.org/abs/2509.17684)|null|
|**2025-09-22**|**Learning Dexterous Manipulation with Quantized Hand State**|Cewu Lu Team|[2509.17450](http://arxiv.org/abs/2509.17450)|null|
|**2025-09-22**|**Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators**|Hamidreza Kasaei Team|[2509.17381](http://arxiv.org/abs/2509.17381)|**[link](https://sites.google.com/view/ftp4rm/home)**|
|**2025-09-21**|**Scalable Multi Agent Diffusion Policies for Coverage Control**|Alejandro Ribeiro Team|[2509.17244](http://arxiv.org/abs/2509.17244)|null|
|**2025-09-21**|**Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation**|Timothy D. Barfoot Team|[2509.17204](http://arxiv.org/abs/2509.17204)|null|
|**2025-09-21**|**MAST: Multi-Agent Spatial Transformer for Learning to Collaborate**|Alejandro Ribeiro Team|[2509.17195](http://arxiv.org/abs/2509.17195)|null|
|**2025-09-21**|**Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation**|Hao Dong Team|[2509.17125](http://arxiv.org/abs/2509.17125)|null|
|**2025-09-21**|**RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments**|Yukiyasu Domae Team|[2509.17057](http://arxiv.org/abs/2509.17057)|null|
|**2025-09-21**|**FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks**|Guyue Zhou Team|[2509.17053](http://arxiv.org/abs/2509.17053)|null|
|**2025-09-21**|**Generalized Momenta-Based Koopman Formalism for Robust Control of Euler-Lagrangian Systems**|Jishnu Keshavan Team|[2509.17010](http://arxiv.org/abs/2509.17010)|null|
|**2025-09-21**|**End2Race: Efficient End-to-End Imitation Learning for Real-Time F1Tenth Racing**|Henry X. Liu Team|[2509.16894](http://arxiv.org/abs/2509.16894)|null|
|**2025-09-20**|**Robot Learning with Sparsity and Scarcity**|Jingxi Xu Team|[2509.16834](http://arxiv.org/abs/2509.16834)|null|
|**2025-09-19**|**Efficient Detection of Objects Near a Robot Manipulator via Miniature Time-of-Flight Sensors**|Michael Gleicher Team|[2509.16122](http://arxiv.org/abs/2509.16122)|null|
|**2025-09-19**|**I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models**|Mohamed Chetouani Team|[2509.16072](http://arxiv.org/abs/2509.16072)|null|
|**2025-09-19**|**Compose by Focus: Scene Graph-based Atomic Skills**|Heng Yang Team|[2509.16053](http://arxiv.org/abs/2509.16053)|null|
|**2025-09-19**|**Learning Safety for Obstacle Avoidance via Control Barrier Functions**|Calin A. Belta Team|[2509.16037](http://arxiv.org/abs/2509.16037)|null|
|**2025-09-19**|**Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder**|Ian Reid Team|[2509.15880](http://arxiv.org/abs/2509.15880)|**[link](https://evggt.github.io/)**|
|**2025-09-19**|**All-Electric Heavy-Duty Robotic Manipulator: Actuator Configuration Optimization and Sensorless Control**|Jouni Mattila Team|[2509.15778](http://arxiv.org/abs/2509.15778)|null|
|**2025-09-19**|**GP3: A 3D Geometry-Aware Policy with Multi-View Images for Robotic Manipulation**|Deli Zhao Team|[2509.15733](http://arxiv.org/abs/2509.15733)|null|
|**2025-09-19**|**Imagination at Inference: Synthesizing In-Hand Views for Robust Visuomotor Policy Inference**|Yoshihiko Nakamura Team|[2509.15717](http://arxiv.org/abs/2509.15717)|null|
|**2025-09-18**|**Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning**|Haodong Zhang Team|[2509.15443](http://arxiv.org/abs/2509.15443)|null|
|**2025-09-18**|**RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation**|Xin Li Team|[2509.15212](http://arxiv.org/abs/2509.15212)|**[link](https://github.com/alibaba-damo-academy/RynnVLA-001)**|
|**2025-09-18**|**Self-Improving Embodied Foundation Models**|Igor Mordatch Team|[2509.15155](http://arxiv.org/abs/2509.15155)|null|
|**2025-09-18**|**A Nonlinear Scaling-based Design of Control Lyapunov-barrier Function for Relative Degree 2 Case and its Application to Safe Feedback Linearization**|Gyunghoon Park Team|[2509.15071](http://arxiv.org/abs/2509.15071)|null|
|**2025-09-18**|**Reinforcement Learning Agent for a 2D Shooter Game**|Hamza A. A. Gardi Team|[2509.15042](http://arxiv.org/abs/2509.15042)|null|
|**2025-09-19**|**Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery**|Yasuhisa Hasegawa Team|[2509.14967](http://arxiv.org/abs/2509.14967)|null|
|**2025-09-18**|**Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale**|Florian Walter Team|[2509.14932](http://arxiv.org/abs/2509.14932)|null|
|**2025-09-18**|**exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation**|Yong-Lu Li Team|[2509.14688](http://arxiv.org/abs/2509.14688)|null|
|**2025-09-18**|**SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching**|Guy Rosman Team|[2509.14548](http://arxiv.org/abs/2509.14548)|null|
|**2025-09-18**|**Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking**|Chen Peng Team|[2509.14530](http://arxiv.org/abs/2509.14530)|null|
|**2025-09-17**|**Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring**|Constantinos Chamzas Team|[2509.14460](http://arxiv.org/abs/2509.14460)|null|
|**2025-09-17**|**LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation**|Han Liu Team|[2509.14349](http://arxiv.org/abs/2509.14349)|null|
|**2025-09-17**|**MIMIC-D: Multi-modal Imitation for MultI-agent Coordination with Decentralized Diffusion Policies**|Negar Mehr Team|[2509.14159](http://arxiv.org/abs/2509.14159)|null|
|**2025-09-17**|**SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model**|Yiming Feng Team|[2509.14138](http://arxiv.org/abs/2509.14138)|null|
|**2025-09-17**|**PhysicalAgent: Towards General Cognitive Robotics with Foundation World Models**|Dzmitry Tsetserukou Team|[2509.13903](http://arxiv.org/abs/2509.13903)|null|
|**2025-09-17**|**Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach**|Yangwei You Team|[2509.13774](http://arxiv.org/abs/2509.13774)|null|
|**2025-09-17**|**Motion Adaptation Across Users and Tasks for Exoskeletons via Meta-Learning**|Houcheng Li Team|[2509.13736](http://arxiv.org/abs/2509.13736)|null|
|**2025-09-17**|**Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings**|Changjoo Nam Team|[2509.13731](http://arxiv.org/abs/2509.13731)|null|
|**2025-09-17**|**HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion**|I-Ming Chen Team|[2509.13692](http://arxiv.org/abs/2509.13692)|null|
|**2025-09-16**|**TreeIRL: Safe Urban Driving with Tree Search and Inverse Reinforcement Learning**|Yunqing Hu Team|[2509.13579](http://arxiv.org/abs/2509.13579)|null|
|**2025-09-18**|**StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening**|Shayegan Omidshafiei Team|[2509.13200](http://arxiv.org/abs/2509.13200)|null|
|**2025-09-16**|**A Design Co-Pilot for Task-Tailored Manipulators**|Matthias Althoff Team|[2509.13077](http://arxiv.org/abs/2509.13077)|null|
|**2025-09-16**|**Deep Learning for Model-Free Prediction of Thermal States of Robot Joint Motors**|Eric Guiffo Kaigom Team|[2509.12739](http://arxiv.org/abs/2509.12739)|null|
|**2025-09-16**|**Safety filtering of robotic manipulation under environment uncertainty: a computational approach**|Martin Servin Team|[2509.12674](http://arxiv.org/abs/2509.12674)|null|
|**2025-09-16**|**ActiveVLN: Towards Active Exploration via Multi-Turn RL in Vision-and-Language Navigation**|Feng Zheng Team|[2509.12618](http://arxiv.org/abs/2509.12618)|null|
|**2025-09-16**|**Robust Online Residual Refinement via Koopman-Guided Dynamics Modeling**|Donglin Wang Team|[2509.12562](http://arxiv.org/abs/2509.12562)|null|
|**2025-09-16**|**Pre-trained Visual Representations Generalize Where it Matters in Model-Based Reinforcement Learning**|Sebastian W. Pattinson Team|[2509.12531](http://arxiv.org/abs/2509.12531)|null|
|**2025-09-15**|**Geometric Red-Teaming for Robotic Manipulation**|Zackory Erickson Team|[2509.12379](http://arxiv.org/abs/2509.12379)|null|
|**2025-09-15**|**Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors**|Anirudha Majumdar Team|[2509.12081](http://arxiv.org/abs/2509.12081)|null|
|**2025-09-15**|**Imitation Learning as Return Distribution Matching**|Alberto Maria Metelli Team|[2509.12026](http://arxiv.org/abs/2509.12026)|null|
|**2025-09-15**|**Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees**|Stephan Sigg Team|[2509.12008](http://arxiv.org/abs/2509.12008)|null|
|**2025-09-15**|**Learning to Generate 4D LiDAR Sequences**|Wei Tsang Ooi Team|[2509.11959](http://arxiv.org/abs/2509.11959)|**[link](https://lidarcrafter.github.io/)**|
|**2025-09-15**|**Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning**|Tim Bradley Team|[2509.11880](http://arxiv.org/abs/2509.11880)|null|
|**2025-09-15**|**Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer**|Luhui Hu Team|[2509.11865](http://arxiv.org/abs/2509.11865)|null|
|**2025-09-17**|**TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning**|Donglin Wang Team|[2509.11839](http://arxiv.org/abs/2509.11839)|null|
|**2025-09-15**|**Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios**|Alois Knoll Team|[2509.11621](http://arxiv.org/abs/2509.11621)|null|
|**2025-09-15**|**RAPTOR: A Foundation Policy for Quadrotor Control**|Giuseppe Loianno Team|[2509.11481](http://arxiv.org/abs/2509.11481)|null|
|**2025-09-17**|**Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations**|Xuanlin Li Team|[2509.11417](http://arxiv.org/abs/2509.11417)|**[link](https://gen-vla.github.io/)**|
|**2025-09-14**|**ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation**|Yizhao Wang Team|[2509.11364](http://arxiv.org/abs/2509.11364)|null|
|**2025-09-14**|**MEMBOT: Memory-Based Robot in Intermittent POMDP**|Eyan Noronha Team|[2509.11225](http://arxiv.org/abs/2509.11225)|null|
|**2025-09-14**|**SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators**|Jun Ma Team|[2509.11185](http://arxiv.org/abs/2509.11185)|null|
|**2025-09-14**|**ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations**|Jun Ma Team|[2509.11125](http://arxiv.org/abs/2509.11125)|null|
|**2025-09-16**|**FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers**|Zhigong Song Team|[2509.11109](http://arxiv.org/abs/2509.11109)|null|
|**2025-09-14**|**End-to-End Visual Autonomous Parking via Control-Aided Attention**|Chen Feng Team|[2509.11090](http://arxiv.org/abs/2509.11090)|null|
|**2025-09-14**|**FragmentGPT: A Unified GPT Model for Fragment Growing, Linking, and Merging in Molecular Design**|Rick Stevens Team|[2509.11044](http://arxiv.org/abs/2509.11044)|null|
|**2025-09-13**|**ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation**|Danfei Xu Team|[2509.10952](http://arxiv.org/abs/2509.10952)|null|
|**2025-09-11**|**Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision**|Yukiyasu Domae Team|[2509.09893](http://arxiv.org/abs/2509.09893)|null|
|**2025-09-11**|**Off Policy Lyapunov Stability in Reinforcement Learning**|Daniela Constantinescu Team|[2509.09863](http://arxiv.org/abs/2509.09863)|null|
|**2025-09-11**|**MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos**|Yuke Zhu Team|[2509.09769](http://arxiv.org/abs/2509.09769)|null|
|**2025-09-11**|**SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning**|Ning Ding Team|[2509.09674](http://arxiv.org/abs/2509.09674)|null|
|**2025-09-11**|**Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration**|Wei Yang Team|[2509.09671](http://arxiv.org/abs/2509.09671)|null|
|**2025-09-11**|**A Neuromorphic Incipient Slip Detection System using Papillae Morphology**|Benjamin Ward-Cherrier Team|[2509.09546](http://arxiv.org/abs/2509.09546)|null|
|**2025-09-11**|**KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning**|M. Ani Hsieh Team|[2509.09074](http://arxiv.org/abs/2509.09074)|null|
|**2025-09-11**|**Joint Model-based Model-free Diffusion for Planning with Constraints**|Shreyas Kousik Team|[2509.08775](http://arxiv.org/abs/2509.08775)|null|
|**2025-09-10**|**SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation**|Peter Stone Team|[2509.08757](http://arxiv.org/abs/2509.08757)|**[link](https://larg.github.io/socialnav-sub)**|
|**2025-09-10**|**PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching**|Liang Ding Team|[2509.08435](http://arxiv.org/abs/2509.08435)|null|
|**2025-09-10**|**Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration**|Huimin Lu Team|[2509.08354](http://arxiv.org/abs/2509.08354)|null|
|**2025-09-10**|**Input-gated Bilateral Teleoperation: An Easy-to-implement Force Feedback Teleoperation Method for Low-cost Hardware**|Tetsuya Ogata Team|[2509.08226](http://arxiv.org/abs/2509.08226)|null|
|**2025-09-09**|**TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models**|Hao Zhao Team|[2509.07962](http://arxiv.org/abs/2509.07962)|**[link](https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/})**|
|**2025-09-09**|**Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation**|Yingbai Hu Team|[2509.07957](http://arxiv.org/abs/2509.07957)|null|
|**2025-09-09**|**RaC: Robot Learning for Long-Horizon Tasks by Scaling Recovery and Correction**|Aviral Kumar Team|[2509.07953](http://arxiv.org/abs/2509.07953)|null|
|**2025-09-09**|**Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions**|Nathan F. Lepora Team|[2509.07445](http://arxiv.org/abs/2509.07445)|null|
|**2025-09-08**|**Quantum Machine Learning and Grover's Algorithm for Quantum Optimization of Robotic Manipulators**|Howard Li Team|[2509.07216](http://arxiv.org/abs/2509.07216)|null|
|**2025-09-08**|**Design of Input-Output Observers for a Population of Systems with Bounded Frequency-Domain Variation using $DK$ -iteration**|James Richard Forbes Team|[2509.07201](http://arxiv.org/abs/2509.07201)|null|
|**2025-09-08**|**First Plan Then Evaluate: Use a Vectorized Motion Planner for Grasping**|Tucker Hermans Team|[2509.07162](http://arxiv.org/abs/2509.07162)|null|
|**2025-09-08**|**Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments**|Deepak Pathak Team|[2509.06953](http://arxiv.org/abs/2509.06953)|null|
|**2025-09-10**|**LLaDA-VLA: Vision Language Diffusion Action Models**|Xiaoyan Sun Team|[2509.06932](http://arxiv.org/abs/2509.06932)|null|
|**2025-09-08**|**Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention**|Mohamed Zayaan S Team|[2509.06705](http://arxiv.org/abs/2509.06705)|null|
|**2025-09-08**|**Group Effect Enhanced Generative Adversarial Imitation Learning for Individual Travel Behavior Modeling under Incentives**|Zhenliang Ma Team|[2509.06656](http://arxiv.org/abs/2509.06656)|null|
|**2025-09-08**|**Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster**|Pavan Ramdya Team|[2509.06426](http://arxiv.org/abs/2509.06426)|null|
|**2025-09-07**|**O $^3$ Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation**|Yen-Ling Kuo Team|[2509.06233](http://arxiv.org/abs/2509.06233)|**[link](https://o3afford.github.io/)**|
|**2025-09-07**|**Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness**|Zhendong Dai Team|[2509.06048](http://arxiv.org/abs/2509.06048)|**[link](https://authors.elsevier.com/c/1lgjX3HdG3supQ)**|
|**2025-09-06**|**TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs**|John Liu Team|[2509.05547](http://arxiv.org/abs/2509.05547)|null|
|**2025-09-05**|**OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation**|Yu Xiang Team|[2509.05513](http://arxiv.org/abs/2509.05513)|null|
|**2025-09-04**|**Long-Horizon Visual Imitation Learning via Plan and Code Reflection**|Yunde Jia Team|[2509.05368](http://arxiv.org/abs/2509.05368)|null|
|**2025-09-08**|**Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework**|Ji-Rong Wen Team|[2509.05007](http://arxiv.org/abs/2509.05007)|null|
|**2025-09-05**|**Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics**|Toshiaki Tsuji Team|[2509.04737](http://arxiv.org/abs/2509.04737)|null|
|**2025-09-04**|**Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision**|Noorbakhsh Amiri Golilarz Team|[2509.04658](http://arxiv.org/abs/2509.04658)|null|
|**2025-09-04**|**Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement**|David Held Team|[2509.04645](http://arxiv.org/abs/2509.04645)|**[link](https://planning-from-point-clouds.github.io/))**|
|**2025-09-04**|**Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control**|Richard Linares Team|[2509.04628](http://arxiv.org/abs/2509.04628)|null|
|**2025-09-04**|**In-Context Policy Adaptation via Cross-Domain Skill Diffusion**|Honguk Woo Team|[2509.04535](http://arxiv.org/abs/2509.04535)|null|
|**2025-09-04**|**EMMA: Scaling Mobile Manipulation via Egocentric Human Data**|Danfei Xu Team|[2509.04443](http://arxiv.org/abs/2509.04443)|null|
|**2025-09-04**|**Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models**|Donglin Wang Team|[2509.04063](http://arxiv.org/abs/2509.04063)|null|
|**2025-09-04**|**FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction**|Jingtai Liu Team|[2509.04018](http://arxiv.org/abs/2509.04018)|null|
|**2025-09-04**|**Weakly-Supervised Learning of Dense Functional Correspondences**|Jiajun Wu Team|[2509.03893](http://arxiv.org/abs/2509.03893)|**[link](https://dense-functional-correspondence.github.io/)**|
|**2025-09-05**|**Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator**|Wei Xu Team|[2509.03859](http://arxiv.org/abs/2509.03859)|null|
|**2025-09-03**|**The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation**|Georgia Chalvatzaki Team|[2509.03222](http://arxiv.org/abs/2509.03222)|null|
|**2025-09-03**|**Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback**|Daniel A. Braun Team|[2509.03206](http://arxiv.org/abs/2509.03206)|null|
|**2025-09-03**|**Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built from a Five Bar Linkage**|Matteo Bottin Team|[2509.03119](http://arxiv.org/abs/2509.03119)|null|
|**2025-09-02**|**Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model**|Carol C. Menassa Team|[2509.02876](http://arxiv.org/abs/2509.02876)|null|
|**2025-09-02**|**Power Grid Control with Graph-Based Distributed Reinforcement Learning**|Marcello Restelli Team|[2509.02861](http://arxiv.org/abs/2509.02861)|null|
|**2025-09-04**|**Plan Verification for LLM-Based Embodied Task Completion Agents**|Gokhan Tur Team|[2509.02761](http://arxiv.org/abs/2509.02761)|null|
|**2025-09-02**|**Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots**|Bingyi Kang Team|[2509.02530](http://arxiv.org/abs/2509.02530)|**[link](https://manipulation-as-in-simulation.github.io/)**|
|**2025-09-02**|**U-ARM : Ultra low-cost general teleoperation interface for robot manipulation**|Bo Zhao Team|[2509.02437](http://arxiv.org/abs/2509.02437)|null|
|**2025-09-05**|**Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance**|Xuelong Li Team|[2509.02055](http://arxiv.org/abs/2509.02055)|null|
|**2025-09-01**|**ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training**|Dieter Fox Team|[2509.01819](http://arxiv.org/abs/2509.01819)|null|
|**2025-09-01**|**Non-conflicting Energy Minimization in Reinforcement Learning based Robot Control**|Stefan Lee Team|[2509.01765](http://arxiv.org/abs/2509.01765)|null|
|**2025-09-01**|**Fail2Progress: Learning from Real-World Robot Failures with Stein Variational Inference**|Tucker Hermans Team|[2509.01746](http://arxiv.org/abs/2509.01746)|null|
|**2025-09-01**|**Articulated Object Estimation in the Wild**|Abhinav Valada Team|[2509.01708](http://arxiv.org/abs/2509.01708)|null|
|**2025-09-01**|**Data Retrieval with Importance Weights for Few-Shot Imitation Learning**|Joey Hejna Team|[2509.01657](http://arxiv.org/abs/2509.01657)|null|
|**2025-09-01**|**Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning**|Seongil Hong Team|[2509.01297](http://arxiv.org/abs/2509.01297)|null|
|**2025-08-31**|**One-Step Model Predictive Path Integral for Manipulator Motion Planning Using Configuration Space Distance Fields**|Kenji Kawashima Team|[2509.00836](http://arxiv.org/abs/2509.00836)|null|
|**2025-08-31**|**An Effective Trajectory Planning and an Optimized Path Planning for a 6-Degree-of-Freedom Robot Manipulator**|Masahiko Mikawa Team|[2509.00828](http://arxiv.org/abs/2509.00828)|null|
|**2025-08-31**|**Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using Comprehensive Gröbner Systems**|Masahiko Mikawa Team|[2509.00823](http://arxiv.org/abs/2509.00823)|null|
|**2025-08-30**|**Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot**|Wenbin Li Team|[2509.00574](http://arxiv.org/abs/2509.00574)|null|
|**2025-08-30**|**NeuralSVCD for Efficient Swept Volume Collision Detection**|Beomjoon Kim Team|[2509.00499](http://arxiv.org/abs/2509.00499)|null|
|**2025-08-29**|**Can a mobile robot learn from a pedestrian model to prevent the sidewalk salsa?**|David Abbink Team|[2508.21690](http://arxiv.org/abs/2508.21690)|null|
|**2025-08-29**|**Robust Convex Model Predictive Control with collision avoidance guarantees for robot manipulators**|Thomas B. Schön Team|[2508.21677](http://arxiv.org/abs/2508.21677)|null|
|**2025-08-29**|**Learning Agile Gate Traversal via Analytical Optimal Policy Gradient**|Lin Zhao Team|[2508.21592](http://arxiv.org/abs/2508.21592)|null|
|**2025-08-29**|**Estimated Informed Anytime Search for Sampling-Based Planning via Adaptive Sampler**|Alois Knoll Team|[2508.21549](http://arxiv.org/abs/2508.21549)|null|
|**2025-08-29**|**Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and Acting**|Matthias Scheutz Team|[2508.21501](http://arxiv.org/abs/2508.21501)|null|
|**2025-08-29**|**RoboInspector: Unveiling the Unreliability of Policy Code for LLM-enabled Robotic Manipulation**|Yuanchao Shu Team|[2508.21378](http://arxiv.org/abs/2508.21378)|null|
|**2025-08-29**|**Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload Manipulation**|Alessandro Roncone Team|[2508.21375](http://arxiv.org/abs/2508.21375)|null|
|**2025-08-29**|**Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe ZYZ Regrasp on a Doosan M0609**|Sawoong Kim Team|[2508.21272](http://arxiv.org/abs/2508.21272)|null|
|**2025-08-28**|**Learning on the Fly: Rapid Policy Adaptation via Differentiable Simulation**|Davide Scaramuzza Team|[2508.21065](http://arxiv.org/abs/2508.21065)|null|
|**2025-08-28**|**Rapid Mismatch Estimation via Neural Network Informed Variational Inference**|Nadia Figueroa Team|[2508.21007](http://arxiv.org/abs/2508.21007)|**[link](https://mateusz-jaszczuk.github.io/rme/)**|
|**2025-08-29**|**UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for Enhanced Robotic Perception**|Wenbo Ding Team|[2508.20982](http://arxiv.org/abs/2508.20982)|null|
|**2025-08-28**|**Deep Fuzzy Optimization for Batch-Size and Nearest Neighbors in Optimal Robot Motion Planning**|Alois Knoll Team|[2508.20884](http://arxiv.org/abs/2508.20884)|null|
|**2025-08-28**|**Learning Primitive Embodied World Models: Towards Scalable Robotic Learning**|Qinying Gu Team|[2508.20840](http://arxiv.org/abs/2508.20840)|null|
|**2025-08-28**|**Non-expert to Expert Motion Translation Using Generative Adversarial Networks**|Seiichiro Katsura Team|[2508.20740](http://arxiv.org/abs/2508.20740)|null|
|**2025-08-28**|**SimShear: Sim-to-Real Shear-based Tactile Servoing**|Nathan F. Lepora Team|[2508.20561](http://arxiv.org/abs/2508.20561)|null|
|**2025-08-31**|**HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation**|Huazhe Xu Team|[2508.20085](http://arxiv.org/abs/2508.20085)|null|
|**2025-08-28**|**Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation**|Donglin Wang Team|[2508.19958](http://arxiv.org/abs/2508.19958)|**[link](https://long-vla.github.io)**|
|**2025-08-28**|**Ego-centric Predictive Model Conditioned on Hand Trajectories**|Mike Zheng Shou Team|[2508.19852](http://arxiv.org/abs/2508.19852)|null|
|**2025-08-27**|**APT*: Asymptotically Optimal Motion Planning via Adaptively Prolated Elliptical R-Nearest Neighbors**|Alois Knoll Team|[2508.19790](http://arxiv.org/abs/2508.19790)|null|
|**2025-08-27**|**Impedance Primitive-augmented Hierarchical Reinforcement Learning for Sequential Tasks**|Jens Kober Team|[2508.19607](http://arxiv.org/abs/2508.19607)|null|
|**2025-08-26**|**Gentle Object Retraction in Dense Clutter Using Multimodal Force Sensing and Imitation Learning**|Mark Cutkosky Team|[2508.19476](http://arxiv.org/abs/2508.19476)|null|
|**2025-08-26**|**LaVA-Man: Learning Visual Action Representations for Robot Manipulation**|Changjae Oh Team|[2508.19391](http://arxiv.org/abs/2508.19391)|null|
|**2025-08-26**|**Inference of Human-derived Specifications of Object Placement via Demonstration**|Julie A Shah Team|[2508.19367](http://arxiv.org/abs/2508.19367)|null|
|**2025-08-26**|**MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation**|Gao Huang Team|[2508.19236](http://arxiv.org/abs/2508.19236)|**[link](https://shihao1895.github.io/MemoryVLA)**|
|**2025-08-26**|**LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding**|Felix Heide Team|[2508.19204](http://arxiv.org/abs/2508.19204)|**[link](https://light.princeton.edu/LSD-3D)**|
|**2025-08-27**|**AutoRing: Imitation Learning--based Autonomous Intraocular Foreign Body Removal Manipulation with Eye Surgical Robot**|Jian Wu Team|[2508.19191](http://arxiv.org/abs/2508.19191)|null|
|**2025-08-28**|**From Tabula Rasa to Emergent Abilities: Discovering Robot Skills via Real-World Unsupervised Quality-Diversity**|Antoine Cully Team|[2508.19172](http://arxiv.org/abs/2508.19172)|null|
|**2025-08-26**|**Playstyle and Artificial Intelligence: An Initial Blueprint Through the Lens of Video Games**|Chiu-Chou Lin Team|[2508.19152](http://arxiv.org/abs/2508.19152)|null|
|**2025-08-26**|**AS2FM: Enabling Statistical Model Checking of ROS 2 Systems for Robust Autonomy**|Matteo Morelli Team|[2508.18820](http://arxiv.org/abs/2508.18820)|null|
|**2025-08-26**|**HyperTASR: Hypernetwork-Driven Task-Aware Scene Representations for Robust Manipulation**|Yanchao Yang Team|[2508.18802](http://arxiv.org/abs/2508.18802)|null|
|**2025-08-26**|**Deep Sensorimotor Control by Imitating Predictive Models of Human Motion**|Antonio Loquercio Team|[2508.18691](http://arxiv.org/abs/2508.18691)|**[link](https://hgaurav2k.github.io/trackr/)**|
|**2025-08-26**|**Integration of Robot and Scene Kinematics for Sequential Mobile Manipulation Planning**|Song-Chun Zhu Team|[2508.18627](http://arxiv.org/abs/2508.18627)|null|
|**2025-08-25**|**PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing**|Wenzhen Yuan Team|[2508.18443](http://arxiv.org/abs/2508.18443)|null|
|**2025-08-25**|**Maintenance automation: methods for robotics manipulation planning and execution**|Alexander Verl Team|[2508.18399](http://arxiv.org/abs/2508.18399)|null|
|**2025-08-26**|**FlowVLA: Thinking in Motion with a Visual Chain of Thought**|Haoang Li Team|[2508.18269](http://arxiv.org/abs/2508.18269)|null|
|**2025-08-25**|**No Need to Look! Locating and Grasping Objects by a Robot Arm Covered with Sensitive Skin**|Matej Hoffmann Team|[2508.17986](http://arxiv.org/abs/2508.17986)|null|
|**2025-08-25**|**SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation**|Bharatesh Chakravarthi Team|[2508.17643](http://arxiv.org/abs/2508.17643)|null|
|**2025-08-25**|**GWM: Towards Scalable Gaussian World Models for Robotic Manipulation**|Siyuan Huang Team|[2508.17600](http://arxiv.org/abs/2508.17600)|**[link](https://gaussian-world-model.github.io/)**|
|**2025-08-24**|**LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations**|Hao Su Team|[2508.17547](http://arxiv.org/abs/2508.17547)|null|
|**2025-08-24**|**Variational Shape Inference for Grasp Diffusion on SE(3)**|Aniket Bera Team|[2508.17482](http://arxiv.org/abs/2508.17482)|null|
|**2025-08-24**|**ReviBranch: Deep Reinforcement Learning for Branch-and-Bound with Revived Trajectories**|Jiaping Xiao Team|[2508.17452](http://arxiv.org/abs/2508.17452)|null|
|**2025-08-24**|**Robotic Manipulation via Imitation Learning: Taxonomy, Evolution, Benchmark, and Challenges**|Liming Chen Team|[2508.17449](http://arxiv.org/abs/2508.17449)|null|
|**2025-08-24**|**OVITA: Open-Vocabulary Interpretable Trajectory Adaptations**|Ravi Prakash Team|[2508.17260](http://arxiv.org/abs/2508.17260)|**[link](https://github.com/anurag1000101/OVITA)**|
|**2025-08-24**|**4D Visual Pre-training for Robot Learning**|Huazhe Xu Team|[2508.17230](http://arxiv.org/abs/2508.17230)|null|
|**2025-08-21**|**UnPose: Uncertainty-Guided Diffusion Priors for Zero-Shot Pose Estimation**|Binbin Xu Team|[2508.15972](http://arxiv.org/abs/2508.15972)|**[link](https://frankzhaodong.github.io/UnPose)**|
|**2025-08-21**|**Spatial Policy: Guiding Visuomotor Robotic Manipulation with Spatial-Aware Modeling and Reasoning**|Wenwu Zhu Team|[2508.15874](http://arxiv.org/abs/2508.15874)|null|
|**2025-08-21**|**Search-Based Credit Assignment for Offline Preference-Based Reinforcement Learning**|Houqiang Li Team|[2508.15327](http://arxiv.org/abs/2508.15327)|null|
|**2025-08-20**|**A Vision-Based Shared-Control Teleoperation Scheme for Controlling the Robotic Arm of a Four-Legged Robot**|Marcelo Becker Team|[2508.14994](http://arxiv.org/abs/2508.14994)|null|
|**2025-08-19**|**Learning to Drive Ethically: Embedding Moral Reasoning into Autonomous Driving**|Ostap Okhrin Team|[2508.14926](http://arxiv.org/abs/2508.14926)|null|
|**2025-08-20**|**FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy**|Cewu Lu Team|[2508.14441](http://arxiv.org/abs/2508.14441)|null|
|**2025-08-20**|**Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations**|Na Li Team|[2508.14383](http://arxiv.org/abs/2508.14383)|null|
|**2025-08-20**|**Action-Constrained Imitation Learning**|Ping-Chun Hsieh Team|[2508.14379](http://arxiv.org/abs/2508.14379)|null|
|**2025-08-20**|**Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation**|Ioannis Stamos Team|[2508.14358](http://arxiv.org/abs/2508.14358)|null|
|**2025-08-19**|**Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation**|Hengshuang Zhao Team|[2508.14042](http://arxiv.org/abs/2508.14042)|null|
|**2025-08-19**|**Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation**|Jianye Hao Team|[2508.13998](http://arxiv.org/abs/2508.13998)|null|
|**2025-08-19**|**Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer**|Paul Asunda Team|[2508.13877](http://arxiv.org/abs/2508.13877)|null|
|**2025-08-18**|**Decoding Communications with Partial Information**|Peter McBurney Team|[2508.13326](http://arxiv.org/abs/2508.13326)|null|
|**2025-08-18**|**Precise Action-to-Video Generation Through Visual Action Prompts**|Ruizhen Hu Team|[2508.13104](http://arxiv.org/abs/2508.13104)|**[link](https://zju3dv.github.io/VAP/)**|
|**2025-08-18**|**Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy**|Zhi Hou Team|[2508.13103](http://arxiv.org/abs/2508.13103)|null|
|**2025-08-18**|**Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey**|Liqiang Nie Team|[2508.13073](http://arxiv.org/abs/2508.13073)|**[link](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)**|
|**2025-08-18**|**PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions**|Hamza El-Kebir Team|[2508.12554](http://arxiv.org/abs/2508.12554)|null|
|**2025-08-17**|**EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos**|Hesheng Wang Team|[2508.12349](http://arxiv.org/abs/2508.12349)|null|
|**2025-08-17**|**Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments**|Jihong Zhu Team|[2508.12274](http://arxiv.org/abs/2508.12274)|null|
|**2025-08-17**|**Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids**|Shuran Song Team|[2508.12252](http://arxiv.org/abs/2508.12252)|null|
|**2025-08-16**|**Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing**|Melkior Ornik Team|[2508.12166](http://arxiv.org/abs/2508.12166)|null|
|**2025-08-16**|**OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments**|Richard Camilli Team|[2508.12071](http://arxiv.org/abs/2508.12071)|null|
|**2025-08-16**|**Fully Spiking Actor-Critic Neural Network for Robotic Manipulation**|Guanghui Sun Team|[2508.12038](http://arxiv.org/abs/2508.12038)|null|
|**2025-08-16**|**OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation**|Xiaozhu Ju Team|[2508.11898](http://arxiv.org/abs/2508.11898)|null|
|**2025-08-15**|**Limitation Learning: Catching Adverse Dialog with GAIL**|Rahul Zalkikar Team|[2508.11767](http://arxiv.org/abs/2508.11767)|null|
|**2025-08-15**|**MultiPark: Multimodal Parking Transformer with Next-Segment Prediction**|Tong Qin Team|[2508.11537](http://arxiv.org/abs/2508.11537)|null|
|**2025-08-15**|**Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation**|Fumio Kanehiro Team|[2508.11275](http://arxiv.org/abs/2508.11275)|null|
|**2025-08-15**|**Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation**|Kwok Wai Samuel Au Team|[2508.11204](http://arxiv.org/abs/2508.11204)|null|
|**2025-08-15**|**Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward**|Yu-Gang Jiang Team|[2508.11143](http://arxiv.org/abs/2508.11143)|null|
|**2025-08-14**|**Robot Policy Evaluation for Sim-to-Real Transfer: A Benchmarking Perspective**|Fabio Ramos Team|[2508.11117](http://arxiv.org/abs/2508.11117)|null|
|**2025-08-14**|**GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning**|Ruohan Gao Team|[2508.11049](http://arxiv.org/abs/2508.11049)|null|
|**2025-08-14**|**3D FlowMatch Actor: Unified 3D Policy for Single- and Dual-Arm Manipulation**|Katerina Fragkiadaki Team|[2508.11002](http://arxiv.org/abs/2508.11002)|null|
|**2025-08-15**|**KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection**|Lorenzo Natale Team|[2508.10511](http://arxiv.org/abs/2508.10511)|null|
|**2025-08-14**|**Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning**|Ping Kuang Team|[2508.10399](http://arxiv.org/abs/2508.10399)|null|
|**2025-08-14**|**Leveraging OS-Level Primitives for Robotic Action Management**|Haibo Chen Team|[2508.10259](http://arxiv.org/abs/2508.10259)|null|
|**2025-08-13**|**Masquerade: Learning from In-the-wild Human Videos using Data-Editing**|Jeannette Bohg Team|[2508.09976](http://arxiv.org/abs/2508.09976)|**[link](https://masquerade-robot.github.io/)**|
|**2025-08-13**|**Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes**|Changjae Oh Team|[2508.09855](http://arxiv.org/abs/2508.09855)|null|
|**2025-08-13**|**Physical Autoregressive Model for Robotic Manipulation without Action Pretraining**|Guangrun Wang Team|[2508.09822](http://arxiv.org/abs/2508.09822)|null|
|**2025-08-13**|**Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions**|Jouni Mattila Team|[2508.09700](http://arxiv.org/abs/2508.09700)|null|
|**2025-08-13**|**CaRoBio: 3D Cable Routing with a Bio-inspired Gripper Fingernail**|Fumin Zhang Team|[2508.09558](http://arxiv.org/abs/2508.09558)|null|
|**2025-08-13**|**Reactive Model Predictive Contouring Control for Robot Manipulators**|Jaeheung Park Team|[2508.09502](http://arxiv.org/abs/2508.09502)|null|
|**2025-08-13**|**DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation**|Liqiang Nie Team|[2508.09444](http://arxiv.org/abs/2508.09444)|null|
|**2025-08-13**|**GeoVLA: Empowering 3D Representations in Vision-Language-Action Models**|Jiale Cao Team|[2508.09071](http://arxiv.org/abs/2508.09071)|**[link](https://linsun449.github.io/GeoVLA/)**|
|**2025-08-12**|**Unsupervised Skill Discovery as Exploration for Learning Agile Locomotion**|Sehoon Ha Team|[2508.08982](http://arxiv.org/abs/2508.08982)|null|
|**2025-08-12**|**Reducing Cognitive Load in Multi-Agent Reinforcement Learning for Mathematical Problem Solving: Decoupling Reasoning and Code Generation**|Yang Li Team|[2508.08882](http://arxiv.org/abs/2508.08882)|null|
|**2025-08-12**|**Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT**|Yukiyasu Domae Team|[2508.08748](http://arxiv.org/abs/2508.08748)|null|
|**2025-08-12**|**Towards Safe Imitation Learning via Potential Field-Guided Flow Matching**|Yoshihiko Nakamura Team|[2508.08707](http://arxiv.org/abs/2508.08707)|null|
|**2025-08-12**|**OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing**|Hengdi Zhang Team|[2508.08706](http://arxiv.org/abs/2508.08706)|null|
|**2025-08-11**|**ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction**|Wenjun Mei Team|[2508.08170](http://arxiv.org/abs/2508.08170)|null|
|**2025-08-11**|**AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies**|Joyce Chai Team|[2508.08113](http://arxiv.org/abs/2508.08113)|null|
|**2025-08-13**|**AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation**|Lei Han Team|[2508.07770](http://arxiv.org/abs/2508.07770)|null|
|**2025-08-11**|**GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions**|Hong Zhang Team|[2508.07650](http://arxiv.org/abs/2508.07650)|null|
|**2025-08-11**|**AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning**|Yang Liu Team|[2508.07626](http://arxiv.org/abs/2508.07626)|null|
|**2025-08-10**|**Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)**|Manoranjan Sinha Team|[2508.07323](http://arxiv.org/abs/2508.07323)|null|
|**2025-08-10**|**Multimodal Spiking Neural Network for Space Robotic Manipulation**|Guanghui Sun Team|[2508.07287](http://arxiv.org/abs/2508.07287)|null|
|**2025-08-09**|**DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit**|Monroe Kennedy III Team|[2508.07118](http://arxiv.org/abs/2508.07118)|null|
|**2025-08-09**|**From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving**|Antonio Guillen-Perez Team|[2508.07029](http://arxiv.org/abs/2508.07029)|null|
|**2025-08-09**|**Manipulator for people with limited abilities**|Arkady Yuschenko Team|[2508.06969](http://arxiv.org/abs/2508.06969)|null|
|**2025-08-09**|**Learning a Vision-Based Footstep Planner for Hierarchical Walking Control**|Michael Posa Team|[2508.06779](http://arxiv.org/abs/2508.06779)|null|
|**2025-08-08**|**Towards Balanced Behavior Cloning from Imbalanced Datasets**|Dylan P. Losey Team|[2508.06319](http://arxiv.org/abs/2508.06319)|null|
|**2025-08-08**|**Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators**|Jouni Mattila Team|[2508.06313](http://arxiv.org/abs/2508.06313)|null|
|**2025-08-08**|**ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints**|Liming Chen Team|[2508.06266](http://arxiv.org/abs/2508.06266)|null|
|**2025-08-08**|**Incremental Language Understanding for Online Motion Planning of Robot Manipulators**|Matthias Scheutz Team|[2508.06095](http://arxiv.org/abs/2508.06095)|null|
|**2025-08-08**|**Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning**|Jonghyun Choi Team|[2508.06042](http://arxiv.org/abs/2508.06042)|null|
|**2025-08-08**|**PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation**|Yao Mu Team|[2508.05976](http://arxiv.org/abs/2508.05976)|null|
|**2025-08-07**|**Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation**|Guanghui Ren Team|[2508.05635](http://arxiv.org/abs/2508.05635)|**[link](https://genie-envisioner.github.io/)**|
|**2025-08-07**|**Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling**|Jiachen Li Team|[2508.05634](http://arxiv.org/abs/2508.05634)|**[link](https://gen-safe-nav.github.io/.)**|
|**2025-08-07**|**Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator**|Nga Nguyen Thi Team|[2508.05584](http://arxiv.org/abs/2508.05584)|null|
|**2025-08-07**|**Do Robots Really Need Anthropomorphic Hands?**|Nicolás Navarro-Guerrero Team|[2508.05415](http://arxiv.org/abs/2508.05415)|null|
|**2025-08-07**|**Real-Time Iteration Scheme for Diffusion Policy**|Danica Kragic Team|[2508.05396](http://arxiv.org/abs/2508.05396)|null|
|**2025-08-07**|**ASkDAgger: Active Skill-level Data Aggregation for Interactive Imitation Learning**|Jens Kober Team|[2508.05310](http://arxiv.org/abs/2508.05310)|null|
|**2025-08-07**|**Learning to See and Act: Task-Aware View Planning for Robotic Manipulation**|Liang Lin Team|[2508.05186](http://arxiv.org/abs/2508.05186)|**[link](https://hcplab-sysu.github.io/TAVP)**|
|**2025-08-07**|**Cognitive Duality for Adaptive Web Agents**|Zheng Hu Team|[2508.05081](http://arxiv.org/abs/2508.05081)|null|
|**2025-08-07**|**Analyzing the Impact of Multimodal Perception on Sample Complexity and Optimization Landscapes in Imitation Learning**|Temitope Lukman Adebanjo Team|[2508.05077](http://arxiv.org/abs/2508.05077)|null|
|**2025-08-06**|**INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM**|Nikos Tsagarakis Team|[2508.04931](http://arxiv.org/abs/2508.04931)|**[link](https://robo-intention.github.io)**|
|**2025-08-06**|**Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)**|Le Tieu Nien Team|[2508.04009](http://arxiv.org/abs/2508.04009)|null|
|**2025-08-05**|**Constraint-Preserving Data Generation for Visuomotor Policy Learning**|Jeannette Bohg Team|[2508.03944](http://arxiv.org/abs/2508.03944)|**[link](https://cp-gen.github.io)**|
|**2025-08-05**|**DiWA: Diffusion Policy Adaptation with World Models**|Abhinav Valada Team|[2508.03645](http://arxiv.org/abs/2508.03645)|null|
|**2025-08-05**|**ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow**|Xiaodan Liang Team|[2508.03218](http://arxiv.org/abs/2508.03218)|null|
|**2025-08-05**|**Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection**|Somil Bansal Team|[2508.03129](http://arxiv.org/abs/2508.03129)|null|
|**2025-08-07**|**Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching**|C. Karen Liu Team|[2508.03068](http://arxiv.org/abs/2508.03068)|null|
|**2025-08-05**|**Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control**|YuFeng Chen Team|[2508.03043](http://arxiv.org/abs/2508.03043)|null|
|**2025-08-04**|**Learning User Interaction Forces using Vision for a Soft Finger Exosuit**|Thomas George Thuruthel Team|[2508.02870](http://arxiv.org/abs/2508.02870)|null|
|**2025-08-04**|**Manip4Care: Robotic Manipulation of Human Limbs for Solving Assistive Tasks**|Ahmed H. Qureshi Team|[2508.02649](http://arxiv.org/abs/2508.02649)|null|
|**2025-08-04**|**D2PPO: Diffusion Policy Policy Optimization with Dispersive Loss**|Haitao Wang Team|[2508.02644](http://arxiv.org/abs/2508.02644)|null|
|**2025-08-01**|**On-Device Diffusion Transformer Policy for Efficient Robot Manipulation**|Dong Xu Team|[2508.00697](http://arxiv.org/abs/2508.00697)|null|
|**2025-08-01**|**HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning**|Lorenzo Natale Team|[2508.00491](http://arxiv.org/abs/2508.00491)|null|
|**2025-08-01**|**Energy Efficient Trajectory Control and Resource Allocation in Multi-UAV-assisted MEC via Deep Reinforcement Learning**|Dusit Niyato Team|[2508.00261](http://arxiv.org/abs/2508.00261)|null|
|**2025-07-31**|**RAGNet: Large-scale Reasoning-based Affordance Segmentation Benchmark towards General Grasping**|Jianbing Shen Team|[2507.23734](http://arxiv.org/abs/2507.23734)|**[link](https://github.com/wudongming97/AffordanceNet)**|
|**2025-07-31**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Jiang Bian Team|[2507.23682](http://arxiv.org/abs/2507.23682)|**[link](https://aka.ms/villa-x)**|
|**2025-08-01**|**H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation**|Jun Zhu Team|[2507.23523](http://arxiv.org/abs/2507.23523)|null|
|**2025-07-31**|**Policy Learning from Large Vision-Language Model Feedback without Reward Modeling**|Chang D. Yoo Team|[2507.23391](http://arxiv.org/abs/2507.23391)|null|
|**2025-07-30**|**In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion**|Peng Lu Team|[2507.23053](http://arxiv.org/abs/2507.23053)|null|
|**2025-07-30**|**Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations**|Brendan Tidd Team|[2507.22380](http://arxiv.org/abs/2507.22380)|null|
|**2025-07-29**|**RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation**|Pengcheng He Team|[2507.22219](http://arxiv.org/abs/2507.22219)|null|
|**2025-07-29**|**A Nonlinear MPC Framework for Loco-Manipulation of Quadrupedal Robots with Non-Negligible Manipulator Dynamics**|Kaveh Akbari Hamed Team|[2507.22042](http://arxiv.org/abs/2507.22042)|null|
|**2025-07-29**|**From Seeing to Experiencing: Scaling Navigation Foundation Models with Reinforcement Learning**|Bolei Zhou Team|[2507.22028](http://arxiv.org/abs/2507.22028)|null|
|**2025-07-29**|**DISCOVERSE: Efficient Robot Simulation in Complex High-Fidelity Environments**|Guyue Zhou Team|[2507.21981](http://arxiv.org/abs/2507.21981)|null|
|**2025-07-29**|**MoDeSuite: Robot Learning Task Suite for Benchmarking Mobile Manipulation with Deformable Objects**|Joni Pajarinen Team|[2507.21796](http://arxiv.org/abs/2507.21796)|null|
|**2025-07-29**|**Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning**|Panpan Cai Team|[2507.21545](http://arxiv.org/abs/2507.21545)|null|
|**2025-07-29**|**Model Predictive Adversarial Imitation Learning for Planning from Observation**|Byron Boots Team|[2507.21533](http://arxiv.org/abs/2507.21533)|null|
|**2025-07-29**|**Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training**|Yutaka Matsuo Team|[2507.21452](http://arxiv.org/abs/2507.21452)|null|
|**2025-07-28**|**Fluidically Innervated Lattices Make Versatile and Durable Tactile Sensors**|Daniela Rus Team|[2507.21225](http://arxiv.org/abs/2507.21225)|null|
|**2025-07-28**|**FMimic: Foundation Models are Fine-grained Action Learners from Human Videos**|Yufeng Yue Team|[2507.20622](http://arxiv.org/abs/2507.20622)|null|
|**2025-07-28**|**Learning Physical Interaction Skills from Human Demonstrations**|Kwonjoon Lee Team|[2507.20445](http://arxiv.org/abs/2507.20445)|null|
|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Hesheng Wang Team|[2507.17462](http://arxiv.org/abs/2507.17462)|null|
|**2025-07-23**|**Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning**|Byeongjoon Noh Team|[2507.17418](http://arxiv.org/abs/2507.17418)|null|
|**2025-07-23**|**Confounded Causal Imitation Learning with Instrumental Variables**|Zhi Geng Team|[2507.17309](http://arxiv.org/abs/2507.17309)|null|
|**2025-07-23**|**Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning**|Takamitsu Matsubara Team|[2507.17275](http://arxiv.org/abs/2507.17275)|null|
|**2025-07-23**|**Towards Human-level Intelligence via Human-like Whole-Body Manipulation**|Zhaohui An Team|[2507.17141](http://arxiv.org/abs/2507.17141)|null|
|**2025-07-22**|**Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots**|Aitor Arrieta Team|[2507.17049](http://arxiv.org/abs/2507.17049)|null|
|**2025-07-19**|**Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning**|Charlie C. L. Wang Team|[2507.16842](http://arxiv.org/abs/2507.16842)|null|
|**2025-07-22**|**ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning**|Fu-En Yang Team|[2507.16815](http://arxiv.org/abs/2507.16815)|null|
|**2025-07-22**|**Equivariant Goal Conditioned Contrastive Reinforcement Learning**|Robert Platt Team|[2507.16139](http://arxiv.org/abs/2507.16139)|null|
|**2025-07-21**|**Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers**|Iman Soltani Team|[2507.15833](http://arxiv.org/abs/2507.15833)|null|
|**2025-07-21**|**Strong, Accurate, and Low-Cost Robot Manipulator**|Donghyun Kim Team|[2507.15693](http://arxiv.org/abs/2507.15693)|null|
|**2025-07-21**|**Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos**|Zongqing Lu Team|[2507.15597](http://arxiv.org/abs/2507.15597)|null|
|**2025-07-22**|**GR-3 Technical Report**|Yichu Yang Team|[2507.15493](http://arxiv.org/abs/2507.15493)|null|
|**2025-07-20**|**Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions**|Eric Diller Team|[2507.15155](http://arxiv.org/abs/2507.15155)|null|
|**2025-07-20**|**Reinforcement Learning for Flow-Matching Policies**|Somayeh Sojoudi Team|[2507.15073](http://arxiv.org/abs/2507.15073)|null|
|**2025-07-20**|**Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper**|Yunzhu Li Team|[2507.15062](http://arxiv.org/abs/2507.15062)|null|
|**2025-07-20**|**LLM-Enhanced Multi-Agent Reinforcement Learning with Expert Workflow for Real-Time P2P Energy Trading**|Lu Zhang Team|[2507.14995](http://arxiv.org/abs/2507.14995)|null|
|**2025-07-20**|**Heterogeneous object manipulation on nonlinear soft surface through linear controller**|Andres Faiña Team|[2507.14967](http://arxiv.org/abs/2507.14967)|null|
|**2025-07-20**|**KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning**|Guangyao Zhai Team|[2507.14820](http://arxiv.org/abs/2507.14820)|null|
|**2025-07-19**|**BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives**|Yongchun Fang Team|[2507.14582](http://arxiv.org/abs/2507.14582)|null|
|**2025-07-18**|**Improving Low-Cost Teleoperation: Augmenting GELLO with Force**|Kai Arulkumaran Team|[2507.13602](http://arxiv.org/abs/2507.13602)|null|
|**2025-07-17**|**The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner**|Kai Chen Team|[2507.13332](http://arxiv.org/abs/2507.13332)|null|
|**2025-07-17**|**ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning**|Johannes A. Stork Team|[2507.13088](http://arxiv.org/abs/2507.13088)|null|
|**2025-07-17**|**Generalist Bimanual Manipulation via Foundation Video Diffusion Models**|Jun Zhu Team|[2507.12898](http://arxiv.org/abs/2507.12898)|null|
|**2025-07-17**|**Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)**|Jost Tobias Springenberg Team|[2507.12856](http://arxiv.org/abs/2507.12856)|null|
|**2025-07-17**|**DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning**|Melanie N. Zeilinger Team|[2507.12855](http://arxiv.org/abs/2507.12855)|null|
|**2025-07-17**|**Learning to Predict Mobile Robot Stability in Off-Road Environments**|Parikshit Maini Team|[2507.12731](http://arxiv.org/abs/2507.12731)|null|
|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Xiaolong Wang Team|[2507.12440](http://arxiv.org/abs/2507.12440)|null|
|**2025-07-16**|**The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey**|Jiming Chen Team|[2507.11840](http://arxiv.org/abs/2507.11840)|null|
|**2025-07-15**|**Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification**|Zsolt Kira Team|[2507.11662](http://arxiv.org/abs/2507.11662)|null|
|**2025-07-15**|**MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments**|Steven Liu Team|[2507.11211](http://arxiv.org/abs/2507.11211)|null|
|**2025-07-15**|**A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty**|Ruggero Carli Team|[2507.11170](http://arxiv.org/abs/2507.11170)|null|
|**2025-07-15**|**Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments**|Kazuya Yoshida Team|[2507.11006](http://arxiv.org/abs/2507.11006)|null|
|**2025-07-15**|**Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning**|Jun Morimoto Team|[2507.10899](http://arxiv.org/abs/2507.10899)|null|
|**2025-07-14**|**Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection**|Colin Bellinger Team|[2507.10814](http://arxiv.org/abs/2507.10814)|null|
|**2025-07-14**|**rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding**|Kaiyu Hang Team|[2507.10776](http://arxiv.org/abs/2507.10776)|null|
|**2025-07-14**|**A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers**|Arko Barman Team|[2507.10775](http://arxiv.org/abs/2507.10775)|null|
|**2025-07-14**|**Vision Language Action Models in Robotic Manipulation: A Systematic Review**|Irfan Hussain Team|[2507.10672](http://arxiv.org/abs/2507.10672)|null|
|**2025-07-16**|**GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning**|Dandan Tu Team|[2507.10628](http://arxiv.org/abs/2507.10628)|null|
|**2025-07-14**|**MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation**|Mengyuan Liu Team|[2507.10543](http://arxiv.org/abs/2507.10543)|null|
|**2025-07-14**|**Prompt Informed Reinforcement Learning for Visual Coverage Path Planning**|Venkat Margapuri Team|[2507.10284](http://arxiv.org/abs/2507.10284)|null|
|**2025-07-14**|**Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?**|Keith Ross Team|[2507.10174](http://arxiv.org/abs/2507.10174)|null|
|**2025-07-16**|**MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping**|Monowar Bhuyan Team|[2507.10158](http://arxiv.org/abs/2507.10158)|null|
|**2025-07-13**|**Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling**|Ali Al-Zawqari Team|[2507.09540](http://arxiv.org/abs/2507.09540)|null|
|**2025-07-13**|**Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles**|Keqiang Li Team|[2507.09537](http://arxiv.org/abs/2507.09537)|null|
|**2025-07-13**|**SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation**|Boyu Wang Team|[2507.09459](http://arxiv.org/abs/2507.09459)|null|
|**2025-07-12**|**DAA*: Deep Angular A Star for Image-based Path Planning**|Zhiwei Xu Team|[2507.09305](http://arxiv.org/abs/2507.09305)|null|
|**2025-07-15**|**Learning and Transferring Better with Depth Information in Visual Reinforcement Learning**|Jingdong Zhao Team|[2507.09180](http://arxiv.org/abs/2507.09180)|null|
|**2025-07-12**|**PRAG: Procedural Action Generator**|Karla Stepanova Team|[2507.09167](http://arxiv.org/abs/2507.09167)|null|
|**2025-07-12**|**Towards Human-level Dexterity via Robot Learning**|Gagan Khandate Team|[2507.09117](http://arxiv.org/abs/2507.09117)|null|
|**2025-07-11**|**Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction**|Max Simchowitz Team|[2507.09061](http://arxiv.org/abs/2507.09061)|null|
|**2025-07-11**|**Behavioral Exploration: Learning to Explore via In-Context Adaptation**|Sergey Levine Team|[2507.09041](http://arxiv.org/abs/2507.09041)|null|
|**2025-07-11**|**Learning human-to-robot handovers through 3D scene reconstruction**|Changjae Oh Team|[2507.08726](http://arxiv.org/abs/2507.08726)|null|
|**2025-07-11**|**Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots**|Yue Gao Team|[2507.08303](http://arxiv.org/abs/2507.08303)|null|
|**2025-07-11**|**CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations**|He Wang Team|[2507.08262](http://arxiv.org/abs/2507.08262)|null|
|**2025-07-10**|**Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion**|Raafat E. Shalaby Team|[2507.08112](http://arxiv.org/abs/2507.08112)|null|
|**2025-07-15**|**EXPO: Stable Reinforcement Learning with Expressive Policies**|Chelsea Finn Team|[2507.07986](http://arxiv.org/abs/2507.07986)|null|
|**2025-07-15**|**Reinforcement Learning with Action Chunking**|Sergey Levine Team|[2507.07969](http://arxiv.org/abs/2507.07969)|null|
|**2025-07-09**|**Self-Wearing Adaptive Garments via Soft Robotic Unfurling**|Allison M. Okamura Team|[2507.07221](http://arxiv.org/abs/2507.07221)|null|
|**2025-07-09**|**Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand**|Xinjun Sheng Team|[2507.06822](http://arxiv.org/abs/2507.06822)|null|
|**2025-07-09**|**Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm**|George A. Vouros Team|[2507.06780](http://arxiv.org/abs/2507.06780)|null|
|**2025-07-13**|**Spatial-Temporal Aware Visuomotor Diffusion Policy Learning**|Yanwei Fu Team|[2507.06710](http://arxiv.org/abs/2507.06710)|null|
|**2025-07-09**|**Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement**|Martin Riedmiller Team|[2507.06701](http://arxiv.org/abs/2507.06701)|null|
|**2025-07-09**|**Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning**|Jian Cheng Team|[2507.06628](http://arxiv.org/abs/2507.06628)|null|
|**2025-07-09**|**Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic**|Fabio Ramos Team|[2507.06625](http://arxiv.org/abs/2507.06625)|null|
|**2025-07-09**|**Token Bottleneck: One Token to Remember Dynamics**|Sangdoo Yun Team|[2507.06543](http://arxiv.org/abs/2507.06543)|null|
|**2025-07-08**|**Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction**|Alessio Del Bue Team|[2507.06404](http://arxiv.org/abs/2507.06404)|null|
|**2025-07-08**|**EC-Flow: Enabling Versatile Robotic Manipulation from Action-Unlabeled Videos via Embodiment-Centric Flow**|Liang Wang Team|[2507.06224](http://arxiv.org/abs/2507.06224)|null|
|**2025-07-08**|**Is Diversity All You Need for Scalable Robotic Manipulation?**|Hongyang Li Team|[2507.06219](http://arxiv.org/abs/2507.06219)|null|
|**2025-07-08**|**Fast Bilateral Teleoperation and Imitation Learning Using Sensorless Force Control via Accurate Dynamics Model**|Toshiaki Tsuji Team|[2507.06174](http://arxiv.org/abs/2507.06174)|null|
|**2025-07-08**|**Learning Agile Tensile Perching for Aerial Robots from Demonstrations**|Basaran Bahadir Kocer Team|[2507.06172](http://arxiv.org/abs/2507.06172)|null|
|**2025-07-08**|**SCCRUB: Surface Cleaning Compliant Robot Utilizing Bristles**|Jeffrey Ian Lipton Team|[2507.06053](http://arxiv.org/abs/2507.06053)|null|
|**2025-07-08**|**LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving**|Jian Sun Team|[2507.05754](http://arxiv.org/abs/2507.05754)|null|
|**2025-07-08**|**Hybrid Diffusion Policies with Projective Geometric Algebra for Efficient Robot Manipulation Learning**|Daniel Rakita Team|[2507.05695](http://arxiv.org/abs/2507.05695)|null|
|**2025-07-08**|**Integrating Diffusion-based Multi-task Learning with Online Reinforcement Learning for Robust Quadruped Robot Control**|Bin Liang Team|[2507.05674](http://arxiv.org/abs/2507.05674)|null|
|**2025-07-08**|**Stable Tracking-in-the-Loop Control of Cable-Driven Surgical Manipulators under Erroneous Kinematic Chains**|Michael C. Yip Team|[2507.05663](http://arxiv.org/abs/2507.05663)|null|
|**2025-07-08**|**DreamGrasp: Zero-Shot 3D Multi-Object Reconstruction from Partial-View Images for Robotic Manipulation**|Frank Chongwoo Park Team|[2507.05627](http://arxiv.org/abs/2507.05627)|null|
|**2025-07-07**|**Gaussian Process-Based Active Exploration Strategies in Vision and Touch**|Nadia Figueroa Team|[2507.05522](http://arxiv.org/abs/2507.05522)|null|
|**2025-07-07**|**A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation**|Russ Tedrake Team|[2507.05331](http://arxiv.org/abs/2507.05331)|null|
|**2025-07-07**|**VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting**|Yanzhi Wang Team|[2507.05116](http://arxiv.org/abs/2507.05116)|null|
|**2025-07-07**|**When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning**|Sebastien Ourselin Team|[2507.05011](http://arxiv.org/abs/2507.05011)|null|
|**2025-07-07**|**Training-free Generation of Temporally Consistent Rewards from VLMs**|Jian Tang Team|[2507.04789](http://arxiv.org/abs/2507.04789)|null|
|**2025-07-07**|**DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics**|Mingsheng Shang Team|[2507.04661](http://arxiv.org/abs/2507.04661)|null|
|**2025-07-07**|**PRISM: Pointcloud Reintegrated Inference via Segmentation and Cross-attention for Manipulation**|Chee-Meng Chew Team|[2507.04633](http://arxiv.org/abs/2507.04633)|null|
|**2025-07-07**|**Learning Robust Stereo Matching in the Wild with Selective Mixture-of-Experts**|Junjie Hu Team|[2507.04631](http://arxiv.org/abs/2507.04631)|null|
|**2025-07-06**|**VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust Long-Horizon Manipulation**|Lei Han Team|[2507.04524](http://arxiv.org/abs/2507.04524)|null|
|**2025-07-06**|**DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge**|Xin Jin Team|[2507.04447](http://arxiv.org/abs/2507.04447)|null|
|**2025-07-06**|**Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks**|Yi Fang Team|[2507.04331](http://arxiv.org/abs/2507.04331)|null|
|**2025-07-05**|**Are Learning-Based Approaches Ready for Real-World Indoor Navigation? A Case for Imitation Learning**|Sebastian Houben Team|[2507.04086](http://arxiv.org/abs/2507.04086)|null|
|**2025-07-05**|**Breaking Imitation Bottlenecks: Reinforced Diffusion Powers Diverse Trajectory Generation**|Yadan Luo Team|[2507.04049](http://arxiv.org/abs/2507.04049)|null|
|**2025-07-08**|**RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot**|Hao Dong Team|[2507.03930](http://arxiv.org/abs/2507.03930)|null|
|**2025-07-05**|**DK-RRT: Deep Koopman RRT for Collision-Aware Motion Planning of Space Manipulators in Dynamic Debris Environments**|Dezhi Yu Team|[2507.03878](http://arxiv.org/abs/2507.03878)|null|
|**2025-07-04**|**Dexterous Teleoperation of 20-DoF ByteDexter Hand via Human Motion Retargeting**|Zeyu Ren Team|[2507.03227](http://arxiv.org/abs/2507.03227)|null|
|**2025-07-02**|**cVLA: Towards Efficient Camera-Space VLAs**|Thomas Brox Team|[2507.02190](http://arxiv.org/abs/2507.02190)|null|
|**2025-07-02**|**Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN**|Matthias Kerzel Team|[2507.02171](http://arxiv.org/abs/2507.02171)|null|
|**2025-07-02**|**TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types**|Wei-Shi Zheng Team|[2507.01857](http://arxiv.org/abs/2507.01857)|null|
|**2025-07-02**|**S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures**|Farshid Alambeigi Team|[2507.01779](http://arxiv.org/abs/2507.01779)|null|
|**2025-07-03**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Yanwei Fu Team|[2507.01424](http://arxiv.org/abs/2507.01424)|null|
|**2025-07-01**|**Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives**|Bakir Lacevic Team|[2507.01198](http://arxiv.org/abs/2507.01198)|null|
|**2025-07-01**|**Imitation Learning for Satellite Attitude Control under Unknown Perturbations**|Xiaoli Bai Team|[2507.01161](http://arxiv.org/abs/2507.01161)|null|
|**2025-07-01**|**SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound**|Philipp Fürnstahl Team|[2507.01152](http://arxiv.org/abs/2507.01152)|null|
|**2025-07-01**|**Geometry-aware 4D Video Generation for Robot Manipulation**|Shuran Song Team|[2507.01099](http://arxiv.org/abs/2507.01099)|null|
|**2025-07-01**|**DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation**|Pulkit Agrawal Team|[2507.01008](http://arxiv.org/abs/2507.01008)|null|
|**2025-07-04**|**Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations**|Yunzhu Li Team|[2507.00990](http://arxiv.org/abs/2507.00990)|null|
|**2025-07-01**|**HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning**|Chenjia Bai Team|[2507.00833](http://arxiv.org/abs/2507.00833)|null|
|**2025-07-01**|**Learning Steerable Imitation Controllers from Unstructured Animal Motions**|Stelian Coros Team|[2507.00677](http://arxiv.org/abs/2507.00677)|null|
|**2025-07-01**|**RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation**|Siddhartha Srinivasa Team|[2507.00435](http://arxiv.org/abs/2507.00435)|null|
|**2025-07-01**|**Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning**|Yang Gao Team|[2506.23944](http://arxiv.org/abs/2506.23944)|null|
|**2025-06-30**|**World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation**|Lin Shao Team|[2506.23919](http://arxiv.org/abs/2506.23919)|null|
|**2025-06-30**|**Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning**|Alexey Skrynnik Team|[2506.23793](http://arxiv.org/abs/2506.23793)|null|
|**2025-06-30**|**PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?**|Ransalu Senanayake Team|[2506.23725](http://arxiv.org/abs/2506.23725)|null|
|**2025-07-04**|**ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation**|Mac Schwager Team|[2506.23126](http://arxiv.org/abs/2506.23126)|null|
|**2025-06-29**|**Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots**|Yue Gao Team|[2506.23125](http://arxiv.org/abs/2506.23125)|null|
|**2025-06-28**|**Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation**|Navid Azizan Team|[2506.22827](http://arxiv.org/abs/2506.22827)|null|
|**2025-06-28**|**SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information**|Yuqiang Wu Team|[2506.22788](http://arxiv.org/abs/2506.22788)|null|
|**2025-06-28**|**Learning Efficient Robotic Garment Manipulation with Standardization**|Bin He Team|[2506.22769](http://arxiv.org/abs/2506.22769)|null|
|**2025-06-28**|**RoboPearls: Editable Video Simulation for Robot Manipulation**|Xiaodan Liang Team|[2506.22756](http://arxiv.org/abs/2506.22756)|null|
|**2025-06-27**|**Spherical Pendulum with Quad-Rotor Thrust Vectoring Actuation -- A Novel Mechatronics and Control Benchmark Platform**|Tsu-Chin Tsao Team|[2506.22410](http://arxiv.org/abs/2506.22410)|null|
|**2025-06-27**|**RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation**|Abhinav Valada Team|[2506.22007](http://arxiv.org/abs/2506.22007)|null|
|**2025-06-26**|**Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation**|Venkat Krovi Team|[2506.21732](http://arxiv.org/abs/2506.21732)|null|
|**2025-06-24**|**Ark: An Open-source Python-based Framework for Robot Learning**|Haitham Bou-Ammar Team|[2506.21628](http://arxiv.org/abs/2506.21628)|null|
|**2025-06-24**|**FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models**|Huiping Zhuang Team|[2506.21627](http://arxiv.org/abs/2506.21627)|null|
|**2025-06-26**|**ACTLLM: Action Consistency Tuned Large Language Model**|Chenliang Xu Team|[2506.21250](http://arxiv.org/abs/2506.21250)|null|
|**2025-07-02**|**World-aware Planning Narratives Enhance Large Vision-Language Model Planner**|Xipeng Qiu Team|[2506.21230](http://arxiv.org/abs/2506.21230)|null|
|**2025-06-26**|**UAIbot: Beginner-friendly web-based simulator for interactive robotics learning and research**|Vinicius Mariano Gonçalves Team|[2506.21178](http://arxiv.org/abs/2506.21178)|null|
|**2025-06-26**|**Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions**|Cewu Lu Team|[2506.21057](http://arxiv.org/abs/2506.21057)|null|
|**2025-06-26**|**Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends**|Zeng-Guang Hou Team|[2506.20966](http://arxiv.org/abs/2506.20966)|null|
|**2025-06-25**|**Learning-Based Distance Estimation for 360° Single-Sensor Setups**|Andreas Zell Team|[2506.20586](http://arxiv.org/abs/2506.20586)|null|
|**2025-06-25**|**Learn to Position -- A Novel Meta Method for Robotic Positioning**|Xiaoming Tao Team|[2506.20445](http://arxiv.org/abs/2506.20445)|null|
|**2025-06-25**|**Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration**|Quanquan Gu Team|[2506.20307](http://arxiv.org/abs/2506.20307)|null|
|**2025-06-24**|**Unified Vision-Language-Action Model**|Zhaoxiang Zhang Team|[2506.19850](http://arxiv.org/abs/2506.19850)|null|
|**2025-06-24**|**T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models**|Qingyao Wu Team|[2506.19498](http://arxiv.org/abs/2506.19498)|null|
|**2025-06-24**|**Is an object-centric representation beneficial for robotic manipulation ?**|Liming Chen Team|[2506.19408](http://arxiv.org/abs/2506.19408)|null|
|**2025-06-24**|**Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference**|Nutan Chen Team|[2506.19303](http://arxiv.org/abs/2506.19303)|null|
|**2025-06-25**|**AnchorDP3: 3D Affordance Guided Sparse Diffusion Policy for Robotic Manipulation**|Hui Shen Team|[2506.19269](http://arxiv.org/abs/2506.19269)|null|
|**2025-06-24**|**Robust Behavior Cloning Via Global Lipschitz Regularization**|Sean B. Andersson Team|[2506.19250](http://arxiv.org/abs/2506.19250)|null|
|**2025-06-23**|**CUPID: Curating Data your Robot Loves with Influence Functions**|Jeannette Bohg Team|[2506.19121](http://arxiv.org/abs/2506.19121)|null|
|**2025-06-23**|**Multimodal Anomaly Detection with a Mixture-of-Experts**|Dongheui Lee Team|[2506.19077](http://arxiv.org/abs/2506.19077)|null|
|**2025-06-25**|**FORTE: Tactile Force and Slip Sensing on Compliant Fingers for Delicate Manipulation**|Lillian Chin Team|[2506.18960](http://arxiv.org/abs/2506.18960)|null|
|**2025-06-23**|**RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base**|Xiangyang Xue Team|[2506.18856](http://arxiv.org/abs/2506.18856)|null|
|**2025-06-23**|**SViP: Sequencing Bimanual Visuomotor Policies with Object-Centric Motion Primitives**|Jia Pan Team|[2506.18825](http://arxiv.org/abs/2506.18825)|null|
|**2025-06-23**|**Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry**|Jan Steinbrener Team|[2506.18580](http://arxiv.org/abs/2506.18580)|null|
|**2025-06-23**|**Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots**|Alessandro Di Nuovo Team|[2506.18365](http://arxiv.org/abs/2506.18365)|null|
|**2025-06-23**|**Robotic Manipulation of a Rotating Chain with Bottom End Fixed**|Quang-Cuong Pham Team|[2506.18355](http://arxiv.org/abs/2506.18355)|null|
|**2025-06-23**|**Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies**|Xiaolin Chang Team|[2506.18304](http://arxiv.org/abs/2506.18304)|null|
|**2025-06-23**|**Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle**|Souma Chowdhury Team|[2506.18264](http://arxiv.org/abs/2506.18264)|null|
|**2025-06-22**|**RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation**|Yao Mu Team|[2506.18088](http://arxiv.org/abs/2506.18088)|null|
|**2025-06-21**|**RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models**|Xiao Li Team|[2506.17639](http://arxiv.org/abs/2506.17639)|null|
|**2025-06-21**|**Imitation Learning for Active Neck Motion Enabling Robot Manipulation beyond the Field of View**|Yasuo Kuniyoshi Team|[2506.17624](http://arxiv.org/abs/2506.17624)|null|
|**2025-06-20**|**Kinematic Model Optimization via Differentiable Contact Manifold for In-Space Manipulation**|Satyandra K. Gupta Team|[2506.17458](http://arxiv.org/abs/2506.17458)|null|
|**2025-06-20**|**Monocular One-Shot Metric-Depth Alignment for RGB-Based Robot Grasping**|Jingjin Yu Team|[2506.17110](http://arxiv.org/abs/2506.17110)|null|
|**2025-06-24**|**Learning Accurate Whole-body Throwing with High-frequency Residual Policy and Pullback Tube Acceleration**|Marco Hutter Team|[2506.16986](http://arxiv.org/abs/2506.16986)|null|
|**2025-06-20**|**Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections**|Shuran Song Team|[2506.16685](http://arxiv.org/abs/2506.16685)|null|
|**2025-06-19**|**CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity**|Yunzhu Li Team|[2506.16652](http://arxiv.org/abs/2506.16652)|null|
|**2025-06-19**|**Reimagination with Test-time Observation Interventions: Distractor-Robust World Model Predictions for Visual Model Predictive Control**|Ran Tian Team|[2506.16565](http://arxiv.org/abs/2506.16565)|null|
|**2025-06-19**|**An Optimization-Augmented Control Framework for Single and Coordinated Multi-Arm Robotic Manipulation**|Ozgur S. Oguz Team|[2506.16555](http://arxiv.org/abs/2506.16555)|null|
|**2025-06-19**|**Human2LocoMan: Learning Versatile Quadrupedal Manipulation with Human Pretraining**|Ding Zhao Team|[2506.16475](http://arxiv.org/abs/2506.16475)|null|
|**2025-06-19**|**GoalLadder: Incremental Goal Discovery with Vision-Language Models**|Shimon Whiteson Team|[2506.16396](http://arxiv.org/abs/2506.16396)|null|
|**2025-06-19**|**CapsDT: Diffusion-Transformer for Capsule Robot Manipulation**|Hongliang Ren Team|[2506.16263](http://arxiv.org/abs/2506.16263)|null|
|**2025-06-19**|**ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models**|Siyuan Huang Team|[2506.16211](http://arxiv.org/abs/2506.16211)|null|
|**2025-06-19**|**FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation**|Wei Tang Team|[2506.16201](http://arxiv.org/abs/2506.16201)|null|
|**2025-06-19**|**ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation**|Jitendra Malik Team|[2506.15953](http://arxiv.org/abs/2506.15953)|null|
|**2025-06-18**|**Learning from Planned Data to Improve Robotic Pick-and-Place Planning Efficiency**|Kensuke Harada Team|[2506.15920](http://arxiv.org/abs/2506.15920)|null|
|**2025-06-18**|**Improving Robotic Manipulation: Techniques for Object Pose Estimation, Accommodating Positional Uncertainty, and Disassembly Tasks from Examples**|Viral Rasik Galaiya Team|[2506.15865](http://arxiv.org/abs/2506.15865)|null|
|**2025-06-18**|**Vision in Action: Learning Active Perception from Human Demonstrations**|Shuran Song Team|[2506.15666](http://arxiv.org/abs/2506.15666)|null|
|**2025-06-18**|**Learning Task-Agnostic Skill Bases to Uncover Motor Primitives in Animal Behaviors**|Anqi Wu Team|[2506.15190](http://arxiv.org/abs/2506.15190)|null|
|**2025-06-18**|**Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation**|Yukiyasu Domae Team|[2506.15157](http://arxiv.org/abs/2506.15157)|null|
|**2025-06-18**|**TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality**|Eiichi Yoshida Team|[2506.15146](http://arxiv.org/abs/2506.15146)|null|
|**2025-06-17**|**RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills**|Chuang Gan Team|[2506.14763](http://arxiv.org/abs/2506.14763)|null|
|**2025-06-17**|**Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation**|Mustafa Mukadam Team|[2506.14754](http://arxiv.org/abs/2506.14754)|null|
|**2025-06-17**|**SENIOR: Efficient Query Selection and Preference-Guided Exploration in Preference-based Reinforcement Learning**|Shuo Wang Team|[2506.14648](http://arxiv.org/abs/2506.14648)|null|
|**2025-06-17**|**Latent Action Diffusion for Cross-Embodiment Manipulation**|Robert K. Katzschmann Team|[2506.14608](http://arxiv.org/abs/2506.14608)|null|
|**2025-06-19**|**ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes**|Hao Dong Team|[2506.14317](http://arxiv.org/abs/2506.14317)|null|
|**2025-06-17**|**Steering Robots with Inference-Time Interactions**|Yanwei Wang Team|[2506.14287](http://arxiv.org/abs/2506.14287)|null|
|**2025-06-17**|**AMPLIFY: Actionless Motion Priors for Robot Learning from Videos**|Animesh Garg Team|[2506.14198](http://arxiv.org/abs/2506.14198)|null|
|**2025-06-17**|**Non-Overlap-Aware Egocentric Pose Estimation for Collaborative Perception in Connected Autonomy**|Peng Gao Team|[2506.14180](http://arxiv.org/abs/2506.14180)|null|
|**2025-06-17**|**GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation**|Yebin Liu Team|[2506.14135](http://arxiv.org/abs/2506.14135)|null|
|**2025-06-16**|**ATK: Automatic Task-driven Keypoint Selection for Robust Policy Learning**|Abhishek Gupta Team|[2506.13867](http://arxiv.org/abs/2506.13867)|null|
|**2025-06-16**|**Touch begins where vision ends: Generalizable policies for contact-rich manipulation**|Raunaq Bhirangi Team|[2506.13762](http://arxiv.org/abs/2506.13762)|null|
|**2025-06-16**|**Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins**|Wei-Chiu Ma Team|[2506.13761](http://arxiv.org/abs/2506.13761)|null|
|**2025-06-16**|**What Matters in Learning from Large-Scale Datasets for Robot Manipulation**|Danfei Xu Team|[2506.13536](http://arxiv.org/abs/2506.13536)|null|
|**2025-06-16**|**A Survey on Imitation Learning for Contact-Rich Tasks in Robotics**|Arash Ajoudani Team|[2506.13498](http://arxiv.org/abs/2506.13498)|null|
|**2025-06-16**|**Learning Swing-up Maneuvers for a Suspended Aerial Manipulation Platform in a Hierarchical Control Framework**|Christian Ott Team|[2506.13478](http://arxiv.org/abs/2506.13478)|null|
|**2025-06-16**|**VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation**|Wei Pan Team|[2506.13428](http://arxiv.org/abs/2506.13428)|null|
|**2025-06-15**|**SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration**|Wenwu Zhu Team|[2506.12723](http://arxiv.org/abs/2506.12723)|null|
|**2025-06-15**|**Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence**|Andrea Bajcsy Team|[2506.12678](http://arxiv.org/abs/2506.12678)|null|
|**2025-06-15**|**Goal-based Self-Adaptive Generative Adversarial Imitation Learning (Goal-SAGAIL) for Multi-goal Robotic Manipulation Tasks**|George Vogiatzis Team|[2506.12676](http://arxiv.org/abs/2506.12676)|null|
|**2025-06-14**|**AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making**|Qingyao Wu Team|[2506.12374](http://arxiv.org/abs/2506.12374)|null|
|**2025-06-13**|**Role of Uncertainty in Model Development and Control Design for a Manufacturing Process**|Francis Assadian Team|[2506.12273](http://arxiv.org/abs/2506.12273)|null|
|**2025-06-13**|**SAIL: Faster-than-Demonstration Execution of Imitation Learning Policies**|Danfei Xu Team|[2506.11948](http://arxiv.org/abs/2506.11948)|null|
|**2025-06-13**|**mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity**|Robert K. Katzschmann Team|[2506.11916](http://arxiv.org/abs/2506.11916)|null|
|**2025-06-13**|**ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations**|Maria Bauza Villalonga Team|[2506.11775](http://arxiv.org/abs/2506.11775)|null|
|**2025-06-13**|**Control Architecture and Design for a Multi-robotic Visual Servoing System in Automated Manufacturing Environment**|Rongfei Li Team|[2506.11387](http://arxiv.org/abs/2506.11387)|null|
|**2025-06-12**|**Influence Functions for Data Attribution in Linear System Identification and LQR Control**|Dongmei Chen Team|[2506.11293](http://arxiv.org/abs/2506.11293)|null|
|**2025-06-12**|**Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation**|Cordelia Schmid Team|[2506.11261](http://arxiv.org/abs/2506.11261)|null|
|**2025-06-12**|**Eye, Robot: Learning to Look to Act with a BC-RL Perception-Action Loop**|Angjoo Kanazawa Team|[2506.10968](http://arxiv.org/abs/2506.10968)|null|
|**2025-06-12**|**GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation**|Jiangmiao Pang Team|[2506.10966](http://arxiv.org/abs/2506.10966)|null|
|**2025-06-12**|**Human-Robot Navigation using Event-based Cameras and Reinforcement Learning**|Rodrigo Verschae Team|[2506.10790](http://arxiv.org/abs/2506.10790)|null|
|**2025-06-12**|**Demonstrating Multi-Suction Item Picking at Scale via Multi-Modal Learning of Pick Success**|Kapil Katyal Team|[2506.10359](http://arxiv.org/abs/2506.10359)|null|
|**2025-06-11**|**Innovative Adaptive Imaged Based Visual Servoing Control of 6 DoFs Industrial Robot Manipulators**|Francis Assadian Team|[2506.10240](http://arxiv.org/abs/2506.10240)|null|
|**2025-06-11**|**One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture**|Stefano Carpin Team|[2506.10106](http://arxiv.org/abs/2506.10106)|null|
|**2025-06-11**|**eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures**|Raunaq Bhirangi Team|[2506.09994](http://arxiv.org/abs/2506.09994)|null|
|**2025-06-11**|**Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation**|Xiao Ma Team|[2506.09990](http://arxiv.org/abs/2506.09990)|null|
|**2025-06-11**|**From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models**|Chen Feng Team|[2506.09930](http://arxiv.org/abs/2506.09930)|null|
|**2025-06-11**|**Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving**|Chen Lv Team|[2506.09800](http://arxiv.org/abs/2506.09800)|null|
|**2025-06-11**|**CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings**|Davide Boscaini Team|[2506.09699](http://arxiv.org/abs/2506.09699)|null|
|**2025-06-11**|**Advances on Affordable Hardware Platforms for Human Demonstration Acquisition in Agricultural Applications**|Néstor García Team|[2506.09494](http://arxiv.org/abs/2506.09494)|null|
|**2025-06-11**|**DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects**|Hong Liu Team|[2506.09491](http://arxiv.org/abs/2506.09491)|null|
|**2025-06-11**|**Time-Unified Diffusion Policy with Action Discrimination for Robotic Manipulation**|Le Wang Team|[2506.09422](http://arxiv.org/abs/2506.09422)|null|
|**2025-06-11**|**Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation**|Xiang Li Team|[2506.09384](http://arxiv.org/abs/2506.09384)|null|
|**2025-06-11**|**ContextBuddy: AI-Enhanced Contextual Insights for Security Alert Investigation (Applied to Intrusion Detection)**|Cecile Paris Team|[2506.09365](http://arxiv.org/abs/2506.09365)|null|
|**2025-06-10**|**UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation**|Li Fei-Fei Team|[2506.09284](http://arxiv.org/abs/2506.09284)|null|
|**2025-06-10**|**Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism**|Bolei Zhou Team|[2506.09176](http://arxiv.org/abs/2506.09176)|null|
|**2025-06-10**|**FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency**|Jian Tang Team|[2506.08822](http://arxiv.org/abs/2506.08822)|null|
|**2025-06-10**|**Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning**|Xianta Jiang Team|[2506.08795](http://arxiv.org/abs/2506.08795)|null|
|**2025-06-10**|**Bayesian Inverse Physics for Neuro-Symbolic Robot Learning**|Frank Kirchner Team|[2506.08756](http://arxiv.org/abs/2506.08756)|null|
|**2025-06-10**|**Deep Reinforcement Learning-Based Motion Planning and PDE Control for Flexible Manipulators**|Jouni Mattila Team|[2506.08639](http://arxiv.org/abs/2506.08639)|null|
|**2025-06-10**|**RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping**|Gitta Kutyniok Team|[2506.08632](http://arxiv.org/abs/2506.08632)|null|
|**2025-06-10**|**Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots**|Lijun Zhu Team|[2506.08416](http://arxiv.org/abs/2506.08416)|null|
|**2025-06-11**|**HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation**|Cong Wang Team|[2506.08296](http://arxiv.org/abs/2506.08296)|null|
|**2025-06-09**|**ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving**|Xinggang Wang Team|[2506.08052](http://arxiv.org/abs/2506.08052)|null|
|**2025-06-09**|**BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models**|Tieniu Tan Team|[2506.07961](http://arxiv.org/abs/2506.07961)|null|
|**2025-06-09**|**BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation**|Xilin Chen Team|[2506.07530](http://arxiv.org/abs/2506.07530)|null|
|**2025-06-09**|**Reinforcement Learning via Implicit Imitation Guidance**|Chelsea Finn Team|[2506.07505](http://arxiv.org/abs/2506.07505)|null|
|**2025-06-09**|**RAPID Hand: A Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platform for Generalist Robot Autonomy**|Hui Cheng Team|[2506.07490](http://arxiv.org/abs/2506.07490)|null|
|**2025-06-08**|**CARoL: Context-aware Adaptation for Robot Learning**|Xuan Wang Team|[2506.07006](http://arxiv.org/abs/2506.07006)|null|
|**2025-06-07**|**SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game**|Shanghang Zhang Team|[2506.06690](http://arxiv.org/abs/2506.06690)|null|
|**2025-06-07**|**RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation**|Si Liu Team|[2506.06677](http://arxiv.org/abs/2506.06677)|null|
|**2025-06-07**|**Self-Adapting Improvement Loops for Robotic Learning**|Chen Sun Team|[2506.06658](http://arxiv.org/abs/2506.06658)|null|
|**2025-06-06**|**Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data**|Somil Bansal Team|[2506.06570](http://arxiv.org/abs/2506.06570)|null|
|**2025-06-06**|**NeSyPack: A Neuro-Symbolic Framework for Bimanual Logistics Packing**|Changliu Liu Team|[2506.06567](http://arxiv.org/abs/2506.06567)|null|
|**2025-06-06**|**MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping**|Farshad Khorrami Team|[2506.06535](http://arxiv.org/abs/2506.06535)|null|
|**2025-06-06**|**3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World Model**|Mingkui Tan Team|[2506.06199](http://arxiv.org/abs/2506.06199)|null|
|**2025-06-06**|**Bridging Perception and Action: Spatially-Grounded Mid-Level Representations for Robot Generalization**|Tingnan Zhang Team|[2506.06196](http://arxiv.org/abs/2506.06196)|null|
|**2025-06-10**|**BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning**|Rudolf Lioutikov Team|[2506.06072](http://arxiv.org/abs/2506.06072)|null|
|**2025-06-06**|**Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning**|Ping Luo Team|[2506.05985](http://arxiv.org/abs/2506.05985)|null|
|**2025-06-06**|**Optimal Robotic Velcro Peeling with Force Feedback**|Volkan Isler Team|[2506.05812](http://arxiv.org/abs/2506.05812)|null|
|**2025-06-06**|**Where Do We Look When We Teach? Analyzing Human Gaze Behavior Across Demonstration Devices in Robot Imitation Learning**|Hiroshi Bito Team|[2506.05808](http://arxiv.org/abs/2506.05808)|null|
|**2025-06-06**|**FlowOE: Imitation Learning with Flow Policy from Ensemble RL Experts for Optimal Execution under Heston Volatility and Concave Market Impacts**|Zhi Chen Team|[2506.05755](http://arxiv.org/abs/2506.05755)|null|
|**2025-06-06**|**You Only Estimate Once: Unified, One-stage, Real-Time Category-level Articulated Object 6D Pose Estimation for Robotic Grasping**|Xiangyang Xue Team|[2506.05719](http://arxiv.org/abs/2506.05719)|null|
|**2025-06-05**|**A Smooth Sea Never Made a Skilled $\texttt{SAILOR}$ : Robust Imitation via Learning to Search**|Gokul Swamy Team|[2506.05294](http://arxiv.org/abs/2506.05294)|null|
|**2025-06-05**|**LiPo: A Lightweight Post-optimization Framework for Smoothing Action Chunks Generated by Learned Policies**|Suhan Park Team|[2506.05165](http://arxiv.org/abs/2506.05165)|null|
|**2025-06-05**|**DemoSpeedup: Accelerating Visuomotor Policies via Entropy-Guided Demonstration Acceleration**|Huazhe Xu Team|[2506.05064](http://arxiv.org/abs/2506.05064)|null|
|**2025-06-06**|**ArtVIP: Articulated Digital Assets of Visual Realism, Modular Interaction, and Physical Fidelity for Robot Learning**|Jian Tang Team|[2506.04941](http://arxiv.org/abs/2506.04941)|null|
|**2025-06-05**|**Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion**|Qi Dou Team|[2506.04716](http://arxiv.org/abs/2506.04716)|null|
|**2025-06-05**|**Advancing Tool-Augmented Large Language Models via Meta-Verification and Reflection Learning**|Wanxiang Che Team|[2506.04625](http://arxiv.org/abs/2506.04625)|null|
|**2025-06-04**|**SGN-CIRL: Scene Graph-based Navigation with Curriculum, Imitation, and Reinforcement Learning**|Aleksandr Panov Team|[2506.04505](http://arxiv.org/abs/2506.04505)|null|
|**2025-06-04**|**Object-centric 3D Motion Field for Robot Learning from Human Videos**|Pieter Abbeel Team|[2506.04227](http://arxiv.org/abs/2506.04227)|null|
|**2025-06-04**|**Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot Data**|Leonard Hasenclever Team|[2506.04120](http://arxiv.org/abs/2506.04120)|null|
|**2025-06-04**|**STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization**|Liqiang Nie Team|[2506.03863](http://arxiv.org/abs/2506.03863)|**[link](https://github.com/jiutian-vl/star)**|
|**2025-06-04**|**SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models**|Jian Tang Team|[2506.03574](http://arxiv.org/abs/2506.03574)|null|
|**2025-06-05**|**Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving**|Hu Chuan Team|[2506.03568](http://arxiv.org/abs/2506.03568)|**[link](https://github.com/lzqw/c-hac)**|
|**2025-06-03**|**ORV: 4D Occupancy-centric Robot Video Generation**|Hao Zhao Team|[2506.03079](http://arxiv.org/abs/2506.03079)|null|
|**2025-06-03**|**Geometric Visual Servo Via Optimal Transport**|Ashutosh Tiwari Team|[2506.02768](http://arxiv.org/abs/2506.02768)|null|
|**2025-06-03**|**Rodrigues Network for Learning Robot Actions**|Leonidas Guibas Team|[2506.02618](http://arxiv.org/abs/2506.02618)|null|
|**2025-06-03**|**Reachability Weighted Offline Goal-conditioned Resampling**|Joni Pajarinen Team|[2506.02577](http://arxiv.org/abs/2506.02577)|null|
|**2025-06-02**|**Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning**|Pheng-Ann Heng Team|[2506.01953](http://arxiv.org/abs/2506.01953)|null|
|**2025-06-02**|**Feel the Force: Contact-Driven Learning from Humans**|Lerrel Pinto Team|[2506.01944](http://arxiv.org/abs/2506.01944)|null|
|**2025-06-02**|**Learning Video Generation for Robotic Manipulation with Collaborative Trajectory Control**|Dahua Lin Team|[2506.01943](http://arxiv.org/abs/2506.01943)|null|
|**2025-06-02**|**FreeTacMan: Robot-free Visuo-Tactile Data Collection System for Contact-rich Manipulation**|Hongyang Li Team|[2506.01941](http://arxiv.org/abs/2506.01941)|null|
|**2025-06-02**|**Learning with pyCub: A New Simulation and Exercise Framework for Humanoid Robotics**|Matej Hoffmann Team|[2506.01756](http://arxiv.org/abs/2506.01756)|null|
|**2025-06-02**|**Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning**|Kang Liu Team|[2506.01710](http://arxiv.org/abs/2506.01710)|**[link](https://github.com/MJinXiang/Reasoning-Table)**|
|**2025-06-02**|**WoMAP: World Models For Embodied Open-Vocabulary Object Localization**|Anirudha Majumdar Team|[2506.01600](http://arxiv.org/abs/2506.01600)|null|
|**2025-06-02**|**FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens**|Yuexin Ma Team|[2506.01583](http://arxiv.org/abs/2506.01583)|null|
|**2025-06-02**|**Trajectory First: A Curriculum for Discovering Diverse Policies**|Marc Toussaint Team|[2506.01568](http://arxiv.org/abs/2506.01568)|null|
|**2025-06-02**|**Variational Adaptive Noise and Dropout towards Stable Recurrent Neural Networks**|Shingo Murata Team|[2506.01350](http://arxiv.org/abs/2506.01350)|null|
|**2025-06-01**|**OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation**|Valts Blukis Team|[2506.01196](http://arxiv.org/abs/2506.01196)|null|
|**2025-06-01**|**HoMeR: Learning In-the-Wild Mobile Manipulation via Hybrid Imitation and Whole-Body Control**|Jeannette Bohg Team|[2506.01185](http://arxiv.org/abs/2506.01185)|null|
|**2025-06-01**|**Jailbreak-R1: Exploring the Jailbreak Capabilities of LLMs via Reinforcement Learning**|Jing Li Team|[2506.00782](http://arxiv.org/abs/2506.00782)|null|
|**2025-05-31**|**XYZ-IBD: High-precision Bin-picking Dataset for Object 6D Pose Estimation Capturing Real-world Industrial Complexity**|Benjamin Busam Team|[2506.00599](http://arxiv.org/abs/2506.00599)|null|
|**2025-05-31**|**Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in AI Agents**|Zhou Yu Team|[2506.00320](http://arxiv.org/abs/2506.00320)|null|
|**2025-05-30**|**3D Gaussian Splat Vulnerabilities**|Polo Chau Team|[2506.00280](http://arxiv.org/abs/2506.00280)|null|
|**2025-05-30**|**Bi-Manual Joint Camera Calibration and Scene Representation**|Weiming Zhi Team|[2505.24819](http://arxiv.org/abs/2505.24819)|null|
|**2025-05-30**|**MagicGripper: A Multimodal Sensor-Integrated Gripper for Contact-Rich Robotic Manipulation**|Dandan Zhang Team|[2505.24382](http://arxiv.org/abs/2505.24382)|null|
|**2025-05-30**|**Imitation Learning-Based Path Generation for the Complex Assembly of Deformable Objects**|Christoffer Sloth Team|[2505.24339](http://arxiv.org/abs/2505.24339)|null|
|**2025-05-30**|**SR3D: Unleashing Single-view 3D Reconstruction for Transparent and Specular Object Grasping**|Hao Dong Team|[2505.24305](http://arxiv.org/abs/2505.24305)|null|
|**2025-05-30**|**Safety-Aware Robust Model Predictive Control for Robotic Arms in Dynamic Environments**|Suwoong Lee Team|[2505.24209](http://arxiv.org/abs/2505.24209)|null|
|**2025-05-30**|**Learning Gentle Humanoid Locomotion and End-Effector Stabilization Control**|Guanya Shi Team|[2505.24198](http://arxiv.org/abs/2505.24198)|null|
|**2025-05-29**|**Mobi- $π$ : Mobilizing Your Robot Learning Policy**|Jeannette Bohg Team|[2505.23692](http://arxiv.org/abs/2505.23692)|null|
|**2025-05-30**|**Normalizing Flows are Capable Models for RL**|Benjamin Eysenbach Team|[2505.23527](http://arxiv.org/abs/2505.23527)|null|
|**2025-05-29**|**Optimization-based Posture Generation for Whole-body Contact Motion by Contact Point Search on the Body Surface**|Masayuki Inaba Team|[2505.23501](http://arxiv.org/abs/2505.23501)|null|
|**2025-05-29**|**Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents**|Lichao Sun Team|[2505.23450](http://arxiv.org/abs/2505.23450)|null|
|**2025-05-29**|**Enhanced DACER Algorithm with High Diffusion Efficiency**|Shengbo Eben Li Team|[2505.23426](http://arxiv.org/abs/2505.23426)|null|
|**2025-05-29**|**RoboTransfer: Geometry-Consistent Video Diffusion for Robotic Visual Policy Transfer**|Zhizhong Su Team|[2505.23171](http://arxiv.org/abs/2505.23171)|null|
|**2025-05-28**|**SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning**|Yuke Zhu Team|[2505.22626](http://arxiv.org/abs/2505.22626)|null|
|**2025-05-28**|**Hybrid Learning for Cold-Start-Aware Microservice Scheduling in Dynamic Edge Environments**|Weijia Jia Team|[2505.22424](http://arxiv.org/abs/2505.22424)|**[link](https://github.com/Blacktower27/CSDCRMDE)**|
|**2025-05-28**|**Efficient Precision-Scalable Hardware for Microscaling (MX) Processing in Robotics Learning**|Marian Verhelst Team|[2505.22404](http://arxiv.org/abs/2505.22404)|null|
|**2025-05-28**|**State and Input Constrained Adaptive Tracking Control of Uncertain Euler-Lagrange Systems with Robustness and Feasibility Analysis**|Shubhendu Bhasin Team|[2505.22352](http://arxiv.org/abs/2505.22352)|null|
|**2025-05-28**|**ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation**|Wenqiang Zhang Team|[2505.22159](http://arxiv.org/abs/2505.22159)|null|
|**2025-05-28**|**Learning Compositional Behaviors from Demonstration and Language**|Jiajun Wu Team|[2505.21981](http://arxiv.org/abs/2505.21981)|null|
|**2025-05-29**|**ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge**|Yi Xu Team|[2505.21906](http://arxiv.org/abs/2505.21906)|null|
|**2025-05-28**|**Streaming Flow Policy: Simplifying diffusion $/$ flow-matching policies by treating action trajectories as flow trajectories**|Siddharth Ancha Team|[2505.21851](http://arxiv.org/abs/2505.21851)|null|
|**2025-05-27**|**PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation**|Tianmin Shu Team|[2505.21652](http://arxiv.org/abs/2505.21652)|null|
|**2025-05-30**|**Right Side Up? Disentangling Orientation Understanding in MLLMs with Fine-grained Multi-axis Perception Tasks**|Bryan A. Plummer Team|[2505.21649](http://arxiv.org/abs/2505.21649)|null|
|**2025-05-27**|**CLAMP: Crowdsourcing a LArge-scale in-the-wild haptic dataset with an open-source device for Multimodal robot Perception**|Tapomayukh Bhattacharjee Team|[2505.21495](http://arxiv.org/abs/2505.21495)|null|
|**2025-05-27**|**EquAct: An SE(3)-Equivariant Multi-Task Transformer for Open-Loop Robotic Manipulation**|Robert Platt Team|[2505.21351](http://arxiv.org/abs/2505.21351)|null|
|**2025-05-27**|**EgoWalk: A Multimodal Dataset for Robot Navigation in the Wild**|Gonzalo Ferrer Team|[2505.21282](http://arxiv.org/abs/2505.21282)|null|
|**2025-05-27**|**Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations**|Tanvi Verma Team|[2505.21182](http://arxiv.org/abs/2505.21182)|null|
|**2025-05-27**|**Object-Centric Action-Enhanced Representations for Robot Visuo-Motor Policy Learning**|George Retsinas Team|[2505.20962](http://arxiv.org/abs/2505.20962)|null|
|**2025-05-27**|**Learning Unified Force and Position Control for Legged Loco-Manipulation**|Siyuan Huang Team|[2505.20829](http://arxiv.org/abs/2505.20829)|null|
|**2025-05-27**|**Spatial RoboGrasp: Generalized Robotic Grasping Control Policy**|Luhui Hu Team|[2505.20814](http://arxiv.org/abs/2505.20814)|null|
|**2025-05-27**|**Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt**|Jianyu Chen Team|[2505.20795](http://arxiv.org/abs/2505.20795)|null|
|**2025-05-28**|**ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image**|Ruohan Gao Team|[2505.20498](http://arxiv.org/abs/2505.20498)|null|
|**2025-05-26**|**OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation**|Farshad Khorrami Team|[2505.20425](http://arxiv.org/abs/2505.20425)|null|
|**2025-05-26**|**Co-Design of Soft Gripper with Neural Physics**|Xiaolong Wang Team|[2505.20404](http://arxiv.org/abs/2505.20404)|null|
|**2025-05-26**|**EgoZero: Robot Learning from Smart Glasses**|Lerrel Pinto Team|[2505.20290](http://arxiv.org/abs/2505.20290)|null|
|**2025-05-26**|**URPlanner: A Universal Paradigm For Collision-Free Robotic Motion Planning Based on Deep Reinforcement Learning**|Marcelo H. Ang Jr Team|[2505.20175](http://arxiv.org/abs/2505.20175)|null|
|**2025-05-27**|**MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents**|Xiaodan Liang Team|[2505.20148](http://arxiv.org/abs/2505.20148)|**[link](https://github.com/mineanybuild/mineanybuild)**|
|**2025-05-26**|**ReasonPlan: Unified Scene Prediction and Decision Reasoning for Closed-loop Autonomous Driving**|Dongbin Zhao Team|[2505.20024](http://arxiv.org/abs/2505.20024)|**[link](https://github.com/liuxueyi/reasonplan)**|
|**2025-05-26**|**Inverse Q-Learning Done Right: Offline Imitation Learning in $Q^π$ -Realizable MDPs**|Luca Viano Team|[2505.19946](http://arxiv.org/abs/2505.19946)|null|
|**2025-05-26**|**TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning**|Dongbin Zhao Team|[2505.19769](http://arxiv.org/abs/2505.19769)|null|
|**2025-05-26**|**Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning**|Jean-Baptiste Mouret Team|[2505.19717](http://arxiv.org/abs/2505.19717)|null|
|**2025-05-25**|**Structured Reinforcement Learning for Combinatorial Decision-Making**|Maximilian Schiffer Team|[2505.19053](http://arxiv.org/abs/2505.19053)|**[link](https://github.com/tumbais/structured-rl)**|
|**2025-05-25**|**WorldEval: World Model as Real-World Robot Policies Evaluator**|Yi Xu Team|[2505.19017](http://arxiv.org/abs/2505.19017)|null|
|**2025-05-25**|**Online Knowledge Distillation with Reward Guidance**|Chen Jia Team|[2505.18952](http://arxiv.org/abs/2505.18952)|null|
|**2025-05-24**|**Guided by Guardrails: Control Barrier Functions as Safety Instructors for Robotic Learning**|Giovanni Beltrame Team|[2505.18858](http://arxiv.org/abs/2505.18858)|null|
|**2025-05-24**|**On the Dual-Use Dilemma in Physical Reasoning and Force**|Nikolaus Correll Team|[2505.18792](http://arxiv.org/abs/2505.18792)|null|
|**2025-05-24**|**VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning**|Ziwei Wang Team|[2505.18719](http://arxiv.org/abs/2505.18719)|null|
|**2025-05-24**|**MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations**|Hong Thanh Nguyen Team|[2505.18595](http://arxiv.org/abs/2505.18595)|null|
|**2025-05-24**|**Grounding Bodily Awareness in Visual Representations for Efficient Policy Learning**|Zhiyun Lin Team|[2505.18487](http://arxiv.org/abs/2505.18487)|null|
|**2025-05-24**|**Canonical Policy: Learning Canonical 3D Representation for Equivariant Policy**|Yu She Team|[2505.18474](http://arxiv.org/abs/2505.18474)|null|
|**2025-05-24**|**ManiFeel: Benchmarking and Understanding Visuotactile Manipulation Policy Learning**|Yu She Team|[2505.18472](http://arxiv.org/abs/2505.18472)|null|
|**2025-05-23**|**ProgRM: Build Better GUI Agents with Progress Rewards**|Kai Yu Team|[2505.18121](http://arxiv.org/abs/2505.18121)|null|
|**2025-05-23**|**Classification of assembly tasks combining multiple primitive actions using Transformers and xLSTMs**|Pedro Neto Team|[2505.18012](http://arxiv.org/abs/2505.18012)|null|
|**2025-05-23**|**Is Single-View Mesh Reconstruction Ready for Robotics?**|Ingmar Posner Team|[2505.17966](http://arxiv.org/abs/2505.17966)|null|
|**2025-05-23**|**SynRES: Towards Referring Expression Segmentation in the Wild via Synthetic Data**|Donghyun Kim Team|[2505.17695](http://arxiv.org/abs/2505.17695)|null|
|**2025-05-23**|**Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning**|Giorgia Ramponi Team|[2505.17610](http://arxiv.org/abs/2505.17610)|null|
|**2025-05-23**|**Dynamic Manipulation of Deformable Objects in 3D: Simulation, Benchmark and Learning Strategy**|Bin Zhao Team|[2505.17434](http://arxiv.org/abs/2505.17434)|null|
|**2025-05-23**|**Bootstrapping Imitation Learning for Long-horizon Manipulation via Hierarchical Data Collection Space**|Hui Cheng Team|[2505.17389](http://arxiv.org/abs/2505.17389)|null|
|**2025-05-22**|**ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems**|Farhad Imani Team|[2505.17295](http://arxiv.org/abs/2505.17295)|null|
|**2025-05-22**|**CoMo: Learning Continuous Latent Motion from Internet Videos for Scalable Robot Learning**|Limin Wang Team|[2505.17006](http://arxiv.org/abs/2505.17006)|null|
|**2025-05-22**|**3D Equivariant Visuomotor Policy Learning via Spherical Projection**|Robin Walters Team|[2505.16969](http://arxiv.org/abs/2505.16969)|null|
|**2025-05-22**|**Efficient Online RL Fine Tuning with Offline Pre-trained Policy Only**|Donglin Wang Team|[2505.16856](http://arxiv.org/abs/2505.16856)|null|
|**2025-05-22**|**Find the Fruit: Designing a Zero-Shot Sim2Real Deep RL Planner for Occlusion Aware Plant Manipulation**|Soumik Sarkar Team|[2505.16547](http://arxiv.org/abs/2505.16547)|null|
|**2025-05-24**|**ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models**|Xiuying Chen Team|[2505.16517](http://arxiv.org/abs/2505.16517)|null|
|**2025-05-22**|**Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)**|Junchi Yan Team|[2505.16394](http://arxiv.org/abs/2505.16394)|null|
|**2025-05-22**|**TacCompress: A Benchmark for Multi-Point Tactile Data Compression in Dexterous Manipulation**|Hengdi Zhang Team|[2505.16289](http://arxiv.org/abs/2505.16289)|null|
|**2025-05-22**|**SEM: Enhancing Spatial Understanding for Robust Robot Manipulation**|Zhizhong Su Team|[2505.16196](http://arxiv.org/abs/2505.16196)|null|
|**2025-05-22**|**Tactile-based Reinforcement Learning for Adaptive Grasping under Observation Uncertainties**|Yang Ye Team|[2505.16167](http://arxiv.org/abs/2505.16167)|null|
|**2025-05-21**|**WaveTouch: Active Tactile Sensing Using Vibro-Feedback for Classification of Variable Stiffness and Infill Density Objects**|Bakhtiyar Orazbayev Team|[2505.16062](http://arxiv.org/abs/2505.16062)|null|
|**2025-05-25**|**Proactive Hierarchical Control Barrier Function-Based Safety Prioritization in Close Human-Robot Interaction Scenarios**|Prashanth Krishnamurthy Team|[2505.16055](http://arxiv.org/abs/2505.16055)|null|
|**2025-05-21**|**UAV-Flow Colosseo: A Real-World Benchmark for Flying-on-a-Word UAV Imitation Learning**|Si Liu Team|[2505.15725](http://arxiv.org/abs/2505.15725)|null|
|**2025-05-21**|**Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization**|Junwei Liang Team|[2505.15660](http://arxiv.org/abs/2505.15660)|null|
|**2025-05-21**|**FLARE: Robot Learning with Implicit World Modeling**|Linxi Fan Team|[2505.15659](http://arxiv.org/abs/2505.15659)|null|
|**2025-05-21**|**Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets**|Ken Goldberg Team|[2505.15517](http://arxiv.org/abs/2505.15517)|null|
|**2025-05-21**|**Guided Policy Optimization under Partial Observability**|Zongqing Lu Team|[2505.15418](http://arxiv.org/abs/2505.15418)|**[link](https://github.com/liyheng/GPO)**|
|**2025-05-21**|**Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control**|Jungwook Choi Team|[2505.15304](http://arxiv.org/abs/2505.15304)|null|
|**2025-05-21**|**Learning-based Autonomous Oversteer Control and Collision Avoidance**|Seung-Hyun Kong Team|[2505.15275](http://arxiv.org/abs/2505.15275)|null|
|**2025-05-21**|**Filtering Learning Histories Enhances In-Context Reinforcement Learning**|Santiago Paternain Team|[2505.15143](http://arxiv.org/abs/2505.15143)|null|
|**2025-05-21**|**Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation**|Xiaodong He Team|[2505.15098](http://arxiv.org/abs/2505.15098)|null|
|**2025-05-20**|**RoboCulture: A Robotics Platform for Automated Biological Experimentation**|Milica Radisic Team|[2505.14941](http://arxiv.org/abs/2505.14941)|null|
|**2025-05-20**|**Imitation Learning via Focused Satisficing**|Brian Ziebart Team|[2505.14820](http://arxiv.org/abs/2505.14820)|null|
|**2025-05-20**|**DORA: Object Affordance-Guided Reinforcement Learning for Dexterous Robotic Manipulation**|Jianwei Zhang Team|[2505.14819](http://arxiv.org/abs/2505.14819)|null|
|**2025-05-20**|**Vid2World: Crafting Video Diffusion Models to Interactive World Models**|Mingsheng Long Team|[2505.14357](http://arxiv.org/abs/2505.14357)|null|
|**2025-05-20**|**AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory**|Ping Luo Team|[2505.14030](http://arxiv.org/abs/2505.14030)|null|
|**2025-05-20**|**RLVR-World: Training World Models with Reinforcement Learning**|Mingsheng Long Team|[2505.13934](http://arxiv.org/abs/2505.13934)|**[link](https://github.com/thuml/RLVR-World)**|
|**2025-05-20**|**Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning**|Yutong Ban Team|[2505.13925](http://arxiv.org/abs/2505.13925)|null|
|**2025-05-20**|**Learning to Insert for Constructive Neural Vehicle Routing Solver**|Qingfu Zhang Team|[2505.13904](http://arxiv.org/abs/2505.13904)|null|
|**2025-05-20**|**Structured Agent Distillation for Large Language Model**|Yanzhi Wang Team|[2505.13820](http://arxiv.org/abs/2505.13820)|null|
|**2025-05-21**|**Adaptive Diffusion Constrained Sampling for Bimanual Robot Manipulation**|Georgia Chalvatzaki Team|[2505.13667](http://arxiv.org/abs/2505.13667)|null|
|**2025-05-19**|**TD-GRPC: Temporal Difference Learning with Group Relative Policy Constraint for Humanoid Locomotion**|Minh Nhat Vu Team|[2505.13549](http://arxiv.org/abs/2505.13549)|null|
|**2025-05-19**|**GraspMolmo: Generalizable Task-Oriented Grasping via Large-Scale Synthetic Data Generation**|Rose Hendrix Team|[2505.13441](http://arxiv.org/abs/2505.13441)|null|
|**2025-05-19**|**KinTwin: Imitation Learning with Torque and Muscle Driven Biomechanical Models Enables Precise Replication of Able-Bodied and Impaired Movement from Markerless Motion Capture**|R. James Cotton Team|[2505.13436](http://arxiv.org/abs/2505.13436)|null|
|**2025-05-19**|**TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation**|Jiangmiao Pang Team|[2505.12748](http://arxiv.org/abs/2505.12748)|null|
|**2025-05-19**|**Incentivizing Multimodal Reasoning in Large Models for Direct Robot Manipulation**|Chi-Wing Fu Team|[2505.12744](http://arxiv.org/abs/2505.12744)|null|
|**2025-05-19**|**Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning**|Taesup Moon Team|[2505.12737](http://arxiv.org/abs/2505.12737)|null|
|**2025-05-19**|**DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories**|Linxi Fan Team|[2505.12705](http://arxiv.org/abs/2505.12705)|null|
|**2025-05-19**|**Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion**|Qi Wu Team|[2505.12679](http://arxiv.org/abs/2505.12679)|null|
|**2025-05-19**|**HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos**|Xue Bin Peng Team|[2505.12619](http://arxiv.org/abs/2505.12619)|null|
|**2025-05-18**|**MTIL: Encoding Full History with Mamba for Temporal Imitation Learning**|Zhouping Yin Team|[2505.12410](http://arxiv.org/abs/2505.12410)|**[link](https://github.com/yulinzhouzyl/mtil)**|
|**2025-05-18**|**PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis**|Zhipong Cai Team|[2505.12294](http://arxiv.org/abs/2505.12294)|null|
|**2025-05-20**|**RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction**|Bo Zhao Team|[2505.12224](http://arxiv.org/abs/2505.12224)|null|
|**2025-05-20**|**Learning Impact-Rich Rotational Maneuvers via Centroidal Velocity Rewards and Sim-to-Real Techniques: A One-Leg Hopper Flip Case Study**|Hae-Won Park Team|[2505.12222](http://arxiv.org/abs/2505.12222)|null|
|**2025-05-17**|**L2D2: Robot Learning from 2D Drawings**|Dylan P. Losey Team|[2505.12072](http://arxiv.org/abs/2505.12072)|null|
|**2025-05-17**|**H2R: A Human-to-Robot Data Augmentation for Robot Pre-training from Videos**|Shanghang Zhang Team|[2505.11920](http://arxiv.org/abs/2505.11920)|null|
|**2025-05-17**|**GLOVER++: Unleashing the Potential of Affordance Learning from Human Behaviors for Robotic Manipulation**|Junwei Liang Team|[2505.11865](http://arxiv.org/abs/2505.11865)|null|
|**2025-05-17**|**Learning IMU Bias with Diffusion Model**|Guoquan Huang Team|[2505.11763](http://arxiv.org/abs/2505.11763)|null|
|**2025-05-16**|**Zero-Shot Visual Generalization in Robot Manipulation**|Gaurav Sukhatme Team|[2505.11719](http://arxiv.org/abs/2505.11719)|null|
|**2025-05-16**|**Employing Laban Shape for Generating Emotionally and Functionally Expressive Trajectories in Robotic Manipulators**|Alessandro Roncone Team|[2505.11716](http://arxiv.org/abs/2505.11716)|null|
|**2025-05-16**|**EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video**|Jian Zhang Team|[2505.11709](http://arxiv.org/abs/2505.11709)|null|
|**2025-05-16**|**Grounded Task Axes: Zero-Shot Semantic Skill Generalization via Task-Axis Controllers and Visual Foundation Models**|Oliver Kroemer Team|[2505.11680](http://arxiv.org/abs/2505.11680)|null|
|**2025-05-16**|**SHIELD: Safety on Humanoids via CBFs In Expectation on Learned Dynamics**|Aaron D. Ames Team|[2505.11494](http://arxiv.org/abs/2505.11494)|null|
|**2025-05-16**|**Exploiting Radiance Fields for Grasp Generation on Novel Synthetic Views**|Todor Stoyanov Team|[2505.11467](http://arxiv.org/abs/2505.11467)|null|
|**2025-05-16**|**ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations**|Jesse Zhang Team|[2505.10911](http://arxiv.org/abs/2505.10911)|null|
|**2025-05-16**|**Counterfactual Behavior Cloning: Offline Imitation Learning from Imperfect Human Demonstrations**|Dylan P. Losey Team|[2505.10760](http://arxiv.org/abs/2505.10760)|null|
|**2025-05-15**|**Infinigen-Sim: Procedural Generation of Articulated Simulation Assets**|Jia Deng Team|[2505.10755](http://arxiv.org/abs/2505.10755)|null|
|**2025-05-15**|**Knowledge capture, adaptation and composition (KCAC): A framework for cross-task curriculum learning in robotic manipulation**|Yan Jin Team|[2505.10522](http://arxiv.org/abs/2505.10522)|null|
|**2025-05-15**|**IN-RIL: Interleaved Reinforcement and Imitation Learning for Policy Fine-Tuning**|Junshan Zhang Team|[2505.10442](http://arxiv.org/abs/2505.10442)|null|
|**2025-05-15**|**NVSPolicy: Adaptive Novel-View Synthesis for Generalizable Language-Conditioned Policy Learning**|Chengyuan Chen Team|[2505.10359](http://arxiv.org/abs/2505.10359)|null|
|**2025-05-15**|**SRT-H: A Hierarchical Framework for Autonomous Surgery via Language Conditioned Imitation Learning**|Axel Krieger Team|[2505.10251](http://arxiv.org/abs/2505.10251)|null|
|**2025-05-15**|**Training People to Reward Robots**|Matthew Howard Team|[2505.10151](http://arxiv.org/abs/2505.10151)|null|
|**2025-05-15**|**EmbodiedMAE: A Unified 3D Multi-Modal Representation for Robot Manipulation**|Jianye Hao Team|[2505.10105](http://arxiv.org/abs/2505.10105)|null|
|**2025-05-15**|**FlowDreamer: A RGB-D World Model with Flow-based Motion Representations for Robot Manipulation**|Qing Li Team|[2505.10075](http://arxiv.org/abs/2505.10075)|null|
|**2025-05-15**|**APEX: Action Priors Enable Efficient Exploration for Skill Imitation on Articulated Robots**|Guillaume Sartoretti Team|[2505.10022](http://arxiv.org/abs/2505.10022)|null|
|**2025-05-15**|**ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts**|Yang Yu Team|[2505.10010](http://arxiv.org/abs/2505.10010)|**[link](https://github.com/lamda-rl/imaginebench)**|
|**2025-05-16**|**PointArena: Probing Multimodal Grounding Through Language-Guided Pointing**|Ranjay Krishna Team|[2505.09990](http://arxiv.org/abs/2505.09990)|null|
|**2025-05-15**|**Learning Diverse Natural Behaviors for Enhancing the Agility of Quadrupedal Robots**|Chunlin Chen Team|[2505.09979](http://arxiv.org/abs/2505.09979)|null|
|**2025-05-14**|**Learning Rock Pushability on Rough Planetary Terrain**|Cagri Kilic Team|[2505.09833](http://arxiv.org/abs/2505.09833)|null|
|**2025-05-14**|**Trailblazer: Learning offroad costmaps for long range planning**|Srikanth Saripalli Team|[2505.09739](http://arxiv.org/abs/2505.09739)|null|
|**2025-05-14**|**EnerVerse-AC: Envisioning Embodied Environments with Action Condition**|Guanghui Ren Team|[2505.09723](http://arxiv.org/abs/2505.09723)|null|
|**2025-05-14**|**ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation**|Daniel Seita Team|[2505.09698](http://arxiv.org/abs/2505.09698)|null|
|**2025-05-14**|**DataMIL: Selecting Data for Robot Imitation Learning with Datamodels**|Roberto Martín-Martín Team|[2505.09603](http://arxiv.org/abs/2505.09603)|null|
|**2025-05-14**|**Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware**|Ken Goldberg Team|[2505.09601](http://arxiv.org/abs/2505.09601)|null|
|**2025-05-14**|**VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation**|Shuo Wang Team|[2505.09577](http://arxiv.org/abs/2505.09577)|null|
|**2025-05-14**|**Learning Long-Context Diffusion Policies via Past-Token Prediction**|Chelsea Finn Team|[2505.09561](http://arxiv.org/abs/2505.09561)|null|
|**2025-05-14**|**Distilling Realizable Students from Unrealizable Teachers**|Sanjiban Choudhury Team|[2505.09546](http://arxiv.org/abs/2505.09546)|null|
|**2025-05-14**|**Exploring Pose-Guided Imitation Learning for Robotic Precise Insertion**|Qixin Cao Team|[2505.09424](http://arxiv.org/abs/2505.09424)|null|
|**2025-05-14**|**Neural Multivariate Regression: Qualitative Insights from the Unconstrained Feature Model**|Keith Ross Team|[2505.09308](http://arxiv.org/abs/2505.09308)|null|
|**2025-05-14**|**Latent Theory of Mind: A Decentralized Diffusion Architecture for Cooperative Manipulation**|Guillaume Sartoretti Team|[2505.09144](http://arxiv.org/abs/2505.09144)|null|
|**2025-05-14**|**FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis**|He Wang Team|[2505.09109](http://arxiv.org/abs/2505.09109)|null|
|**2025-05-14**|**Imitation Learning for Adaptive Control of a Virtual Soft Exoglove**|Letizia Gionfrida Team|[2505.09099](http://arxiv.org/abs/2505.09099)|null|
|**2025-05-13**|**ChicGrasp: Imitation-Learning based Customized Dual-Jaw Gripper Control for Delicate, Irregular Bio-products Manipulation**|Dongyi Wang Team|[2505.08986](http://arxiv.org/abs/2505.08986)|null|
|**2025-05-13**|**Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness**|Wolfram Burgard Team|[2505.08627](http://arxiv.org/abs/2505.08627)|null|
|**2025-05-13**|**Beyond Predefined Actions: Integrating Behavior Trees and Dynamic Movement Primitives for Robot Learning from Demonstration**|Todor Stoyanov Team|[2505.08625](http://arxiv.org/abs/2505.08625)|null|
|**2025-05-13**|**From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation**|Jianye Hao Team|[2505.08548](http://arxiv.org/abs/2505.08548)|null|
|**2025-05-13**|**Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges**|Weisi Guo Team|[2505.08453](http://arxiv.org/abs/2505.08453)|null|
|**2025-05-13**|**Adaptive Diffusion Policy Optimization for Robotic Manipulation**|Zhuang Yang Team|[2505.08376](http://arxiv.org/abs/2505.08376)|null|
|**2025-05-13**|**Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation**|Qianchun Lu Team|[2505.08364](http://arxiv.org/abs/2505.08364)|null|
|**2025-05-13**|**Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning**|Biwei Huang Team|[2505.08361](http://arxiv.org/abs/2505.08361)|null|
|**2025-05-13**|**HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands**|Yunhui Liu Team|[2505.08213](http://arxiv.org/abs/2505.08213)|null|
|**2025-05-13**|**CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding**|Shuo Wang Team|[2505.08194](http://arxiv.org/abs/2505.08194)|null|
|**2025-05-12**|**What Matters for Batch Online Reinforcement Learning in Robotics?**|Chelsea Finn Team|[2505.08078](http://arxiv.org/abs/2505.08078)|null|
|**2025-05-12**|**H $^{\mathbf{3}}$ DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning**|Huazhe Xu Team|[2505.07819](http://arxiv.org/abs/2505.07819)|null|
|**2025-05-12**|**Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models**|Jia-Bin Huang Team|[2505.07815](http://arxiv.org/abs/2505.07815)|null|
|**2025-05-12**|**Improving Trajectory Stitching with Flow Models**|Ioannis Havoutis Team|[2505.07802](http://arxiv.org/abs/2505.07802)|null|
|**2025-05-12**|**Guiding Data Collection via Factored Scaling Curves**|Anirudha Majumdar Team|[2505.07728](http://arxiv.org/abs/2505.07728)|null|
|**2025-05-12**|**GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion**|Peng Yin Team|[2505.07455](http://arxiv.org/abs/2505.07455)|null|
|**2025-05-12**|**ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning**|Donglin Wang Team|[2505.07395](http://arxiv.org/abs/2505.07395)|null|
|**2025-05-11**|**X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real**|Sanjiban Choudhury Team|[2505.07096](http://arxiv.org/abs/2505.07096)|null|
|**2025-05-11**|**YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action**|Bailing Tian Team|[2505.06923](http://arxiv.org/abs/2505.06923)|null|
|**2025-05-10**|**JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes**|Harish Ravichandar Team|[2505.06771](http://arxiv.org/abs/2505.06771)|null|
|**2025-05-10**|**Learned IMU Bias Prediction for Invariant Visual Inertial Odometry**|Nikolay Atanasov Team|[2505.06748](http://arxiv.org/abs/2505.06748)|null|
|**2025-05-10**|**ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation**|Zixian Yue Team|[2505.06628](http://arxiv.org/abs/2505.06628)|null|
|**2025-05-10**|**Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach**|Xiaokang Yang Team|[2505.06482](http://arxiv.org/abs/2505.06482)|null|
|**2025-05-09**|**Adaptive Wiping: Adaptive contact-rich manipulation through few-shot imitation learning with Force-Torque feedback and pre-trained object representations**|Gentiane Venture Team|[2505.06451](http://arxiv.org/abs/2505.06451)|null|
|**2025-05-09**|**VIN-NBV: A View Introspection Network for Next-Best-View Selection for Resource-Efficient 3D Reconstruction**|Roni Sengupta Team|[2505.06219](http://arxiv.org/abs/2505.06219)|null|
|**2025-05-09**|**Neuro-Symbolic Concepts**|Jiajun Wu Team|[2505.06191](http://arxiv.org/abs/2505.06191)|null|
|**2025-05-07**|**Efficient Sensorimotor Learning for Open-world Robot Manipulation**|Yifeng Zhu Team|[2505.06136](http://arxiv.org/abs/2505.06136)|null|
|**2025-05-09**|**Robot Learning Using Multi-Coordinate Elastic Maps**|Reza Azadeh Team|[2505.06092](http://arxiv.org/abs/2505.06092)|null|
|**2025-05-09**|**TREND: Tri-teaching for Robust Preference-based Reinforcement Learning with Demonstrations**|Abhinav Shrivastava Team|[2505.06079](http://arxiv.org/abs/2505.06079)|null|
|**2025-05-09**|**3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks**|Farshad Khorrami Team|[2505.05800](http://arxiv.org/abs/2505.05800)|null|
|**2025-05-09**|**Demystifying Diffusion Policies: Action Memorization and Simple Lookup Table Alternatives**|Mac Schwager Team|[2505.05787](http://arxiv.org/abs/2505.05787)|null|
|**2025-05-09**|**FlowHFT: Flow Policy Induced Optimal High-Frequency Trading under Diverse Market Conditions**|Steve Yang Team|[2505.05784](http://arxiv.org/abs/2505.05784)|null|
|**2025-05-08**|**CLAM: Continuous Latent Action Models for Robot Learning from Unlabeled Demonstrations**|Stephen Tu Team|[2505.04999](http://arxiv.org/abs/2505.04999)|null|
|**2025-05-08**|**CubeDAgger: Improved Robustness of Interactive Imitation Learning without Violation of Dynamic Stability**|Taisuke Kobayashi Team|[2505.04897](http://arxiv.org/abs/2505.04897)|null|
|**2025-05-08**|**D-CODA: Diffusion for Coordinated Dual-Arm Data Augmentation**|Daniel Seita Team|[2505.04860](http://arxiv.org/abs/2505.04860)|null|
|**2025-05-07**|**Steerable Scene Generation with Post Training and Inference-Time Search**|Russ Tedrake Team|[2505.04831](http://arxiv.org/abs/2505.04831)|null|
|**2025-05-07**|**Primal-dual algorithm for contextual stochastic combinatorial optimization**|Axel Parmentier Team|[2505.04757](http://arxiv.org/abs/2505.04757)|null|
|**2025-05-07**|**Merging and Disentangling Views in Visual Reinforcement Learning for Robotic Manipulation**|Henrik I. Christensen Team|[2505.04619](http://arxiv.org/abs/2505.04619)|null|
|**2025-05-06**|**OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation**|Donglin Wang Team|[2505.03912](http://arxiv.org/abs/2505.03912)|null|
|**2025-05-06**|**AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control**|Xiaolong Wang Team|[2505.03738](http://arxiv.org/abs/2505.03738)|null|
|**2025-05-06**|**Meta-Optimization and Program Search using Language Models for Task and Motion Planning**|Marc Toussaint Team|[2505.03725](http://arxiv.org/abs/2505.03725)|null|
|**2025-05-06**|**Ergodic Generative Flows**|Yinchuan Li Team|[2505.03561](http://arxiv.org/abs/2505.03561)|null|
|**2025-05-06**|**RIFT: Closed-Loop RL Fine-Tuning for Realistic and Controllable Traffic Simulation**|Sifa Zheng Team|[2505.03344](http://arxiv.org/abs/2505.03344)|null|
|**2025-05-06**|**The Unreasonable Effectiveness of Discrete-Time Gaussian Process Mixtures for Robot Policy Learning**|Abhinav Valada Team|[2505.03296](http://arxiv.org/abs/2505.03296)|null|
|**2025-05-05**|**Sim2Real Transfer for Vision-Based Grasp Verification**|Markus Vincze Team|[2505.03046](http://arxiv.org/abs/2505.03046)|**[link](https://github.com/pauamargant/hsr-graspsynth)**|
|**2025-05-05**|**Zero-shot Sim2Real Transfer for Magnet-Based Tactile Sensor on Insertion Tasks**|Jia Deng Team|[2505.02915](http://arxiv.org/abs/2505.02915)|null|
|**2025-05-05**|**Re-purposing a modular origami manipulator into an adaptive physical computer for machine learning and robotic perception**|Suyi Li Team|[2505.02744](http://arxiv.org/abs/2505.02744)|null|
|**2025-05-05**|**Spatiotemporal Non-Uniformity-Aware Online Task Scheduling in Collaborative Edge Computing for Industrial Internet of Things**|Bo Lei Team|[2505.02597](http://arxiv.org/abs/2505.02597)|null|
|**2025-05-05**|**Automated Hybrid Reward Scheduling via Large Language Models for Robotic Skill Learning**|Jianqiang Li Team|[2505.02483](http://arxiv.org/abs/2505.02483)|null|
|**2025-05-05**|**MetaScenes: Towards Automated Replica Creation for Real-world 3D Scans**|Siyuan Huang Team|[2505.02388](http://arxiv.org/abs/2505.02388)|null|
|**2025-05-04**|**Coupled Distributional Random Expert Distillation for World Model Online Imitation Learning**|Hao Su Team|[2505.02228](http://arxiv.org/abs/2505.02228)|null|
|**2025-05-04**|**CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation**|Hao Dong Team|[2505.02166](http://arxiv.org/abs/2505.02166)|null|
|**2025-05-04**|**Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions**|Mingyu Ding Team|[2505.02152](http://arxiv.org/abs/2505.02152)|null|
|**2025-05-03**|**Act Natural! Extending Naturalistic Projection to Multimodal Behavior Scenarios**|David Fridovich-Keil Team|[2505.01945](http://arxiv.org/abs/2505.01945)|null|
|**2025-05-07**|**RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation**|Xiaodan Liang Team|[2505.01709](http://arxiv.org/abs/2505.01709)|null|
|**2025-05-02**|**FalconWing: An Open-Source Platform for Ultra-Light Fixed-Wing Aircraft Research**|Sayan Mitra Team|[2505.01383](http://arxiv.org/abs/2505.01383)|null|
|**2025-05-06**|**Robotic Visual Instruction**|Xianzheng Ma Team|[2505.00693](http://arxiv.org/abs/2505.00693)|null|
|**2025-05-01**|**Towards Autonomous Micromobility through Scalable Urban Simulation**|Bolei Zhou Team|[2505.00690](http://arxiv.org/abs/2505.00690)|null|
|**2025-05-01**|**DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation**|Yang Gao Team|[2505.00527](http://arxiv.org/abs/2505.00527)|null|
|**2025-05-01**|**Optimal Interactive Learning on the Job via Facility Location Planning**|George Konidaris Team|[2505.00490](http://arxiv.org/abs/2505.00490)|null|
|**2025-04-30**|**LLM-based Interactive Imitation Learning for Robotic Manipulation**|Stefan Wermter Team|[2504.21769](http://arxiv.org/abs/2504.21769)|null|
|**2025-04-30**|**RoboGround: Robotic Manipulation with Grounded Vision-Language Priors**|Zhou Zhao Team|[2504.21530](http://arxiv.org/abs/2504.21530)|null|
|**2025-04-30**|**Provably-Safe, Online System Identification**|Ram Vasudevan Team|[2504.21486](http://arxiv.org/abs/2504.21486)|null|
|**2025-04-29**|**TesserAct: Learning 4D Embodied World Models**|Chuang Gan Team|[2504.20995](http://arxiv.org/abs/2504.20995)|null|
|**2025-04-29**|**XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search**|Elena Shrestha Team|[2504.20969](http://arxiv.org/abs/2504.20969)|null|
|**2025-04-29**|**PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations**|Xuguang Lan Team|[2504.20520](http://arxiv.org/abs/2504.20520)|null|
|**2025-04-29**|**SPARK Hand: Scooping-Pinching Adaptive Robotic Hand with Kempe Mechanism for Vertical Passive Grasp in Environmental Constraints**|Wenzeng Zhang Team|[2504.20506](http://arxiv.org/abs/2504.20506)|null|
|**2025-04-28**|**UTTG_ A Universal Teleoperation Approach via Online Trajectory Generation**|Hesheng Wang Team|[2504.19736](http://arxiv.org/abs/2504.19736)|null|
|**2025-04-28**|**GPA-RAM: Grasp-Pretraining Augmented Robotic Attention Mamba for Spatial Task Learning**|Mengyuan Liu Team|[2504.19683](http://arxiv.org/abs/2504.19683)|null|
|**2025-04-27**|**PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies**|Edward Adelson Team|[2504.19341](http://arxiv.org/abs/2504.19341)|null|
|**2025-04-29**|**Learned Perceptive Forward Dynamics Model for Safe and Platform-aware Robotic Navigation**|Marco Hutter Team|[2504.19322](http://arxiv.org/abs/2504.19322)|**[link](https://github.com/leggedrobotics/fdm)**|
|**2025-04-27**|**Learning to Drive from a World Model**|Yassine Yousfi Team|[2504.19077](http://arxiv.org/abs/2504.19077)|null|
|**2025-04-26**|**RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning**|Pieter Abbeel Team|[2504.18904](http://arxiv.org/abs/2504.18904)|null|
|**2025-04-26**|**Imitation Learning for Autonomous Driving: Insights from Real-World Testing**|Tufan Kumbasar Team|[2504.18847](http://arxiv.org/abs/2504.18847)|null|
|**2025-04-26**|**Hierarchical Reinforcement Learning in Multi-Goal Spatial Navigation with Autonomous Mobile Robots**|Alfredo Weitzenfeld Team|[2504.18794](http://arxiv.org/abs/2504.18794)|null|
|**2025-04-26**|**STDArm: Transferring Visuomotor Policies From Static Data Training to Dynamic Robot Manipulation**|Yanyong Zhang Team|[2504.18792](http://arxiv.org/abs/2504.18792)|null|
|**2025-04-25**|**Generalization Capability for Imitation Learning**|Yixiao Wang Team|[2504.18538](http://arxiv.org/abs/2504.18538)|null|
|**2025-04-25**|**Instrumentation for Better Demonstrations: A Case Study**|Francis wyffels Team|[2504.18481](http://arxiv.org/abs/2504.18481)|null|
|**2025-04-25**|**Action Flow Matching for Continual Robot Learning**|Lantao Liu Team|[2504.18471](http://arxiv.org/abs/2504.18471)|null|
|**2025-04-25**|**Design and Evaluation of a UGV-Based Robotic Platform for Precision Soil Moisture Remote Sensing**|George Nikolakopoulos Team|[2504.18284](http://arxiv.org/abs/2504.18284)|null|
|**2025-04-28**|**Implementation Analysis of Collaborative Robot Digital Twins in Physics Engines**|Hans D. Schotten Team|[2504.18200](http://arxiv.org/abs/2504.18200)|null|
|**2025-04-25**|**Offline Learning of Controllable Diverse Behaviors**|Ludovic Denoyer Team|[2504.18160](http://arxiv.org/abs/2504.18160)|null|
|**2025-04-24**|**CIVIL: Causal and Intuitive Visual Imitation Learning**|Dylan P. Losey Team|[2504.17959](http://arxiv.org/abs/2504.17959)|null|
|**2025-04-24**|**Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning**|Prithviraj Ammanabrolu Team|[2504.17950](http://arxiv.org/abs/2504.17950)|null|
|**2025-04-24**|**Learning Attentive Neural Processes for Planning with Pushing Actions**|Nicholas Roy Team|[2504.17924](http://arxiv.org/abs/2504.17924)|null|
|**2025-04-24**|**CaRL: Learning Scalable Planning Policies with Simple Rewards**|Andreas Geiger Team|[2504.17838](http://arxiv.org/abs/2504.17838)|null|
|**2025-04-23**|**Learning Underwater Active Perception in Simulation**|Donald G. Dansereau Team|[2504.17817](http://arxiv.org/abs/2504.17817)|null|
|**2025-04-24**|**Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation**|Jiangmiao Pang Team|[2504.17784](http://arxiv.org/abs/2504.17784)|null|
|**2025-04-24**|**Integrating Learning-Based Manipulation and Physics-Based Locomotion for Whole-Body Badminton Robot Control**|Dong Xuan Team|[2504.17771](http://arxiv.org/abs/2504.17771)|null|
|**2025-04-24**|**Robotic Grinding Skills Learning Based on Geodesic Length Dynamic Motion Primitives**|Han Ding Team|[2504.17216](http://arxiv.org/abs/2504.17216)|null|
|**2025-04-23**|**Geometric Formulation of Unified Force-Impedance Control on SE(3) for Robotic Manipulators**|Roberto Horowitz Team|[2504.17080](http://arxiv.org/abs/2504.17080)|null|
|**2025-04-23**|**A Systematic Approach to Design Real-World Human-in-the-Loop Deep Reinforcement Learning: Salient Features, Challenges and Trade-offs**|Younes Zerouali Team|[2504.17006](http://arxiv.org/abs/2504.17006)|null|
|**2025-04-23**|**Latent Diffusion Planning for Imitation Learning**|Chelsea Finn Team|[2504.16925](http://arxiv.org/abs/2504.16925)|null|
|**2025-04-23**|**MOSAIC: A Skill-Centric Algorithmic Framework for Long-Horizon Manipulation Planning**|Maxim Likhachev Team|[2504.16738](http://arxiv.org/abs/2504.16738)|null|
|**2025-04-23**|**ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance**|Shanghang Zhang Team|[2504.16464](http://arxiv.org/abs/2504.16464)|null|
|**2025-04-22**|**Mass-Adaptive Admittance Control for Robotic Manipulators**|Logan E. Beaver Team|[2504.16224](http://arxiv.org/abs/2504.16224)|null|
|**2025-04-22**|**$π_{0.5}$ : a Vision-Language-Action Model with Open-World Generalization**|Ury Zhilinsky Team|[2504.16054](http://arxiv.org/abs/2504.16054)|null|
|**2025-04-22**|**SPECI: Skill Prompts based Hierarchical Continual Imitation Learning for Robot Manipulation**|Xiangli Nie Team|[2504.15561](http://arxiv.org/abs/2504.15561)|null|
|**2025-04-22**|**VibeCheck: Using Active Acoustic Tactile Sensing for Contact-Rich Manipulation**|Matei Ciocarlie Team|[2504.15535](http://arxiv.org/abs/2504.15535)|null|
|**2025-04-22**|**Few-Shot Vision-Language Action-Incremental Policy Learning**|Weili Guan Team|[2504.15517](http://arxiv.org/abs/2504.15517)|null|
|**2025-04-21**|**LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning**|Boyuan Chen Team|[2504.15472](http://arxiv.org/abs/2504.15472)|null|
|**2025-04-23**|**Advancing Embodied Intelligence in Robotic-Assisted Endovascular Procedures: A Systematic Review of AI Solutions**|Peng Qi Team|[2504.15327](http://arxiv.org/abs/2504.15327)|null|
|**2025-04-21**|**Immersive Teleoperation Framework for Locomanipulation Tasks**|Dimitrios Kanoulas Team|[2504.15229](http://arxiv.org/abs/2504.15229)|null|
|**2025-04-21**|**A Genetic Fuzzy-Enabled Framework on Robotic Manipulation for In-Space Servicing**|Kelly Cohen Team|[2504.15226](http://arxiv.org/abs/2504.15226)|null|
|**2025-04-21**|**A General Infrastructure and Workflow for Quadrotor Deep Reinforcement Learning and Reality Deployment**|Huaping Liu Team|[2504.15129](http://arxiv.org/abs/2504.15129)|null|
|**2025-04-21**|**SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks**|Animesh Garg Team|[2504.14857](http://arxiv.org/abs/2504.14857)|null|
|**2025-04-20**|**Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline**|Hongsheng Li Team|[2504.14709](http://arxiv.org/abs/2504.14709)|null|
|**2025-04-24**|**Latent Representations for Visual Proprioception in Inexpensive Robots**|Ladislau Bölöni Team|[2504.14634](http://arxiv.org/abs/2504.14634)|null|
|**2025-04-18**|**DiffOG: Differentiable Policy Trajectory Optimization with Generalizability**|Yu She Team|[2504.13807](http://arxiv.org/abs/2504.13807)|null|
|**2025-04-18**|**Imitation Learning with Precisely Labeled Human Demonstrations**|Yilong Song Team|[2504.13803](http://arxiv.org/abs/2504.13803)|null|
|**2025-04-21**|**SLAM&Render: A Benchmark for the Intersection Between Neural Rendering, Gaussian Splatting and SLAM**|Javier Civera Team|[2504.13713](http://arxiv.org/abs/2504.13713)|**[link](https://github.com/samuel-cerezo/SLAM-Render)**|
|**2025-04-18**|**Self-Mixing Laser Interferometry: In Search of an Ambient Noise-Resilient Alternative to Acoustic Sensing**|Francis wyffels Team|[2504.13711](http://arxiv.org/abs/2504.13711)|null|
|**2025-04-18**|**On the Importance of Tactile Sensing for Imitation Learning: A Case Study on Robotic Match Lighting**|Jan Peters Team|[2504.13618](http://arxiv.org/abs/2504.13618)|null|
|**2025-04-18**|**A Model-Based Approach to Imitation Learning through Multi-Step Predictions**|Na Li Team|[2504.13413](http://arxiv.org/abs/2504.13413)|null|
|**2025-04-17**|**RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins**|Ping Luo Team|[2504.13059](http://arxiv.org/abs/2504.13059)|null|
|**2025-04-17**|**Adaptive Task Space Non-Singular Terminal Super-Twisting Sliding Mode Control of a 7-DOF Robotic Manipulator**|E. Witrant Team|[2504.13056](http://arxiv.org/abs/2504.13056)|null|
|**2025-04-17**|**Krysalis Hand: A Lightweight, High-Payload, 18-DoF Anthropomorphic End-Effector for Robotic Learning and Dexterous Manipulation**|Iman Soltani Team|[2504.12967](http://arxiv.org/abs/2504.12967)|null|
|**2025-04-17**|**TSGS: Improving Gaussian Splatting for Transparent Surface Reconstruction via Normal and De-lighting Priors**|Yi Yang Team|[2504.12799](http://arxiv.org/abs/2504.12799)|null|
|**2025-04-17**|**Trajectory Adaptation using Large Language Models**|Ravi Prakash Team|[2504.12755](http://arxiv.org/abs/2504.12755)|null|
|**2025-04-17**|**Embodied Neuromorphic Control Applied on a 7-DOF Robotic Manipulator**|Lei Wang Team|[2504.12702](http://arxiv.org/abs/2504.12702)|**[link](https://bitbucket.org/icubdataset/inverse-dynamic)**|
|**2025-04-21**|**A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation**|Xiaodan Liang Team|[2504.12636](http://arxiv.org/abs/2504.12636)|null|
|**2025-04-17**|**Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration**|Jeannette Bohg Team|[2504.12609](http://arxiv.org/abs/2504.12609)|null|
|**2025-04-16**|**Adapting a World Model for Trajectory Following in a 3D Game**|Raluca Georgescu Team|[2504.12299](http://arxiv.org/abs/2504.12299)|null|
|**2025-04-16**|**Towards Forceful Robotic Foundation Models: a Literature Survey**|Nikolaus Correll Team|[2504.11827](http://arxiv.org/abs/2504.11827)|null|
|**2025-04-17**|**Toward Aligning Human and Robot Actions via Multi-Modal Demonstration Learning**|Fei Liu Team|[2504.11493](http://arxiv.org/abs/2504.11493)|null|
|**2025-04-15**|**Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks**|Suryansh Kumar Team|[2504.11247](http://arxiv.org/abs/2504.11247)|null|
|**2025-04-17**|**CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image**|Yi Zhu Team|[2504.11230](http://arxiv.org/abs/2504.11230)|null|
|**2025-04-15**|**Superfast Configuration-Space Convex Set Computation on GPUs for Online Motion Planning**|Daniela Rus Team|[2504.10783](http://arxiv.org/abs/2504.10783)|**[link](https://github.com/wernerpe/csdecomp)**|
|**2025-04-14**|**Improving In-Context Learning with Reasoning Distillation**|Xiang Gao Team|[2504.10647](http://arxiv.org/abs/2504.10647)|null|
|**2025-04-14**|**Flying Hand: End-Effector-Centric Framework for Versatile Aerial Manipulation Teleoperation and Policy Learning**|Guanya Shi Team|[2504.10334](http://arxiv.org/abs/2504.10334)|null|
|**2025-04-14**|**Look-to-Touch: A Vision-Enhanced Proximity and Tactile Sensor for Distance and Geometry Perception in Robotic Manipulation**|Guoying Gu Team|[2504.10280](http://arxiv.org/abs/2504.10280)|null|
|**2025-04-14**|**Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models**|Hui Cheng Team|[2504.10041](http://arxiv.org/abs/2504.10041)|**[link](https://github.com/hren20/naivibridger)**|
|**2025-04-14**|**Efficient Task-specific Conditional Diffusion Policies: Shortcut Model Acceleration and SO(3) Optimization**|Wei Sui Team|[2504.09927](http://arxiv.org/abs/2504.09927)|null|
|**2025-04-12**|**Compliant Explicit Reference Governor for Contact Friendly Robotic Manipulators**|Marco M. Nicotra Team|[2504.09188](http://arxiv.org/abs/2504.09188)|null|
|**2025-04-11**|**BiFlex: A Passive Bimodal Stiffness Flexible Wrist for Manipulation in Unstructured Environments**|Roberto Martín-Martín Team|[2504.08706](http://arxiv.org/abs/2504.08706)|null|
|**2025-04-11**|**Diffusion Models for Robotic Manipulation: A Survey**|Rania Rayyes Team|[2504.08438](http://arxiv.org/abs/2504.08438)|null|
|**2025-04-10**|**Echo: An Open-Source, Low-Cost Teleoperation System with Force Feedback for Dataset Collection in Robot Learning**|Dzmitry Tsetserukou Team|[2504.07939](http://arxiv.org/abs/2504.07939)|null|
|**2025-04-10**|**TOCALib: Optimal control library with interpolation for bimanual manipulation and obstacles avoidance**|Aleksandr Panov Team|[2504.07708](http://arxiv.org/abs/2504.07708)|null|
|**2025-04-10**|**Novel Diffusion Models for Multimodal 3D Hand Trajectory Prediction**|Hesheng Wang Team|[2504.07375](http://arxiv.org/abs/2504.07375)|**[link](https://github.com/irmvlab/mmtwin)**|
|**2025-04-09**|**Adaptive Vision-Guided Robotic Arm Control for Precision Pruning in Dynamic Orchard Environments**|Manoj Karkee Team|[2504.07309](http://arxiv.org/abs/2504.07309)|null|
|**2025-04-09**|**AssistanceZero: Scalably Solving Assistance Games**|Anca Dragan Team|[2504.07091](http://arxiv.org/abs/2504.07091)|**[link](https://github.com/cassidylaidlaw/minecraft-building-assistance-game)**|
|**2025-04-09**|**Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation**|Huazhe Xu Team|[2504.06961](http://arxiv.org/abs/2504.06961)|null|
|**2025-04-09**|**Developing Modular Grasping and Manipulation Pipeline Infrastructure to Streamline Performance Benchmarking**|Holly Yanco Team|[2504.06819](http://arxiv.org/abs/2504.06819)|null|
|**2025-04-09**|**Interactive Expressive Motion Generation Using Dynamic Movement Primitives**|Kai O. Arras Team|[2504.06735](http://arxiv.org/abs/2504.06735)|null|
|**2025-04-09**|**Overcoming Dynamic Environments: A Hybrid Approach to Motion Planning for Manipulators**|Gavin Paul Team|[2504.06596](http://arxiv.org/abs/2504.06596)|null|
|**2025-04-09**|**CAFE-AD: Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving**|Yanyong Zhang Team|[2504.06584](http://arxiv.org/abs/2504.06584)|**[link](https://github.com/alniyatrui/cafe-ad)**|
|**2025-04-09**|**OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning**|Tyler Fenstermaker Team|[2504.06538](http://arxiv.org/abs/2504.06538)|null|
|**2025-04-08**|**ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface**|Rui Chen Team|[2504.06156](http://arxiv.org/abs/2504.06156)|null|
|**2025-04-08**|**MAPLE: Encoding Dexterous Robotic Manipulation Priors Learned From Egocentric Videos**|Marc Pollefeys Team|[2504.06084](http://arxiv.org/abs/2504.06084)|null|
|**2025-04-08**|**Learning-enhanced electronic skin for tactile sensing on deformable surface based on electrical impedance tomography**|Yunjie Yang Team|[2504.05987](http://arxiv.org/abs/2504.05987)|null|
|**2025-04-08**|**Stratified Expert Cloning with Adaptive Selection for User Retention in Large-Scale Recommender Systems**|Yongqi Liu Team|[2504.05628](http://arxiv.org/abs/2504.05628)|null|
|**2025-04-08**|**TW-CRL: Time-Weighted Contrastive Reward Learning for Efficient Inverse Reinforcement Learning**|Stephen Xia Team|[2504.05585](http://arxiv.org/abs/2504.05585)|null|
|**2025-04-07**|**SPARK-Remote: A Cost-Effective System for Remote Bimanual Robot Teleoperation**|Karthik Desingh Team|[2504.05488](http://arxiv.org/abs/2504.05488)|null|
|**2025-04-07**|**RobustDexGrasp: Robust Dexterous Grasping of General Objects from Single-view Perception**|Jie Song Team|[2504.05287](http://arxiv.org/abs/2504.05287)|null|
|**2025-04-07**|**Vision-Language Model Predictive Control for Manipulation Planning and Trajectory Generation**|Wei Zhang Team|[2504.05225](http://arxiv.org/abs/2504.05225)|**[link](https://github.com/ppjmchen/vlmpc)**|
|**2025-04-07**|**Wavelet Policy: Imitation Policy Learning in Frequency Domain with Wavelet Transforms**|Hongrui Zhu Team|[2504.04991](http://arxiv.org/abs/2504.04991)|null|
|**2025-04-07**|**Embodied Perception for Test-time Grasping Detection Adaptation with Knowledge Infusion**|Fengyu Zhou Team|[2504.04795](http://arxiv.org/abs/2504.04795)|null|
|**2025-04-06**|**Tool-as-Interface: Learning Robot Policies from Human Tool Usage through Imitation Learning**|Katherine Driggs-Campbell Team|[2504.04612](http://arxiv.org/abs/2504.04612)|null|
|**2025-04-06**|**Diffusion-Based Approximate MPC: Fast and Consistent Imitation of Multi-Modal Action Distributions**|Katherine J. Kuchenbecker Team|[2504.04603](http://arxiv.org/abs/2504.04603)|null|
|**2025-04-06**|**DexTOG: Learning Task-Oriented Dexterous Grasp with Language**|Cewu Lu Team|[2504.04573](http://arxiv.org/abs/2504.04573)|null|
|**2025-04-06**|**DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation and Grasping in Cluttered Environments**|Lin Shao Team|[2504.04516](http://arxiv.org/abs/2504.04516)|null|
|**2025-04-06**|**Human-Level Competitive Pokémon via Scalable Offline Reinforcement Learning with Transformers**|Yuke Zhu Team|[2504.04395](http://arxiv.org/abs/2504.04395)|null|
|**2025-04-05**|**ORCA: An Open-Source, Reliable, Cost-Effective, Anthropomorphic Robotic Hand for Uninterrupted Dexterous Task Learning**|Robert K. Katzschmann Team|[2504.04259](http://arxiv.org/abs/2504.04259)|null|
|**2025-04-09**|**Digital Gene: Learning about the Physical World through Analytic Concepts**|Cewu Lu Team|[2504.04170](http://arxiv.org/abs/2504.04170)|null|
|**2025-04-04**|**Dexterous Manipulation through Imitation Learning: A Survey**|Hong Zhang Team|[2504.03515](http://arxiv.org/abs/2504.03515)|null|
|**2025-04-04**|**GraphSeg: Segmented 3D Representations via Graph Edge Addition and Contraction**|Weiming Zhi Team|[2504.03129](http://arxiv.org/abs/2504.03129)|null|
|**2025-04-03**|**Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets**|Abhishek Gupta Team|[2504.02792](http://arxiv.org/abs/2504.02792)|null|
|**2025-04-03**|**Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision**|Shibiao Xu Team|[2504.02477](http://arxiv.org/abs/2504.02477)|null|
|**2025-04-02**|**RoboAct-CLIP: Video-Driven Pre-training of Atomic Action Understanding for Robotics**|Qiang Nie Team|[2504.02069](http://arxiv.org/abs/2504.02069)|null|
|**2025-04-02**|**Slot-Level Robotic Placement via Visual Imitation from Single Human Video**|Arsalan Mousavian Team|[2504.01959](http://arxiv.org/abs/2504.01959)|null|
|**2025-04-02**|**Learning with Imperfect Models: When Multi-step Prediction Mitigates Compounding Error**|Nikolai Matni Team|[2504.01766](http://arxiv.org/abs/2504.01766)|null|
|**2025-04-02**|**TransforMerger: Transformer-based Voice-Gesture Fusion for Robust Human-Robot Communication**|Karla Stepanova Team|[2504.01708](http://arxiv.org/abs/2504.01708)|null|
|**2025-04-02**|**8-DoFs Cable Driven Parallel Robots for Bimanual Teleportation**|Josie Hughes Team|[2504.01554](http://arxiv.org/abs/2504.01554)|null|
|**2025-04-02**|**Bi-LAT: Bilateral Control-Based Imitation Learning via Natural Language and Action Chunking with Transformers**|Yuki Uranishi Team|[2504.01301](http://arxiv.org/abs/2504.01301)|null|
|**2025-04-02**|**The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction**|Matthew K. X. J Pan Team|[2504.01260](http://arxiv.org/abs/2504.01260)|null|
|**2025-04-01**|**Energy Weighted Learning Progress Guided Interleaved Multi-Task Learning**|Erhan Oztop Team|[2504.00707](http://arxiv.org/abs/2504.00707)|null|
|**2025-04-01**|**Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs**|Masaya Kinoshita Team|[2504.00614](http://arxiv.org/abs/2504.00614)|null|
|**2025-04-01**|**Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation**|Dong Wang Team|[2504.00420](http://arxiv.org/abs/2504.00420)|null|
|**2025-03-31**|**CBIL: Collective Behavior Imitation Learning for Fish from Real Videos**|Taku Komura Team|[2504.00234](http://arxiv.org/abs/2504.00234)|null|
|**2025-04-02**|**Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation**|Yuke Zhu Team|[2503.24361](http://arxiv.org/abs/2503.24361)|null|
|**2025-04-02**|**AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World**|Sergey Levine Team|[2503.24278](http://arxiv.org/abs/2503.24278)|**[link](https://github.com/zhouzypaul/auto_eval)**|
|**2025-03-31**|**HACTS: a Human-As-Copilot Teleoperation System for Robot Learning**|Jian Tang Team|[2503.24070](http://arxiv.org/abs/2503.24070)|null|
|**2025-03-31**|**Learning 3D-Gaussian Simulators from RGB Videos**|Georg Martius Team|[2503.24009](http://arxiv.org/abs/2503.24009)|null|
|**2025-03-31**|**ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos**|Dinesh Jayaraman Team|[2503.23877](http://arxiv.org/abs/2503.23877)|**[link](https://github.com/everloom-129/rekep)**|
|**2025-03-31**|**Disambiguate Gripper State in Grasp-Based Tasks: Pseudo-Tactile as Feedback Enables Pure Simulation Learning**|Yue Wang Team|[2503.23835](http://arxiv.org/abs/2503.23835)|null|
|**2025-03-30**|**Can Visuo-motor Policies Benefit from Random Exploration Data? A Case Study on Stacking**|Florian T. Pokorny Team|[2503.23571](http://arxiv.org/abs/2503.23571)|null|
|**2025-08-26**|**BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities**|Li Fei-Fei Team|[2503.05652](http://arxiv.org/abs/2503.05652)|**[link](https://behavior-robot-suite.github.io/)**|
|**2024-12-17**|**TidyBot++: An Open-Source Holonomic Mobile Manipulator for Robot Learning**|Jeannette Bohg Team|[2412.10447](http://arxiv.org/abs/2412.10447)|**[link](https://tidybot2.github.io)**|
|**2025-01-08**|**3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing**|Yunzhu Li Team|[2410.24091](http://arxiv.org/abs/2410.24091)|null|
|**2024-10-24**|**SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation**|Ajay Mandlekar Team|[2410.18065](http://arxiv.org/abs/2410.18065)|null|
|**2024-11-05**|**ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data**|Shuran Song Team|[2406.19464](http://arxiv.org/abs/2406.19464)|**[link](https://maniwav.github.io/)**|
|**2023-10-31**|**Learning Robot Manipulation from Cross-Morphology Demonstration**|Gaurav Sukhatme Team|[2304.03833](http://arxiv.org/abs/2304.03833)|null|
|**2022-11-17**|**ToolFlowNet: Robotic Manipulation with Tools via Predicting Tool Flow from Point Clouds**|David Held Team|[2211.09006](http://arxiv.org/abs/2211.09006)|**[link](https://sites.google.com/view/point-cloud-policy/home)**|
|**2022-11-16**|**Learning and Retrieval from Prior Data for Skill-based Imitation Learning**|Yuke Zhu Team|[2210.11435](http://arxiv.org/abs/2210.11435)|null|
|**2023-03-09**|**VIOLA: Imitation Learning for Vision-Based Manipulation with Object Proposal Priors**|Yuke Zhu Team|[2210.11339](http://arxiv.org/abs/2210.11339)|null|
|**2022-10-12**|**Self-Supervised Learning of Multi-Object Keypoints for Robotic Manipulation**|Abhinav Valada Team|[2205.08316](http://arxiv.org/abs/2205.08316)|null|
|**2022-11-21**|**R3M: A Universal Visual Representation for Robot Manipulation**|Abhinav Gupta Team|[2203.12601](http://arxiv.org/abs/2203.12601)|null|
|**2022-02-07**|**BC-Z: Zero-Shot Task Generalization with Robotic Imitation Learning**|Chelsea Finn Team|[2202.02005](http://arxiv.org/abs/2202.02005)|null|
|**2021-11-02**|**Learning Language-Conditioned Robot Behavior from Offline Data and Crowd-Sourced Annotation**|Chelsea Finn Team|[2109.01115](http://arxiv.org/abs/2109.01115)|null|
|**2021-06-11**|**Coarse-to-Fine Imitation Learning: Robot Manipulation from a Single Demonstration**|Edward Johns Team|[2105.06411](http://arxiv.org/abs/2105.06411)|**[link](https://www.robot-learning.uk/coarse-to-fine-imitation-learning)**|
|**2022-03-09**|**Interactive Imitation Learning in State-Space**|Jens Kober Team|[2008.00524](http://arxiv.org/abs/2008.00524)|null|
|**2020-05-19**|**On-Policy Robot Imitation Learning from a Converging Supervisor**|Ken Goldberg Team|[1907.03423](http://arxiv.org/abs/1907.03423)|null|
|**2018-11-08**|**RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through Imitation**|Li Fei-Fei Team|[1811.02790](http://arxiv.org/abs/1811.02790)|null|
|**2018-10-09**|**Robustness via Retrying: Closed-Loop Robotic Manipulation with Self-Supervised Learning**|Chelsea Finn Team|[1810.03043](http://arxiv.org/abs/1810.03043)|null|
|**2017-10-27**|**Learning Robotic Manipulation of Granular Media**|Sergey Levine Team|[1709.02833](http://arxiv.org/abs/1709.02833)|null|

## VLM

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning**|Xinggang Wang Team|[2602.21186](http://arxiv.org/abs/2602.21186)|null|
|**2026-02-24**|**Seeing Through Words: Controlling Visual Retrieval Quality with Language Models**|Yun Fu Team|[2602.21175](http://arxiv.org/abs/2602.21175)|null|
|**2026-02-24**|**LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis**|Marius George Linguraru Team|[2602.21142](http://arxiv.org/abs/2602.21142)|null|
|**2026-02-24**|**VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation**|Sharon Li Team|[2602.21054](http://arxiv.org/abs/2602.21054)|null|
|**2026-02-24**|**OCR-Agent: Agentic OCR with Capability and Memory Reflection**|Ying Cai Team|[2602.21053](http://arxiv.org/abs/2602.21053)|null|
|**2026-02-24**|**Not Just What's There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning**|Zejiang He Team|[2602.21035](http://arxiv.org/abs/2602.21035)|null|
|**2026-02-24**|**From Perception to Action: An Interactive Benchmark for Vision Reasoning**|Roy Ka-Wei Lee Team|[2602.21015](http://arxiv.org/abs/2602.21015)|**[link](https://social-ai-studio.github.io/CHAIN/)**|
|**2026-02-24**|**SpatiaLQA: A Benchmark for Evaluating Spatial Logical Reasoning in Vision-Language Models**|Jie Song Team|[2602.20901](http://arxiv.org/abs/2602.20901)|null|
|**2026-02-24**|**Diagnosing Causal Reasoning in Vision-Language Models via Structured Relevance Graphs**|Yihao Ding Team|[2602.20878](http://arxiv.org/abs/2602.20878)|null|
|**2026-02-24**|**MUSE: Harnessing Precise and Diverse Semantics for Few-Shot Whole Slide Image Classification**|Nankun Mu Team|[2602.20873](http://arxiv.org/abs/2602.20873)|null|
|**2026-02-24**|**On the Explainability of Vision-Language Models in Art History**|Stefanie Schneider Team|[2602.20853](http://arxiv.org/abs/2602.20853)|null|
|**2026-02-24**|**GatedCLIP: Gated Multimodal Fusion for Hateful Memes Detection**|Zirong Zeng Team|[2602.20818](http://arxiv.org/abs/2602.20818)|null|
|**2026-02-24**|**VGGDrive: Empowering Vision-Language Models with Cross-View Geometric Grounding for Autonomous Driving**|Long Chen Team|[2602.20794](http://arxiv.org/abs/2602.20794)|null|
|**2026-02-24**|**NGL-Prompter: Training-Free Sewing Pattern Estimation from a Single Image**|Michael Black Team|[2602.20700](http://arxiv.org/abs/2602.20700)|null|
|**2026-02-24**|**PromptCD: Test-Time Behavior Enhancement via Polarity-Prompt Contrastive Decoding**|Xueqi Cheng Team|[2602.20696](http://arxiv.org/abs/2602.20696)|null|
|**2026-02-24**|**How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective**|Tong Xu Team|[2602.20687](http://arxiv.org/abs/2602.20687)|null|
|**2026-02-24**|**Recursive Belief Vision Language Model**|Nirav Patel Team|[2602.20659](http://arxiv.org/abs/2602.20659)|null|
|**2026-02-24**|**Vision-Language Models for Ergonomic Assessment of Manual Lifting Tasks: Estimating Horizontal and Vertical Hand Distances from RGB Video**|Maury A. Nussbaum Team|[2602.20658](http://arxiv.org/abs/2602.20658)|null|
|**2026-02-24**|**Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion**|Ziran Wang Team|[2602.20577](http://arxiv.org/abs/2602.20577)|null|
|**2026-02-24**|**An interactive enhanced driving dataset for autonomous driving**|Lu Xiong Team|[2602.20575](http://arxiv.org/abs/2602.20575)|null|
|**2026-02-23**|**NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning**|George Konidaris Team|[2602.20119](http://arxiv.org/abs/2602.20119)|**[link](https://nova-plan.github.io/)**|
|**2026-02-23**|**StructXLIP: Enhancing Vision-language Models with Multimodal Structural Cues**|Marco Cristani Team|[2602.20089](http://arxiv.org/abs/2602.20089)|null|
|**2026-02-23**|**Do Large Language Models Understand Data Visualization Principles?**|Emmanuel Iarussi Team|[2602.20084](http://arxiv.org/abs/2602.20084)|null|
|**2026-02-23**|**HeatPrompt: Zero-Shot Vision-Language Modeling of Urban Heat Demand from Satellite Images**|Veit Hagenmeyer Team|[2602.20066](http://arxiv.org/abs/2602.20066)|null|
|**2026-02-23**|**Contextual Safety Reasoning and Grounding for Open-World Robots**|George J. Pappas Team|[2602.19983](http://arxiv.org/abs/2602.19983)|null|
|**2026-02-23**|**BeamVLM for Low-altitude Economy: Generative Beam Prediction via Vision-language Models**|Chengwen Xing Team|[2602.19929](http://arxiv.org/abs/2602.19929)|null|
|**2026-02-23**|**Multi-Modal Representation Learning via Semi-Supervised Rate Reduction for Generalized Category Discovery**|Chun-Guang Li Team|[2602.19910](http://arxiv.org/abs/2602.19910)|null|
|**2026-02-23**|**ApET: Approximation-Error Guided Token Compression for Efficient VLMs**|Hairong Zheng Team|[2602.19870](http://arxiv.org/abs/2602.19870)|null|
|**2026-02-23**|**TraceVision: Trajectory-Aware Vision-Language Model for Human-Like Spatial Understanding**|Jinqiao Wang Team|[2602.19768](http://arxiv.org/abs/2602.19768)|null|
|**2026-02-23**|**Seeing Clearly, Reasoning Confidently: Plug-and-Play Remedies for Vision Language Model Blindness**|Zhengming Ding Team|[2602.19615](http://arxiv.org/abs/2602.19615)|null|
|**2026-02-23**|**VALD: Multi-Stage Vision Attack Detection for Efficient LVLM Defense**|Ayellet Tal Team|[2602.19570](http://arxiv.org/abs/2602.19570)|null|
|**2026-02-23**|**Can a Teenager Fool an AI? Evaluating Low-Cost Cosmetic Attacks on Age Estimation Systems**|Simiao Ren Team|[2602.19539](http://arxiv.org/abs/2602.19539)|null|
|**2026-02-23**|**ORION: ORthonormal Text Encoding for Universal VLM AdaptatION**|Ismail Ben Ayed Team|[2602.19530](http://arxiv.org/abs/2602.19530)|null|
|**2026-02-23**|**Forgetting-Resistant and Lesion-Aware Source-Free Domain Adaptive Fundus Image Analysis with Vision-Language Model**|Xiaomeng Li Team|[2602.19471](http://arxiv.org/abs/2602.19471)|null|
|**2026-02-23**|**Decoupling Vision and Language: Codebook Anchored Visual Adaptation**|Jonathan Wu Team|[2602.19449](http://arxiv.org/abs/2602.19449)|null|
|**2026-02-23**|**UrbanAlign: Post-hoc Semantic Calibration for VLM-Human Preference Alignment**|Chunlei Shi Team|[2602.19442](http://arxiv.org/abs/2602.19442)|null|
|**2026-02-23**|**PA-Attack: Guiding Gray-Box Attacks on LVLM Vision Encoders with Prototypes and Attention**|Minjing Dong Team|[2602.19418](http://arxiv.org/abs/2602.19418)|null|
|**2026-02-22**|**Seeing Farther and Smarter: Value-Guided Multi-Path Reflection for VLM Policy Optimization**|Dimitris N. Metaxas Team|[2602.19372](http://arxiv.org/abs/2602.19372)|null|
|**2026-02-22**|**MentalBlackboard: Evaluating Spatial Visualization via Mathematical Transformations**|Yezhou Yang Team|[2602.19357](http://arxiv.org/abs/2602.19357)|null|
|**2026-02-22**|**TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics**|Ranjay Krishna Team|[2602.19313](http://arxiv.org/abs/2602.19313)|null|
|**2026-02-20**|**CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation**|Jon Froehlich Team|[2602.18424](http://arxiv.org/abs/2602.18424)|null|
|**2026-02-20**|**Zero-shot Interactive Perception**|Amir Ghalamzan Team|[2602.18374](http://arxiv.org/abs/2602.18374)|**[link](https://openreview.net/forum?id=7MhpFcr5Nx)**|
|**2026-02-20**|**Simplifying Outcomes of Language Model Component Analyses with ELIA**|Nils Feldhus Team|[2602.18262](http://arxiv.org/abs/2602.18262)|**[link](https://github.com/aaron0eidt/ELIA)**|
|**2026-02-20**|**FENCE: A Financial and Multimodal Jailbreak Detection Dataset**|Youngjun Kwak Team|[2602.18154](http://arxiv.org/abs/2602.18154)|null|
|**2026-02-20**|**OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models**|Jingrun Chen Team|[2602.18094](http://arxiv.org/abs/2602.18094)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-19**|**Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models**|Ludwig Schmidt Team|[2602.17871](http://arxiv.org/abs/2602.17871)|null|
|**2026-02-19**|**Enabling Training-Free Text-Based Remote Sensing Segmentation**|Djamila Aouada Team|[2602.17799](http://arxiv.org/abs/2602.17799)|null|
|**2026-02-19**|**CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild**|Justus Thies Team|[2602.17770](http://arxiv.org/abs/2602.17770)|**[link](https://balamuruganthambiraja.github.io/CLUTCH/)**|
|**2026-02-19**|**Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting**|Zhiqiang Shen Team|[2602.17645](http://arxiv.org/abs/2602.17645)|**[link](https://github.com/vila-lab/M-Attack-V2)**|
|**2026-02-19**|**Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning**|Monowar Bhuyan Team|[2602.17625](http://arxiv.org/abs/2602.17625)|null|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games**|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|null|
|**2026-02-19**|**LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs**|Zongyuan Ge Team|[2602.17535](http://arxiv.org/abs/2602.17535)|null|
|**2026-02-19**|**Selective Training for Large Vision Language Models via Visual Information Gain**|Sangheum Hwang Team|[2602.17186](http://arxiv.org/abs/2602.17186)|null|
|**2026-02-18**|**Narrow fine-tuning erodes safety alignment in vision-language agents**|Shivam Raval Team|[2602.16931](http://arxiv.org/abs/2602.16931)|null|
|**2026-02-20**|**MALLVI: A Multi-Agent Framework for Integrated Generalized Robotics Manipulation**|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|null|
|**2026-02-18**|**DODO: Discrete OCR Diffusion Models**|Niv Nayman Team|[2602.16872](http://arxiv.org/abs/2602.16872)|null|
|**2026-02-18**|**Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**|Jundong Li Team|[2602.16702](http://arxiv.org/abs/2602.16702)|null|
|**2026-02-18**|**A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**|James Haworth Team|[2602.16590](http://arxiv.org/abs/2602.16590)|null|
|**2026-02-18**|**DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**|Chenfanfu Jiang Team|[2602.16502](http://arxiv.org/abs/2602.16502)|null|
|**2026-02-18**|**Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**|Dahua Lin Team|[2602.16455](http://arxiv.org/abs/2602.16455)|null|
|**2026-02-18**|**Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**|Shubham Agarwal Team|[2602.16430](http://arxiv.org/abs/2602.16430)|null|
|**2026-02-18**|**Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**|Takeo Igarashi Team|[2602.16157](http://arxiv.org/abs/2602.16157)|null|
|**2026-02-18**|**Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**|Jean Oh Team|[2602.16149](http://arxiv.org/abs/2602.16149)|null|
|**2026-02-18**|**IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**|Miguel P. Eckstein Team|[2602.16138](http://arxiv.org/abs/2602.16138)|null|
|**2026-02-18**|**OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**|Beng Chin Ooi Team|[2602.16110](http://arxiv.org/abs/2602.16110)|null|
|**2026-02-17**|**MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**|Gongbo Liang Team|[2602.16019](http://arxiv.org/abs/2602.16019)|null|
|**2026-02-17**|**BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**|Mehmet Kurt Team|[2602.16006](http://arxiv.org/abs/2602.16006)|null|
|**2026-02-17**|**Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**|Yuval Levental Team|[2602.15950](http://arxiv.org/abs/2602.15950)|null|
|**2026-02-17**|**Visual Memory Injection Attacks for Multi-Turn Conversations**|Matthias Hein Team|[2602.15927](http://arxiv.org/abs/2602.15927)|null|
|**2026-02-17**|**Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**|Valerio Guarrasi Team|[2602.15650](http://arxiv.org/abs/2602.15650)|null|
|**2026-02-17**|**CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**|Arkady Zgonnikov Team|[2602.15645](http://arxiv.org/abs/2602.15645)|null|
|**2026-02-17**|**Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**|Alois Knoll Team|[2602.15591](http://arxiv.org/abs/2602.15591)|null|
|**2026-02-17**|**VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**|Ning Ji Team|[2602.15549](http://arxiv.org/abs/2602.15549)|null|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**|Priyesh Shukla Team|[2602.15516](http://arxiv.org/abs/2602.15516)|null|
|**2026-02-17**|**On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**|Francesco Croce Team|[2602.15460](http://arxiv.org/abs/2602.15460)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-17**|**The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems**|Jing Gao Team|[2602.15382](http://arxiv.org/abs/2602.15382)|null|
|**2026-02-17**|**GMAIL: Generative Modality Alignment for generated Image Learning**|Sukmin Yun Team|[2602.15368](http://arxiv.org/abs/2602.15368)|null|
|**2026-02-17**|**Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs**|Dongsheng Li Team|[2602.15318](http://arxiv.org/abs/2602.15318)|null|
|**2026-02-17**|**Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models**|Jaehyun Ahn Team|[2602.15315](http://arxiv.org/abs/2602.15315)|null|
|**2026-02-17**|**EAA: Automating materials characterization with vision language model agents**|Mathew J. Cherukara Team|[2602.15294](http://arxiv.org/abs/2602.15294)|null|
|**2026-02-17**|**Visual Persuasion: What Influences Decisions of Vision-Language Models?**|Nikhil Singh Team|[2602.15278](http://arxiv.org/abs/2602.15278)|null|
|**2026-02-16**|**How to Train Your Long-Context Visual Document Model**|Austin Veselka Team|[2602.15257](http://arxiv.org/abs/2602.15257)|null|
|**2026-02-16**|**Ground-Truth Depth in Vision Language Models: Spatial Context Understanding in Conversational AI for XR-Robotic Support in Emergency First Response**|Manfred Tscheligi Team|[2602.15237](http://arxiv.org/abs/2602.15237)|null|
|**2026-02-16**|**Seeing to Generalize: How Visual Data Corrects Binding Shortcuts**|Rodrigo Toro Icarte Team|[2602.15183](http://arxiv.org/abs/2602.15183)|null|
|**2026-02-16**|**Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition**|Jinhui Tang Team|[2602.15124](http://arxiv.org/abs/2602.15124)|null|
|**2026-02-16**|**BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**|Aviral Kumar Team|[2602.15010](http://arxiv.org/abs/2602.15010)|null|
|**2026-02-16**|**ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**|Nipun Batra Team|[2602.14989](http://arxiv.org/abs/2602.14989)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**|Giuseppe Riccardi Team|[2602.14589](http://arxiv.org/abs/2602.14589)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**|Mikko Tolonen Team|[2602.14524](http://arxiv.org/abs/2602.14524)|null|
|**2026-02-16**|**S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**|Deepak Gupta Team|[2602.14432](http://arxiv.org/abs/2602.14432)|null|
|**2026-02-16**|**Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**|Yiliao Song Team|[2602.14399](http://arxiv.org/abs/2602.14399)|null|
|**2026-02-15**|**Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**|Peter Staar Team|[2602.14276](http://arxiv.org/abs/2602.14276)|null|
|**2026-02-15**|**Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**|Priyesh Shukla Team|[2602.14236](http://arxiv.org/abs/2602.14236)|null|
|**2026-02-15**|**Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**|Tao Xu Team|[2602.14162](http://arxiv.org/abs/2602.14162)|null|
|**2026-02-15**|**Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**|Wojciech Kusa Team|[2602.14073](http://arxiv.org/abs/2602.14073)|null|
|**2026-02-15**|**MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**|Hongxin Wei Team|[2602.13961](http://arxiv.org/abs/2602.13961)|null|
|**2026-02-14**|**RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**|Yu Hong Team|[2602.13748](http://arxiv.org/abs/2602.13748)|null|
|**2026-02-14**|**Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**|Nouar AlDahoul Team|[2602.13712](http://arxiv.org/abs/2602.13712)|null|
|**2026-02-14**|**LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**|Luyl-Da Quach Team|[2602.13662](http://arxiv.org/abs/2602.13662)|null|
|**2026-02-14**|**KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**|Edward Choi Team|[2602.13650](http://arxiv.org/abs/2602.13650)|null|
|**2026-02-14**|**Towards Sparse Video Understanding and Reasoning**|Han Liu Team|[2602.13602](http://arxiv.org/abs/2602.13602)|null|
|**2026-02-14**|**AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**|Tianyu Pang Team|[2602.13600](http://arxiv.org/abs/2602.13600)|null|
|**2026-02-14**|**OpAgent: Operator Agent for Web Navigation**|Peng Di Team|[2602.13559](http://arxiv.org/abs/2602.13559)|null|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**|Jiangpeng He Team|[2602.13041](http://arxiv.org/abs/2602.13041)|**[link](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**|
|**2026-02-13**|**Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**|Lianwen Jin Team|[2602.12957](http://arxiv.org/abs/2602.12957)|null|
|**2026-02-13**|**HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**|Theodore S. Rappaport Team|[2602.12942](http://arxiv.org/abs/2602.12942)|null|
|**2026-02-13**|**RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**|Jyothikamalesh S Team|[2602.12877](http://arxiv.org/abs/2602.12877)|null|
|**2026-02-13**|**Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**|Wei Shen Team|[2602.12843](http://arxiv.org/abs/2602.12843)|null|
|**2026-02-13**|**X-SYS: A Reference Architecture for Interactive Explanation Systems**|Sebastian Lapuschkin Team|[2602.12748](http://arxiv.org/abs/2602.12748)|null|
|**2026-02-13**|**SignScene: Visual Sign Grounding for Mapless Navigation**|David Hsu Team|[2602.12686](http://arxiv.org/abs/2602.12686)|null|
|**2026-02-13**|**IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**|Jiechao Gao Team|[2602.12659](http://arxiv.org/abs/2602.12659)|null|
|**2026-02-13**|**PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**|Alireza Taheri Team|[2602.12597](http://arxiv.org/abs/2602.12597)|null|
|**2026-02-13**|**On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**|Arnab Mondal Team|[2602.12506](http://arxiv.org/abs/2602.12506)|null|
|**2026-02-13**|**Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**|Rahmatollah Beheshti Team|[2602.12498](http://arxiv.org/abs/2602.12498)|null|
|**2026-02-12**|**Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**|Yesh Dattatreya Team|[2602.12405](http://arxiv.org/abs/2602.12405)|null|
|**2026-02-12**|**What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**|Tianyi Zhou Team|[2602.12395](http://arxiv.org/abs/2602.12395)|null|
|**2026-02-12**|**Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**|Michael Graber Team|[2602.12381](http://arxiv.org/abs/2602.12381)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-12**|**LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**|Yulun Tian Team|[2602.12314](http://arxiv.org/abs/2602.12314)|null|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|null|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|null|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|null|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**|Øyvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|null|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|null|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|null|
|**2026-02-12**|**LAMP: Implicit Language Map for Robot Navigation**|Sunwook Choi Team|[2602.11862](http://arxiv.org/abs/2602.11862)|**[link](https://lab-of-ai-and-robotics.github.io/LAMP/)**|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**|Zhou Yang Team|[2602.11824](http://arxiv.org/abs/2602.11824)|null|
|**2026-02-12**|**Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**|Jianfeng Lu Team|[2602.11743](http://arxiv.org/abs/2602.11743)|null|
|**2026-02-12**|**Adapting Vision-Language Models for E-commerce Understanding at Scale**|Shahram Khadivi Team|[2602.11733](http://arxiv.org/abs/2602.11733)|null|
|**2026-02-12**|**STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**|Qing Li Team|[2602.11730](http://arxiv.org/abs/2602.11730)|null|
|**2026-02-12**|**ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**|Kai Chen Team|[2602.11636](http://arxiv.org/abs/2602.11636)|**[link](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**|
|**2026-02-12**|**SkillRater: Untangling Capabilities in Multimodal Data**|Akshat Shrivastava Team|[2602.11615](http://arxiv.org/abs/2602.11615)|null|
|**2026-02-11**|**Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**|René Vidal Team|[2602.11448](http://arxiv.org/abs/2602.11448)|null|
|**2026-02-11**|**Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**|Tat-Seng Chua Team|[2602.11241](http://arxiv.org/abs/2602.11241)|null|
|**2026-02-11**|**Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**|Wenhan Luo Team|[2602.11146](http://arxiv.org/abs/2602.11146)|**[link](https://github.com/HKUST-C4G/diffusion-rm)**|
|**2026-02-12**|**Chatting with Images for Introspective Visual Thinking**|Tieniu Tan Team|[2602.11073](http://arxiv.org/abs/2602.11073)|null|
|**2026-02-11**|**Safe mobility support system using crowd mapping and avoidance route planning using VLM**|Koichi Ozaki Team|[2602.10910](http://arxiv.org/abs/2602.10910)|null|
|**2026-02-11**|**Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation**|Yuya Ieiri Team|[2602.10880](http://arxiv.org/abs/2602.10880)|null|
|**2026-02-11**|**SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios**|Haonan Li Team|[2602.10840](http://arxiv.org/abs/2602.10840)|null|
|**2026-02-11**|**Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training**|Yanan Sun Team|[2602.10815](http://arxiv.org/abs/2602.10815)|null|
|**2026-02-11**|**DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories**|Zhicheng Dou Team|[2602.10809](http://arxiv.org/abs/2602.10809)|null|
|**2026-02-11**|**From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?**|Vedasri Nakka Team|[2602.10771](http://arxiv.org/abs/2602.10771)|null|
|**2026-02-11**|**Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs**|Edith C. H. Ngai Team|[2602.10740](http://arxiv.org/abs/2602.10740)|null|
|**2026-02-11**|**Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**|Yanwei Fu Team|[2602.10717](http://arxiv.org/abs/2602.10717)|null|
|**2026-02-11**|**Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software**|Shuai Wang Team|[2602.10655](http://arxiv.org/abs/2602.10655)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-11**|**MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps**|Vivek Gupta Team|[2602.10518](http://arxiv.org/abs/2602.10518)|null|
|**2026-02-11**|**Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**|Sikai Chen Team|[2602.10458](http://arxiv.org/abs/2602.10458)|null|
|**2026-02-11**|**HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images**|Chengming Zhang Team|[2602.10425](http://arxiv.org/abs/2602.10425)|null|
|**2026-02-11**|**LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies**|Hyun Myung Team|[2602.10399](http://arxiv.org/abs/2602.10399)|**[link](https://locovlm.github.io)**|
|**2026-02-11**|**When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents**|Djamé Seddah Team|[2602.10384](http://arxiv.org/abs/2602.10384)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-11**|**Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection**|Wei Lu Team|[2602.10042](http://arxiv.org/abs/2602.10042)|null|
|**2026-02-10**|**RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**|Jiangmiao Pang Team|[2602.09973](http://arxiv.org/abs/2602.09973)|null|
|**2026-02-10**|**Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning**|Yiming Gan Team|[2602.09972](http://arxiv.org/abs/2602.09972)|null|
|**2026-02-10**|**Kelix Technique Report**|Ziqi Wang Team|[2602.09843](http://arxiv.org/abs/2602.09843)|null|
|**2026-02-10**|**SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding**|Xudong Jiang Team|[2602.09825](http://arxiv.org/abs/2602.09825)|null|
|**2026-02-10**|**GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation**|Uma Mahesh Team|[2602.09701](http://arxiv.org/abs/2602.09701)|null|
|**2026-02-10**|**AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models**|Linlin Wang Team|[2602.09611](http://arxiv.org/abs/2602.09611)|null|
|**2026-02-10**|**Delving into Spectral Clustering with Vision-Language Representations**|Zhen Fang Team|[2602.09586](http://arxiv.org/abs/2602.09586)|null|
|**2026-02-10**|**Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination**|Koichi Shirahata Team|[2602.09541](http://arxiv.org/abs/2602.09541)|null|
|**2026-02-10**|**DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment**|Runze Hu Team|[2602.09531](http://arxiv.org/abs/2602.09531)|null|
|**2026-02-10**|**Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs**|Rujie Liu Team|[2602.09521](http://arxiv.org/abs/2602.09521)|null|
|**2026-02-10**|**SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning**|Yu Liu Team|[2602.09463](http://arxiv.org/abs/2602.09463)|null|
|**2026-02-10**|**P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads**|Ganqu Cui Team|[2602.09443](http://arxiv.org/abs/2602.09443)|null|
|**2026-02-10**|**Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models**|Haibo Hu Team|[2602.09431](http://arxiv.org/abs/2602.09431)|null|
|**2026-02-10**|**Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification**|Jie Wei Team|[2602.09425](http://arxiv.org/abs/2602.09425)|null|
|**2026-02-10**|**K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge**|Kurt Keutzer Team|[2602.09411](http://arxiv.org/abs/2602.09411)|**[link](https://github.com/zkkli/K-Sort-Eval)**|
|**2026-02-09**|**VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models**|Tianyu Luan Team|[2602.09252](http://arxiv.org/abs/2602.09252)|null|
|**2026-02-09**|**Barycentric alignment for instance-level comparison of neural representations**|Meenakshi Khosla Team|[2602.09225](http://arxiv.org/abs/2602.09225)|null|
|**2026-02-09**|**VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models**|Wenchao Li Team|[2602.09214](http://arxiv.org/abs/2602.09214)|null|
|**2026-02-09**|**From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection**|Gim Hee Lee Team|[2602.09002](http://arxiv.org/abs/2602.09002)|null|
|**2026-02-09**|**Learning Self-Correction in Vision-Language Models via Rollout Augmentation**|Ruqi Zhang Team|[2602.08503](http://arxiv.org/abs/2602.08503)|null|
|**2026-02-09**|**SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios**|Chelsea Finn Team|[2602.08440](http://arxiv.org/abs/2602.08440)|null|
|**2026-02-09**|**What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning**|Sirui Han Team|[2602.08346](http://arxiv.org/abs/2602.08346)|null|
|**2026-02-09**|**CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT**|Luxin Xu Team|[2602.08339](http://arxiv.org/abs/2602.08339)|null|
|**2026-02-09**|**Moral Sycophancy in Vision Language Models**|Irfan Ahmad Team|[2602.08311](http://arxiv.org/abs/2602.08311)|null|
|**2026-02-08**|**Robustness of Vision Language Models Against Split-Image Harmful Input Attacks**|Shagufta Mehnaz Team|[2602.08136](http://arxiv.org/abs/2602.08136)|null|
|**2026-02-08**|**Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models**|Peng Wang Team|[2602.07899](http://arxiv.org/abs/2602.07899)|null|
|**2026-02-08**|**Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds**|Jiansheng Fan Team|[2602.07864](http://arxiv.org/abs/2602.07864)|null|
|**2026-02-08**|**LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge**|Tao Gu Team|[2602.07849](http://arxiv.org/abs/2602.07849)|null|
|**2026-02-08**|**Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures**|Simiao Ren Team|[2602.07815](http://arxiv.org/abs/2602.07815)|null|
|**2026-02-08**|**MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training**|Volkan Cevher Team|[2602.07790](http://arxiv.org/abs/2602.07790)|null|
|**2026-02-08**|**PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification**|Chang Kong Team|[2602.07768](http://arxiv.org/abs/2602.07768)|null|
|**2026-02-07**|**Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning**|Mohan Trivedi Team|[2602.07680](http://arxiv.org/abs/2602.07680)|null|
|**2026-02-07**|**From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding**|Leonardo Gonzalez Team|[2602.07645](http://arxiv.org/abs/2602.07645)|null|
|**2026-02-07**|**LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation**|Soumik Sarkar Team|[2602.07629](http://arxiv.org/abs/2602.07629)|null|
|**2026-02-07**|**HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology**|M. Khalid Khan Niazi Team|[2602.07608](http://arxiv.org/abs/2602.07608)|null|
|**2026-02-07**|**From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection**|Usman Naseem Team|[2602.07497](http://arxiv.org/abs/2602.07497)|null|
|**2026-02-07**|**Toward Vision-Language Assistants for Radio Astronomical Source Analysis**|S. Riggi Team|[2602.07469](http://arxiv.org/abs/2602.07469)|null|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models**|Joo-Young Kim Team|[2602.06822](http://arxiv.org/abs/2602.06822)|null|
|**2026-02-06**|**Same Answer, Different Representations: Hidden instability in VLMs**|Pasquale Minervini Team|[2602.06652](http://arxiv.org/abs/2602.06652)|null|
|**2026-02-06**|**CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling**|Cheng Xue Team|[2602.06619](http://arxiv.org/abs/2602.06619)|null|
|**2026-02-06**|**SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs**|Mattia Rigotti Team|[2602.06566](http://arxiv.org/abs/2602.06566)|null|
|**2026-02-06**|**Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance**|Anastasia Antsiferova Team|[2602.06530](http://arxiv.org/abs/2602.06530)|null|
|**2026-02-06**|**FloorplanVLM: A Vision-Language Model for Floorplan Vectorization**|Yue Yang Team|[2602.06507](http://arxiv.org/abs/2602.06507)|null|
|**2026-02-06**|**MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing**|Hong Li Team|[2602.06402](http://arxiv.org/abs/2602.06402)|null|
|**2026-02-06**|**POINTS-GUI-G: GUI-Grounding Journey**|Jie Zhou Team|[2602.06391](http://arxiv.org/abs/2602.06391)|null|
|**2026-02-05**|**Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings**|Agustin Picard Team|[2602.06218](http://arxiv.org/abs/2602.06218)|null|
|**2026-02-05**|**DeDPO: Debiased Direct Preference Optimization for Diffusion Models**|Ramin Zabih Team|[2602.06195](http://arxiv.org/abs/2602.06195)|null|
|**2026-02-05**|**PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining**|Weidi Xie Team|[2602.06184](http://arxiv.org/abs/2602.06184)|null|
|**2026-02-05**|**Compressing LLMs with MoP: Mixture of Pruners**|Artur Jordao Team|[2602.06127](http://arxiv.org/abs/2602.06127)|**[link](https://github.com/c2d-usp/Efficient-LLMs-with-MoP)**|
|**2026-02-05**|**Can vision language models learn intuitive physics from interaction?**|Eric Schulz Team|[2602.06033](http://arxiv.org/abs/2602.06033)|null|
|**2026-02-05**|**GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?**|Jiaqi Wang Team|[2602.06013](http://arxiv.org/abs/2602.06013)|**[link](https://genarena.github.io/)**|
|**2026-02-05**|**Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning**|Xianming Liu Team|[2602.05809](http://arxiv.org/abs/2602.05809)|null|
|**2026-02-05**|**Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**|Weiming Zhang Team|[2602.05789](http://arxiv.org/abs/2602.05789)|null|
|**2026-02-05**|**Ethology of Latent Spaces**|Philippe Boisnard Team|[2602.05710](http://arxiv.org/abs/2602.05710)|**[link](https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf)**|
|**2026-02-05**|**LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation**|Yiguo Qiao Team|[2602.05578](http://arxiv.org/abs/2602.05578)|null|
|**2026-02-05**|**TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?**|Cheston Tan Team|[2602.05570](http://arxiv.org/abs/2602.05570)|null|
|**2026-02-05**|**VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator**|Miguel Cazorla Team|[2602.05552](http://arxiv.org/abs/2602.05552)|null|
|**2026-02-05**|**Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification**|Liping Jing Team|[2602.05535](http://arxiv.org/abs/2602.05535)|**[link](https://github.com/HT86159/EUQ)**|
|**2026-02-05**|**Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models**|Nadir Durrani Team|[2602.05437](http://arxiv.org/abs/2602.05437)|null|
|**2026-02-05**|**Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting**|Can Huang Team|[2602.05384](http://arxiv.org/abs/2602.05384)|null|
|**2026-02-05**|**VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs**|Konstantinos Psounis Team|[2602.05382](http://arxiv.org/abs/2602.05382)|null|
|**2026-02-05**|**Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions**|Tao Zhang Team|[2602.05273](http://arxiv.org/abs/2602.05273)|null|
|**2026-02-05**|**Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR**|Haibo Qiu Team|[2602.05261](http://arxiv.org/abs/2602.05261)|null|
|**2026-02-05**|**GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling**|Tong Zhang Team|[2602.05202](http://arxiv.org/abs/2602.05202)|null|
|**2026-02-04**|**ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation**|Yapeng Tian Team|[2602.05132](http://arxiv.org/abs/2602.05132)|null|
|**2026-02-04**|**VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**|Dongdong Chen Team|[2602.05049](http://arxiv.org/abs/2602.05049)|**[link](https://vista-vla.github.io/)**|
|**2026-02-04**|**Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?**|Alan Ritter Team|[2602.05023](http://arxiv.org/abs/2602.05023)|null|
|**2026-02-04**|**When LLaVA Meets Objects: Token Composition for Vision-Language-Models**|Hilde Kuehne Team|[2602.04864](http://arxiv.org/abs/2602.04864)|null|
|**2026-02-04**|**El Agente Estructural: An Artificially Intelligent Molecular Editor**|Varinia Bernales Team|[2602.04849](http://arxiv.org/abs/2602.04849)|null|
|**2026-02-04**|**VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?**|Huchuan Lu Team|[2602.04802](http://arxiv.org/abs/2602.04802)|null|
|**2026-02-04**|**Annotation Free Spacecraft Detection and Segmentation using Vision Language Models**|Djamila Aouada Team|[2602.04699](http://arxiv.org/abs/2602.04699)|null|
|**2026-02-04**|**AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**|Chunhua Shen Team|[2602.04672](http://arxiv.org/abs/2602.04672)|null|
|**2026-02-04**|**PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective**|Chunhua Shen Team|[2602.04657](http://arxiv.org/abs/2602.04657)|null|
|**2026-02-04**|**Relational Scene Graphs for Object Grounding of Natural Language Commands**|Ville Kyrki Team|[2602.04635](http://arxiv.org/abs/2602.04635)|null|
|**2026-02-04**|**LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation**|Yan Song Team|[2602.04617](http://arxiv.org/abs/2602.04617)|null|
|**2026-02-04**|**VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration**|Kunwoo Park Team|[2602.04587](http://arxiv.org/abs/2602.04587)|null|
|**2026-02-04**|**Understanding Degradation with Vision Language Model**|Xuelong Li Team|[2602.04565](http://arxiv.org/abs/2602.04565)|null|
|**2026-02-04**|**EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**|Börje F. Karlsson Team|[2602.04515](http://arxiv.org/abs/2602.04515)|null|
|**2026-02-04**|**When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models**|Se-Young Yun Team|[2602.04356](http://arxiv.org/abs/2602.04356)|null|
|**2026-02-04**|**Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models**|Deyu Zhou Team|[2602.04355](http://arxiv.org/abs/2602.04355)|null|
|**2026-02-04**|**Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning**|Shu-Tao Xia Team|[2602.04340](http://arxiv.org/abs/2602.04340)|null|
|**2026-02-04**|**Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner**|Shu-Tao Xia Team|[2602.04337](http://arxiv.org/abs/2602.04337)|null|
|**2026-02-04**|**Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement**|Lin Gui Team|[2602.04304](http://arxiv.org/abs/2602.04304)|null|
|**2026-02-04**|**AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models**|Yunjiang Lou Team|[2602.04256](http://arxiv.org/abs/2602.04256)|null|
|**2026-02-04**|**VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding**|Weining Shen Team|[2602.04094](http://arxiv.org/abs/2602.04094)|null|
|**2026-02-03**|**Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**|Rex Ying Team|[2602.03983](http://arxiv.org/abs/2602.03983)|null|
|**2026-02-03**|**VLS: Steering Pretrained Robot Policies via Vision-Language Models**|Ranjay Krishna Team|[2602.03973](http://arxiv.org/abs/2602.03973)|**[link](https://vision-language-steering.github.io/webpage/)**|
|**2026-02-03**|**They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References**|Usman Naseem Team|[2602.03822](http://arxiv.org/abs/2602.03822)|null|
|**2026-02-03**|**Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives**|Katherine D. Van Schaik Team|[2602.03750](http://arxiv.org/abs/2602.03750)|null|
|**2026-02-03**|**Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment**|Mahdi Abdelguerfi Team|[2602.03742](http://arxiv.org/abs/2602.03742)|null|
|**2026-02-03**|**RegionReasoner: Region-Grounded Multi-Round Visual Reasoning**|Cees G. M. Snoek Team|[2602.03733](http://arxiv.org/abs/2602.03733)|null|
|**2026-02-03**|**MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment**|Gunhee Kim Team|[2602.03665](http://arxiv.org/abs/2602.03665)|null|
|**2026-02-03**|**KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs**|Jianyuan Guo Team|[2602.03615](http://arxiv.org/abs/2602.03615)|null|
|**2026-02-03**|**TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection**|Mohammad Sabokrou Team|[2602.03594](http://arxiv.org/abs/2602.03594)|null|
|**2026-02-03**|**Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning**|Jianxiong Wang Team|[2602.03530](http://arxiv.org/abs/2602.03530)|null|
|**2026-02-03**|**Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance**|Min Zhang Team|[2602.03491](http://arxiv.org/abs/2602.03491)|null|
|**2026-02-03**|**Contextualized Visual Personalization in Vision-Language Models**|Sungroh Yoon Team|[2602.03454](http://arxiv.org/abs/2602.03454)|**[link](https://github.com/oyt9306/CoViP)**|
|**2026-02-03**|**AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation**|Jimmy Xiangji Huang Team|[2602.03416](http://arxiv.org/abs/2602.03416)|null|
|**2026-02-03**|**Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility**|Ming Li Team|[2602.03402](http://arxiv.org/abs/2602.03402)|null|
|**2026-02-03**|**POP: Prefill-Only Pruning for Efficient Large Model Inference**|Qingan Li Team|[2602.03295](http://arxiv.org/abs/2602.03295)|null|
|**2026-02-03**|**LaVPR: Benchmarking Language and Vision for Place Recognition**|Yoli Shavit Team|[2602.03253](http://arxiv.org/abs/2602.03253)|null|
|**2026-02-03**|**Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration**|Haixia Bi Team|[2602.03151](http://arxiv.org/abs/2602.03151)|null|
|**2026-02-03**|**SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass**|Xin Miao Team|[2602.03134](http://arxiv.org/abs/2602.03134)|null|
|**2026-02-03**|**FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation**|Rongjunchen Zhang Team|[2602.03130](http://arxiv.org/abs/2602.03130)|null|
|**2026-02-03**|**Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors**|Wenbo Ding Team|[2602.03119](http://arxiv.org/abs/2602.03119)|null|
|**2026-02-03**|**IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning**|Yongchao Xu Team|[2602.03060](http://arxiv.org/abs/2602.03060)|null|
|**2026-02-03**|**Bongards at the Boundary of Perception and Reasoning: Programs or Language?**|Kevin Ellis Team|[2602.03038](http://arxiv.org/abs/2602.03038)|null|
|**2026-02-02**|**Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning**|Kostas Alexis Team|[2602.02456](http://arxiv.org/abs/2602.02456)|null|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|**[link](https://world-gymnast.github.io/)**|
|**2026-02-02**|**ReasonEdit: Editing Vision-Language Models using Human Reasoning**|Thomas Hartvigsen Team|[2602.02408](http://arxiv.org/abs/2602.02408)|null|
|**2026-02-02**|**LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization**|Limin Wang Team|[2602.02341](http://arxiv.org/abs/2602.02341)|null|
|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|null|
|**2026-02-02**|**Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models**|Toshihiko Yamasaki Team|[2602.02043](http://arxiv.org/abs/2602.02043)|null|
|**2026-02-02**|**Rethinking Genomic Modeling Through Optical Character Recognition**|Xiangxiang Zeng Team|[2602.02014](http://arxiv.org/abs/2602.02014)|null|
|**2026-02-02**|**Enhancing Multi-Image Understanding through Delimiter Token Scaling**|Junsuk Choe Team|[2602.01984](http://arxiv.org/abs/2602.01984)|null|
|**2026-02-02**|**VLM-Guided Experience Replay**|Shie Mannor Team|[2602.01915](http://arxiv.org/abs/2602.01915)|null|
|**2026-02-02**|**Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery**|J. Marius Zöllner Team|[2602.01836](http://arxiv.org/abs/2602.01836)|null|
|**2026-02-02**|**CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding**|Xiaodong Gu Team|[2602.01785](http://arxiv.org/abs/2602.01785)|**[link](https://github.com/YerbaPage/CodeOCR)**|
|**2026-02-02**|**Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models**|Bin Li Team|[2602.01738](http://arxiv.org/abs/2602.01738)|null|
|**2026-02-02**|**AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**|Yu She Team|[2602.01662](http://arxiv.org/abs/2602.01662)|null|
|**2026-02-02**|**ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval**|Tat-Seng Chua Team|[2602.01639](http://arxiv.org/abs/2602.01639)|null|
|**2026-02-02**|**PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards**|Mei Chen Team|[2602.01624](http://arxiv.org/abs/2602.01624)|null|
|**2026-02-02**|**Generative Visual Code Mobile World Models**|Jamin Shin Team|[2602.01576](http://arxiv.org/abs/2602.01576)|null|
|**2026-02-02**|**SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models**|Xiaochun Cao Team|[2602.01574](http://arxiv.org/abs/2602.01574)|null|
|**2026-02-02**|**Preserving Localized Patch Semantics in VLMs**|Longin Jan Latecki Team|[2602.01530](http://arxiv.org/abs/2602.01530)|null|
|**2026-02-02**|**Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition**|Brian Keith-Norambuena Team|[2602.01527](http://arxiv.org/abs/2602.01527)|null|
|**2026-02-01**|**Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles**|Jiachen Bian Team|[2602.01452](http://arxiv.org/abs/2602.01452)|null|
|**2026-01-30**|**User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**|Maria Gorlatova Team|[2601.23281](http://arxiv.org/abs/2601.23281)|null|
|**2026-01-30**|**Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**|Liang-Jie Zhang Team|[2601.23253](http://arxiv.org/abs/2601.23253)|null|
|**2026-01-30**|**Structured Over Scale: Learning Spatial Reasoning from Educational Video**|Sarah Ostadabbas Team|[2601.23251](http://arxiv.org/abs/2601.23251)|null|
|**2026-01-30**|**Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**|Lijie Hu Team|[2601.23149](http://arxiv.org/abs/2601.23149)|null|
|**2026-01-30**|**One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**|Dong Liu Team|[2601.23041](http://arxiv.org/abs/2601.23041)|null|
|**2026-01-30**|**Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**|Jianzong Wang Team|[2601.22959](http://arxiv.org/abs/2601.22959)|null|
|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Stefano Nolfi Team|[2601.22948](http://arxiv.org/abs/2601.22948)|null|
|**2026-01-30**|**A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions**|Arno Eichberger Team|[2601.22830](http://arxiv.org/abs/2601.22830)|null|
|**2026-01-30**|**Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA**|Yinghuan Shi Team|[2601.22828](http://arxiv.org/abs/2601.22828)|null|
|**2026-01-30**|**HeatMat: Simulation of City Material Impact on Urban Heat Island Effect**|Rosalie Martin Team|[2601.22796](http://arxiv.org/abs/2601.22796)|null|
|**2026-01-30**|**Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models**|Christos Emmanouilidis Team|[2601.22754](http://arxiv.org/abs/2601.22754)|null|
|**2026-01-30**|**StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing**|Roy Ka-Wei Lee Team|[2601.22738](http://arxiv.org/abs/2601.22738)|null|
|**2026-01-30**|**Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models**|Tat-Seng Chua Team|[2601.22737](http://arxiv.org/abs/2601.22737)|null|
|**2026-01-30**|**Vision-Language Models Unlock Task-Centric Latent Actions**|Vladislav Kurenkov Team|[2601.22714](http://arxiv.org/abs/2601.22714)|null|
|**2026-01-30**|**Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs**|Yawei Li Team|[2601.22709](http://arxiv.org/abs/2601.22709)|null|
|**2026-01-30**|**Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**|Kai Yuan Team|[2601.22701](http://arxiv.org/abs/2601.22701)|null|
|**2026-01-30**|**Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding**|Hyun Gyu Lee Team|[2601.22696](http://arxiv.org/abs/2601.22696)|null|
|**2026-01-30**|**Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction**|Nuno Vasconcelos Team|[2601.22570](http://arxiv.org/abs/2601.22570)|null|
|**2026-01-30**|**Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**|Jinsong Su Team|[2601.22451](http://arxiv.org/abs/2601.22451)|**[link](https://github.com/Liushiyu-0709/SelfVal)**|
|**2026-01-29**|**Jailbreaks on Vision Language Model via Multimodal Reasoning**|Yuguang Yao Team|[2601.22398](http://arxiv.org/abs/2601.22398)|null|
|**2026-01-29**|**Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions**|Serena Yeung-Levy Team|[2601.22150](http://arxiv.org/abs/2601.22150)|**[link](https://sites.google.com/view/vi-probe/)**|
|**2026-01-29**|**SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence**|Morteza Fayazi Team|[2601.22114](http://arxiv.org/abs/2601.22114)|null|
|**2026-01-29**|**VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning**|Dacheng Tao Team|[2601.22069](http://arxiv.org/abs/2601.22069)|**[link](https://github.com/w-yibo/VTC-R1)**|
|**2026-01-29**|**Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models**|Diego Marcos Team|[2601.21944](http://arxiv.org/abs/2601.21944)|null|
|**2026-01-29**|**Mil-SCORE: Benchmarking Long-Context Geospatial Reasoning and Planning in Large Language Models**|Xiaomin Lin Team|[2601.21826](http://arxiv.org/abs/2601.21826)|null|
|**2026-01-29**|**MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods**|Lijun Wu Team|[2601.21821](http://arxiv.org/abs/2601.21821)|null|
|**2026-01-29**|**Moral Outrage Shapes Commitments Beyond Attention: Multimodal Moral Emotions on YouTube in Korea and the US**|Wonjae Lee Team|[2601.21815](http://arxiv.org/abs/2601.21815)|null|
|**2026-01-29**|**Knowledge Vector Weakening: Efficient Training-free Unlearning for Large Vision-Language Models**|Junsuk Choe Team|[2601.21794](http://arxiv.org/abs/2601.21794)|null|
|**2026-01-29**|**Epistemic Uncertainty Quantification for Pre-trained VLMs via Riemannian Flow Matching**|Prashant Singh Team|[2601.21662](http://arxiv.org/abs/2601.21662)|null|
|**2026-01-29**|**OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models**|Zhixiong Zeng Team|[2601.21639](http://arxiv.org/abs/2601.21639)|null|
|**2026-01-29**|**PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization**|Yongbing Zhang Team|[2601.21617](http://arxiv.org/abs/2601.21617)|null|
|**2026-01-29**|**WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models**|Nenghai Yu Team|[2601.21610](http://arxiv.org/abs/2601.21610)|null|
|**2026-01-29**|**On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression**|Haibo Hu Team|[2601.21531](http://arxiv.org/abs/2601.21531)|null|
|**2026-01-29**|**IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation**|Jeonggil Ko Team|[2601.21506](http://arxiv.org/abs/2601.21506)|null|
|**2026-01-29**|**DSCD-Nav: Dual-Stance Cooperative Debate for Object Navigation**|Cheng Deng Team|[2601.21409](http://arxiv.org/abs/2601.21409)|null|
|**2026-01-29**|**GeoRC: A Benchmark for Geolocation Reasoning Chains**|James Hays Team|[2601.21278](http://arxiv.org/abs/2601.21278)|null|
|**2026-01-29**|**Thinker: A vision-language foundation model for embodied intelligence**|Jichao Jiao Team|[2601.21199](http://arxiv.org/abs/2601.21199)|null|
|**2026-01-29**|**FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models**|Tao Chen Team|[2601.21187](http://arxiv.org/abs/2601.21187)|null|
|**2026-01-28**|**Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization**|Yu Guan Team|[2601.21078](http://arxiv.org/abs/2601.21078)|null|
|**2026-01-28**|**Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning**|Anna Korhonen Team|[2601.21037](http://arxiv.org/abs/2601.21037)|null|
|**2026-01-28**|**Open-Vocabulary Functional 3D Human-Scene Interaction Generation**|Yan Zhang Team|[2601.20835](http://arxiv.org/abs/2601.20835)|null|
|**2026-01-28**|**bi-modal textual prompt learning for vision-language models in remote sensing**|Biplab Banerjee Team|[2601.20675](http://arxiv.org/abs/2601.20675)|null|
|**2026-01-28**|**Investigating the Development of Task-Oriented Communication in Vision-Language Models**|Ron Meir Team|[2601.20641](http://arxiv.org/abs/2601.20641)|null|
|**2026-01-28**|**SketchDynamics: Exploring Free-Form Sketches for Dynamic Intent Expression in Animation Generation**|Hongbo Fu Team|[2601.20622](http://arxiv.org/abs/2601.20622)|null|
|**2026-01-28**|**DeepSeek-OCR 2: Visual Causal Flow**|Yukun Li Team|[2601.20552](http://arxiv.org/abs/2601.20552)|null|
|**2026-01-28**|**AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors**|Danijel Skočaj Team|[2601.20524](http://arxiv.org/abs/2601.20524)|null|
|**2026-01-28**|**MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models**|Jiantao Zhou Team|[2601.20433](http://arxiv.org/abs/2601.20433)|null|
|**2026-01-28**|**Let's Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models**|Feng Liu Team|[2601.20419](http://arxiv.org/abs/2601.20419)|null|
|**2026-01-28**|**TABED: Test-Time Adaptive Ensemble Drafting for Robust Speculative Decoding in LVLMs**|Kangwook Lee Team|[2601.20357](http://arxiv.org/abs/2601.20357)|null|
|**2026-01-28**|**Physically Guided Visual Mass Estimation from a Single RGB Image**|Kwang In Kim Team|[2601.20303](http://arxiv.org/abs/2601.20303)|null|
|**2026-01-28**|**Hallucination Begins Where Saliency Drops**|Hao Tang Team|[2601.20279](http://arxiv.org/abs/2601.20279)|null|
|**2026-01-27**|**Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing**|Yu Xiao Team|[2601.20107](http://arxiv.org/abs/2601.20107)|null|
|**2026-01-27**|**Quantization-Aware Distillation for NVFP4 Inference Accuracy Recovery**|Huizi Mao Team|[2601.20088](http://arxiv.org/abs/2601.20088)|null|
|**2026-01-27**|**Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning**|Stefan Scherer Team|[2601.20075](http://arxiv.org/abs/2601.20075)|null|
|**2026-01-27**|**DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation**|Mooi Choo Chuah Team|[2601.20064](http://arxiv.org/abs/2601.20064)|null|
|**2026-01-27**|**EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning**|Pheng-Ann Heng Team|[2601.19850](http://arxiv.org/abs/2601.19850)|**[link](https://github.com/Nicous20/EgoHandICL)**|
|**2026-01-27**|**Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision**|Xiaotian Li Team|[2601.19798](http://arxiv.org/abs/2601.19798)|null|
|**2026-01-27**|**Physics-Aware Novel-View Acoustic Synthesis with Vision-Language Priors and 3D Acoustic Environment Modeling**|Wenwu Wang Team|[2601.19712](http://arxiv.org/abs/2601.19712)|**[link](https://physnvas.github.io/)**|
|**2026-01-27**|**KeepLoRA: Continual Learning with Residual Gradient Adaptation**|Min-Ling Zhang Team|[2601.19659](http://arxiv.org/abs/2601.19659)|null|
|**2026-01-27**|**ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving**|Hong Chen Team|[2601.19582](http://arxiv.org/abs/2601.19582)|null|
|**2026-01-27**|**Automated Safety Benchmarking: A Multi-agent Pipeline for LVLMs**|Wei Sun Team|[2601.19507](http://arxiv.org/abs/2601.19507)|null|
|**2026-01-27**|**From Internal Diagnosis to External Auditing: A VLM-Driven Paradigm for Online Test-Time Backdoor Defense**|Kehuan Zhang Team|[2601.19448](http://arxiv.org/abs/2601.19448)|null|
|**2026-01-27**|**RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming**|Xiaopeng Fan Team|[2601.19433](http://arxiv.org/abs/2601.19433)|null|
|**2026-01-27**|**Gazeify Then Voiceify: Physical Object Referencing Through Gaze and Voice Interaction with Displayless Smart Glasses**|Tanya Jonker Team|[2601.19281](http://arxiv.org/abs/2601.19281)|null|
|**2026-01-27**|**GhostUI: Unveiling Hidden Interactions in Mobile UI**|Jinwook Seo Team|[2601.19258](http://arxiv.org/abs/2601.19258)|null|
|**2026-01-27**|**Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP**|Xilin Chen Team|[2601.19210](http://arxiv.org/abs/2601.19210)|null|
|**2026-01-27**|**MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning**|Hamid Rezatofighi Team|[2601.19204](http://arxiv.org/abs/2601.19204)|null|
|**2026-01-27**|**Before Smelling the Video: A Two-Stage Pipeline for Interpretable Video-to-Scent Plans**|Denise Wilson Team|[2601.19203](http://arxiv.org/abs/2601.19203)|null|
|**2026-01-27**|**Do Images Speak Louder than Words? Investigating the Effect of Textual Misinformation in VLMs**|Ray Mooney Team|[2601.19202](http://arxiv.org/abs/2601.19202)|null|
|**2026-01-27**|**m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning**|Igor Molybog Team|[2601.19099](http://arxiv.org/abs/2601.19099)|null|
|**2026-01-26**|**Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery**|Yansha Deng Team|[2601.18765](http://arxiv.org/abs/2601.18765)|null|
|**2026-01-26**|**Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems**|Keze Wang Team|[2601.18735](http://arxiv.org/abs/2601.18735)|null|
|**2026-01-26**|**Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge**|Luc Van Gool Team|[2601.18733](http://arxiv.org/abs/2601.18733)|**[link](https://mars-eai.github.io/MARS-Challenge-Webpage/)**|
|**2026-01-26**|**Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge**|Jiawei Zhang Team|[2601.18698](http://arxiv.org/abs/2601.18698)|null|
|**2026-01-26**|**DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment**|Michael Felsberg Team|[2601.18493](http://arxiv.org/abs/2601.18493)|null|
|**2026-01-26**|**SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation**|Ziyuan Pu Team|[2601.18442](http://arxiv.org/abs/2601.18442)|null|
|**2026-01-26**|**ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks**|Konstantinos N. Plataniotis Team|[2601.18386](http://arxiv.org/abs/2601.18386)|null|
|**2026-01-26**|**Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning**|Tingbo Zhang Team|[2601.18356](http://arxiv.org/abs/2601.18356)|null|
|**2026-01-26**|**Beyond Rigid: Benchmarking Non-Rigid Video Editing**|Min Zhang Team|[2601.18340](http://arxiv.org/abs/2601.18340)|null|
|**2026-01-26**|**TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion**|Jian Tang Team|[2601.18323](http://arxiv.org/abs/2601.18323)|null|
|**2026-01-26**|**Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation**|Jihong Park Team|[2601.18242](http://arxiv.org/abs/2601.18242)|null|
|**2026-01-26**|**GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models**|Jian Luan Team|[2601.18197](http://arxiv.org/abs/2601.18197)|null|
|**2026-01-26**|**Spatial-Conditioned Reasoning in Long-Egocentric Videos**|Abolfazl Razi Team|[2601.18100](http://arxiv.org/abs/2601.18100)|null|
|**2026-01-26**|**Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models**|Christopher J. MacLellan Team|[2601.18065](http://arxiv.org/abs/2601.18065)|null|
|**2026-01-25**|**RemEdit: Efficient Diffusion Editing with Riemannian Geometry**|Brian D. Davison Team|[2601.17927](http://arxiv.org/abs/2601.17927)|null|
|**2026-01-25**|**Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models**|Jaewoo Kang Team|[2601.17918](http://arxiv.org/abs/2601.17918)|null|
|**2026-01-25**|**RAICL: Retrieval-Augmented In-Context Learning for Vision-Language-Model Based EEG Seizure Detection**|Dongrui Wu Team|[2601.17844](http://arxiv.org/abs/2601.17844)|null|
|**2026-01-25**|**ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning**|LiQun Huang Team|[2601.17818](http://arxiv.org/abs/2601.17818)|null|
|**2026-01-25**|**A Computational Approach to Visual Metonymy**|Tianyu Jiang Team|[2601.17706](http://arxiv.org/abs/2601.17706)|null|
|**2026-01-25**|**StyleDecoupler: Generalizable Artistic Style Disentanglement**|Jie Zhou Team|[2601.17697](http://arxiv.org/abs/2601.17697)|null|
|**2026-01-23**|**VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents**|Joseph E. Gonzalez Team|[2601.16973](http://arxiv.org/abs/2601.16973)|**[link](https://visgym.github.io/)**|
|**2026-01-23**|**Evaluating Large Vision-language Models for Surgical Tool Detection**|Cristian A. Linte Team|[2601.16895](http://arxiv.org/abs/2601.16895)|null|
|**2026-01-23**|**X-Aligner: Composed Visual Retrieval without the Bells and Whistles**|Mariana-Iuliana Georgescu Team|[2601.16582](http://arxiv.org/abs/2601.16582)|null|
|**2026-01-23**|**LLM is Not All You Need: A Systematic Evaluation of ML vs. Foundation Models for text and image based Medical Classification**|Dhvani Upadhyay Team|[2601.16549](http://arxiv.org/abs/2601.16549)|null|
|**2026-01-23**|**AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose**|Jinhong Yang Team|[2601.16429](http://arxiv.org/abs/2601.16429)|null|
|**2026-01-23**|**Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)**|Walid G. Aref Team|[2601.16409](http://arxiv.org/abs/2601.16409)|null|
|**2026-01-22**|**Point Bridge: 3D Representations for Cross Domain Policy Learning**|Ajay Mandlekar Team|[2601.16212](http://arxiv.org/abs/2601.16212)|null|
|**2026-01-22**|**Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources**|Hamed Ghodrati Team|[2601.16108](http://arxiv.org/abs/2601.16108)|null|
|**2026-01-22**|**DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models**|Jingqun Tang Team|[2601.16065](http://arxiv.org/abs/2601.16065)|null|
|**2026-01-22**|**DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning**|Minsu Cho Team|[2601.16046](http://arxiv.org/abs/2601.16046)|null|
|**2026-01-22**|**RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture**|Kshitij Jadhav Team|[2601.15891](http://arxiv.org/abs/2601.15891)|null|
|**2026-01-22**|**Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video**|Jan van Gemert Team|[2601.15780](http://arxiv.org/abs/2601.15780)|null|
|**2026-01-22**|**Hallucination Mitigating for Medical Report Generation**|Piji Li Team|[2601.15745](http://arxiv.org/abs/2601.15745)|null|
|**2026-01-22**|**Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework**|Kunal Sonalkar Team|[2601.15711](http://arxiv.org/abs/2601.15711)|null|
|**2026-01-21**|**CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation**|Arash Ajoudani Team|[2601.15541](http://arxiv.org/abs/2601.15541)|null|
|**2026-01-21**|**DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection**|Marc Lalonde Team|[2601.15453](http://arxiv.org/abs/2601.15453)|null|
|**2026-01-21**|**CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation**|Bernard Ghanem Team|[2601.15408](http://arxiv.org/abs/2601.15408)|null|
|**2026-01-21**|**Improving MoE Compute Efficiency by Composing Weight and Data Sparsity**|Armen Aghajanyan Team|[2601.15370](http://arxiv.org/abs/2601.15370)|null|
|**2026-01-21**|**Towards Understanding Best Practices for Quantization of Vision-Language Models**|Matthew Gwilliam Team|[2601.15287](http://arxiv.org/abs/2601.15287)|null|
|**2026-01-21**|**Iterative Refinement Improves Compositional Image Generation**|Deepak Pathak Team|[2601.15286](http://arxiv.org/abs/2601.15286)|**[link](https://iterative-img-gen.github.io/)**|
|**2026-01-21**|**PROGRESSLM: Towards Progress Reasoning in Vision-Language Models**|Han Liu Team|[2601.15224](http://arxiv.org/abs/2601.15224)|**[link](https://progresslm.github.io/ProgressLM/)**|
|**2026-01-21**|**Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning**|Zeyu Fu Team|[2601.15115](http://arxiv.org/abs/2601.15115)|null|
|**2026-01-21**|**Unified Multi-Dataset Training for TBPS**|Brejesh Lall Team|[2601.14978](http://arxiv.org/abs/2601.14978)|null|
|**2026-01-21**|**Vision-Language Models on the Edge for Real-Time Robotic Perception**|Syed Ali Raza Zaidi Team|[2601.14921](http://arxiv.org/abs/2601.14921)|null|
|**2026-01-21**|**HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation**|Dzmitry Tsetserukou Team|[2601.14874](http://arxiv.org/abs/2601.14874)|null|
|**2026-01-21**|**Measuring and Aligning Abstraction in Vision-Language Models with Medical Taxonomies**|Cosmin I. Bercea Team|[2601.14827](http://arxiv.org/abs/2601.14827)|null|
|**2026-01-21**|**Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis**|Tomoya Sakai Team|[2601.14774](http://arxiv.org/abs/2601.14774)|null|
|**2026-01-21**|**Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning**|Zheng Wei Team|[2601.14750](http://arxiv.org/abs/2601.14750)|null|
|**2026-01-21**|**DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling**|Jung Sun Yoo Team|[2601.14732](http://arxiv.org/abs/2601.14732)|null|
|**2026-01-21**|**Typhoon OCR: Open Vision-Language Model For Thai Document Extraction**|Kunat Pipatanakul Team|[2601.14722](http://arxiv.org/abs/2601.14722)|null|
|**2026-01-21**|**Unified Multimodal and Multilingual Retrieval via Multi-Task Learning with NLU Integration**|Guoquan Zhang Team|[2601.14714](http://arxiv.org/abs/2601.14714)|null|
|**2026-01-21**|**AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving**|Yu Zhang Team|[2601.14702](http://arxiv.org/abs/2601.14702)|null|
|**2026-01-21**|**Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis**|Nantheera Anantrasirichai Team|[2601.14637](http://arxiv.org/abs/2601.14637)|null|
|**2026-01-21**|**Probing Prompt Design for Socially Compliant Robot Navigation with Vision Language Models**|Toshihiko Yamasaki Team|[2601.14622](http://arxiv.org/abs/2601.14622)|null|
|**2026-01-21**|**Learning Consistent Taxonomic Classification through Hierarchical Reasoning**|Haibin Ling Team|[2601.14610](http://arxiv.org/abs/2601.14610)|null|
|**2026-01-21**|**3D Space as a Scratchpad for Editable Text-to-Image Generation**|Kevin Blackburn-Matzen Team|[2601.14602](http://arxiv.org/abs/2601.14602)|null|
|**2026-01-21**|**Explainable OOHRI: Communicating Robot Capabilities and Limitations as Augmented Reality Affordances**|Parastoo Abtahi Team|[2601.14587](http://arxiv.org/abs/2601.14587)|null|
|**2026-01-20**|**GutenOCR: A Grounded Vision-Language Front-End for Documents**|Yosheb Getachew Team|[2601.14490](http://arxiv.org/abs/2601.14490)|null|
|**2026-01-20**|**LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR**|Baptiste Aubertin Team|[2601.14251](http://arxiv.org/abs/2601.14251)|null|
|**2026-01-20**|**Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis**|Gregory Furman Team|[2601.14227](http://arxiv.org/abs/2601.14227)|null|
|**2026-01-20**|**IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models**|Yun Fu Team|[2601.14188](http://arxiv.org/abs/2601.14188)|null|
|**2026-01-20**|**TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers**|Kai Chen Team|[2601.14133](http://arxiv.org/abs/2601.14133)|**[link](https://github.com/ZGC-EmbodyAI/TwinBrainVLA)**|
|**2026-01-20**|**Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems**|Abiola Akanmu Team|[2601.14091](http://arxiv.org/abs/2601.14091)|null|
|**2026-01-20**|**DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning**|Burak Temelkuran Team|[2601.14084](http://arxiv.org/abs/2601.14084)|null|
|**2026-01-20**|**Feature-Aware Test Generation for Deep Learning Models**|Andrea Stocco Team|[2601.14081](http://arxiv.org/abs/2601.14081)|null|
|**2026-01-20**|**Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology**|Keze Wang Team|[2601.14044](http://arxiv.org/abs/2601.14044)|null|
|**2026-01-20**|**HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs**|Lei Bi Team|[2601.13919](http://arxiv.org/abs/2601.13919)|null|
|**2026-01-20**|**Revisiting Multi-Task Visual Representation Learning**|Weidi Xie Team|[2601.13886](http://arxiv.org/abs/2601.13886)|**[link](https://github.com/Becomebright/MTV)**|
|**2026-01-20**|**DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes**|Ferda Ofli Team|[2601.13839](http://arxiv.org/abs/2601.13839)|null|
|**2026-01-20**|**PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval**|Chaim Baskin Team|[2601.13797](http://arxiv.org/abs/2601.13797)|null|
|**2026-01-20**|**Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search**|Yan Lu Team|[2601.13719](http://arxiv.org/abs/2601.13719)|null|
|**2026-01-20**|**Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs**|Taesup Kim Team|[2601.13707](http://arxiv.org/abs/2601.13707)|null|
|**2026-01-20**|**Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles**|Athanasios Voulodimos Team|[2601.13705](http://arxiv.org/abs/2601.13705)|null|
|**2026-01-20**|**CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models**|Zhe Zhao Team|[2601.13622](http://arxiv.org/abs/2601.13622)|null|
|**2026-01-20**|**ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch**|Lijun Wu Team|[2601.13606](http://arxiv.org/abs/2601.13606)|null|
|**2026-01-19**|**PhysicsSolutionAgent: Towards Multimodal Explanations for Numerical Physics Problem Solving**|Dhruv Kumar Team|[2601.13453](http://arxiv.org/abs/2601.13453)|null|
|**2026-01-19**|**Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation**|Lawrence Swaminathan Xavier Prince Team|[2601.13440](http://arxiv.org/abs/2601.13440)|null|
|**2026-01-19**|**Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics**|Eric Cosatto Team|[2601.13401](http://arxiv.org/abs/2601.13401)|null|
|**2026-01-16**|**ShapeR: Robust Conditional 3D Shape Generation from Casual Captures**|Jakob Engel Team|[2601.11514](http://arxiv.org/abs/2601.11514)|**[link](http://facebookresearch.github.io/ShapeR)**|
|**2026-01-16**|**MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Models**|Tao Gui Team|[2601.11464](http://arxiv.org/abs/2601.11464)|null|
|**2026-01-16**|**ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models**|Guanghui Ren Team|[2601.11404](http://arxiv.org/abs/2601.11404)|null|
|**2026-01-16**|**Enhancing Vision Language Models with Logic Reasoning for Situational Awareness**|Suya Yu Team|[2601.11322](http://arxiv.org/abs/2601.11322)|null|
|**2026-01-16**|**X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning**|Huazhe Xu Team|[2601.11269](http://arxiv.org/abs/2601.11269)|null|
|**2026-01-16**|**Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval**|Yining Chen Team|[2601.11248](http://arxiv.org/abs/2601.11248)|null|
|**2026-01-16**|**Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification**|Gaurav Sharma Team|[2601.11243](http://arxiv.org/abs/2601.11243)|null|
|**2026-01-16**|**MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement**|Linlin Shen Team|[2601.10949](http://arxiv.org/abs/2601.10949)|null|
|**2026-01-16**|**PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis**|Anand Mishra Team|[2601.10945](http://arxiv.org/abs/2601.10945)|null|
|**2026-01-15**|**Can Vision-Language Models Understand Construction Workers? An Exploratory Study**|Arash Tavakoli Team|[2601.10835](http://arxiv.org/abs/2601.10835)|null|
|**2026-01-15**|**Future Optical Flow Prediction Improves Robot Control & Video Generation**|Juan Carlos Niebles Team|[2601.10781](http://arxiv.org/abs/2601.10781)|**[link](https://fofpred.github.io)**|
|**2026-01-15**|**Alterbute: Editing Intrinsic Attributes of Objects in Images**|Yedid Hoshen Team|[2601.10714](http://arxiv.org/abs/2601.10714)|**[link](https://talreiss.github.io/alterbute/)**|
|**2026-01-15**|**From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion**|Lianli Gao Team|[2601.10710](http://arxiv.org/abs/2601.10710)|null|
|**2026-01-15**|**Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding**|Ranjay Krishna Team|[2601.10611](http://arxiv.org/abs/2601.10611)|null|
|**2026-01-15**|**Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure**|Zhen Dong Team|[2601.10551](http://arxiv.org/abs/2601.10551)|null|
|**2026-01-15**|**SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery**|Bisheng Yang Team|[2601.10535](http://arxiv.org/abs/2601.10535)|null|
|**2026-01-16**|**MERGETUNE: Continued fine-tuning of vision-language models**|Josef Kittler Team|[2601.10497](http://arxiv.org/abs/2601.10497)|null|
|**2026-01-15**|**Urban Socio-Semantic Segmentation with Vision-Language Reasoning**|Yansheng Li Team|[2601.10477](http://arxiv.org/abs/2601.10477)|null|
|**2026-01-15**|**SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability**|Nassir Navab Team|[2601.10455](http://arxiv.org/abs/2601.10455)|null|
|**2026-01-15**|**Global Context Compression with Interleaved Vision-Text Transformation**|Feng Huang Team|[2601.10378](http://arxiv.org/abs/2601.10378)|null|
|**2026-01-15**|**Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models**|Zi Huang Team|[2601.10313](http://arxiv.org/abs/2601.10313)|null|
|**2026-01-15**|**V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation**|Wei Chen Team|[2601.10094](http://arxiv.org/abs/2601.10094)|null|
|**2026-01-15**|**The Spatial Blindspot of Vision-Language Models**|Bala Krishna S Vegesna Team|[2601.09954](http://arxiv.org/abs/2601.09954)|null|
|**2026-01-14**|**MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation**|Kuang Gong Team|[2601.09879](http://arxiv.org/abs/2601.09879)|null|
|**2026-01-14**|**Bears, all bears, and some bears. Language Constraints on Language Models' Inductive Inferences**|Kanishka Misra Team|[2601.09852](http://arxiv.org/abs/2601.09852)|null|
|**2026-01-14**|**ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning**|Sandeep Chinchali Team|[2601.09851](http://arxiv.org/abs/2601.09851)|null|
|**2026-01-14**|**GUI-Eyes: Tool-Augmented Perception for Visual Grounding in GUI Agents**|Wu Liu Team|[2601.09770](http://arxiv.org/abs/2601.09770)|null|
|**2026-01-14**|**Self-Supervised Animal Identification for Long Videos**|Neill Campbell Team|[2601.09663](http://arxiv.org/abs/2601.09663)|null|
|**2026-01-14**|**LiteEmbed: Adapting CLIP to Rare Classes**|Vineet Gandhi Team|[2601.09661](http://arxiv.org/abs/2601.09661)|null|
|**2026-01-14**|**Image2Garment: Simulation-ready Garment Generation from a Single Image**|Gordon Wetzstein Team|[2601.09658](http://arxiv.org/abs/2601.09658)|null|
|**2026-01-14**|**OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding**|Cheng Sun Team|[2601.09575](http://arxiv.org/abs/2601.09575)|**[link](https://peterjohnsonhuang.github.io/openvoxel-pages/)**|
|**2026-01-14**|**PrivLEX: Detecting legal concepts in images through Vision-Language Models**|Andrea Cavallaro Team|[2601.09449](http://arxiv.org/abs/2601.09449)|null|
|**2026-01-14**|**SSVP: Synergistic Semantic-Visual Prompting for Industrial Zero-Shot Anomaly Detection**|Xuelong Li Team|[2601.09147](http://arxiv.org/abs/2601.09147)|null|
|**2026-01-14**|**SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL**|Hong-Yu Zhou Team|[2601.09136](http://arxiv.org/abs/2601.09136)|null|
|**2026-01-14**|**LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models**|Hongbin Liu Team|[2601.09116](http://arxiv.org/abs/2601.09116)|null|
|**2026-01-13**|**Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation**|Miao Li Team|[2601.09031](http://arxiv.org/abs/2601.09031)|null|
|**2026-01-13**|**SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning**|Jose Dolz Team|[2601.08617](http://arxiv.org/abs/2601.08617)|null|
|**2026-01-13**|**VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations**|Pål Halvorsen Team|[2601.08557](http://arxiv.org/abs/2601.08557)|null|
|**2026-01-13**|**Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse**|Haoran Xie Team|[2601.08531](http://arxiv.org/abs/2601.08531)|null|
|**2026-01-13**|**Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models**|Jing Qin Team|[2601.08476](http://arxiv.org/abs/2601.08476)|null|
|**2026-01-13**|**Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs**|Atsushi Hashimoto Team|[2601.08470](http://arxiv.org/abs/2601.08470)|null|
|**2026-01-13**|**Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling**|Andrew Morris Team|[2601.08467](http://arxiv.org/abs/2601.08467)|null|
|**2026-01-13**|**CoMa: Contextual Massing Generation with Vision-Language Models**|Ivan Oseledets Team|[2601.08464](http://arxiv.org/abs/2601.08464)|null|
|**2026-01-13**|**Real2Sim based on Active Perception with automatically VLM-generated Behavior Trees**|Pietro Falco Team|[2601.08454](http://arxiv.org/abs/2601.08454)|null|
|**2026-01-13**|**MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP**|Biplab Banerjee Team|[2601.08420](http://arxiv.org/abs/2601.08420)|null|
|**2026-01-13**|**Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2**|Tian Wang Team|[2601.08408](http://arxiv.org/abs/2601.08408)|null|
|**2026-01-13**|**Semantic Misalignment in Vision-Language Models under Perceptual Degradation**|Guo Cheng Team|[2601.08355](http://arxiv.org/abs/2601.08355)|null|
|**2026-01-13**|**ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation**|Yanwei Fu Team|[2601.08325](http://arxiv.org/abs/2601.08325)|null|
|**2026-01-13**|**Route, Retrieve, Reflect, Repair: Self-Improving Agentic Framework for Visual Detection and Linguistic Reasoning in Medical Imaging**|AKM Mahbubur Rahman Team|[2601.08192](http://arxiv.org/abs/2601.08192)|null|
|**2026-01-13**|**Subspace Alignment for Vision-Language Model Test-time Adaptation**|Hanghang Tong Team|[2601.08139](http://arxiv.org/abs/2601.08139)|null|
|**2026-01-12**|**Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling**|Prem Natarajan Team|[2601.08040](http://arxiv.org/abs/2601.08040)|null|
|**2026-01-12**|**A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs**|Lionel Briand Team|[2601.08024](http://arxiv.org/abs/2601.08024)|null|
|**2026-01-12**|**Representations of Text and Images Align From Layer One**|Stanislav Fort Team|[2601.08017](http://arxiv.org/abs/2601.08017)|null|
|**2026-01-12**|**CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation**|Pooyan Fazli Team|[2601.08010](http://arxiv.org/abs/2601.08010)|null|
|**2026-01-12**|**VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding**|Qiufeng Yi Team|[2601.07986](http://arxiv.org/abs/2601.07986)|null|
|**2026-01-12**|**Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models**|Qiufeng Yi Team|[2601.07984](http://arxiv.org/abs/2601.07984)|null|
|**2026-01-12**|**Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests**|Hendrik Buschmeier Team|[2601.07820](http://arxiv.org/abs/2601.07820)|null|
|**2026-01-12**|**More Images, More Problems? A Controlled Analysis of VLM Failure Modes**|Brais Martinez Team|[2601.07812](http://arxiv.org/abs/2601.07812)|null|
|**2026-01-12**|**Vision-Language Model for Accurate Crater Detection**|Hichem Snoussi Team|[2601.07795](http://arxiv.org/abs/2601.07795)|null|
|**2026-01-12**|**OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent**|Zichen Ding Team|[2601.07779](http://arxiv.org/abs/2601.07779)|null|
|**2026-01-12**|**Video Evidence to Reasoning Efficient Video Understanding via Explicit Evidence Grounding**|Jianyuan Ni Team|[2601.07761](http://arxiv.org/abs/2601.07761)|null|
|**2026-01-12**|**Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model**|Yang Cai Team|[2601.07695](http://arxiv.org/abs/2601.07695)|null|
|**2026-01-12**|**VirtualEnv: A Platform for Embodied AI Research**|Antonio Torralba Team|[2601.07553](http://arxiv.org/abs/2601.07553)|null|
|**2026-01-12**|**Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions**|Yongbin Li Team|[2601.07516](http://arxiv.org/abs/2601.07516)|null|
|**2026-01-12**|**VLM-CAD: VLM-Optimized Collaborative Agent Design Workflow for Analog Circuit Sizing**|Yaqi Wang Team|[2601.07315](http://arxiv.org/abs/2601.07315)|null|
|**2026-01-12**|**A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model**|Xuming Hu Team|[2601.07291](http://arxiv.org/abs/2601.07291)|null|
|**2026-01-12**|**MEDVISTAGYM: A Scalable Training Environment for Thinking with Medical Images via Tool-Integrated Reinforcement Learning**|Xuan Wang Team|[2601.07107](http://arxiv.org/abs/2601.07107)|null|
|**2026-01-11**|**Efficient Visual Question Answering Pipeline for Autonomous Driving via Scene Region Compression**|Chongruo Wu Team|[2601.07092](http://arxiv.org/abs/2601.07092)|null|
|**2026-01-11**|**Measuring Social Bias in Vision-Language Models with Face-Only Counterfactuals from Real Photos**|Jun Yu Team|[2601.06931](http://arxiv.org/abs/2601.06931)|null|
|**2026-01-11**|**CLIMP: Contrastive Language-Image Mamba Pretraining**|Raja Giryes Team|[2601.06891](http://arxiv.org/abs/2601.06891)|null|
|**2026-01-11**|**MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data**|Yisheng Lv Team|[2601.06847](http://arxiv.org/abs/2601.06847)|null|
|**2026-01-11**|**Forest Before Trees: Latent Superposition for Efficient Visual Reasoning**|Yuhan Liu Team|[2601.06803](http://arxiv.org/abs/2601.06803)|null|
|**2026-01-10**|**ArrowGEV: Grounding Events in Video via Learning the Arrow of Time**|Jie Zhou Team|[2601.06559](http://arxiv.org/abs/2601.06559)|null|
|**2026-01-10**|**VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment**|Qiong Luo Team|[2601.06475](http://arxiv.org/abs/2601.06475)|null|
|**2026-01-10**|**SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning**|Yan Wang Team|[2601.06474](http://arxiv.org/abs/2601.06474)|null|
|**2026-01-10**|**On the Adversarial Robustness of 3D Large Vision-Language Models**|Ngai-Man Cheung Team|[2601.06464](http://arxiv.org/abs/2601.06464)|null|
|**2026-01-09**|**Open-Vocabulary 3D Instruction Ambiguity Detection**|Ge Li Team|[2601.05991](http://arxiv.org/abs/2601.05991)|null|
|**2026-01-09**|**Context-Aware Decoding for Faithful Vision-Language Generation**|Ziwei Zhu Team|[2601.05939](http://arxiv.org/abs/2601.05939)|null|
|**2026-01-09**|**Router-Suggest: Dynamic Routing for Multimodal Auto-Completion in Visually-Grounded Dialogs**|Manish Gupta Team|[2601.05851](http://arxiv.org/abs/2601.05851)|null|
|**2026-01-09**|**From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation**|Yan Lu Team|[2601.05787](http://arxiv.org/abs/2601.05787)|null|
|**2026-01-09**|**PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility**|Zhouxing Shi Team|[2601.05739](http://arxiv.org/abs/2601.05739)|null|
|**2026-01-09**|**SGDrive: Scene-to-Goal Hierarchical World Cognition for Autonomous Driving**|Li Zhang Team|[2601.05640](http://arxiv.org/abs/2601.05640)|null|
|**2026-01-09**|**LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction**|Hongyang Li Team|[2601.05611](http://arxiv.org/abs/2601.05611)|null|
|**2026-01-09**|**VIB-Probe: Detecting and Mitigating Hallucinations in Vision-Language Models via Variational Information Bottleneck**|Xiaoqing Zheng Team|[2601.05547](http://arxiv.org/abs/2601.05547)|null|
|**2026-01-09**|**Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making**|Jihie Kim Team|[2601.05529](http://arxiv.org/abs/2601.05529)|null|
|**2026-01-08**|**Thinking with Map: Reinforced Parallel Map-Augmented Agent for Geolocalization**|Xiangxiang Chu Team|[2601.05432](http://arxiv.org/abs/2601.05432)|null|
|**2026-01-08**|**Coding the Visual World: From Image to Simulation Using Vision Language Models**|Sagi Eppel Team|[2601.05344](http://arxiv.org/abs/2601.05344)|null|
|**2026-01-08**|**Intent at a Glance: Gaze-Guided Robotic Manipulation via Foundation Models**|Yuchen Cui Team|[2601.05336](http://arxiv.org/abs/2601.05336)|null|
|**2026-01-08**|**Mechanisms of Prompt-Induced Hallucination in Vision-Language Models**|Kyle Mahowald Team|[2601.05201](http://arxiv.org/abs/2601.05201)|null|
|**2026-01-09**|**CoV: Chain-of-View Prompting for Spatial Reasoning**|Bohan Zhuang Team|[2601.05172](http://arxiv.org/abs/2601.05172)|**[link](https://github.com/ziplab/CoV)**|
|**2026-01-08**|**VERSE: Visual Embedding Reduction and Space Exploration. Clustering-Guided Insights for Training Data Enhancement in Visually-Rich Document Understanding**|Jaime Boal Team|[2601.05125](http://arxiv.org/abs/2601.05125)|null|
|**2026-01-08**|**From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)**|Anubhav Girdhar Team|[2601.05059](http://arxiv.org/abs/2601.05059)|null|
|**2026-01-08**|**Scaling Vision Language Models for Pharmaceutical Long Form Video Reasoning on Industrial GenAI Platform**|Baddu Narendra Team|[2601.04891](http://arxiv.org/abs/2601.04891)|null|
|**2026-01-09**|**SOVABench: A Vehicle Surveillance Action Retrieval Benchmark for Multimodal Large Language Models**|Sergio Escalera Team|[2601.04824](http://arxiv.org/abs/2601.04824)|null|
|**2026-01-08**|**AECV-Bench: Benchmarking Multimodal Models on Architectural and Engineering Drawings Understanding**|Guido Maciocci Team|[2601.04819](http://arxiv.org/abs/2601.04819)|null|
|**2026-01-08**|**Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition**|Masahiro Okuda Team|[2601.04752](http://arxiv.org/abs/2601.04752)|null|
|**2026-01-08**|**Agri-R1: Empowering Generalizable Agricultural Reasoning in Vision-Language Models with Reinforcement Learning**|Tao Fang Team|[2601.04672](http://arxiv.org/abs/2601.04672)|null|
|**2026-01-08**|**When More Words Say Less: Decoupling Length and Specificity in Image Description Evaluation**|Elisa Kreiss Team|[2601.04609](http://arxiv.org/abs/2601.04609)|null|
|**2026-01-08**|**Vision-Language Agents for Interactive Forest Change Analysis**|Nantheera Anantrasirichai Team|[2601.04497](http://arxiv.org/abs/2601.04497)|null|
|**2026-01-07**|**UniDrive-WM: Unified Understanding, Planning and Generation World Model For Autonomous Driving**|Liu Ren Team|[2601.04453](http://arxiv.org/abs/2601.04453)|**[link](https://unidrive-wm.github.io/UniDrive-WM)**|
|**2026-01-07**|**Addressing Overthinking in Large Vision-Language Models via Gated Perception-Reasoning Optimization**|Jiang Gui Team|[2601.04442](http://arxiv.org/abs/2601.04442)|null|
|**2026-01-07**|**3D-Agent:Tri-Modal Multi-Agent Collaboration for Scalable 3D Object Annotation**|Keze Wang Team|[2601.04404](http://arxiv.org/abs/2601.04404)|null|
|**2026-01-07**|**Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models**|Mattias Rantalainen Team|[2601.04163](http://arxiv.org/abs/2601.04163)|null|
|**2026-01-07**|**Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning**|Anil Kag Team|[2601.04153](http://arxiv.org/abs/2601.04153)|null|
|**2026-01-08**|**GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning**|Yuxin Hu Team|[2601.04118](http://arxiv.org/abs/2601.04118)|null|
|**2026-01-07**|**CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM**|Chang Liu Team|[2601.03956](http://arxiv.org/abs/2601.03956)|null|
|**2026-01-07**|**FocusUI: Efficient UI Grounding via Position-Preserving Visual Token Selection**|Hwee Tou Ng Team|[2601.03928](http://arxiv.org/abs/2601.03928)|null|
|**2026-01-07**|**Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models**|Hwanhee Lee Team|[2601.03926](http://arxiv.org/abs/2601.03926)|null|
|**2026-01-07**|**HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis**|Petru Manescu Team|[2601.03915](http://arxiv.org/abs/2601.03915)|null|
|**2026-01-07**|**Current Agents Fail to Leverage World Model as Tool for Foresight**|Heng Ji Team|[2601.03905](http://arxiv.org/abs/2601.03905)|null|
|**2026-01-07**|**RadDiff: Describing Differences in Radiology Image Sets with Natural Language**|Serena Yeung-Levy Team|[2601.03733](http://arxiv.org/abs/2601.03733)|null|
|**2026-01-07**|**BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion**|Hongbin Liu Team|[2601.03713](http://arxiv.org/abs/2601.03713)|null|
|**2026-01-07**|**e5-omni: Explicit Cross-modal Alignment for Omni-modal Embeddings**|Zhicheng Dou Team|[2601.03666](http://arxiv.org/abs/2601.03666)|null|
|**2026-01-07**|**Jailbreaking LLMs & VLMs: Mechanisms, Evaluation, and Unified Defense**|Yiming He Team|[2601.03594](http://arxiv.org/abs/2601.03594)|null|
|**2026-01-07**|**Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions**|Ping Jian Team|[2601.03590](http://arxiv.org/abs/2601.03590)|null|
|**2026-01-07**|**Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach**|Xiang Yan Team|[2601.03534](http://arxiv.org/abs/2601.03534)|null|
|**2026-01-07**|**SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models**|Peng Li Team|[2601.03500](http://arxiv.org/abs/2601.03500)|null|
|**2026-01-06**|**FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder**|Yu Sun Team|[2601.03460](http://arxiv.org/abs/2601.03460)|null|
|**2026-01-06**|**FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin**|Fatemeh Afghah Team|[2601.03449](http://arxiv.org/abs/2601.03449)|null|
|**2026-01-06**|**Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning**|Ehsaneddin Asgari Team|[2601.03400](http://arxiv.org/abs/2601.03400)|null|
|**2026-01-06**|**MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models**|Zhiqi Huang Team|[2601.03331](http://arxiv.org/abs/2601.03331)|null|
|**2026-01-06**|**ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios**|Enwen Hu Team|[2601.03011](http://arxiv.org/abs/2601.03011)|null|
|**2026-01-06**|**Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning**|Shangchen Zhou Team|[2601.02918](http://arxiv.org/abs/2601.02918)|**[link](https://ethanliang99.github.io/ZOOMIQA-Projectpage)**|
|**2026-01-06**|**RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance**|Liyong Ren Team|[2601.02888](http://arxiv.org/abs/2601.02888)|null|
|**2026-01-05**|**WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks**|Spencer Whitehead Team|[2601.02439](http://arxiv.org/abs/2601.02439)|null|
|**2026-01-05**|**VINO: A Unified Visual Generator with Interleaved OmniModal Context**|Weicai Ye Team|[2601.02358](http://arxiv.org/abs/2601.02358)|**[link](https://sotamak1r.github.io/VINO-web/)**|
|**2026-01-05**|**DatBench: Discriminative, Faithful, and Efficient VLM Evaluations**|Matthew Leavitt Team|[2601.02316](http://arxiv.org/abs/2601.02316)|null|
|**2026-01-05**|**BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models**|Amit Sethi Team|[2601.02147](http://arxiv.org/abs/2601.02147)|null|
|**2026-01-05**|**Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot**|Maoqing Yao Team|[2601.02078](http://arxiv.org/abs/2601.02078)|null|
|**2026-01-05**|**Agentic Retoucher for Text-To-Image Generation**|Guangtao Zhai Team|[2601.02046](http://arxiv.org/abs/2601.02046)|null|
|**2026-01-05**|**Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation**|Jiang Bian Team|[2601.01984](http://arxiv.org/abs/2601.01984)|null|
|**2026-01-05**|**AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing**|Xianglong Liu Team|[2601.01957](http://arxiv.org/abs/2601.01957)|null|
|**2026-01-05**|**Learning Diffusion Policy from Primitive Skills for Robot Manipulation**|Dong Xu Team|[2601.01948](http://arxiv.org/abs/2601.01948)|null|
|**2026-01-05**|**MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning**|Huynh Thi Thanh Binh Team|[2601.01910](http://arxiv.org/abs/2601.01910)|null|
|**2026-01-05**|**A Hybrid Architecture for Multi-Stage Claim Document Understanding: Combining Vision-Language Models and Machine Learning for Real-Time Processing**|Sean Ho Team|[2601.01897](http://arxiv.org/abs/2601.01897)|null|
|**2026-01-05**|**Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence**|Hongxiao Wang Team|[2601.01875](http://arxiv.org/abs/2601.01875)|null|
|**2026-01-05**|**Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion**|Ruili Wang Team|[2601.01870](http://arxiv.org/abs/2601.01870)|null|
|**2026-01-05**|**VerLM: Explaining Face Verification Using Natural Language**|Bhiksha Raj Team|[2601.01798](http://arxiv.org/abs/2601.01798)|null|
|**2026-01-05**|**Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization**|Haohan Wang Team|[2601.01747](http://arxiv.org/abs/2601.01747)|null|
|**2026-01-04**|**FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation**|Peiyu Liu Team|[2601.01513](http://arxiv.org/abs/2601.01513)|null|
|**2026-01-04**|**Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization**|Linchao Zhu Team|[2601.01483](http://arxiv.org/abs/2601.01483)|null|
|**2026-01-04**|**AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval**|Xingzhao Liu Team|[2601.01416](http://arxiv.org/abs/2601.01416)|null|
|**2026-01-04**|**LinMU: Multimodal Understanding Made Linear**|Niraj K. Jha Team|[2601.01322](http://arxiv.org/abs/2601.01322)|null|
|**2026-01-03**|**SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models**|Yunlin Zeng Team|[2601.01062](http://arxiv.org/abs/2601.01062)|null|
|**2026-01-03**|**ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval**|Thanh Duc Ngo Team|[2601.01024](http://arxiv.org/abs/2601.01024)|null|
|**2026-01-02**|**Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection**|Abhinav Dhall Team|[2601.00777](http://arxiv.org/abs/2601.00777)|null|
|**2026-01-02**|**Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model**|Li Zhou Team|[2601.00716](http://arxiv.org/abs/2601.00716)|null|
|**2026-01-02**|**RoboReward: General-Purpose Vision-Language Reward Models for Robotics**|Chelsea Finn Team|[2601.00675](http://arxiv.org/abs/2601.00675)|null|
|**2026-01-02**|**CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models**|Rahul Rahaman Team|[2601.00659](http://arxiv.org/abs/2601.00659)|null|
|**2026-01-02**|**DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations**|Xuming He Team|[2601.00623](http://arxiv.org/abs/2601.00623)|null|
|**2026-01-01**|**CPPO: Contrastive Perception for Vision Language Policy Optimization**|Mohammad Akbari Team|[2601.00501](http://arxiv.org/abs/2601.00501)|null|
|**2026-01-01**|**Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach**|Jun Wang Team|[2601.00388](http://arxiv.org/abs/2601.00388)|null|
|**2026-01-01**|**FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering**|Yanbing Liu Team|[2601.00269](http://arxiv.org/abs/2601.00269)|null|
|**2026-01-01**|**TotalFM: An Organ-Separated Framework for 3D-CT Vision Foundation Models**|Tomohiro Kikuchi Team|[2601.00260](http://arxiv.org/abs/2601.00260)|null|
|**2026-01-01**|**Focal-RegionFace: Generating Fine-Grained Multi-attribute Descriptions for Arbitrarily Selected Face Focal Regions**|Xuri Ge Team|[2601.00156](http://arxiv.org/abs/2601.00156)|null|
|**2026-01-01**|**FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications**|Tao Chen Team|[2601.00150](http://arxiv.org/abs/2601.00150)|null|
|**2025-12-31**|**Explicit Abstention Knobs for Predictable Reliability in Video Question Answering**|Jorge Ortiz Team|[2601.00138](http://arxiv.org/abs/2601.00138)|null|
|**2025-12-31**|**DarkEQA: Benchmarking Vision-Language Models for Embodied Question Answering in Low-Light Indoor Environments**|Tae-Hyun Oh Team|[2512.24985](http://arxiv.org/abs/2512.24985)|null|
|**2025-12-31**|**CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement**|Weihe Zhong Team|[2512.24947](http://arxiv.org/abs/2512.24947)|null|
|**2025-12-31**|**Video and Language Alignment in 2D Systems for 3D Multi-object Scenes with Multi-Information Derivative-Free Control**|Rico Sennnrich Team|[2512.24826](http://arxiv.org/abs/2512.24826)|null|
|**2025-12-31**|**LSRE: Latent Semantic Rule Encoding for Real-Time Semantic Risk Detection in Autonomous Driving**|Diange Yang Team|[2512.24712](http://arxiv.org/abs/2512.24712)|null|
|**2025-12-31**|**Improving Few-Shot Change Detection Visual Question Answering via Decision-Ambiguity-guided Reinforcement Fine-Tuning**|Quan Wang Team|[2512.24591](http://arxiv.org/abs/2512.24591)|null|
|**2025-12-31**|**PhyGDPO: Physics-Aware Groupwise Direct Preference Optimization for Physically Consistent Text-to-Video Generation**|Ji Hou Team|[2512.24551](http://arxiv.org/abs/2512.24551)|null|
|**2025-12-30**|**Foundation models on the bridge: Semantic hazard detection and safety maneuvers for maritime autonomy with vision-language models**|Martin Steinert Team|[2512.24470](http://arxiv.org/abs/2512.24470)|null|
|**2025-12-30**|**DermaVQA-DAS: Dermatology Assessment Schema (DAS) & Datasets for Closed-Ended Question Answering & Segmentation in Patient-Generated Dermatology Images**|Josep Malvehy Team|[2512.24340](http://arxiv.org/abs/2512.24340)|null|
|**2025-12-30**|**Spatial-aware Vision Language Model for Autonomous Driving**|Venice Erin Liong Team|[2512.24331](http://arxiv.org/abs/2512.24331)|null|
|**2025-12-30**|**SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning**|Lewei Lu Team|[2512.24330](http://arxiv.org/abs/2512.24330)|null|
|**2025-12-30**|**RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation**|Wenjun Wu Team|[2512.24212](http://arxiv.org/abs/2512.24212)|null|
|**2025-12-30**|**Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training**|Jianlan Luo Team|[2512.24125](http://arxiv.org/abs/2512.24125)|null|
|**2025-12-30**|**GeoBench: Rethinking Multimodal Geometric Problem-Solving via Hierarchical Evaluation**|Junchi Yan Team|[2512.24119](http://arxiv.org/abs/2512.24119)|null|
|**2025-12-30**|**FUSE-RSVLM: Feature Fusion Vision-Language Model for Remote Sensing**|Yang Gao Team|[2512.24022](http://arxiv.org/abs/2512.24022)|null|
|**2025-12-29**|**Same or Not? Enhancing Visual Perception in Vision-Language Models**|Georgia Gkioxari Team|[2512.23592](http://arxiv.org/abs/2512.23592)|**[link](https://glab-caltech.github.io/twin/)**|
|**2025-12-29**|**Instruction-Following Evaluation of Large Vision-Language Models**|Jun Suzuki Team|[2512.23572](http://arxiv.org/abs/2512.23572)|null|
|**2025-12-29**|**VL-RouterBench: A Benchmark for Vision-Language Model Routing**|Xiaolin Huang Team|[2512.23562](http://arxiv.org/abs/2512.23562)|null|
|**2025-12-29**|**Toward Trustworthy Agentic AI: A Multimodal Framework for Preventing Prompt Injection Attacks**|Mahmoud Abdel Moaty Team|[2512.23557](http://arxiv.org/abs/2512.23557)|null|
|**2025-12-29**|**PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis**|Xiaofan Zhang Team|[2512.23545](http://arxiv.org/abs/2512.23545)|null|
|**2025-12-30**|**UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?**|Xuezhi Cao Team|[2512.23512](http://arxiv.org/abs/2512.23512)|null|
|**2025-12-29**|**CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models**|Zepeng Wang Team|[2512.23453](http://arxiv.org/abs/2512.23453)|null|
|**2025-12-29**|**SpatialMosaic: A Multiview VLM Dataset for Partial Visibility**|Jaesik Park Team|[2512.23365](http://arxiv.org/abs/2512.23365)|null|
|**2025-12-29**|**Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control**|Syed Bahauddin Alam Team|[2512.23292](http://arxiv.org/abs/2512.23292)|null|
|**2025-12-29**|**ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing**|Bin Wang Team|[2512.23244](http://arxiv.org/abs/2512.23244)|null|
|**2025-12-29**|**Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism**|Runhe Qiu Team|[2512.23243](http://arxiv.org/abs/2512.23243)|null|
|**2025-12-29**|**GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation**|Hesheng Wang Team|[2512.23180](http://arxiv.org/abs/2512.23180)|null|
|**2025-12-28**|**How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure**|Paul M. Thompson Team|[2512.23109](http://arxiv.org/abs/2512.23109)|null|
|**2025-12-28**|**Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients**|Rafet Sifa Team|[2512.23090](http://arxiv.org/abs/2512.23090)|null|
|**2025-12-28**|**Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models**|Yanan Sui Team|[2512.23077](http://arxiv.org/abs/2512.23077)|null|
|**2025-12-28**|**Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models**|Yun Fu Team|[2512.23073](http://arxiv.org/abs/2512.23073)|null|
|**2025-12-28**|**Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion**|Yuanchun Shi Team|[2512.23035](http://arxiv.org/abs/2512.23035)|null|
|**2025-12-28**|**An Architecture-Led Hybrid Report on Body Language Detection Project**|Diba Darooneh Team|[2512.23028](http://arxiv.org/abs/2512.23028)|null|
|**2025-12-28**|**ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving**|Hongsheng Li Team|[2512.22939](http://arxiv.org/abs/2512.22939)|null|
|**2025-12-28**|**Multimodal Fact-Checking: An Agent-based Approach**|Mohan Kankanhalli Team|[2512.22933](http://arxiv.org/abs/2512.22933)|**[link](https://github.com/xudanni0927/AgentFact)**|
|**2025-12-26**|**See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning**|Rui Wang Team|[2512.22120](http://arxiv.org/abs/2512.22120)|null|
|**2025-12-26**|**Look Closer! An Adversarial Parametric Editing Framework for Hallucination Mitigation in VLMs**|Zhongshi He Team|[2512.21999](http://arxiv.org/abs/2512.21999)|null|
|**2025-12-26**|**LVLM-Aided Alignment of Task-Specific Vision Models**|Florian Buettner Team|[2512.21985](http://arxiv.org/abs/2512.21985)|null|
|**2025-12-26**|**Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?**|Shouling Ji Team|[2512.21871](http://arxiv.org/abs/2512.21871)|null|
|**2025-12-26**|**Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion**|Xiaoyu Du Team|[2512.21863](http://arxiv.org/abs/2512.21863)|null|
|**2025-12-26**|**Training-free Conditional Image Embedding Framework Leveraging Large Vision Language Models**|Naoto Inoue Team|[2512.21860](http://arxiv.org/abs/2512.21860)|null|
|**2025-12-26**|**Few Tokens Matter: Entropy Guided Attacks on Vision-Language Models**|Jing Zhang Team|[2512.21815](http://arxiv.org/abs/2512.21815)|null|
|**2025-12-25**|**Scene-VLM: Multimodal Video Scene Segmentation via Vision-Language Models**|Igor Kviatkovsky Team|[2512.21778](http://arxiv.org/abs/2512.21778)|null|
|**2025-12-25**|**MAction-SocialNav: Multi-Action Socially Compliant Navigation via Reasoning-enhanced Prompt Tuning**|Ling Xiao Team|[2512.21722](http://arxiv.org/abs/2512.21722)|null|
|**2025-12-25**|**Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning**|Nilaan Loganathan Team|[2512.21699](http://arxiv.org/abs/2512.21699)|null|
|**2025-12-25**|**SlideChain: Semantic Provenance for Lecture Understanding via Blockchain Registration**|Ge Wang Team|[2512.21684](http://arxiv.org/abs/2512.21684)|null|
|**2025-12-25**|**The Deepfake Detective: Interpreting Neural Forensics Through Sparse Features and Manifolds**|Jared Junkin Team|[2512.21670](http://arxiv.org/abs/2512.21670)|null|
|**2025-12-25**|**Training-Free Disentangled Text-Guided Image Editing via Sparse Latent Constraints**|Novanto Yudistira Team|[2512.21637](http://arxiv.org/abs/2512.21637)|null|
|**2025-12-25**|**A Medical Multimodal Diagnostic Framework Integrating Vision-Language Models and Logic Tree Reasoning**|Fuji Yang Team|[2512.21583](http://arxiv.org/abs/2512.21583)|null|
|**2025-12-25**|**Towards Long-window Anchoring in Vision-Language Model Distillation**|Jianxin Li Team|[2512.21576](http://arxiv.org/abs/2512.21576)|null|
|**2025-12-25**|**Toward Intelligent Scene Augmentation for Context-Aware Object Placement and Sponsor-Logo Integration**|Dhruv Kumar Team|[2512.21560](http://arxiv.org/abs/2512.21560)|null|
|**2025-12-25**|**Hierarchy-Aware Fine-Tuning of Vision-Language Models**|Juhua Hu Team|[2512.21529](http://arxiv.org/abs/2512.21529)|null|
|**2025-12-25**|**Fixed-Budget Parameter-Efficient Training with Frozen Encoders Improves Multimodal Chest X-Ray Classification**|Md Nahid Siddique Team|[2512.21508](http://arxiv.org/abs/2512.21508)|null|
|**2025-12-25**|**RLLaVA: An RL-central Framework for Language and Vision Assistants**|Lei Huang Team|[2512.21450](http://arxiv.org/abs/2512.21450)|**[link](https://github.com/TinyLoopX/RLLaVA)**|
|**2025-12-24**|**A Tool Bottleneck Framework for Clinically-Informed and Interpretable Medical Image Understanding**|Ehsan Adeli Team|[2512.21414](http://arxiv.org/abs/2512.21414)|null|
|**2025-12-24**|**Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models**|Yu-Lun Liu Team|[2512.21337](http://arxiv.org/abs/2512.21337)|**[link](https://sytwu.github.io/BeyondMemo/)**|
|**2025-12-24**|**Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks**|Jiaqi W. Ma Team|[2512.21329](http://arxiv.org/abs/2512.21329)|null|
|**2025-12-24**|**LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation**|Aleksandr I. Panov Team|[2512.21243](http://arxiv.org/abs/2512.21243)|null|
|**2025-12-24**|**Leveraging Lightweight Entity Extraction for Scalable Event-Based Image Retrieval**|Tran Chi Nguyen Team|[2512.21221](http://arxiv.org/abs/2512.21221)|null|
|**2025-12-24**|**RoboSafe: Safeguarding Embodied Agents via Executable Safety Logic**|Xianglong Liu Team|[2512.21220](http://arxiv.org/abs/2512.21220)|null|
|**2025-12-24**|**VisRes Bench: On Evaluating the Visual Reasoning Capabilities of VLMs**|Sanath Narayan Team|[2512.21194](http://arxiv.org/abs/2512.21194)|null|
|**2025-12-24**|**MarineEval: Assessing the Marine Intelligence of Vision-Language Models**|Sai-Kit Yeung Team|[2512.21126](http://arxiv.org/abs/2512.21126)|null|
|**2025-12-24**|**UniRec-0.1B: Unified Text and Formula Recognition with 0.1B Parameters**|Yu-Gang Jiang Team|[2512.21095](http://arxiv.org/abs/2512.21095)|null|
|**2025-12-24**|**GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs**|Ahmad-Reza Sadeghi Team|[2512.21008](http://arxiv.org/abs/2512.21008)|null|
|**2025-12-24**|**ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments**|Yue Wang Team|[2512.20940](http://arxiv.org/abs/2512.20940)|null|
|**2025-12-24**|**Transductive Visual Programming: Evolving Tool Libraries from Experience for Spatial Reasoning**|Serena Yeung-Levy Team|[2512.20934](http://arxiv.org/abs/2512.20934)|**[link](https://transductive-visualprogram.github.io/)**|
|**2025-12-24**|**PanoGrounder: Bridging 2D and 3D with Panoramic Scene Representations for VLM-based 3D Visual Grounding**|Jongwoo Lim Team|[2512.20907](http://arxiv.org/abs/2512.20907)|null|
|**2025-12-24**|**Benchmarking and Enhancing VLM for Compressed Image Understanding**|Yan Wang Team|[2512.20901](http://arxiv.org/abs/2512.20901)|null|
|**2025-12-24**|**Proprioception Enhances Vision Language Model in Generating Captions and Subtask Segmentations for Robot Task**|Tetsuya Ogata Team|[2512.20876](http://arxiv.org/abs/2512.20876)|null|
|**2025-12-23**|**Input-Adaptive Visual Preprocessing for Efficient Fast Vision-Language Model Inference**|Novanto Yudistira Team|[2512.20839](http://arxiv.org/abs/2512.20839)|null|
|**2025-12-23**|**Soft Filtering: Guiding Zero-shot Composed Image Retrieval with Prescriptive and Proscriptive Constraints**|Sungchul Choi Team|[2512.20781](http://arxiv.org/abs/2512.20781)|null|
|**2025-12-23**|**VL4Gaze: Unleashing Vision-Language Models for Gaze Following**|Yihua Cheng Team|[2512.20735](http://arxiv.org/abs/2512.20735)|null|
|**2025-12-23**|**LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing**|Ding Zhao Team|[2512.20591](http://arxiv.org/abs/2512.20591)|null|
|**2025-12-23**|**FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models**|Keze Wang Team|[2512.20561](http://arxiv.org/abs/2512.20561)|null|
|**2025-12-23**|**Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models**|Xiaojuan Qi Team|[2512.20557](http://arxiv.org/abs/2512.20557)|null|
|**2025-12-23**|**Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios**|Jie Li Team|[2512.20556](http://arxiv.org/abs/2512.20556)|null|
|**2025-12-23**|**Chain-of-Anomaly Thoughts with Large Vision-Language Models**|David Semedo Team|[2512.20417](http://arxiv.org/abs/2512.20417)|null|
|**2025-12-23**|**CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation**|D. Timonin Team|[2512.20362](http://arxiv.org/abs/2512.20362)|null|
|**2025-12-23**|**ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge**|Xuehai Zhou Team|[2512.20276](http://arxiv.org/abs/2512.20276)|null|
|**2025-12-23**|**LADLE-MM: Limited Annotation based Detector with Learned Ensembles for Multimodal Misinformation**|Irene Amerini Team|[2512.20257](http://arxiv.org/abs/2512.20257)|null|
|**2025-12-23**|**Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation**|Lianyang Ma Team|[2512.20188](http://arxiv.org/abs/2512.20188)|null|
|**2025-12-23**|**Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark**|Hailun Lin Team|[2512.20174](http://arxiv.org/abs/2512.20174)|null|
|**2025-12-23**|**LoLA: Long Horizon Latent Action Learning for General Robot Manipulation**|Baining Guo Team|[2512.20166](http://arxiv.org/abs/2512.20166)|null|
|**2025-12-23**|**SE360: Semantic Edit in 360 $^\circ$ Panoramas via Hierarchical Data Construction**|Taehyun Rhee Team|[2512.19943](http://arxiv.org/abs/2512.19943)|null|
|**2025-12-22**|**Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis**|Pabitra Mitra Team|[2512.19663](http://arxiv.org/abs/2512.19663)|null|
|**2025-12-22**|**CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion**|Patrick Pérez Team|[2512.19535](http://arxiv.org/abs/2512.19535)|null|
|**2025-12-22**|**QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models**|Ehsan Adeli Team|[2512.19526](http://arxiv.org/abs/2512.19526)|null|
|**2025-12-22**|**MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation**|Siyuan Xu Team|[2512.19453](http://arxiv.org/abs/2512.19453)|null|
|**2025-12-22**|**EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration**|Liang Wang Team|[2512.19396](http://arxiv.org/abs/2512.19396)|null|
|**2025-12-22**|**SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models**|Michael Wijaya Team|[2512.19317](http://arxiv.org/abs/2512.19317)|null|
|**2025-12-22**|**Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing**|Jimin Liang Team|[2512.19302](http://arxiv.org/abs/2512.19302)|null|
|**2025-12-22**|**Towards Minimal Fine-Tuning of VLMs**|Honglak Lee Team|[2512.19219](http://arxiv.org/abs/2512.19219)|null|
|**2025-12-22**|**Vision-Language-Policy Model for Dynamic Robot Task Planning**|Ioannis Havoutis Team|[2512.19178](http://arxiv.org/abs/2512.19178)|null|
|**2025-12-22**|**Watch Closely: Mitigating Object Hallucinations in Large Vision-Language Models with Disentangled Decoding**|Zheng Hu Team|[2512.19070](http://arxiv.org/abs/2512.19070)|null|
|**2025-12-21**|**Delta-LLaVA: Base-then-Specialize Alignment for Token-Efficient Vision-Language Models**|Diksha Shukla Team|[2512.18910](http://arxiv.org/abs/2512.18910)|null|
|**2025-12-21**|**Thinking Beyond Labels: Vocabulary-Free Fine-Grained Recognition using Reasoning-Augmented LMMs**|Rao Anwer Team|[2512.18897](http://arxiv.org/abs/2512.18897)|null|
|**2025-12-21**|**Revealing Perception and Generation Dynamics in LVLMs: Mitigating Hallucinations via Validated Dominance Correction**|Cheng Deng Team|[2512.18813](http://arxiv.org/abs/2512.18813)|null|
|**2025-12-21**|**ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning**|Dan Negrut Team|[2512.18619](http://arxiv.org/abs/2512.18619)|null|
|**2025-12-21**|**SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback**|Yun Fu Team|[2512.18599](http://arxiv.org/abs/2512.18599)|null|
|**2025-12-21**|**Enhancing Medical Large Vision-Language Models via Alignment Distillation**|Fenglong Ma Team|[2512.18554](http://arxiv.org/abs/2512.18554)|null|
|**2025-12-20**|**GTMA: Dynamic Representation Optimization for OOD Vision-Language Models**|Keze Wang Team|[2512.18504](http://arxiv.org/abs/2512.18504)|null|
|**2025-12-20**|**Adaptive-VoCo: Complexity-Aware Visual Token Compression for Vision-Language Models**|Keze Wang Team|[2512.18496](http://arxiv.org/abs/2512.18496)|null|
|**2025-12-20**|**AmPLe: Supporting Vision-Language Models via Adaptive-Debiased Ensemble Multi-Prompt Learning**|Hui Xiong Team|[2512.18411](http://arxiv.org/abs/2512.18411)|null|
|**2025-12-20**|**Learning Semantic Atomic Skills for Multi-Task Robotic Manipulation**|Jingya Wang Team|[2512.18368](http://arxiv.org/abs/2512.18368)|null|
|**2025-12-19**|**AdaptPrompt: Parameter-Efficient Adaptation of VLMs for Generalizable Deepfake Detection**|Fakhri Karray Team|[2512.17730](http://arxiv.org/abs/2512.17730)|null|
|**2025-12-19**|**PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology**|Yongbing Zhang Team|[2512.17621](http://arxiv.org/abs/2512.17621)|null|
|**2025-12-19**|**Xiaomi MiMo-VL-Miloco Technical Report**|Pei Fu Team|[2512.17436](http://arxiv.org/abs/2512.17436)|null|
|**2025-12-19**|**ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**|Changyin Sun Team|[2512.17435](http://arxiv.org/abs/2512.17435)|null|
|**2025-12-19**|**RadImageNet-VQA: A Large-Scale CT and MRI Dataset for Radiologic Visual Question Answering**|Corentin Dancette Team|[2512.17396](http://arxiv.org/abs/2512.17396)|null|
|**2025-12-19**|**Are Vision Language Models Cross-Cultural Theory of Mind Reasoners?**|Wei Peng Team|[2512.17394](http://arxiv.org/abs/2512.17394)|null|
|**2025-12-19**|**Democratizing Pathology Co-Pilots: An Open Pipeline and Dataset for Whole-Slide Vision-Language Modelling**|Francesco Ciompi Team|[2512.17326](http://arxiv.org/abs/2512.17326)|null|
|**2025-12-19**|**A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs**|Yang Gao Team|[2512.17319](http://arxiv.org/abs/2512.17319)|null|
|**2025-12-19**|**Auxiliary Descriptive Knowledge for Few-Shot Adaptation of Vision-Language Model**|Jae-Pil Heo Team|[2512.17313](http://arxiv.org/abs/2512.17313)|null|
|**2025-12-19**|**Deep But Reliable: Advancing Multi-turn Reasoning for Thinking with Images**|Lijun Zhang Team|[2512.17306](http://arxiv.org/abs/2512.17306)|null|
|**2025-12-19**|**Vision-Language Model Guided Image Restoration**|Kin-Man Lam Team|[2512.17292](http://arxiv.org/abs/2512.17292)|null|
|**2025-12-19**|**LUMIA: A Handheld Vision-to-Music System for Real-Time, Embodied Composition**|Vealy Lai Team|[2512.17228](http://arxiv.org/abs/2512.17228)|null|
|**2025-12-19**|**DAVE: A VLM Vision Encoder for Document Understanding and Web Agents**|Roei Herzig Team|[2512.17221](http://arxiv.org/abs/2512.17221)|null|
|**2025-12-19**|**CheXPO-v2: Preference Optimization for Chest X-ray VLMs with Knowledge Graph Consistency**|Quan Wang Team|[2512.17213](http://arxiv.org/abs/2512.17213)|null|
|**2025-12-19**|**Reasoning Palette: Modulating Reasoning via Latent Contextualization for Controllable Exploration for (V)LMs**|Bo Zheng Team|[2512.17206](http://arxiv.org/abs/2512.17206)|null|
|**2025-12-19**|**Anatomical Region-Guided Contrastive Decoding: A Plug-and-Play Strategy for Mitigating Hallucinations in Medical VLMs**|Yuanyuan Shi Team|[2512.17189](http://arxiv.org/abs/2512.17189)|null|
|**2025-12-19**|**Can Synthetic Images Serve as Effective and Efficient Class Prototypes?**|Jun Wang Team|[2512.17160](http://arxiv.org/abs/2512.17160)|null|
|**2025-12-18**|**The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining**|Shivanand Sheshappanavar Team|[2512.17121](http://arxiv.org/abs/2512.17121)|null|
|**2025-12-18**|**MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning**|Koushil Sreenath Team|[2512.16909](http://arxiv.org/abs/2512.16909)|**[link](https://hybridrobotics.github.io/MomaGraph/)**|
|**2025-12-18**|**PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence**|Kai Chen Team|[2512.16793](http://arxiv.org/abs/2512.16793)|null|
|**2025-12-18**|**CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?**|Haofen Wang Team|[2512.16755](http://arxiv.org/abs/2512.16755)|null|
|**2025-12-18**|**N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models**|Dong Yu Team|[2512.16561](http://arxiv.org/abs/2512.16561)|**[link](https://n3d-vlm.github.io)**|
|**2025-12-18**|**Scaling Laws for Energy Efficiency of Local LLMs**|Román Orús Team|[2512.16531](http://arxiv.org/abs/2512.16531)|null|
|**2025-12-18**|**TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models**|Qi Li Team|[2512.16523](http://arxiv.org/abs/2512.16523)|null|
|**2025-12-18**|**SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning**|Eric Sax Team|[2512.16461](http://arxiv.org/abs/2512.16461)|null|
|**2025-12-18**|**E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion**|Dimitrios Kanoulas Team|[2512.16446](http://arxiv.org/abs/2512.16446)|null|
|**2025-12-18**|**Collaborative Edge-to-Server Inference for Vision-Language Models**|Yongjune Kim Team|[2512.16349](http://arxiv.org/abs/2512.16349)|null|
|**2025-12-18**|**ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation**|Yang Gao Team|[2512.16302](http://arxiv.org/abs/2512.16302)|null|
|**2025-12-18**|**Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis**|Yao Yuan Team|[2512.16237](http://arxiv.org/abs/2512.16237)|null|
|**2025-12-18**|**Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation**|Srimat Chakradhar Team|[2512.16201](http://arxiv.org/abs/2512.16201)|null|
|**2025-12-18**|**C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation**|Yuncheng Shen Team|[2512.16164](http://arxiv.org/abs/2512.16164)|null|
|**2025-12-18**|**MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation**|Jinman Kim Team|[2512.16145](http://arxiv.org/abs/2512.16145)|null|
|**2025-12-18**|**Auto-Vocabulary 3D Object Detection**|Raymond A. Yeh Team|[2512.16077](http://arxiv.org/abs/2512.16077)|null|
|**2025-12-17**|**Are vision-language models ready to zero-shot replace supervised classification models in agriculture?**|Mason J. Earles Team|[2512.15977](http://arxiv.org/abs/2512.15977)|null|
|**2025-12-17**|**From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection**|Sébastien Lefèvre Team|[2512.15971](http://arxiv.org/abs/2512.15971)|null|
|**2025-12-17**|**Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models**|Marco Aiello Team|[2512.15957](http://arxiv.org/abs/2512.15957)|null|
|**2025-12-17**|**R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space**|Eric Sax Team|[2512.15940](http://arxiv.org/abs/2512.15940)|null|
|**2025-12-17**|**DSO: Direct Steering Optimization for Bias Mitigation**|Nicholas Apostoloff Team|[2512.15926](http://arxiv.org/abs/2512.15926)|null|
|**2025-12-17**|**DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models**|Xinggang Wang Team|[2512.15713](http://arxiv.org/abs/2512.15713)|null|
|**2025-12-17**|**VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression**|Jason Zhang Team|[2512.15701](http://arxiv.org/abs/2512.15701)|null|
|**2025-12-17**|**VTCBench: Can Vision-Language Models Understand Long Context with Vision-Text Compression?**|Zhaoxiang Zhang Team|[2512.15649](http://arxiv.org/abs/2512.15649)|null|
|**2025-12-17**|**Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models**|Georgina Cosma Team|[2512.15372](http://arxiv.org/abs/2512.15372)|null|
|**2025-12-17**|**SynthSeg-Agents: Multi-Agent Synthetic Data Generation for Zero-Shot Weakly Supervised Semantic Segmentation**|Jimin Xiao Team|[2512.15310](http://arxiv.org/abs/2512.15310)|null|
|**2025-12-17**|**Assessing the Visual Enumeration Abilities of Specialized Counting Architectures and Vision-Language Models**|Alberto Testolin Team|[2512.15254](http://arxiv.org/abs/2512.15254)|null|
|**2025-12-17**|**Intersectional Fairness in Vision-Language Models for Medical Image Disease Classification**|Jinman Kim Team|[2512.15249](http://arxiv.org/abs/2512.15249)|null|
|**2025-12-17**|**EagleVision: A Dual-Stage Framework with BEV-grounding-based Chain-of-Thought for Spatial Intelligence**|Yifan Yang Team|[2512.15160](http://arxiv.org/abs/2512.15160)|null|
|**2025-12-16**|**Puzzle Curriculum GRPO for Vision-Centric Reasoning**|Radek Grzeszczuk Team|[2512.14944](http://arxiv.org/abs/2512.14944)|**[link](https://pcgrpo.github.io)**|
|**2025-12-16**|**Parameter Efficient Multimodal Instruction Tuning for Romanian Vision Language Models**|Dumitru-Clementin Cercel Team|[2512.14926](http://arxiv.org/abs/2512.14926)|null|
|**2025-12-16**|**Improving VQA Reliability: A Dual-Assessment Approach with Self-Reflection and Cross-Model Verification**|Longwen Gao Team|[2512.14770](http://arxiv.org/abs/2512.14770)|null|
|**2025-12-16**|**Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models**|Yiran Chen Team|[2512.14661](http://arxiv.org/abs/2512.14661)|null|
|**2025-12-16**|**A4-Agent: An Agentic Framework for Zero-Shot Affordance Reasoning**|Ying-Cong Chen Team|[2512.14442](http://arxiv.org/abs/2512.14442)|null|
|**2025-12-16**|**DISCODE: Distribution-Aware Score Decoder for Robust Automatic Evaluation of Image Captioning**|Yusuke Sekikawa Team|[2512.14420](http://arxiv.org/abs/2512.14420)|null|
|**2025-12-16**|**Vector Prism: Animating Vector Graphics by Stratifying Semantic Structure**|Jaegul Choo Team|[2512.14336](http://arxiv.org/abs/2512.14336)|null|
|**2025-12-16**|**From YOLO to VLMs: Advancing Zero-Shot and Few-Shot Detection of Wastewater Treatment Plants Using Satellite Imagery in MENA Region**|Garcia Andarcia Mariangel Team|[2512.14312](http://arxiv.org/abs/2512.14312)|null|
|**2025-12-16**|**Improving Semantic Uncertainty Quantification in LVLMs with Semantic Gaussian Processes**|Gianni Franchi Team|[2512.14177](http://arxiv.org/abs/2512.14177)|null|
|**2025-12-16**|**UIXPOSE: Mobile Malware Detection via Intention-Behaviour Discrepancy Analysis**|Van-Thuan Pham Team|[2512.14130](http://arxiv.org/abs/2512.14130)|null|
|**2025-12-16**|**Neurosymbolic Inference On Foundation Models For Remote Sensing Text-to-image Retrieval With Complex Queries**|Maarten Kruithof Team|[2512.14102](http://arxiv.org/abs/2512.14102)|null|
|**2025-12-16**|**SDAR-VL: Stable and Efficient Block-wise Diffusion for Vision-Language Understanding**|Bowen Zhou Team|[2512.14068](http://arxiv.org/abs/2512.14068)|null|
|**2025-12-16**|**OmniDrive-R1: Reinforcement-driven Interleaved Multi-modal Chain-of-Thought for Trustworthy Vision-Language Autonomous Driving**|Wuxiong Huang Team|[2512.14044](http://arxiv.org/abs/2512.14044)|null|
|**2025-12-16**|**MobileWorldBench: Towards Semantic World Modeling For Mobile Agents**|Aditya Grover Team|[2512.14014](http://arxiv.org/abs/2512.14014)|null|
|**2025-12-16**|**Autonomous Construction-Site Safety Inspection Using Mobile Robots: A Multilayer VLM-LLM Pipeline**|Abiola Akanmu Team|[2512.13974](http://arxiv.org/abs/2512.13974)|null|
|**2025-12-15**|**From Unlearning to UNBRANDING: A Benchmark for Trademark-Safe Text-to-Image Generation**|Przemysław Spurek Team|[2512.13953](http://arxiv.org/abs/2512.13953)|null|
|**2025-12-15**|**Improvise, Adapt, Overcome -- Telescopic Adapters for Efficient Fine-tuning of Vision Language Models in Medical Imaging**|Amit Shukla Team|[2512.13855](http://arxiv.org/abs/2512.13855)|null|
|**2025-12-15**|**AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection**|Yan Wang Team|[2512.13671](http://arxiv.org/abs/2512.13671)|null|
|**2025-12-15**|**RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics**|Shanghang Zhang Team|[2512.13660](http://arxiv.org/abs/2512.13660)|**[link](https://zhoues.github.io/RoboTracer)**|
|**2025-12-15**|**Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models**|Fatih Porikli Team|[2512.13609](http://arxiv.org/abs/2512.13609)|null|
|**2025-12-15**|**Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning**|Zongqing Lu Team|[2512.13380](http://arxiv.org/abs/2512.13380)|null|
|**2025-12-16**|**CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images**|Qinghui He Team|[2512.13285](http://arxiv.org/abs/2512.13285)|null|
|**2025-12-15**|**Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection**|Minhyuk Sung Team|[2512.13250](http://arxiv.org/abs/2512.13250)|**[link](https://active-view-selection.github.io/)**|
|**2025-12-15**|**Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection**|Zechang Li Team|[2512.13240](http://arxiv.org/abs/2512.13240)|null|
|**2025-12-15**|**MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion**|Weiping Ding Team|[2512.13177](http://arxiv.org/abs/2512.13177)|null|
|**2025-12-15**|**Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos**|Zongqing Lu Team|[2512.13080](http://arxiv.org/abs/2512.13080)|null|
|**2025-12-15**|**Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models**|Lihua Zhang Team|[2512.13072](http://arxiv.org/abs/2512.13072)|null|
|**2025-12-15**|**GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training**|Deheng Ye Team|[2512.13043](http://arxiv.org/abs/2512.13043)|null|
|**2025-12-15**|**TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading**|Huaxiong Huang Team|[2512.13008](http://arxiv.org/abs/2512.13008)|null|
|**2025-12-14**|**SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition**|Keith Redmill Team|[2512.12885](http://arxiv.org/abs/2512.12885)|null|
|**2025-12-14**|**VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps**|Ryo Yonetani Team|[2512.12793](http://arxiv.org/abs/2512.12793)|null|
|**2025-12-14**|**Efficient Vision-Language Reasoning via Adaptive Token Pruning**|Henry Hu Team|[2512.12701](http://arxiv.org/abs/2512.12701)|null|
|**2025-12-14**|**Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning**|Jian Liang Team|[2512.12690](http://arxiv.org/abs/2512.12690)|null|
|**2025-12-14**|**Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models**|Kazuhide Nakata Team|[2512.12596](http://arxiv.org/abs/2512.12596)|null|
|**2025-12-14**|**From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models**|Hyung-Sin Kim Team|[2512.12571](http://arxiv.org/abs/2512.12571)|null|
|**2025-12-13**|**Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings**|Yuqi Ouyang Team|[2512.12492](http://arxiv.org/abs/2512.12492)|null|
|**2025-12-13**|**More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models**|Jing Shi Team|[2512.12487](http://arxiv.org/abs/2512.12487)|null|
|**2025-12-12**|**Extending a Parliamentary Corpus with MPs' Tweets: Automatic Annotation and Evaluation Using MultiParTweet**|Alexander Mehler Team|[2512.11567](http://arxiv.org/abs/2512.11567)|null|
|**2025-12-12**|**VLM2GeoVec: Toward Universal Multimodal Embeddings for Remote Sensing**|Michael Felsberg Team|[2512.11490](http://arxiv.org/abs/2512.11490)|null|
|**2025-12-12**|**Minimal Clips, Maximum Salience: Long Video Summarization via Key Moment Extraction**|Nancy F. Chen Team|[2512.11399](http://arxiv.org/abs/2512.11399)|null|
|**2025-12-12**|**The N-Body Problem: Parallel Execution from Single-Person Egocentric Video**|Dima Damen Team|[2512.11393](http://arxiv.org/abs/2512.11393)|**[link](https://zhifanzhu.github.io/ego-nbody)**|
|**2025-12-12**|**Surveillance Video-Based Traffic Accident Detection Using Transformer Architecture**|Long T. Truong Team|[2512.11350](http://arxiv.org/abs/2512.11350)|null|
|**2025-12-12**|**Benchmarking the Generality of Vision-Language-Action Models**|Yangyue Wang Team|[2512.11315](http://arxiv.org/abs/2512.11315)|null|
|**2025-12-12**|**Towards Logic-Aware Manipulation: A Knowledge Primitive for VLM-Based Assistants in Smart Manufacturing**|Daqiang Guo Team|[2512.11275](http://arxiv.org/abs/2512.11275)|null|
|**2025-12-12**|**Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy**|Yue Wang Team|[2512.11218](http://arxiv.org/abs/2512.11218)|null|
|**2025-12-11**|**Image Tiling for High-Resolution Reasoning: Balancing Local Detail with Global Context**|Irina Rish Team|[2512.11167](http://arxiv.org/abs/2512.11167)|null|
|**2025-12-11**|**Limits and Gains of Test-Time Scaling in Vision-Language Reasoning**|Mahdieh Soleymani Baghshah Team|[2512.11109](http://arxiv.org/abs/2512.11109)|null|
|**2025-12-11**|**Vision-Language Models for Infrared Industrial Sensing in Additive Manufacturing Scene Description**|Vinh Nguyen Team|[2512.11098](http://arxiv.org/abs/2512.11098)|null|
|**2025-12-11**|**VDAWorld: World Modelling via VLM-Directed Abstraction and Simulation**|Ayush Tewari Team|[2512.11061](http://arxiv.org/abs/2512.11061)|**[link](https://felixomahony.github.io/vdaworld/)**|
|**2025-12-11**|**Synthetic Vasculature and Pathology Enhance Vision-Language Model Reasoning**|Johannes C. Paetzold Team|[2512.11060](http://arxiv.org/abs/2512.11060)|null|
|**2025-12-11**|**VL-JEPA: Joint Embedding Predictive Architecture for Vision-language**|Pascale Fung Team|[2512.10942](http://arxiv.org/abs/2512.10942)|null|
|**2025-12-11**|**BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models**|Boqing Gong Team|[2512.10932](http://arxiv.org/abs/2512.10932)|null|
|**2025-12-11**|**DuetSVG: Unified Multimodal SVG Generation with Internal Visual Guidance**|Difan Liu Team|[2512.10894](http://arxiv.org/abs/2512.10894)|**[link](https://intchous.github.io/DuetSVG-site)**|
|**2025-12-11**|**PubTables-v2: A new large-scale dataset for full-page and multi-page table extraction**|Maury Courtland Team|[2512.10888](http://arxiv.org/abs/2512.10888)|null|
|**2025-12-12**|**From Macro to Micro: Benchmarking Microscopic Spatial Intelligence on Molecules via Vision-Language Models**|Wenbing Huang Team|[2512.10867](http://arxiv.org/abs/2512.10867)|null|
|**2025-12-11**|**SpaceDrive: Infusing Spatial Awareness into VLM-based Autonomous Driving**|Andreas Zell Team|[2512.10719](http://arxiv.org/abs/2512.10719)|null|
|**2025-12-11**|**Enhancing Radiology Report Generation and Visual Grounding using Reinforcement Learning**|Michael Krauthammer Team|[2512.10691](http://arxiv.org/abs/2512.10691)|null|
|**2025-12-11**|**DOCR-Inspector: Fine-Grained and Automated Evaluation of Document Parsing with VLM**|Wentao Zhang Team|[2512.10619](http://arxiv.org/abs/2512.10619)|null|
|**2025-12-11**|**Beyond Pixels: A Training-Free, Text-to-Text Framework for Remote Sensing Image Retrieval**|M. Prasad Team|[2512.10596](http://arxiv.org/abs/2512.10596)|null|
|**2025-12-11**|**Boosting RL-Based Visual Reasoning with Selective Adversarial Entropy Intervention**|Xiaomeng Li Team|[2512.10414](http://arxiv.org/abs/2512.10414)|null|
|**2025-12-11**|**Towards Fine-Grained Recognition with Large Visual Language Models: Benchmark and Optimization Strategies**|Xin Lou Team|[2512.10384](http://arxiv.org/abs/2512.10384)|null|
|**2025-12-11**|**CoSPlan: Corrective Sequential Planning via Scene Graph Incremental Updates**|Yogesh S Rawat Team|[2512.10342](http://arxiv.org/abs/2512.10342)|null|
|**2025-12-11**|**Multilingual VLM Training: Adapting an English-Trained VLM to French**|Alexis Roger Team|[2512.10336](http://arxiv.org/abs/2512.10336)|null|
|**2025-12-11**|**ConStruct: Structural Distillation of Foundation Models for Prototype-Based Weakly Supervised Histopathology Segmentation**|Hien Van Nguyen Team|[2512.10316](http://arxiv.org/abs/2512.10316)|null|
|**2025-12-11**|**Investigating The Functional Roles of Attention Heads in Vision Language Models: Evidence for Reasoning Modules**|Krista A. Ehinger Team|[2512.10300](http://arxiv.org/abs/2512.10300)|null|
|**2025-12-11**|**Solving Semi-Supervised Few-Shot Learning from an Auto-Annotation Perspective**|Shu Kong Team|[2512.10244](http://arxiv.org/abs/2512.10244)|**[link](https://tian1327.github.io/SWIFT)**|
|**2025-12-10**|**Independent Density Estimation**|Jiahao Liu Team|[2512.10067](http://arxiv.org/abs/2512.10067)|null|
|**2025-12-10**|**SimWorld-Robotics: Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration**|Tianmin Shu Team|[2512.10046](http://arxiv.org/abs/2512.10046)|null|
|**2025-12-11**|**ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning**|Yike Guo Team|[2512.09924](http://arxiv.org/abs/2512.09924)|**[link](https://github.com/Liuxinyv/ReViSE))**|
|**2025-12-10**|**VisualActBench: Can VLMs See and Act like a Human?**|Jiebo Luo Team|[2512.09907](http://arxiv.org/abs/2512.09907)|null|
|**2025-12-10**|**Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs**|Janis Keuper Team|[2512.09874](http://arxiv.org/abs/2512.09874)|null|
|**2025-12-10**|**An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence**|Israel Cohen Team|[2512.09670](http://arxiv.org/abs/2512.09670)|null|
|**2025-12-10**|**Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment**|Shin'ya Nishida Team|[2512.09555](http://arxiv.org/abs/2512.09555)|null|
|**2025-12-10**|**Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation**|Steffen Staab Team|[2512.09446](http://arxiv.org/abs/2512.09446)|null|
|**2025-12-10**|**Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model**|Ruixuan Wang Team|[2512.09441](http://arxiv.org/abs/2512.09441)|null|
|**2025-12-10**|**COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning**|Chen Lv Team|[2512.09349](http://arxiv.org/abs/2512.09349)|null|
|**2025-12-10**|**View-on-Graph: Zero-shot 3D Visual Grounding via Vision-Language Reasoning on Scene Graphs**|Xin Yang Team|[2512.09215](http://arxiv.org/abs/2512.09215)|null|
|**2025-12-09**|**Prompt-Based Continual Compositional Zero-Shot Learning**|Mohsen Ali Team|[2512.09172](http://arxiv.org/abs/2512.09172)|null|
|**2025-12-09**|**ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors**|Benjamin Busam Team|[2512.09056](http://arxiv.org/abs/2512.09056)|null|
|**2025-12-09**|**Unified Diffusion Transformer for High-fidelity Text-Aware Image Restoration**|Seungryong Kim Team|[2512.08922](http://arxiv.org/abs/2512.08922)|null|
|**2025-12-09**|**SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing**|Jiankang Deng Team|[2512.08881](http://arxiv.org/abs/2512.08881)|null|
|**2025-12-09**|**Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference**|Amit Bendkhale Team|[2512.08860](http://arxiv.org/abs/2512.08860)|**[link](https://github.com/Amiton7/Tri-Bench.)**|
|**2025-12-09**|**InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models**|Xinggang Wang Team|[2512.08829](http://arxiv.org/abs/2512.08829)|null|
|**2025-12-09**|**Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning**|Angelica I. Aviles-Rivero Team|[2512.08820](http://arxiv.org/abs/2512.08820)|null|
|**2025-12-09**|**Trajectory Densification and Depth from Perspective-based Blur**|Yueting Chen Team|[2512.08627](http://arxiv.org/abs/2512.08627)|null|
|**2025-12-10**|**Mind to Hand: Purposeful Robotic Control via Embodied Reasoning**|Jianan Wang Team|[2512.08580](http://arxiv.org/abs/2512.08580)|null|
|**2025-12-09**|**Beyond Real Weights: Hypercomplex Representations for Stable Quantization**|Shafin Rahman Team|[2512.08524](http://arxiv.org/abs/2512.08524)|null|
|**2025-12-10**|**OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation**|Harry Yang Team|[2512.08294](http://arxiv.org/abs/2512.08294)|null|
|**2025-12-09**|**PAVAS: Physics-Aware Video-to-Audio Synthesis**|Yuki Mitsufuji Team|[2512.08282](http://arxiv.org/abs/2512.08282)|null|
|**2025-12-09**|**HybridToken-VLM: Hybrid Token Compression for Vision-Language Models**|Keze Wang Team|[2512.08240](http://arxiv.org/abs/2512.08240)|null|
|**2025-12-09**|**Semantic-Metric Bayesian Risk Fields: Learning Robot Safety from Human Videos with a VLM Prior**|Mac Schwager Team|[2512.08233](http://arxiv.org/abs/2512.08233)|null|
|**2025-12-09**|**MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models**|Keze Wang Team|[2512.08228](http://arxiv.org/abs/2512.08228)|null|
|**2025-12-09**|**Ground Slow, Move Fast: A Dual-System Foundation Model for Generalizable Vision-and-Language Navigation**|Xihui Liu Team|[2512.08186](http://arxiv.org/abs/2512.08186)|null|
|**2025-12-08**|**FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models**|Yao-Yi Chiang Team|[2512.08016](http://arxiv.org/abs/2512.08016)|null|
|**2025-12-08**|**Relational Visual Similarity**|Yuheng Li Team|[2512.07833](http://arxiv.org/abs/2512.07833)|**[link](https://thaoshibe.github.io/relsim)**|
|**2025-12-08**|**Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models**|Renzo Ardiccioni Team|[2512.07564](http://arxiv.org/abs/2512.07564)|**[link](https://github.com/kassoumsanogo1/self-correcting-vlm-re-Attention.git)**|
|**2025-12-08**|**From Show Programmes to Data: Designing a Workflow to Make Performing Arts Ephemera Accessible Through Language Models**|Jeanne Fras Team|[2512.07452](http://arxiv.org/abs/2512.07452)|null|
|**2025-12-08**|**Structure-Aware Feature Rectification with Region Adjacency Graphs for Training-Free Open-Vocabulary Semantic Segmentation**|Jianbo Jiao Team|[2512.07360](http://arxiv.org/abs/2512.07360)|null|
|**2025-12-08**|**Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding**|Xu Chen Team|[2512.07344](http://arxiv.org/abs/2512.07344)|null|
|**2025-12-08**|**Towards Accurate UAV Image Perception: Guiding Vision-Language Models with Stronger Task Prompts**|Chao Tao Team|[2512.07302](http://arxiv.org/abs/2512.07302)|null|
|**2025-12-08**|**Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery**|Naoto Yokoya Team|[2512.07276](http://arxiv.org/abs/2512.07276)|null|
|**2025-12-08**|**RVLF: A Reinforcing Vision-Language Framework for Gloss-Free Sign Language Translation**|Jun Wan Team|[2512.07273](http://arxiv.org/abs/2512.07273)|null|
|**2025-12-08**|**Zero-Shot Textual Explanations via Translating Decision-Critical Features**|Kazuhiko Kawamoto Team|[2512.07245](http://arxiv.org/abs/2512.07245)|null|
|**2025-12-08**|**Dropout Prompt Learning: Towards Robust and Adaptive Vision-Language Models**|Yuchen Wang Team|[2512.07234](http://arxiv.org/abs/2512.07234)|null|
|**2025-12-08**|**Pay Less Attention to Function Words for Free Robustness of Vision-Language Models**|Chao Shen Team|[2512.07222](http://arxiv.org/abs/2512.07222)|null|
|**2025-12-08**|**VFM-VLM: Vision Foundation Model and Vision Language Model based Visual Comparison for 3D Pose Estimation**|Sungho Kim Team|[2512.07215](http://arxiv.org/abs/2512.07215)|null|
|**2025-12-08**|**MMRPT: MultiModal Reinforcement Pre-Training via Masked Vision-Dependent Reasoning**|Yichao Wu Team|[2512.07203](http://arxiv.org/abs/2512.07203)|null|
|**2025-12-08**|**Using Vision-Language Models as Proxies for Social Intelligence in Human-Robot Interaction**|Wendy Ju Team|[2512.07177](http://arxiv.org/abs/2512.07177)|null|
|**2025-12-08**|**CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics**|Jihyong Oh Team|[2512.07155](http://arxiv.org/abs/2512.07155)|**[link](https://cmlab-korea.github.io/CHIMERA/)**|
|**2025-12-08**|**Think-Reflect-Revise: A Policy-Guided Reflective Framework for Safety Alignment in Large Vision Language Models**|Wenjie Wang Team|[2512.07141](http://arxiv.org/abs/2512.07141)|null|
|**2025-12-08**|**A Large-Scale Multimodal Dataset and Benchmarks for Human Activity Scene Understanding and Reasoning**|Guoliang Xing Team|[2512.07136](http://arxiv.org/abs/2512.07136)|null|
|**2025-12-08**|**DART: Leveraging Multi-Agent Disagreement for Tool Recruitment in Multimodal Reasoning**|Mohit Bansal Team|[2512.07132](http://arxiv.org/abs/2512.07132)|**[link](https://github.com/nsivaku/dart)**|
|**2025-12-08**|**MulCLIP: A Multi-level Alignment Framework for Enhancing Fine-grained Long-context CLIP**|Dung D. Le Team|[2512.07128](http://arxiv.org/abs/2512.07128)|null|
|**2025-12-07**|**Structural and Disentangled Adaptation of Large Vision Language Models for Multimodal Recommendation**|Nan Tang Team|[2512.06883](http://arxiv.org/abs/2512.06883)|null|
|**2025-12-05**|**Training-Time Action Conditioning for Efficient Real-Time Chunking**|Sergey Levine Team|[2512.05964](http://arxiv.org/abs/2512.05964)|null|
|**2025-12-05**|**M4-RAG: A Massive-Scale Multilingual Multi-Cultural Multimodal RAG**|Genta Indra Winata Team|[2512.05959](http://arxiv.org/abs/2512.05959)|null|
|**2025-12-05**|**SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models**|Yilun Du Team|[2512.05955](http://arxiv.org/abs/2512.05955)|null|
|**2025-12-05**|**TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models**|Babak Damavandi Team|[2512.05943](http://arxiv.org/abs/2512.05943)|null|
|**2025-12-05**|**Zoom in, Click out: Unlocking and Evaluating the Potential of Zooming for GUI Grounding**|Shilong Liu Team|[2512.05941](http://arxiv.org/abs/2512.05941)|**[link](https://github.com/Princeton-AI2-Lab/ZoomClick)**|
|**2025-12-05**|**PRiSM: An Agentic Multimodal Benchmark for Scientific Reasoning via Python-Grounded Evaluation**|Babak Damavandi Team|[2512.05930](http://arxiv.org/abs/2512.05930)|null|
|**2025-12-05**|**VRSA: Jailbreaking Multimodal Large Language Models through Visual Reasoning Sequential Attack**|Xingxing Wei Team|[2512.05853](http://arxiv.org/abs/2512.05853)|null|
|**2025-12-05**|**Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling**|Sarath Chandar Team|[2512.05809](http://arxiv.org/abs/2512.05809)|null|
|**2025-12-05**|**Distilling Expert Surgical Knowledge: How to train local surgical VLMs for anatomy explanation in Complete Mesocolic Excision**|Alexander Schlaefer Team|[2512.05740](http://arxiv.org/abs/2512.05740)|null|
|**2025-12-05**|**HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies**|Yu-Gang Jiang Team|[2512.05693](http://arxiv.org/abs/2512.05693)|null|
|**2025-12-05**|**Interleaved Latent Visual Reasoning with Selective Perceptual Modeling**|Zhongyu Wei Team|[2512.05665](http://arxiv.org/abs/2512.05665)|**[link](https://github.com/XD111ds/ILVR)**|
|**2025-12-05**|**Fast SceneScript: Accurate and Efficient Structured Language Model via Multi-Token Prediction**|Theo Gevers Team|[2512.05597](http://arxiv.org/abs/2512.05597)|null|
|**2025-12-05**|**ProPhy: Progressive Physical Alignment for Dynamic World Simulation**|Xiaodan Liang Team|[2512.05564](http://arxiv.org/abs/2512.05564)|null|
|**2025-12-05**|**Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models**|Guixian Zhang Team|[2512.05546](http://arxiv.org/abs/2512.05546)|null|
|**2025-12-05**|**Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement**|Tilmann Rabl Team|[2512.05525](http://arxiv.org/abs/2512.05525)|null|
|**2025-12-05**|**VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation**|Basura Fernando Team|[2512.05524](http://arxiv.org/abs/2512.05524)|null|
|**2025-12-05**|**Concept-based Explainable Data Mining with VLM for 3D Detection**|Mai Tsujimoto Team|[2512.05482](http://arxiv.org/abs/2512.05482)|**[link](https://github.com/mm1129/concept_based_rare_detector_2025)**|
|**2025-12-05**|**ParaUni: Enhance Generation in Unified Multimodal Model with Reinforcement-driven Hierarchical Parallel Information Interaction**|Feng Zhao Team|[2512.05422](http://arxiv.org/abs/2512.05422)|null|
|**2025-12-05**|**The Dynamic Prior: Understanding 3D Structures for Casual Dynamic Videos**|Jun Gao Team|[2512.05398](http://arxiv.org/abs/2512.05398)|**[link](https://github.com/wuzy2115/DYNAPO)**|
|**2025-12-05**|**LoC-Path: Learning to Compress for Pathology Multimodal Large Language Models**|Chao Chen Team|[2512.05391](http://arxiv.org/abs/2512.05391)|null|
|**2025-12-04**|**From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model**|Mohammad Akbari Team|[2512.05277](http://arxiv.org/abs/2512.05277)|null|
|**2025-12-04**|**Semore: VLM-guided Enhanced Semantic Motion Representations for Visual Reinforcement Learning**|Yan Wang Team|[2512.05172](http://arxiv.org/abs/2512.05172)|null|
|**2025-12-04**|**DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation**|Hongsheng Li Team|[2512.05112](http://arxiv.org/abs/2512.05112)|**[link](https://github.com/CaraJ7/DraCo)**|
|**2025-12-04**|**ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning**|Jiaqi Wang Team|[2512.05111](http://arxiv.org/abs/2512.05111)|null|
|**2025-12-04**|**STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models**|Benjamin Busam Team|[2512.05107](http://arxiv.org/abs/2512.05107)|null|
|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|Emily Dinan Team|[2512.05103](http://arxiv.org/abs/2512.05103)|null|
|**2025-12-04**|**Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark**|Ming-Hsuan Yang Team|[2512.05091](http://arxiv.org/abs/2512.05091)|**[link](https://harboryuan.github.io/visual-reasoning-tracer)**|
|**2025-12-04**|**Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models**|Hyunjung Shim Team|[2512.04981](http://arxiv.org/abs/2512.04981)|**[link](https://fairpro-t2i.github.io)**|
|**2025-12-04**|**FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization**|Hang Zhao Team|[2512.04952](http://arxiv.org/abs/2512.04952)|null|
|**2025-12-04**|**Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems**|Saud Satti Team|[2512.04895](http://arxiv.org/abs/2512.04895)|null|
|**2025-12-04**|**ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications**|Nilaan Loganathan Team|[2512.04785](http://arxiv.org/abs/2512.04785)|null|
|**2025-12-04**|**MemLoRA: Distilling Expert Adapters for On-Device Memory Systems**|Taha Ceritli Team|[2512.04763](http://arxiv.org/abs/2512.04763)|null|
|**2025-12-04**|**E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving**|Chengzhong Xu Team|[2512.04733](http://arxiv.org/abs/2512.04733)|null|
|**2025-12-04**|**Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild**|Jie Liu Team|[2512.04728](http://arxiv.org/abs/2512.04728)|null|
|**2025-12-04**|**Towards Cross-View Point Correspondence in Vision-Language Models**|Xiaolong Zheng Team|[2512.04686](http://arxiv.org/abs/2512.04686)|null|
|**2025-12-04**|**Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation**|Min Zhang Team|[2512.04678](http://arxiv.org/abs/2512.04678)|null|
|**2025-12-04**|**SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding**|Yu-Chiang Frank Wang Team|[2512.04643](http://arxiv.org/abs/2512.04643)|null|
|**2025-12-04**|**Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning**|Mathieu Serrurier Team|[2512.04632](http://arxiv.org/abs/2512.04632)|null|
|**2025-12-04**|**Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning**|Blaise Yvert Team|[2512.04618](http://arxiv.org/abs/2512.04618)|null|
|**2025-12-04**|**Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot**|Shuo Wang Team|[2512.04599](http://arxiv.org/abs/2512.04599)|null|
|**2025-12-04**|**When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering**|Jianfei Yang Team|[2512.04597](http://arxiv.org/abs/2512.04597)|null|
|**2025-12-04**|**SAM3-I: Segment Anything with Instructions**|Li Cheng Team|[2512.04585](http://arxiv.org/abs/2512.04585)|null|
|**2025-12-03**|**SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL**|Jonathan Tremblay Team|[2512.04069](http://arxiv.org/abs/2512.04069)|null|
|**2025-12-03**|**Stable Signer: Hierarchical Sign Language Generative Model**|Dimitris N. Metaxas Team|[2512.04048](http://arxiv.org/abs/2512.04048)|**[link](https://stablesigner.github.io)**|
|**2025-12-03**|**Jina-VLM: Small Multilingual Vision Language Model**|Han Xiao Team|[2512.04032](http://arxiv.org/abs/2512.04032)|null|
|**2025-12-03**|**DIQ-H: Evaluating Hallucination Persistence in VLMs Under Temporal Visual Degradation**|Xiaoqiang Team|[2512.03992](http://arxiv.org/abs/2512.03992)|null|
|**2025-12-03**|**TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning**|Limin Wang Team|[2512.03963](http://arxiv.org/abs/2512.03963)|null|
|**2025-12-03**|**Hierarchical Vision Language Action Model Using Success and Failure Demonstrations**|Sungjoon Choi Team|[2512.03913](http://arxiv.org/abs/2512.03913)|**[link](https://vine-vla.github.io/)**|
|**2025-12-03**|**OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance**|Jianwei Zhang Team|[2512.03874](http://arxiv.org/abs/2512.03874)|**[link](https://sites.google.com/view/omnidexvlg)**|
|**2025-12-03**|**PULSE: A Unified Multi-Task Architecture for Cardiac Segmentation, Diagnosis, and Few-Shot Cross-Modality Clinical Adaptation**|Muzammil Behzad Team|[2512.03848](http://arxiv.org/abs/2512.03848)|null|
|**2025-12-03**|**AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition**|Deheng Ye Team|[2512.03794](http://arxiv.org/abs/2512.03794)|null|
|**2025-12-03**|**Universally Converging Representations of Matter Across Scientific Foundation Models**|Rafael Gómez-Bombarelli Team|[2512.03750](http://arxiv.org/abs/2512.03750)|null|
|**2025-12-03**|**Thinking with Programming Vision: Towards a Unified View for Thinking with Images**|Tao Jin Team|[2512.03746](http://arxiv.org/abs/2512.03746)|null|
|**2025-12-03**|**PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention**|Mingming Gong Team|[2512.03724](http://arxiv.org/abs/2512.03724)|null|
|**2025-12-03**|**ConvRot: Rotation-Based Plug-and-Play 4-bit Quantization for Diffusion Transformers**|Haoqian Wang Team|[2512.03673](http://arxiv.org/abs/2512.03673)|null|
|**2025-12-03**|**Colon-X: Advancing Intelligent Colonoscopy from Multimodal Understanding to Clinical Reasoning**|Nick Barnes Team|[2512.03667](http://arxiv.org/abs/2512.03667)|null|
|**2025-12-03**|**Optical Context Compression Is Just (Bad) Autoencoding**|Taylor Berg-Kirkpatrick Team|[2512.03643](http://arxiv.org/abs/2512.03643)|null|
|**2025-12-03**|**MemVerse: Multimodal Memory for Lifelong Learning Agents**|Ding Wang Team|[2512.03627](http://arxiv.org/abs/2512.03627)|null|
|**2025-12-03**|**The promising potential of vision language models for the generation of textual weather forecasts**|Charles Ewen Team|[2512.03623](http://arxiv.org/abs/2512.03623)|null|
|**2025-12-03**|**LAMP: Language-Assisted Motion Planning for Controllable Video Generation**|Duygu Ceylan Team|[2512.03619](http://arxiv.org/abs/2512.03619)|null|
|**2025-12-03**|**CartoMapQA: A Fundamental Benchmark Dataset Evaluating Vision-Language Models on Cartographic Map Understanding**|Yan Liu Team|[2512.03558](http://arxiv.org/abs/2512.03558)|null|
|**2025-12-03**|**Dynamic Content Moderation in Livestreams: Combining Supervised Classification with MLLM-Boosted Similarity Matching**|Danhui Guan Team|[2512.03553](http://arxiv.org/abs/2512.03553)|null|
|**2025-12-02**|**OneThinker: All-in-one Reasoning Model for Image and Video**|Xiangyu Yue Team|[2512.03043](http://arxiv.org/abs/2512.03043)|**[link](https://github.com/tulerfeng/OneThinker)**|
|**2025-12-02**|**InEx: Hallucination Mitigation via Introspection and Cross-Modal Multi-Agent Collaboration**|Wei Pang Team|[2512.02981](http://arxiv.org/abs/2512.02981)|null|
|**2025-12-02**|**Contextual Image Attack: How Visual Context Exposes Multimodal Safety Vulnerabilities**|Jing Shao Team|[2512.02973](http://arxiv.org/abs/2512.02973)|null|
|**2025-12-02**|**Lumos: Let there be Language Model System Certification**|Gagandeep Singh Team|[2512.02966](http://arxiv.org/abs/2512.02966)|null|
|**2025-12-02**|**AutoNeural: Co-Designing Vision-Language Models for NPU Inference**|Han Yang Team|[2512.02924](http://arxiv.org/abs/2512.02924)|null|
|**2025-12-02**|**MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding**|Kaihao Zhang Team|[2512.02906](http://arxiv.org/abs/2512.02906)|null|
|**2025-12-02**|**VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling**|Guangrun Wang Team|[2512.02902](http://arxiv.org/abs/2512.02902)|null|
|**2025-12-02**|**MindGPT-4ov: An Enhanced MLLM via a Multi-Stage Post-Training Paradigm**|Xuhan Zhu Team|[2512.02895](http://arxiv.org/abs/2512.02895)|null|
|**2025-12-02**|**Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?**|Jose Garcia-Rodriguez Team|[2512.02846](http://arxiv.org/abs/2512.02846)|null|
|**2025-12-02**|**VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion**|Yong Shen Team|[2512.02844](http://arxiv.org/abs/2512.02844)|null|
|**2025-12-02**|**ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning**|Yanwei Fu Team|[2512.02835](http://arxiv.org/abs/2512.02835)|null|
|**2025-12-02**|**Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach**|Xuelong Li Team|[2512.02834](http://arxiv.org/abs/2512.02834)|null|
|**2025-12-02**|**Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control**|Xiaofan Zhang Team|[2512.02814](http://arxiv.org/abs/2512.02814)|null|
|**2025-12-02**|**Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols**|Yong-Lu Li Team|[2512.02787](http://arxiv.org/abs/2512.02787)|null|
|**2025-12-02**|**Reasoning-Aware Multimodal Fusion for Hateful Video Detection**|Zeyu Fu Team|[2512.02743](http://arxiv.org/abs/2512.02743)|null|
|**2025-12-02**|**RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning**|Haoqian Wang Team|[2512.02729](http://arxiv.org/abs/2512.02729)|null|
|**2025-12-02**|**Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs**|Zafeirios Fountas Team|[2512.02719](http://arxiv.org/abs/2512.02719)|null|
|**2025-12-02**|**GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding**|Lei Wang Team|[2512.02715](http://arxiv.org/abs/2512.02715)|null|
|**2025-12-02**|**VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm**|Xinghao Chen Team|[2512.02700](http://arxiv.org/abs/2512.02700)|null|
|**2025-12-02**|**GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization**|Bo Du Team|[2512.02697](http://arxiv.org/abs/2512.02697)|null|
|**2025-12-01**|**ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation**|Shanghang Zhang Team|[2512.02013](http://arxiv.org/abs/2512.02013)|null|
|**2025-12-01**|**PAI-Bench: A Comprehensive Benchmark For Physical AI**|Humphrey Shi Team|[2512.01989](http://arxiv.org/abs/2512.01989)|null|
|**2025-12-01**|**Low-Rank Prehab: Preparing Neural Networks for SVD Compression**|Soheil Kolouri Team|[2512.01980](http://arxiv.org/abs/2512.01980)|null|
|**2025-12-01**|**Chain-of-Ground: Improving GUI Grounding via Iterative Reasoning and Reference Feedback**|Shilong Liu Team|[2512.01979](http://arxiv.org/abs/2512.01979)|null|
|**2025-12-01**|**GrndCtrl: Grounding World Models via Self-Supervised Reward Alignment**|Sebastian Scherer Team|[2512.01952](http://arxiv.org/abs/2512.01952)|null|
|**2025-12-01**|**Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models**|Yingfang Yuan Team|[2512.01949](http://arxiv.org/abs/2512.01949)|**[link](https://01yzzyu.github.io/script.github.io/)**|
|**2025-12-01**|**Guardian: Detecting Robotic Planning and Execution Errors with Vision-Language Models**|Cordelia Schmid Team|[2512.01946](http://arxiv.org/abs/2512.01946)|null|
|**2025-12-01**|**Med-VCD: Mitigating Hallucination for Medical Large Vision Language Models through Visual Contrastive Decoding**|Omid Nejati Manzari Team|[2512.01922](http://arxiv.org/abs/2512.01922)|null|
|**2025-12-01**|**PhyDetEx: Detecting and Explaining the Physical Plausibility of T2V Models**|Lei Zhang Team|[2512.01843](http://arxiv.org/abs/2512.01843)|null|
|**2025-12-01**|**OpenREAD: Reinforced Open-Ended Reasoing for End-to-End Autonomous Driving with LLM-as-Critic**|Chen Lv Team|[2512.01830](http://arxiv.org/abs/2512.01830)|null|
|**2025-12-01**|**CauSight: Learning to Supersense for Visual Causal Discovery**|Chaochao Lu Team|[2512.01827](http://arxiv.org/abs/2512.01827)|**[link](https://github.com/OpenCausaLab/CauSight)**|
|**2025-12-01**|**Seeing through Imagination: Learning Scene Geometry via Implicit Spatial World Modeling**|Xiaodan Liang Team|[2512.01821](http://arxiv.org/abs/2512.01821)|null|
|**2025-12-01**|**Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos**|Deepti Ghadiyaram Team|[2512.01803](http://arxiv.org/abs/2512.01803)|null|
|**2025-12-01**|**GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation**|Yonghui Wu Team|[2512.01801](http://arxiv.org/abs/2512.01801)|null|
|**2025-12-01**|**IGen: Scalable Data Generation for Robot Learning from Open-World Images**|Zhi Wang Team|[2512.01773](http://arxiv.org/abs/2512.01773)|null|
|**2025-12-01**|**VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis**|Hafsa Billah Team|[2512.01769](http://arxiv.org/abs/2512.01769)|null|
|**2025-12-01**|**FreqEdit: Preserving High-Frequency Features for Robust Multi-Turn Image Editing**|Xudong Mao Team|[2512.01755](http://arxiv.org/abs/2512.01755)|null|
|**2025-12-01**|**DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models**|Zongqing Lu Team|[2512.01715](http://arxiv.org/abs/2512.01715)|null|
|**2025-12-01**|**StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos**|Mohit Bansal Team|[2512.01707](http://arxiv.org/abs/2512.01707)|**[link](https://streamgaze.github.io/)**|
|**2025-12-01**|**DreamingComics: A Story Visualization Pipeline via Subject and Layout Customized Generation using Video Models**|Chen Chen Team|[2512.01686](http://arxiv.org/abs/2512.01686)|null|
|**2025-11-28**|**Video-R2: Reinforcing Consistent and Grounded Reasoning in Multimodal Language Models**|Salman Khan Team|[2511.23478](http://arxiv.org/abs/2511.23478)|null|
|**2025-11-28**|**Video-CoM: Interactive Video Reasoning via Chain of Manipulations**|Salman Khan Team|[2511.23477](http://arxiv.org/abs/2511.23477)|null|
|**2025-11-28**|**Visual Generation Tuning**|Xinggang Wang Team|[2511.23469](http://arxiv.org/abs/2511.23469)|null|
|**2025-11-28**|**Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model**|Qinglin Lu Team|[2511.23429](http://arxiv.org/abs/2511.23429)|**[link](https://hunyuan-gamecraft-2.github.io/)**|
|**2025-11-28**|**LFM2 Technical Report**|Neehal Tumma Team|[2511.23404](http://arxiv.org/abs/2511.23404)|null|
|**2025-11-28**|**DEAL-300K: Diffusion-based Editing Area Localization with a 300K-Scale Dataset and Frequency-Prompted Baseline**|Qiang Zeng Team|[2511.23377](http://arxiv.org/abs/2511.23377)|null|
|**2025-11-28**|**Optimizing Multimodal Language Models through Attention-based Interpretability**|Evgeny Kotelnikov Team|[2511.23375](http://arxiv.org/abs/2511.23375)|null|
|**2025-11-28**|**Toward Automatic Safe Driving Instruction: A Large-Scale Vision Language Model Approach**|Taro Watanabe Team|[2511.23311](http://arxiv.org/abs/2511.23311)|null|
|**2025-11-28**|**SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot**|Dzmitry Tsetserukou Team|[2511.23300](http://arxiv.org/abs/2511.23300)|null|
|**2025-11-28**|**Transformer-Driven Triple Fusion Framework for Enhanced Multimodal Author Intent Classification in Low-Resource Bangla**|Md Rifat Hossen Team|[2511.23287](http://arxiv.org/abs/2511.23287)|null|
|**2025-11-28**|**OctoMed: Data Recipes for State-of-the-Art Multimodal Medical Reasoning**|Hoifung Poon Team|[2511.23269](http://arxiv.org/abs/2511.23269)|null|
|**2025-11-28**|**Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning**|Jun Wang Team|[2511.23262](http://arxiv.org/abs/2511.23262)|null|
|**2025-11-28**|**AgriCoT: A Chain-of-Thought Benchmark for Evaluating Reasoning in Vision-Language Models for Agriculture**|Juepeng Zheng Team|[2511.23253](http://arxiv.org/abs/2511.23253)|null|
|**2025-11-28**|**Unlocking Multilingual Reasoning Capability of LLMs and LVLMs through Representation Engineering**|Bing Qin Team|[2511.23231](http://arxiv.org/abs/2511.23231)|null|
|**2025-11-28**|**TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies**|Jianxin Wu Team|[2511.23225](http://arxiv.org/abs/2511.23225)|null|
|**2025-11-28**|**Instruction Tuning of Large Language Models for Tabular Data Generation-in One Day**|Biplab Sikdar Team|[2511.23220](http://arxiv.org/abs/2511.23220)|null|
|**2025-11-28**|**Obstruction reasoning for robotic grasping**|Fabio Poiesi Team|[2511.23186](http://arxiv.org/abs/2511.23186)|null|
|**2025-11-28**|**Learning to Refuse: Refusal-Aware Reinforcement Fine-Tuning for Hard-Irrelevant Queries in Video Temporal Grounding**|Jee-Hyong Lee Team|[2511.23151](http://arxiv.org/abs/2511.23151)|null|
|**2025-11-28**|**Analyzing Image Beyond Visual Aspect: Image Emotion Classification via Multiple-Affective Captioning**|Hansen Yang Team|[2511.23115](http://arxiv.org/abs/2511.23115)|null|
|**2025-11-28**|**MathSight: A Benchmark Exploring Have Vision-Language Models Really Seen in University-Level Mathematical Reasoning?**|Zhenzhou Shao Team|[2511.23112](http://arxiv.org/abs/2511.23112)|**[link](https://cnu-bot-group.github.io/MathSight/)**|
|**2025-11-26**|**G $^2$ VLM: Geometry Grounded Vision Language Model with Unified 3D Reconstruction and Spatial Reasoning**|Jiangmiao Pang Team|[2511.21688](http://arxiv.org/abs/2511.21688)|**[link](https://github.com/InternRobotics/G2VLM)**|
|**2025-11-26**|**Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models**|Nan Zhang Team|[2511.21663](http://arxiv.org/abs/2511.21663)|null|
|**2025-11-26**|**Qwen3-VL Technical Report**|Ke Zhu Team|[2511.21631](http://arxiv.org/abs/2511.21631)|null|
|**2025-11-26**|**Automated Protein Motif Localization using Concept Activation Vectors in Protein Language Model Embedding Space**|Claire D. McWhite Team|[2511.21614](http://arxiv.org/abs/2511.21614)|null|
|**2025-11-26**|**VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation**|Shaoshuai Shi Team|[2511.21557](http://arxiv.org/abs/2511.21557)|null|
|**2025-11-26**|**$\mathcal{E}_0$ : Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion**|Guangrun Wang Team|[2511.21542](http://arxiv.org/abs/2511.21542)|null|
|**2025-11-26**|**Video Generation Models Are Good Latent Reward Models**|Fan Tang Team|[2511.21541](http://arxiv.org/abs/2511.21541)|null|
|**2025-11-26**|**EoS-FM: Can an Ensemble of Specialist Models act as a Generalist Feature Extractor?**|Sébastien Lefèvre Team|[2511.21523](http://arxiv.org/abs/2511.21523)|null|
|**2025-11-26**|**IntAttention: A Fully Integer Attention Pipeline for Efficient Edge Inference**|Shiqi Yu Team|[2511.21513](http://arxiv.org/abs/2511.21513)|null|
|**2025-11-26**|**From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings**|Alexander Kleiner Team|[2511.21428](http://arxiv.org/abs/2511.21428)|null|
|**2025-11-26**|**SAM Guided Semantic and Motion Changed Region Mining for Remote Sensing Change Captioning**|Jin Tang Team|[2511.21420](http://arxiv.org/abs/2511.21420)|null|
|**2025-11-26**|**Do Reasoning Vision-Language Models Inversely Scale in Test-Time Compute? A Distractor-centric Empirical Analysis**|Jaeho Lee Team|[2511.21397](http://arxiv.org/abs/2511.21397)|null|
|**2025-11-26**|**Monet: Reasoning in Latent Visual Space Beyond Images and Language**|Yisen Wang Team|[2511.21395](http://arxiv.org/abs/2511.21395)|null|
|**2025-11-26**|**Thinking With Bounding Boxes: Enhancing Spatio-Temporal Video Grounding via Reinforcement Fine-Tuning**|Sijie Zhu Team|[2511.21375](http://arxiv.org/abs/2511.21375)|null|
|**2025-11-26**|**SurgMLLMBench: A Multimodal Large Language Model Benchmark Dataset for Surgical Scene Understanding**|Juyoun Park Team|[2511.21339](http://arxiv.org/abs/2511.21339)|null|
|**2025-11-26**|**Co-Training Vision Language Models for Remote Sensing Multi-task Learning**|Junchi Yan Team|[2511.21272](http://arxiv.org/abs/2511.21272)|null|
|**2025-11-26**|**Multi-Reward GRPO for Stable and Prosodic Single-Codebook TTS LLMs at Scale**|Zhisheng Wang Team|[2511.21270](http://arxiv.org/abs/2511.21270)|null|
|**2025-11-26**|**AVFakeBench: A Comprehensive Audio-Video Forgery Detection Benchmark for AV-LMMs**|Zekun Li Team|[2511.21251](http://arxiv.org/abs/2511.21251)|null|
|**2025-11-26**|**Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition**|Zhiyong Wang Team|[2511.21202](http://arxiv.org/abs/2511.21202)|null|
|**2025-11-26**|**When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models**|Xudong Jiang Team|[2511.21192](http://arxiv.org/abs/2511.21192)|null|
|**2025-11-25**|**LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight**|Zhiding Yu Team|[2511.20648](http://arxiv.org/abs/2511.20648)|**[link](https://nvlabs.github.io/LocateAnything3D/)**|
|**2025-11-25**|**Vision-Language Memory for Spatial Reasoning**|Chen Wang Team|[2511.20644](http://arxiv.org/abs/2511.20644)|null|
|**2025-11-25**|**Concept-Aware Batch Sampling Improves Language-Image Pretraining**|Matthias Bethge Team|[2511.20643](http://arxiv.org/abs/2511.20643)|null|
|**2025-11-25**|**Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition**|Min-Ling Zhang Team|[2511.20641](http://arxiv.org/abs/2511.20641)|null|
|**2025-11-25**|**Reinforcing Action Policies by Prophesying**|Li Zhang Team|[2511.20633](http://arxiv.org/abs/2511.20633)|**[link](https://LogosRoboticsGroup.github.io/ProphRL)**|
|**2025-11-25**|**MapReduce LoRA: Advancing the Pareto Front in Multi-Preference Optimization for Generative Models**|Humphrey Shi Team|[2511.20629](http://arxiv.org/abs/2511.20629)|null|
|**2025-11-25**|**Fighting AI with AI: Leveraging Foundation Models for Assuring AI-Enabled Safety-Critical Systems**|Corina S. Păsăreanu Team|[2511.20627](http://arxiv.org/abs/2511.20627)|null|
|**2025-11-25**|**Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward**|Li Yuan Team|[2511.20561](http://arxiv.org/abs/2511.20561)|null|
|**2025-11-25**|**Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models**|Shamima Hossain Team|[2511.20531](http://arxiv.org/abs/2511.20531)|null|
|**2025-11-25**|**Adam Simplified: Bias Correction Simplified**|Antonio Orvieto Team|[2511.20516](http://arxiv.org/abs/2511.20516)|null|
|**2025-11-25**|**DesignPref: Capturing Personal Preferences in Visual Design Generation**|Jason Wu Team|[2511.20513](http://arxiv.org/abs/2511.20513)|null|
|**2025-11-25**|**NVIDIA Nemotron Parse 1.1**|Bryan Catanzaro Team|[2511.20478](http://arxiv.org/abs/2511.20478)|null|
|**2025-11-25**|**Object-Centric Vision Token Pruning for Vision Language Models**|Joni Pajarinen Team|[2511.20439](http://arxiv.org/abs/2511.20439)|null|
|**2025-11-25**|**VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning**|Sheng Li Team|[2511.20422](http://arxiv.org/abs/2511.20422)|null|
|**2025-11-25**|**NNGPT: Rethinking AutoML with Large Language Models**|Radu Timofte Team|[2511.20333](http://arxiv.org/abs/2511.20333)|null|
|**2025-11-25**|**Bootstrapping Physics-Grounded Video Generation through VLM-Guided Iterative Self-Refinement**|Qingming Huang Team|[2511.20280](http://arxiv.org/abs/2511.20280)|null|
|**2025-11-25**|**ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis**|Abhijit Das Team|[2511.20274](http://arxiv.org/abs/2511.20274)|null|
|**2025-11-25**|**VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs**|Yi Wang Team|[2511.20272](http://arxiv.org/abs/2511.20272)|null|
|**2025-11-25**|**Rectified Flow for Vision-Aided mmWave V2I Beam Prediction**|Henk Wymeersch Team|[2511.20265](http://arxiv.org/abs/2511.20265)|null|
|**2025-11-25**|**V-Attack: Targeting Disentangled Value Features for Controllable Adversarial Attacks on LVLMs**|Xilin Chen Team|[2511.20223](http://arxiv.org/abs/2511.20223)|null|
|**2025-11-24**|**Mixture of Horizons in Action Chunking**|Mingyu Ding Team|[2511.19433](http://arxiv.org/abs/2511.19433)|null|
|**2025-11-24**|**Cook and Clean Together: Teaching Embodied Agents for Parallel Task Execution**|Xiang Bai Team|[2511.19430](http://arxiv.org/abs/2511.19430)|**[link](https://github.com/H-EmbodVis/GRANT})**|
|**2025-11-24**|**Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens**|Xudong Wang Team|[2511.19418](http://arxiv.org/abs/2511.19418)|**[link](https://wakalsprojectpage.github.io/comt-website/)**|
|**2025-11-24**|**Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration**|Hoifung Poon Team|[2511.19417](http://arxiv.org/abs/2511.19417)|null|
|**2025-11-24**|**UISearch: Graph-Based Embeddings for Multimodal Enterprise UI Screenshots Retrieval**|Rima Kilany Team|[2511.19380](http://arxiv.org/abs/2511.19380)|null|
|**2025-11-24**|**Rethinking Intermediate Representation for VLM-based Robot Manipulation**|Chi-Wing Fu Team|[2511.19315](http://arxiv.org/abs/2511.19315)|null|
|**2025-11-24**|**LAST: LeArning to Think in Space and Time for Generalist Vision-Language Models**|Jiaheng Wei Team|[2511.19261](http://arxiv.org/abs/2511.19261)|null|
|**2025-11-24**|**Medusa: Cross-Modal Transferable Adversarial Attacks on Multimodal Medical Retrieval-Augmented Generation**|Yefeng Zheng Team|[2511.19257](http://arxiv.org/abs/2511.19257)|null|
|**2025-11-24**|**Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving**|Hang Xu Team|[2511.19221](http://arxiv.org/abs/2511.19221)|null|
|**2025-11-24**|**Are Large Vision Language Models Truly Grounded in Medical Images? Evidence from Italian Clinical Visual Question Answering**|Marcello Di Pumpo Team|[2511.19220](http://arxiv.org/abs/2511.19220)|null|
|**2025-11-24**|**Can Modern Vision Models Understand the Difference Between an Object and a Look-alike?**|Amir Rosenfeld Team|[2511.19200](http://arxiv.org/abs/2511.19200)|null|
|**2025-11-24**|**EEG-VLM: A Hierarchical Vision-Language Model with Multi-Level Feature Alignment and Visually Enhanced Language-Guided Reasoning for EEG Image-Based Sleep Stage Prediction**|Shuo Li Team|[2511.19155](http://arxiv.org/abs/2511.19155)|null|
|**2025-11-24**|**From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation**|Asma Ahmad Farhan Team|[2511.19149](http://arxiv.org/abs/2511.19149)|null|
|**2025-11-24**|**ABM-LoRA: Activation Boundary Matching for Fast Convergence in Low-Rank Adaptation**|Junseok Kwon Team|[2511.19145](http://arxiv.org/abs/2511.19145)|null|
|**2025-11-24**|**MonoSR: Open-Vocabulary Spatial Reasoning from Monocular Images**|Shijie Li Team|[2511.19119](http://arxiv.org/abs/2511.19119)|null|
|**2025-11-24**|**DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection**|Mike Zheng Shou Team|[2511.19111](http://arxiv.org/abs/2511.19111)|null|
|**2025-11-24**|**LLMAID: Identifying AI Capabilities in Android Apps with LLMs**|Hongyu Zhang Team|[2511.19059](http://arxiv.org/abs/2511.19059)|null|
|**2025-11-24**|**MedSAM3: Delving into Segment Anything with Medical Concepts**|Jintai Chen Team|[2511.19046](http://arxiv.org/abs/2511.19046)|null|
|**2025-11-24**|**Benchmarking Corruption Robustness of LVLMs: A Discriminative Benchmark and Robustness Alignment Metric**|Xin Sun Team|[2511.19032](http://arxiv.org/abs/2511.19032)|null|
|**2025-11-24**|**Rethinking Plant Disease Diagnosis: Bridging the Academic-Practical Gap with Vision Transformers and Zero-Shot Learning**|Bilal Fortas Team|[2511.18989](http://arxiv.org/abs/2511.18989)|null|
|**2025-11-21**|**RynnVLA-002: A Unified Vision-Language-Action and World Model**|Hao Chen Team|[2511.17502](http://arxiv.org/abs/2511.17502)|null|
|**2025-11-21**|**Downscaling Intelligence: Exploring Perception and Reasoning Bottlenecks in Small Multimodal Models**|Serena Yeung-Levy Team|[2511.17487](http://arxiv.org/abs/2511.17487)|**[link](https://web.stanford.edu/~markendo/projects/downscaling_intelligence)**|
|**2025-11-21**|**Counterfactual World Models via Digital Twin-conditioned Video Diffusion**|Mathias Unberath Team|[2511.17481](http://arxiv.org/abs/2511.17481)|null|
|**2025-11-21**|**MMT-ARD: Multimodal Multi-Teacher Adversarial Distillation for Robust Vision-Language Models**|Yew-Soon Ong Team|[2511.17448](http://arxiv.org/abs/2511.17448)|null|
|**2025-11-21**|**REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing**|Begüm Demir Team|[2511.17442](http://arxiv.org/abs/2511.17442)|**[link](https://github.com/be-chen/REMSA)**|
|**2025-11-21**|**SMILE: A Composite Lexical-Semantic Metric for Question-Answering Evaluation**|Juan Carlos Niebles Team|[2511.17432](http://arxiv.org/abs/2511.17432)|null|
|**2025-11-21**|**SPEAR-1: Scaling Beyond Robot Demonstrations via 3D Understanding**|Danda Pani Paudel Team|[2511.17411](http://arxiv.org/abs/2511.17411)|null|
|**2025-11-21**|**IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation**|Yu Kong Team|[2511.17384](http://arxiv.org/abs/2511.17384)|null|
|**2025-11-21**|**METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model**|Shanghang Zhang Team|[2511.17366](http://arxiv.org/abs/2511.17366)|null|
|**2025-11-21**|**UAM: A Unified Attention-Mamba Backbone of Multimodal Framework for Tumor Cell Classification**|Nancy Guo Team|[2511.17355](http://arxiv.org/abs/2511.17355)|null|
|**2025-11-21**|**Agentic Program Verification**|Abhik Roychoudhury Team|[2511.17330](http://arxiv.org/abs/2511.17330)|null|
|**2025-11-21**|**SpatialGeo:Boosting Spatial Reasoning in Multimodal LLMs via Geometry-Semantics Fusion**|Weida Wang Team|[2511.17308](http://arxiv.org/abs/2511.17308)|null|
|**2025-11-21**|**MolSight: Optical Chemical Structure Recognition with SMILES Pretraining, Multi-Granularity Learning and Reinforcement Learning**|Wenyu Liu Team|[2511.17300](http://arxiv.org/abs/2511.17300)|null|
|**2025-11-21**|**Where Culture Fades: Revealing the Cultural Gap in Text-to-Image Generation**|Tat-Seng Chua Team|[2511.17282](http://arxiv.org/abs/2511.17282)|null|
|**2025-11-21**|**A Little More Like This: Text-to-Image Retrieval with Vision-Language Models Using Relevance Feedback**|Nava Tintarev Team|[2511.17255](http://arxiv.org/abs/2511.17255)|null|
|**2025-11-21**|**Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats**|Sibei Yang Team|[2511.17254](http://arxiv.org/abs/2511.17254)|**[link](https://github.com/SooLab/AllPath)**|
|**2025-11-21**|**Lost in Translation and Noise: A Deep Dive into the Failure Modes of VLMs on Real-World Tables**|Abhay Kumary Team|[2511.17238](http://arxiv.org/abs/2511.17238)|null|
|**2025-11-21**|**Scaling Self-Supervised and Cross-Modal Pretraining for Volumetric CT Transformers**|Fons van der Sommen Team|[2511.17209](http://arxiv.org/abs/2511.17209)|null|
|**2025-11-21**|**VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation**|Gim Hee Lee Team|[2511.17199](http://arxiv.org/abs/2511.17199)|null|
|**2025-11-21**|**Device-Guided Music Transfer**|Dong Ma Team|[2511.17136](http://arxiv.org/abs/2511.17136)|null|
|**2025-11-20**|**Learning to Think Fast and Slow for Visual Language Models**|Kaiyang Zhou Team|[2511.16670](http://arxiv.org/abs/2511.16670)|null|
|**2025-11-20**|**Video-as-Answer: Predict and Generate Next Video Event with Joint-GRPO**|Jing Liao Team|[2511.16669](http://arxiv.org/abs/2511.16669)|**[link](https://video-as-answer.github.io/)**|
|**2025-11-20**|**Cognitive Foundations for Reasoning and Their Manifestation in LLMs**|Yulia Tsvetkov Team|[2511.16660](http://arxiv.org/abs/2511.16660)|null|
|**2025-11-20**|**InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy**|Jiangmiao Pang Team|[2511.16651](http://arxiv.org/abs/2511.16651)|null|
|**2025-11-20**|**Bridging VLMs and Embodied Intelligence with Deliberate Practice Policy Optimization**|Xiaozhu Ju Team|[2511.16602](http://arxiv.org/abs/2511.16602)|null|
|**2025-11-20**|**TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding**|Qin Jin Team|[2511.16595](http://arxiv.org/abs/2511.16595)|**[link](https://xuboshen.github.io/TimeViper)**|
|**2025-11-20**|**Contrastive vision-language learning with paraphrasing and negation**|Artur d'Avila Garcez Team|[2511.16527](http://arxiv.org/abs/2511.16527)|null|
|**2025-11-20**|**MiMo-Embodied: X-Embodied Foundation Model Technical Report**|Long Chen Team|[2511.16518](http://arxiv.org/abs/2511.16518)|**[link](https://github.com/XiaomiMiMo/MiMo-Embodied)**|
|**2025-11-20**|**Arctic-Extract Technical Report**|Wojciech Jaśkowski Team|[2511.16470](http://arxiv.org/abs/2511.16470)|null|
|**2025-11-20**|**LLaVA $^3$ : Representing 3D Scenes like a Cubist Painter to Boost 3D Scene Understanding of VLMs**|Loïc Barthe Team|[2511.16454](http://arxiv.org/abs/2511.16454)|null|
|**2025-11-20**|**VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference**|Bo Zhao Team|[2511.16449](http://arxiv.org/abs/2511.16449)|null|
|**2025-11-20**|**Beyond Visual Cues: Leveraging General Semantics as Support for Few-Shot Segmentation**|Weifeng Liu Team|[2511.16435](http://arxiv.org/abs/2511.16435)|null|
|**2025-11-20**|**TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models**|Chaochao Chen Team|[2511.16423](http://arxiv.org/abs/2511.16423)|null|
|**2025-11-20**|**The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks**|Jianfeng Ma Team|[2511.16347](http://arxiv.org/abs/2511.16347)|null|
|**2025-11-20**|**FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models**|Mingsheng Shang Team|[2511.16233](http://arxiv.org/abs/2511.16233)|null|
|**2025-11-20**|**Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions**|Yoichi Sato Team|[2511.16221](http://arxiv.org/abs/2511.16221)|null|
|**2025-11-20**|**FlipVQA-Miner: Cross-Page Visual Question-Answer Mining from Textbooks**|Wentao Zhang Team|[2511.16216](http://arxiv.org/abs/2511.16216)|null|
|**2025-11-20**|**When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models**|Yaochu Jin Team|[2511.16203](http://arxiv.org/abs/2511.16203)|null|
|**2025-11-20**|**From Performance to Understanding: A Vision for Explainable Automated Algorithm Design**|Thomas Bäck Team|[2511.16201](http://arxiv.org/abs/2511.16201)|null|
|**2025-11-20**|**Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight**|Zhijie Deng Team|[2511.16175](http://arxiv.org/abs/2511.16175)|null|
|**2025-11-19**|**Think Visually, Reason Textually: Vision-Language Synergy in ARC**|Jiaqi Wang Team|[2511.15703](http://arxiv.org/abs/2511.15703)|null|
|**2025-11-19**|**MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping**|Jun Zhang Team|[2511.15690](http://arxiv.org/abs/2511.15690)|null|
|**2025-11-19**|**Walrus: A Cross-Domain Foundation Model for Continuum Dynamics**|Shirley Ho Team|[2511.15684](http://arxiv.org/abs/2511.15684)|null|
|**2025-11-19**|**VisPlay: Self-Evolving Vision-Language Models from Images**|Yonghui Yang Team|[2511.15661](http://arxiv.org/abs/2511.15661)|null|
|**2025-11-19**|**Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning**|Da-Wei Zhou Team|[2511.15633](http://arxiv.org/abs/2511.15633)|null|
|**2025-11-19**|**The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification**|Didac Suris Team|[2511.15622](http://arxiv.org/abs/2511.15622)|null|
|**2025-11-19**|**When to Think and When to Look: Uncertainty-Guided Lookback**|Chenliang Xu Team|[2511.15613](http://arxiv.org/abs/2511.15613)|null|
|**2025-11-19**|**SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models**|Xipeng Qiu Team|[2511.15605](http://arxiv.org/abs/2511.15605)|null|
|**2025-11-19**|**AVATAAR: Agentic Video Answering via Temporal Adaptive Alignment and Reasoning**|Chinmay Gondhalekar Team|[2511.15578](http://arxiv.org/abs/2511.15578)|null|
|**2025-11-19**|**Computer-Use Agents as Judges for Generative User Interface**|Mike Zheng Shou Team|[2511.15567](http://arxiv.org/abs/2511.15567)|**[link](https://showlab.github.io/AUI)**|
|**2025-11-19**|**Multimodal Evaluation of Russian-language Architectures**|Alena Fenogenova Team|[2511.15552](http://arxiv.org/abs/2511.15552)|null|
|**2025-11-19**|**Learning to Expand Images for Efficient Visual Autoregressive Modeling**|Tao Huang Team|[2511.15499](http://arxiv.org/abs/2511.15499)|null|
|**2025-11-19**|**SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome**|Mohammad Lotfollahi Team|[2511.15464](http://arxiv.org/abs/2511.15464)|null|
|**2025-11-19**|**D4C: Data-free Quantization for Contrastive Language-Image Pre-training Models**|Kentaro Yoshioka Team|[2511.15411](http://arxiv.org/abs/2511.15411)|null|
|**2025-11-19**|**Breaking Expert Knowledge Limits: Self-Pruning for Large Language Models**|Hao Wang Team|[2511.15390](http://arxiv.org/abs/2511.15390)|null|
|**2025-11-19**|**Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training**|Jianfei Yang Team|[2511.15379](http://arxiv.org/abs/2511.15379)|null|
|**2025-11-19**|**C2F-Space: Coarse-to-Fine Space Grounding for Spatial Instructions using Vision-Language Models**|Daehyung Park Team|[2511.15333](http://arxiv.org/abs/2511.15333)|null|
|**2025-11-19**|**What Your Features Reveal: Data-Efficient Black-Box Feature Inversion Attack for Split DNNs**|Fan Li Team|[2511.15316](http://arxiv.org/abs/2511.15316)|null|
|**2025-11-19**|**Adapt-As-You-Walk Through the Clouds: Training-Free Online Test-Time Adaptation of 3D Vision-Language Foundation Models**|Morteza Saberi Team|[2511.15311](http://arxiv.org/abs/2511.15311)|null|
|**2025-11-19**|**Text2Loc++: Generalizing 3D Point Cloud Localization from Natural Language**|Daniel Cremers Team|[2511.15308](http://arxiv.org/abs/2511.15308)|null|
|**2025-11-18**|**ARC Is a Vision Problem!**|Kaiming He Team|[2511.14761](http://arxiv.org/abs/2511.14761)|**[link](https://github.com/lillian039/VARC)**|
|**2025-11-18**|**UniGen-1.5: Enhancing Image Generation and Editing through Reward Unification in Reinforcement Learning**|Afshin Dehghan Team|[2511.14760](http://arxiv.org/abs/2511.14760)|null|
|**2025-11-18**|**$π^{*}_{0.6}$ : a VLA That Learns From Experience**|Zhiyuan Zhou Team|[2511.14759](http://arxiv.org/abs/2511.14759)|null|
|**2025-11-18**|**Vision Large Language Models Are Good Noise Handlers in Engagement Analysis**|Xiaobai Li Team|[2511.14749](http://arxiv.org/abs/2511.14749)|null|
|**2025-11-18**|**Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge**|Günter Klambauer Team|[2511.14744](http://arxiv.org/abs/2511.14744)|null|
|**2025-11-18**|**Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer**|Ankush Kumar Team|[2511.14691](http://arxiv.org/abs/2511.14691)|null|
|**2025-11-18**|**NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards**|Soujanya Poria Team|[2511.14659](http://arxiv.org/abs/2511.14659)|**[link](https://declare-lab.github.io/nora-1.5)**|
|**2025-11-18**|**Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities**|Inigo Zubeldia Team|[2511.14631](http://arxiv.org/abs/2511.14631)|null|
|**2025-11-18**|**Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks**|Xiaoshuai Hao Team|[2511.14592](http://arxiv.org/abs/2511.14592)|null|
|**2025-11-18**|**OmniZip: Audio-Guided Dynamic Token Compression for Fast Omnimodal Large Language Models**|Huan Wang Team|[2511.14582](http://arxiv.org/abs/2511.14582)|**[link](https://github.com/KD-TAO/OmniZip)**|
|**2025-11-18**|**Task Addition and Weight Disentanglement in Closed-Vocabulary Models**|Pascal Frossard Team|[2511.14569](http://arxiv.org/abs/2511.14569)|null|
|**2025-11-18**|**Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM**|Siyuan Cheng Team|[2511.14499](http://arxiv.org/abs/2511.14499)|null|
|**2025-11-18**|**Agentic Video Intelligence: A Flexible Framework for Advanced Video Exploration and Understanding**|Min-Ling Zhang Team|[2511.14446](http://arxiv.org/abs/2511.14446)|null|
|**2025-11-18**|**Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems**|Angelo Ferrando Team|[2511.14435](http://arxiv.org/abs/2511.14435)|null|
|**2025-11-18**|**Enhancing LLM-based Autonomous Driving with Modular Traffic Light and Sign Recognition**|Abhinav Valada Team|[2511.14391](http://arxiv.org/abs/2511.14391)|null|
|**2025-11-18**|**O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model**|Anirban Chakraborty Team|[2511.14368](http://arxiv.org/abs/2511.14368)|null|
|**2025-11-18**|**ArchMap: Arch-Flattening and Knowledge-Guided Vision Language Model for Tooth Counting and Structured Dental Understanding**|Jionglong Su Team|[2511.14336](http://arxiv.org/abs/2511.14336)|null|
|**2025-11-18**|**When Words Change the Model: Sensitivity of LLMs for Constraint Programming Modelling**|Jacopo Mauro Team|[2511.14334](http://arxiv.org/abs/2511.14334)|null|
|**2025-11-18**|**Step by Step Network**|Gao Huang Team|[2511.14329](http://arxiv.org/abs/2511.14329)|null|
|**2025-11-18**|**Segmentwise Pruning in Audio-Language Models**|Jean-François Bonastre Team|[2511.14293](http://arxiv.org/abs/2511.14293)|null|
|**2025-11-17**|**Scaling Spatial Intelligence with Multimodal Foundation Models**|Lei Yang Team|[2511.13719](http://arxiv.org/abs/2511.13719)|**[link](https://huggingface.co/collections/sensenova/sensenova-si)**|
|**2025-11-17**|**TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models**|Ying-Cong Chen Team|[2511.13704](http://arxiv.org/abs/2511.13704)|**[link](https://haroldchen19.github.io/TiViBench-Page/)**|
|**2025-11-17**|**Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation**|Joseph K J Team|[2511.13689](http://arxiv.org/abs/2511.13689)|null|
|**2025-11-17**|**Training-Free Multi-View Extension of IC-Light for Textual Position-Aware Scene Relighting**|Haoji Hu Team|[2511.13684](http://arxiv.org/abs/2511.13684)|null|
|**2025-11-17**|**Part-X-MLLM: Part-aware 3D Multimodal Large Language Model**|Chunchao Guo Team|[2511.13647](http://arxiv.org/abs/2511.13647)|null|
|**2025-11-17**|**CacheFlow: Compressive Streaming Memory for Efficient Long-Form Video Understanding**|Daivik Patel Team|[2511.13644](http://arxiv.org/abs/2511.13644)|null|
|**2025-11-17**|**CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product**|Jiayi Cen Team|[2511.13626](http://arxiv.org/abs/2511.13626)|null|
|**2025-11-17**|**FreeAskWorld: An Interactive and Closed-Loop Simulator for Human-Centric Embodied AI**|Jiangtao Gong Team|[2511.13524](http://arxiv.org/abs/2511.13524)|null|
|**2025-11-17**|**Language-Guided Invariance Probing of Vision-Language Models**|Jae Joong Lee Team|[2511.13494](http://arxiv.org/abs/2511.13494)|null|
|**2025-11-17**|**Semantic Document Derendering: SVG Reconstruction via Vision-Language Modeling**|Pascal Frossard Team|[2511.13478](http://arxiv.org/abs/2511.13478)|null|
|**2025-11-17**|**Trust in Vision-Language Models: Insights from a Participatory User Workshop**|Viola Schiaffonati Team|[2511.13458](http://arxiv.org/abs/2511.13458)|null|
|**2025-11-17**|**Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline**|Ziqian Lu Team|[2511.13442](http://arxiv.org/abs/2511.13442)|null|
|**2025-11-17**|**VOPE: Revisiting Hallucination of Vision-Language Models in Voluntary Imagination Task**|Xilin Chen Team|[2511.13420](http://arxiv.org/abs/2511.13420)|null|
|**2025-11-17**|**Attention Grounded Enhancement for Visual Document Retrieval**|Keping Bi Team|[2511.13415](http://arxiv.org/abs/2511.13415)|null|
|**2025-11-17**|**Descriptor: Distance-Annotated Traffic Perception Question Answering (DTPQA)**|Ciaran Eising Team|[2511.13397](http://arxiv.org/abs/2511.13397)|null|
|**2025-11-17**|**Generalized Denoising Diffusion Codebook Models (gDDCM): Tokenizing images using a pre-trained diffusion model**|Fei Kong Team|[2511.13387](http://arxiv.org/abs/2511.13387)|null|
|**2025-11-17**|**Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models**|Dario Rodighiero Team|[2511.13378](http://arxiv.org/abs/2511.13378)|null|
|**2025-11-17**|**Tab-PET: Graph-Based Positional Encodings for Tabular Transformers**|Mehul Motani Team|[2511.13338](http://arxiv.org/abs/2511.13338)|null|
|**2025-11-17**|**TabFlash: Efficient Table Understanding with Progressive Question Conditioning and Token Focusing**|Hyunwoo J. Kim Team|[2511.13283](http://arxiv.org/abs/2511.13283)|null|
|**2025-11-17**|**Is your VLM Sky-Ready? A Comprehensive Spatial Intelligence Benchmark for UAV Navigation**|Wenbo Ding Team|[2511.13269](http://arxiv.org/abs/2511.13269)|null|
|**2025-11-14**|**DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding**|Jinsung Yoon Team|[2511.11552](http://arxiv.org/abs/2511.11552)|null|
|**2025-11-14**|**Bridging Hidden States in Vision-Language Models**|Jacob Fein-Ashley Team|[2511.11526](http://arxiv.org/abs/2511.11526)|null|
|**2025-11-14**|**Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities**|Jingyuan Chen Team|[2511.11512](http://arxiv.org/abs/2511.11512)|null|
|**2025-11-14**|**PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models**|Manish Bhattarai Team|[2511.11502](http://arxiv.org/abs/2511.11502)|null|
|**2025-11-14**|**Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective**|Ngan Le Team|[2511.11478](http://arxiv.org/abs/2511.11478)|null|
|**2025-11-14**|**Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents**|Fabrizio Battiloro Team|[2511.11468](http://arxiv.org/abs/2511.11468)|null|
|**2025-11-14**|**VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation**|Klaus Maier-Hein Team|[2511.11450](http://arxiv.org/abs/2511.11450)|null|
|**2025-11-14**|**From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs**|Giuseppe Riccardi Team|[2511.11440](http://arxiv.org/abs/2511.11440)|null|
|**2025-11-14**|**VP-Bench: A Comprehensive Benchmark for Visual Prompting in Multimodal Large Language Models**|Wenqiang Lei Team|[2511.11438](http://arxiv.org/abs/2511.11438)|null|
|**2025-11-14**|**Comprehension of Multilingual Expressions Referring to Target Objects in Visual Inputs**|Bruno Martins Team|[2511.11427](http://arxiv.org/abs/2511.11427)|null|
|**2025-11-14**|**BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning**|De-Chuan Zhan Team|[2511.11421](http://arxiv.org/abs/2511.11421)|null|
|**2025-11-14**|**Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models**|Baoliang Chen Team|[2511.11410](http://arxiv.org/abs/2511.11410)|null|
|**2025-11-14**|**MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model**|Bo Yan Team|[2511.11407](http://arxiv.org/abs/2511.11407)|null|
|**2025-11-14**|**DocSLM: A Small Vision-Language Model for Long Multimodal Document Understanding**|Sunando Sengupta Team|[2511.11313](http://arxiv.org/abs/2511.11313)|null|
|**2025-11-14**|**EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment**|Hongyi Zhang Team|[2511.11301](http://arxiv.org/abs/2511.11301)|null|
|**2025-11-14**|**AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models**|Volker Tresp Team|[2511.11299](http://arxiv.org/abs/2511.11299)|**[link](https://github.com/HaokunChen245/AUVIC)**|
|**2025-11-14**|**Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation**|Xi Zheng Team|[2511.11298](http://arxiv.org/abs/2511.11298)|null|
|**2025-11-14**|**GraphPilot: Grounded Scene Graph Conditioning for Language-Based Autonomous Driving**|Abhinav Valada Team|[2511.11266](http://arxiv.org/abs/2511.11266)|null|
|**2025-11-14**|**Discovering Meaningful Units with Visually Grounded Semantics from Image Captions**|James Henderson Team|[2511.11262](http://arxiv.org/abs/2511.11262)|null|
|**2025-11-14**|**CountSteer: Steering Attention for Object Counting in Diffusion Models**|Hyunsoo Cho Team|[2511.11253](http://arxiv.org/abs/2511.11253)|null|
|**2025-11-13**|**Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling**|Jinguo Zhu Team|[2511.10648](http://arxiv.org/abs/2511.10648)|null|
|**2025-11-13**|**Querying Labeled Time Series Data with Scenario Programs**|Sanjit A Seshia Team|[2511.10627](http://arxiv.org/abs/2511.10627)|null|
|**2025-11-13**|**Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals**|Pawan Goyal Team|[2511.10615](http://arxiv.org/abs/2511.10615)|null|
|**2025-11-13**|**Impact of Layer Norm on Memorization and Generalization in Transformers**|Jung-Eun Kim Team|[2511.10566](http://arxiv.org/abs/2511.10566)|null|
|**2025-11-13**|**OmniVGGT: Omni-Modality Driven Visual Geometry Grounded**|Ziwei Liu Team|[2511.10560](http://arxiv.org/abs/2511.10560)|**[link](https://livioni.github.io/OmniVGGT-offcial/)**|
|**2025-11-13**|**SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation**|Liqiang Nie Team|[2511.10518](http://arxiv.org/abs/2511.10518)|**[link](https://github.com/JiuTian-VL/SemanticVLA)**|
|**2025-11-13**|**LLM-YOLOMS: Large Language Model-based Semantic Interpretation and Fault Diagnosis for Wind Turbine Components**|Jianbo Feng Team|[2511.10394](http://arxiv.org/abs/2511.10394)|null|
|**2025-11-13**|**MonkeyOCR v1.5 Technical Report: Unlocking Robust Document Parsing for Complex Patterns**|Xiang Bai Team|[2511.10390](http://arxiv.org/abs/2511.10390)|null|
|**2025-11-13**|**Rethinking Visual Information Processing in Multimodal LLMs**|Amit Kumar K C Team|[2511.10301](http://arxiv.org/abs/2511.10301)|null|
|**2025-11-13**|**Adaptive Residual-Update Steering for Low-Overhead Hallucination Mitigation in Large Vision Language Models**|Pekka Marttinen Team|[2511.10292](http://arxiv.org/abs/2511.10292)|null|
|**2025-11-13**|**PROPA: Toward Process-level Optimization in Visual Reasoning via Reinforcement Learning**|Jey Han Lau Team|[2511.10279](http://arxiv.org/abs/2511.10279)|null|
|**2025-11-13**|**Causal-HalBench: Uncovering LVLMs Object Hallucinations Through Causal Intervention**|Xiang Wang Team|[2511.10268](http://arxiv.org/abs/2511.10268)|null|
|**2025-11-13**|**Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis**|Min Cao Team|[2511.10254](http://arxiv.org/abs/2511.10254)|null|
|**2025-11-13**|**TubeRMC: Tube-conditioned Reconstruction with Mutual Constraints for Weakly-supervised Spatio-Temporal Video Grounding**|Beihao Xia Team|[2511.10241](http://arxiv.org/abs/2511.10241)|null|
|**2025-11-13**|**Intilligence Foundation Model: A New Perspective to Approach Artificial General Intelligence**|Yao Zhao Team|[2511.10119](http://arxiv.org/abs/2511.10119)|null|
|**2025-11-13**|**MTAttack: Multi-Target Backdoor Attacks against Large Vision-Language Models**|Xiao Bai Team|[2511.10098](http://arxiv.org/abs/2511.10098)|null|
|**2025-11-13**|**How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders**|Dianbo Liu Team|[2511.10094](http://arxiv.org/abs/2511.10094)|null|
|**2025-11-13**|**SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition**|Zitong Yu Team|[2511.10091](http://arxiv.org/abs/2511.10091)|null|
|**2025-11-13**|**GridPrune: From "Where to Look" to "What to Select" in Visual Token Pruning for MLLMs**|Pengwei Wang Team|[2511.10081](http://arxiv.org/abs/2511.10081)|null|
|**2025-11-13**|**VLF-MSC: Vision-Language Feature-Based Multimodal Semantic Communication System**|Joonhyuk Kang Team|[2511.10074](http://arxiv.org/abs/2511.10074)|null|
|**2025-11-10**|**Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective**|Somil Bansal Team|[2511.07410](http://arxiv.org/abs/2511.07410)|null|
|**2025-11-10**|**CAMP-VQA: Caption-Embedded Multimodal Perception for No-Reference Quality Assessment of Compressed Video**|David Bull Team|[2511.07290](http://arxiv.org/abs/2511.07290)|null|
|**2025-11-10**|**Leveraging Text-Driven Semantic Variation for Robust OOD Segmentation**|Jaekoo Lee Team|[2511.07238](http://arxiv.org/abs/2511.07238)|null|
|**2025-11-10**|**Federated Learning for Video Violence Detection: Complementary Roles of Lightweight CNNs and Vision-Language Models for Energy-Efficient Use**|Rachid Chelouah Team|[2511.07171](http://arxiv.org/abs/2511.07171)|null|
|**2025-11-10**|**ClusterMine: Robust Label-Free Visual Out-Of-Distribution Detection via Concept Mining from Text Corpora**|Markus Kollmann Team|[2511.07068](http://arxiv.org/abs/2511.07068)|**[link](https://github.com/HHU-MMBS/clustermine_wacv_official)**|
|**2025-11-10**|**CoLM: Collaborative Large Models via A Client-Server Paradigm**|Hongyuan Zhang Team|[2511.06991](http://arxiv.org/abs/2511.06991)|null|
|**2025-11-10**|**RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation**|Yu Zhang Team|[2511.06899](http://arxiv.org/abs/2511.06899)|null|
|**2025-11-10**|**Flexible Concept Bottleneck Model**|Rui Zhang Team|[2511.06678](http://arxiv.org/abs/2511.06678)|null|
|**2025-11-10**|**HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment**|Shiguo Lian Team|[2511.06653](http://arxiv.org/abs/2511.06653)|null|
|**2025-11-10**|**NOVO: Bridging LLaVA and SAM with Visual-only Prompts for Reasoning Segmentation**|Yeong-Jun Cho Team|[2511.06651](http://arxiv.org/abs/2511.06651)|null|
|**2025-11-10**|**How Do VLAs Effectively Inherit from VLMs?**|Jiang Bian Team|[2511.06619](http://arxiv.org/abs/2511.06619)|null|
|**2025-11-09**|**A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving**|Xiaopeng Li Team|[2511.06496](http://arxiv.org/abs/2511.06496)|null|
|**2025-11-09**|**Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models**|Sabine Süsstrunk Team|[2511.06490](http://arxiv.org/abs/2511.06490)|null|
|**2025-11-09**|**GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding**|Riad Souissi Team|[2511.06348](http://arxiv.org/abs/2511.06348)|null|
|**2025-11-09**|**ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning**|Moazzem Hossain Team|[2511.06316](http://arxiv.org/abs/2511.06316)|null|
|**2025-11-09**|**TinyChemVL: Advancing Chemical Vision-Language Models via Efficient Visual Token Reduction and Complex Reaction Tasks**|Bo Xu Team|[2511.06283](http://arxiv.org/abs/2511.06283)|null|
|**2025-11-09**|**WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation**|Jie Tang Team|[2511.06251](http://arxiv.org/abs/2511.06251)|null|
|**2025-11-09**|**Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation**|Winston H. Hsu Team|[2511.06240](http://arxiv.org/abs/2511.06240)|null|
|**2025-11-09**|**MoRA: Missing Modality Low-Rank Adaptation for Visual Recognition**|Vijaykrishnan Narayanan Team|[2511.06225](http://arxiv.org/abs/2511.06225)|null|
|**2025-11-09**|**Scene-Aware Urban Design: A Human-AI Recommendation Framework Using Co-Occurrence Embeddings and Vision-Language Models**|Alexander Htet Kyaw Team|[2511.06201](http://arxiv.org/abs/2511.06201)|null|
|**2025-11-07**|**Visual Spatial Tuning**|Hengshuang Zhao Team|[2511.05491](http://arxiv.org/abs/2511.05491)|null|
|**2025-11-07**|**Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval**|Hongda Shen Team|[2511.05325](http://arxiv.org/abs/2511.05325)|null|
|**2025-11-07**|**Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings**|Furong Huang Team|[2511.05017](http://arxiv.org/abs/2511.05017)|null|
|**2025-11-07**|**iFlyBot-VLM Technical Report**|Jia Pan Team|[2511.04976](http://arxiv.org/abs/2511.04976)|null|
|**2025-11-07**|**A benchmark multimodal oro-dental dataset for large vision-language models**|Muhammad Saqib Team|[2511.04948](http://arxiv.org/abs/2511.04948)|null|
|**2025-11-06**|**Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning**|Yiannis Kantaros Team|[2511.04835](http://arxiv.org/abs/2511.04835)|null|
|**2025-11-06**|**IndicVisionBench: Benchmarking Cultural and Multilingual Understanding in VLMs**|Shubham Agarwal Team|[2511.04727](http://arxiv.org/abs/2511.04727)|null|
|**2025-11-05**|**SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking**|Dacheng Tao Team|[2511.04711](http://arxiv.org/abs/2511.04711)|null|
|**2025-11-06**|**SAFe-Copilot: Unified Shared Autonomy Framework**|Daniela Rus Team|[2511.04664](http://arxiv.org/abs/2511.04664)|null|
|**2025-11-06**|**Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm**|Xipeng Qiu Team|[2511.04570](http://arxiv.org/abs/2511.04570)|null|
|**2025-11-06**|**Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment**|Bo Zhao Team|[2511.04555](http://arxiv.org/abs/2511.04555)|**[link](https://github.com/MINT-SJTU/Evo-1)**|
|**2025-11-07**|**ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai**|Kunat Pipatanakul Team|[2511.04479](http://arxiv.org/abs/2511.04479)|null|
|**2025-11-06**|**GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents**|Dongmei Zhang Team|[2511.04307](http://arxiv.org/abs/2511.04307)|null|
|**2025-11-07**|**On the Brittleness of CLIP Text Encoders**|Luca Rossetto Team|[2511.04247](http://arxiv.org/abs/2511.04247)|**[link](https://github.com/allie-tran/clip-brittleness)**|
|**2025-11-06**|**Text to Sketch Generation with Multi-Styles**|Lei Xu Team|[2511.04123](http://arxiv.org/abs/2511.04123)|null|
|**2025-11-05**|**Context informs pragmatic interpretation in vision-language models**|Michael C. Frank Team|[2511.03908](http://arxiv.org/abs/2511.03908)|null|
|**2025-11-05**|**Contamination Detection for VLMs using Multi-Modal Semantic Perturbation**|Yong Jae Lee Team|[2511.03774](http://arxiv.org/abs/2511.03774)|null|
|**2025-11-05**|**GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement**|Jiachen Li Team|[2511.03400](http://arxiv.org/abs/2511.03400)|null|
|**2025-11-05**|**Decoupling Augmentation Bias in Prompt Learning for Vision-Language Models**|Seokju Lee Team|[2511.03367](http://arxiv.org/abs/2511.03367)|null|
|**2025-11-04**|**LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation**|Jinyoung Yeo Team|[2511.03001](http://arxiv.org/abs/2511.03001)|null|
|**2025-11-04**|**SCALE-VLP: Soft-Weighted Contrastive Volumetric Vision-Language Pre-training with Spatial-Knowledge Semantics**|Leonid Sigal Team|[2511.02996](http://arxiv.org/abs/2511.02996)|null|
|**2025-11-04**|**XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations**|Jian Tang Team|[2511.02776](http://arxiv.org/abs/2511.02776)|null|
|**2025-11-04**|**Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes**|Yi Jiang Team|[2511.02503](http://arxiv.org/abs/2511.02503)|null|
|**2025-11-04**|**RxnCaption: Reformulating Reaction Diagram Parsing as Visual Prompt Guided Captioning**|Conghui He Team|[2511.02384](http://arxiv.org/abs/2511.02384)|null|
|**2025-11-04**|**The Pervasive Blind Spot: Benchmarking VLM Inference Risks on Everyday Personal Videos**|Hewu Li Team|[2511.02367](http://arxiv.org/abs/2511.02367)|null|
|**2025-11-04**|**CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning**|Han Yan Team|[2511.02360](http://arxiv.org/abs/2511.02360)|null|
|**2025-11-04**|**LACY: A Vision-Language Model-based Language-Action Cycle for Self-Improving Robotic Manipulation**|Changhyun Choi Team|[2511.02239](http://arxiv.org/abs/2511.02239)|**[link](https://vla2026.github.io/LACY/)**|
|**2025-11-04**|**Text to Robotic Assembly of Multi Component Objects using 3D Generative AI and Vision Language Models**|Randall Davis Team|[2511.02162](http://arxiv.org/abs/2511.02162)|null|
|**2025-11-03**|**Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion**|Dung D. Le Team|[2511.02113](http://arxiv.org/abs/2511.02113)|null|
|**2025-11-03**|**TRACE: Textual Reasoning for Affordance Coordinate Extraction**|Matthew S. Brown Team|[2511.01999](http://arxiv.org/abs/2511.01999)|null|
|**2025-11-03**|**Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing**|Tao Qi Team|[2511.01952](http://arxiv.org/abs/2511.01952)|null|
|**2025-11-04**|**Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models**|Mingwei Shen Team|[2511.01831](http://arxiv.org/abs/2511.01831)|null|
|**2025-11-03**|**SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art**|Alona Strugatski Team|[2511.01817](http://arxiv.org/abs/2511.01817)|null|
|**2025-11-03**|**GenDexHand: Generative Simulation for Dexterous Hands**|Yi Ma Team|[2511.01791](http://arxiv.org/abs/2511.01791)|null|
|**2025-11-03**|**3EED: Ground Everything Everywhere in 3D**|Ziwei Liu Team|[2511.01755](http://arxiv.org/abs/2511.01755)|**[link](https://project-3eed.github.io/)**|
|**2025-11-03**|**UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback**|Fan Wang Team|[2511.01678](http://arxiv.org/abs/2511.01678)|null|
|**2025-11-03**|**Vote-in-Context: Turning VLMs into Zero-Shot Rank Fusers**|Naeemullah Khan Team|[2511.01617](http://arxiv.org/abs/2511.01617)|null|
|**2025-11-03**|**Analyzing Sustainability Messaging in Large-Scale Corporate Social Media**|Marcel Worring Team|[2511.01550](http://arxiv.org/abs/2511.01550)|null|
|**2025-11-03**|**AERMANI-VLM: Structured Prompting and Reasoning for Aerial Manipulation with Vision Language Models**|Spandan Roy Team|[2511.01472](http://arxiv.org/abs/2511.01472)|null|
|**2025-11-03**|**HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA**|Shihong Xia Team|[2511.01463](http://arxiv.org/abs/2511.01463)|null|
|**2025-11-03**|**When to Trust the Answer: Question-Aligned Semantic Nearest Neighbor Entropy for Safer Surgical VQA**|Mobarak I. Hoque Team|[2511.01458](http://arxiv.org/abs/2511.01458)|null|
|**2025-10-31**|**PETAR: Localized Findings Generation with Mask-Aware Vision-Language Modeling for PET Automated Reporting**|Tyler J. Bradshaw Team|[2510.27680](http://arxiv.org/abs/2510.27680)|null|
|**2025-10-31**|**Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning**|Jiaqi Wang Team|[2510.27606](http://arxiv.org/abs/2510.27606)|null|
|**2025-10-31**|**From Pixels to Paths: A Multi-Agent Framework for Editable Scientific Illustration**|Kaipeng Zhang Team|[2510.27452](http://arxiv.org/abs/2510.27452)|null|
|**2025-10-31**|**Modality Alignment across Trees on Heterogeneous Hyperbolic Manifolds**|Mehrtash Harandi Team|[2510.27391](http://arxiv.org/abs/2510.27391)|null|
|**2025-10-31**|**FOCUS: Efficient Keyframe Selection for Long Video Understanding**|Yang You Team|[2510.27280](http://arxiv.org/abs/2510.27280)|null|
|**2025-10-31**|**T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging Analysis**|Mohammad Yaqub Team|[2510.27265](http://arxiv.org/abs/2510.27265)|null|
|**2025-10-31**|**ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models**|Tengxiang Zhang Team|[2510.27256](http://arxiv.org/abs/2510.27256)|null|
|**2025-11-03**|**Enhancing Spatio-Temporal Zero-shot Action Recognition with Language-driven Description Attributes**|Seong-Whan Lee Team|[2510.27255](http://arxiv.org/abs/2510.27255)|null|
|**2025-10-31**|**Generating Accurate and Detailed Captions for High-Resolution Images**|Jiyoung Jung Team|[2510.27164](http://arxiv.org/abs/2510.27164)|null|
|**2025-10-30**|**MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation**|Xiaohui Xie Team|[2510.26996](http://arxiv.org/abs/2510.26996)|null|
|**2025-10-30**|**MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models**|Ziliang Chen Team|[2510.26937](http://arxiv.org/abs/2510.26937)|null|
|**2025-10-30**|**NaviTrace: Evaluating Embodied Navigation of Vision-Language Models**|Jonas Frey Team|[2510.26909](http://arxiv.org/abs/2510.26909)|null|
|**2025-10-30**|**Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations**|Jane Cleland-Huang Team|[2510.26905](http://arxiv.org/abs/2510.26905)|null|
|**2025-10-30**|**Do Vision-Language Models Measure Up? Benchmarking Visual Measurement Reading with MeasureBench**|Xi Yang Team|[2510.26865](http://arxiv.org/abs/2510.26865)|**[link](https://flageval-baai.github.io/MeasureBenchPage/)**|
|**2025-11-03**|**ChartAB: A Benchmark for Chart Grounding & Dense Alignment**|Tianyi Zhou Team|[2510.26781](http://arxiv.org/abs/2510.26781)|null|
|**2025-10-30**|**SteerVLM: Robust Model Control through Lightweight Activation Steering for Vision Language Models**|Chris Thomas Team|[2510.26769](http://arxiv.org/abs/2510.26769)|null|
|**2025-10-30**|**All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles**|Abolfazl Razi Team|[2510.26641](http://arxiv.org/abs/2510.26641)|null|
|**2025-10-30**|**Counteracting Matthew Effect in Self-Improvement of LVLMs through Head-Tail Re-balancing**|Xuanjing Huang Team|[2510.26474](http://arxiv.org/abs/2510.26474)|null|
|**2025-11-03**|**Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition**|ShengJun Huang Team|[2510.26466](http://arxiv.org/abs/2510.26466)|null|
|**2025-10-30**|**Towards Fine-Grained Vision-Language Alignment for Few-Shot Anomaly Detection**|Chengjie Wang Team|[2510.26464](http://arxiv.org/abs/2510.26464)|null|
|**2025-10-30**|**A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models**|Muhammad Haris Khan Team|[2510.26441](http://arxiv.org/abs/2510.26441)|null|
|**2025-10-30**|**MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders**|Marco Grangetto Team|[2510.26411](http://arxiv.org/abs/2510.26411)|null|
|**2025-10-30**|**Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual**|Peerat Limkonchotiwat Team|[2510.26271](http://arxiv.org/abs/2510.26271)|null|
|**2025-10-30**|**Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models**|Shigeru Kitazawa Team|[2510.26241](http://arxiv.org/abs/2510.26241)|null|
|**2025-10-30**|**MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction**|Ali Diba Team|[2510.26151](http://arxiv.org/abs/2510.26151)|null|
|**2025-10-30**|**GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks**|Qing Li Team|[2510.26098](http://arxiv.org/abs/2510.26098)|null|
|**2025-10-30**|**Dynamic VLM-Guided Negative Prompting for Diffusion Models**|Yoonseok Choi Team|[2510.26052](http://arxiv.org/abs/2510.26052)|null|
|**2025-10-29**|**CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments**|Antoine Bosselut Team|[2510.26006](http://arxiv.org/abs/2510.26006)|null|
|**2025-10-30**|**PairUni: Pairwise Training for Unified Multimodal Language Models**|Zhuochen Wang Team|[2510.25682](http://arxiv.org/abs/2510.25682)|null|
|**2025-10-29**|**ALDEN: Reinforcement Learning for Active Navigation and Evidence Gathering in Long Documents**|Bela Gipp Team|[2510.25668](http://arxiv.org/abs/2510.25668)|null|
|**2025-10-29**|**Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization**|Aleksandr I. Panov Team|[2510.25616](http://arxiv.org/abs/2510.25616)|null|
|**2025-10-29**|**Using VLM Reasoning to Constrain Task and Motion Planning**|Zachary Kingston Team|[2510.25548](http://arxiv.org/abs/2510.25548)|null|
|**2025-10-29**|**Seeing, Signing, and Saying: A Vision-Language Model-Assisted Pipeline for Sign Language Data Acquisition and Curation from Social Media**|Josef van Genabith Team|[2510.25413](http://arxiv.org/abs/2510.25413)|null|
|**2025-10-29**|**SoraNav: Adaptive UAV Task-Centric Navigation via Zeroshot VLM Reasoning**|Wei Pan Team|[2510.25191](http://arxiv.org/abs/2510.25191)|null|
|**2025-10-29**|**Agentic Moderation: Multi-Agent Design for Safer Vision-Language Models**|Usman Naseem Team|[2510.25179](http://arxiv.org/abs/2510.25179)|null|
|**2025-10-29**|**Learning Spatial-Aware Manipulation Ordering**|Jian Pu Team|[2510.25138](http://arxiv.org/abs/2510.25138)|null|
|**2025-10-29**|**NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies**|Jinghui Lu Team|[2510.25122](http://arxiv.org/abs/2510.25122)|null|
|**2025-10-29**|**Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection**|Hyunwoo J. Kim Team|[2510.25094](http://arxiv.org/abs/2510.25094)|null|
|**2025-10-29**|**DRIP: Dynamic patch Reduction via Interpretable Pooling**|Sachin Kumar Team|[2510.25067](http://arxiv.org/abs/2510.25067)|null|
|**2025-10-28**|**Efficient License Plate Recognition via Pseudo-Labeled Supervision with Grounding DINO and YOLOv8**|Ching Yee Suen Team|[2510.25032](http://arxiv.org/abs/2510.25032)|null|
|**2025-10-28**|**SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving**|Mykel J. Kochenderfer Team|[2510.24949](http://arxiv.org/abs/2510.24949)|null|
|**2025-10-28**|**Finding Culture-Sensitive Neurons in Vision-Language Models**|Ivan Titov Team|[2510.24942](http://arxiv.org/abs/2510.24942)|null|
|**2025-10-28**|**Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning**|Arnold W. Schumann Team|[2510.24650](http://arxiv.org/abs/2510.24650)|null|
|**2025-10-28**|**OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid Validation in Realistic Workflows**|Lingpeng Kong Team|[2510.24411](http://arxiv.org/abs/2510.24411)|null|
|**2025-10-28**|**What do vision-language models see in the context? Investigating multimodal in-context learning**|Sandra Avila Team|[2510.24331](http://arxiv.org/abs/2510.24331)|null|
|**2025-10-28**|**Few-Shot Remote Sensing Image Scene Classification with CLIP and Prompt Learning**|Ivan Kitanovski Team|[2510.24321](http://arxiv.org/abs/2510.24321)|null|
|**2025-10-28**|**ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model**|Rui Yan Team|[2510.24285](http://arxiv.org/abs/2510.24285)|null|
|**2025-10-28**|**Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models**|Yue Gao Team|[2510.24242](http://arxiv.org/abs/2510.24242)|null|
|**2025-10-28**|**V-SAT: Video Subtitle Annotation Tool**|Vishwanathan Raman Team|[2510.24180](http://arxiv.org/abs/2510.24180)|null|
|**2025-10-28**|**Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning**|Xubo Luo Team|[2510.24152](http://arxiv.org/abs/2510.24152)|null|
|**2025-10-28**|**Compositional Image Synthesis with Inference-Time Scaling**|Namhyuk Ahn Team|[2510.24133](http://arxiv.org/abs/2510.24133)|**[link](https://github.com/gcl-inha/ReFocus)**|
|**2025-10-28**|**HistoLens: An Interactive XAI Toolkit for Verifying and Mitigating Flaws in Vision-Language Models for Histopathology**|Vandita Singh Team|[2510.24115](http://arxiv.org/abs/2510.24115)|null|
|**2025-10-28**|**PFEA: An LLM-based High-Level Natural Language Planning and Feedback Embodied Agent for Human-Centered AI**|Philip Dames Team|[2510.24109](http://arxiv.org/abs/2510.24109)|null|
|**2025-10-28**|**Enhancing CLIP Robustness via Cross-Modality Alignment**|Hanwang Zhang Team|[2510.24038](http://arxiv.org/abs/2510.24038)|null|
|**2025-10-28**|**Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks**|Hannah Kerner Team|[2510.24010](http://arxiv.org/abs/2510.24010)|null|
|**2025-10-28**|**Reasoning Visual Language Model for Chest X-Ray Analysis**|Daguang Xu Team|[2510.23968](http://arxiv.org/abs/2510.23968)|null|
|**2025-10-27**|**Latent Chain-of-Thought for Visual Reasoning**|Zhiqiang Tao Team|[2510.23925](http://arxiv.org/abs/2510.23925)|null|
|**2025-10-27**|**Explainable Detection of AI-Generated Images with Artifact Localization Using Faster-Than-Lies and Vision-Language Models for Edge Devices**|Madesh Kuppusamy Team|[2510.23775](http://arxiv.org/abs/2510.23775)|null|
|**2025-10-27**|**RobotArena $\infty$ : Scalable Robot Benchmarking via Real-to-Sim Translation**|Katerina Fragkiadaki Team|[2510.23571](http://arxiv.org/abs/2510.23571)|**[link](https://robotarenainf.github.io)**|
|**2025-10-28**|**VOLD: Reasoning Transfer from LLMs to Vision-Language Models via On-Policy Distillation**|Cordelia Schmid Team|[2510.23497](http://arxiv.org/abs/2510.23497)|null|
|**2025-10-27**|**On the Faithfulness of Visual Thinking: Measurement and Enhancement**|Guisong Xia Team|[2510.23482](http://arxiv.org/abs/2510.23482)|null|
|**2025-10-27**|**A Video Is Not Worth a Thousand Words**|Michael Wray Team|[2510.23253](http://arxiv.org/abs/2510.23253)|null|
|**2025-10-27**|**Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports**|Curtis P. Langlotz Team|[2510.23217](http://arxiv.org/abs/2510.23217)|null|
|**2025-10-27**|**DecoDINO: 3D Human-Scene Contact Prediction with Semantic Classification**|Angelo Broere Team|[2510.23203](http://arxiv.org/abs/2510.23203)|null|
|**2025-10-27**|**Evaluation of Vision-LLMs in Surveillance Video**|Jelte P. Mense Team|[2510.23190](http://arxiv.org/abs/2510.23190)|null|
|**2025-10-27**|**Finding 3D Scene Analogies with Multimodal Foundation Models**|Young Min Kim Team|[2510.23184](http://arxiv.org/abs/2510.23184)|null|
|**2025-10-27**|**Revisiting Multimodal Positional Encoding in Vision-Language Models**|Shuai Bai Team|[2510.23095](http://arxiv.org/abs/2510.23095)|null|
|**2025-10-27**|**Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models**|Donald MacDonald Team|[2510.23066](http://arxiv.org/abs/2510.23066)|null|
|**2025-10-27**|**VoMP: Predicting Volumetric Mechanical Property Fields**|Maria Shugrina Team|[2510.22975](http://arxiv.org/abs/2510.22975)|**[link](https://research.nvidia.com/labs/sil/projects/vomp)**|
|**2025-10-28**|**HyPerNav: Hybrid Perception for Object-Oriented Navigation in Unknown Environment**|Zhen Li Team|[2510.22917](http://arxiv.org/abs/2510.22917)|null|
|**2025-10-26**|**Seeing the Unseen: Towards Zero-Shot Inspection for Wind Turbine Blades using Knowledge-Augmented Vision Language Models**|Jiong Tang Team|[2510.22868](http://arxiv.org/abs/2510.22868)|null|
|**2025-10-26**|**Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models**|Kaito Tanaka Team|[2510.22838](http://arxiv.org/abs/2510.22838)|null|
|**2025-10-26**|**VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions**|Taehwan Kim Team|[2510.22798](http://arxiv.org/abs/2510.22798)|**[link](https://vehme.github.io/)**|
|**2025-10-26**|**Self-Calibrated Consistency can Fight Back for Adversarial Robustness in Vision-Language Models**|Mingkun Xu Team|[2510.22785](http://arxiv.org/abs/2510.22785)|null|
|**2025-10-26**|**MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion**|Chien-Sheng Wu Team|[2510.22768](http://arxiv.org/abs/2510.22768)|null|
|**2025-10-26**|**Jarvis: Towards Personalized AI Assistant via Personal KV-Cache Retrieval**|Wentao Zhang Team|[2510.22765](http://arxiv.org/abs/2510.22765)|null|
|**2025-10-26**|**S-Chain: Structured Visual Chain-of-Thought For Medicine**|Anh Totti Nguyen Team|[2510.22728](http://arxiv.org/abs/2510.22728)|null|
|**2025-10-26**|**Atlas Urban Index: A VLM-Based Approach for Spatially and Temporally Calibrated Urban Development Monitoring**|Prathamesh Mayekar Team|[2510.22702](http://arxiv.org/abs/2510.22702)|null|
|**2025-10-24**|**A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection**|Peter Henderson Team|[2510.21679](http://arxiv.org/abs/2510.21679)|null|
|**2025-10-24**|**Modest-Align: Data-Efficient Alignment for Vision-Language Models**|Zuozhu Liu Team|[2510.21606](http://arxiv.org/abs/2510.21606)|null|
|**2025-10-24**|**Head Pursuit: Probing Attention Specialization in Multimodal Transformers**|Alberto Cazzaniga Team|[2510.21518](http://arxiv.org/abs/2510.21518)|null|
|**2025-10-24**|**MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection**|Jie Qin Team|[2510.21449](http://arxiv.org/abs/2510.21449)|null|
|**2025-10-24**|**Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings**|Fakhri Karray Team|[2510.21424](http://arxiv.org/abs/2510.21424)|null|
|**2025-10-24**|**Bridging the gap to real-world language-grounded visual concept learning**|Seunghoon Hong Team|[2510.21412](http://arxiv.org/abs/2510.21412)|null|
|**2025-10-24**|**VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set**|Shuhui Wang Team|[2510.21323](http://arxiv.org/abs/2510.21323)|null|
|**2025-10-24**|**Memory-Free Continual Learning with Null Space Adaptation for Zero-Shot Vision-Language Models**|Taesup Kim Team|[2510.21175](http://arxiv.org/abs/2510.21175)|null|
|**2025-10-24**|**Generalizable Hierarchical Skill Learning via Object-Centric Representation**|Robert Platt Team|[2510.21121](http://arxiv.org/abs/2510.21121)|null|
|**2025-10-24**|**SafetyPairs: Isolating Safety Critical Image Features with Counterfactual Image Generation**|Joseph Yitan Cheng Team|[2510.21120](http://arxiv.org/abs/2510.21120)|null|
|**2025-10-24**|**MedAlign: A Synergistic Framework of Multimodal Preference Optimization and Federated Meta-Cognitive Reasoning**|Dong In Kim Team|[2510.21093](http://arxiv.org/abs/2510.21093)|null|
|**2025-10-24**|**Knowledge-Driven Vision-Language Model for Plexus Detection in Hirschsprung's Disease**|Adrian D. C. Chan Team|[2510.21083](http://arxiv.org/abs/2510.21083)|null|
|**2025-10-24**|**ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models**|Jimmy Chiun Team|[2510.21069](http://arxiv.org/abs/2510.21069)|null|
|**2025-10-23**|**3DReasonKnee: Advancing Grounded Reasoning in Medical Vision Language Models**|Pranav Rajpurkar Team|[2510.20967](http://arxiv.org/abs/2510.20967)|null|
|**2025-10-23**|**Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation**|Shengjie Wang Team|[2510.20812](http://arxiv.org/abs/2510.20812)|null|
|**2025-10-23**|**Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models**|Linfeng Zhang Team|[2510.20707](http://arxiv.org/abs/2510.20707)|**[link](https://github.com/xuyang-liu16/MixKV)**|
|**2025-10-23**|**Diagnosing Visual Reasoning: Challenges, Insights, and a Path Forward**|Chenliang Xu Team|[2510.20696](http://arxiv.org/abs/2510.20696)|null|
|**2025-10-23**|**Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging**|Bjoern Menze Team|[2510.20639](http://arxiv.org/abs/2510.20639)|null|
|**2025-10-23**|**Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models**|Lan-Zhe Guo Team|[2510.20477](http://arxiv.org/abs/2510.20477)|null|
|**2025-10-23**|**GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?**|Yingchun Wang Team|[2510.20333](http://arxiv.org/abs/2510.20333)|null|
|**2025-10-23**|**Breakdance Video classification in the age of Generative AI**|Michelle Munson Team|[2510.20287](http://arxiv.org/abs/2510.20287)|null|
|**2025-10-23**|**Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding**|Sangyoun Lee Team|[2510.20244](http://arxiv.org/abs/2510.20244)|null|
|**2025-10-23**|**Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context**|Sibei Yang Team|[2510.20229](http://arxiv.org/abs/2510.20229)|null|
|**2025-10-24**|**Surfer 2: The Next Generation of Cross-Platform Computer Use Agents**|Jevgenij Zubovskij Team|[2510.19949](http://arxiv.org/abs/2510.19949)|null|
|**2025-10-22**|**Semantic World Models**|Abhishek Gupta Team|[2510.19818](http://arxiv.org/abs/2510.19818)|null|
|**2025-10-22**|**olmOCR 2: Unit Test Rewards for Document OCR**|Kyle Lo Team|[2510.19817](http://arxiv.org/abs/2510.19817)|**[link](https://olmocr.allen.ai/)**|
|**2025-10-22**|**Class-Aware Prototype Learning with Negative Contrast for Test-Time Adaptation of Vision-Language Models**|Xuelong Li Team|[2510.19802](http://arxiv.org/abs/2510.19802)|null|
|**2025-10-22**|**MedReason-R1: Learning to Reason for CT Diagnosis with Reinforcement Learning and Local Zoom**|Shaohua Kevin Zhou Team|[2510.19626](http://arxiv.org/abs/2510.19626)|**[link](https://github.com/Leevan001/MedReason-R1)**|
|**2025-10-22**|**XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography**|Mauricio Reyes Team|[2510.19599](http://arxiv.org/abs/2510.19599)|null|
|**2025-10-22**|**Can You Trust What You See? Alpha Channel No-Box Attacks on Video Object Detection**|Qiben Yan Team|[2510.19574](http://arxiv.org/abs/2510.19574)|null|
|**2025-10-22**|**A Matter of Time: Revealing the Structure of Time in Vision-Language Models**|Matthias Zeppelzauer Team|[2510.19559](http://arxiv.org/abs/2510.19559)|null|
|**2025-10-22**|**[De|Re]constructing VLMs' Reasoning in Counting**|Giuseppe Riccardi Team|[2510.19555](http://arxiv.org/abs/2510.19555)|null|
|**2025-10-22**|**CARES: Context-Aware Resolution Selector for VLMs**|Eli Schwartz Team|[2510.19496](http://arxiv.org/abs/2510.19496)|null|
|**2025-10-22**|**Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes**|Baining Guo Team|[2510.19400](http://arxiv.org/abs/2510.19400)|**[link](https://github.com/microsoft/MV-RoboBench)**|
|**2025-10-22**|**A Training-Free Framework for Open-Vocabulary Image Segmentation and Recognition with EfficientNet and CLIP**|Wei Yu Chen Team|[2510.19333](http://arxiv.org/abs/2510.19333)|null|
|**2025-10-22**|**Unified Reinforcement and Imitation Learning for Vision-Language Models**|Yueh-Hua Wu Team|[2510.19307](http://arxiv.org/abs/2510.19307)|**[link](https://byungkwanlee.github.io/RIL-page)**|
|**2025-10-22**|**Hierarchical DLO Routing with Reinforcement Learning and In-Context Vision-language Models**|Changhyun Choi Team|[2510.19268](http://arxiv.org/abs/2510.19268)|null|
|**2025-10-22**|**Preliminary Use of Vision Language Model Driven Extraction of Mouse Behavior Towards Understanding Fear Expression**|Evangelos E. Papalexakis Team|[2510.19160](http://arxiv.org/abs/2510.19160)|null|
|**2025-10-21**|**PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions**|Kathleen McKeown Team|[2510.19060](http://arxiv.org/abs/2510.19060)|**[link](https://github.com/amith-ananthram/posh)**|
|**2025-10-21**|**Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts**|Hyunjung Shim Team|[2510.19001](http://arxiv.org/abs/2510.19001)|null|
|**2025-10-21**|**DSI-Bench: A Benchmark for Dynamic Spatial Intelligence**|Zhou Zhao Team|[2510.18873](http://arxiv.org/abs/2510.18873)|null|
|**2025-10-21**|**FedDEAP: Adaptive Dual-Prompt Tuning for Multi-Domain Federated Learning**|Jagath C. Rajapakse Team|[2510.18837](http://arxiv.org/abs/2510.18837)|null|
|**2025-10-21**|**Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation**|Elvis Hsieh Team|[2510.18751](http://arxiv.org/abs/2510.18751)|null|
|**2025-10-21**|**Exploring a Unified Vision-Centric Contrastive Alternatives on Multi-Modal Web Documents**|Mike Zheng Shou Team|[2510.18703](http://arxiv.org/abs/2510.18703)|**[link](https://linyq17.github.io/VC2L/)**|
|**2025-10-21**|**Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views**|Ruqi Huang Team|[2510.18632](http://arxiv.org/abs/2510.18632)|null|
|**2025-10-21**|**CUARewardBench: A Benchmark for Evaluating Reward Models on Computer-using Agent**|Xing Sun Team|[2510.18596](http://arxiv.org/abs/2510.18596)|null|
|**2025-10-21**|**CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder**|Hye Won Chung Team|[2510.18583](http://arxiv.org/abs/2510.18583)|null|
|**2025-10-21**|**Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation**|Yan-Ann Chen Team|[2510.18502](http://arxiv.org/abs/2510.18502)|null|
|**2025-10-21**|**StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking**|Donglin Yu Team|[2510.18483](http://arxiv.org/abs/2510.18483)|null|
|**2025-10-21**|**Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation**|Cristina España-Bonet Team|[2510.18439](http://arxiv.org/abs/2510.18439)|null|
|**2025-10-21**|**ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization**|Hongyi Wen Team|[2510.18433](http://arxiv.org/abs/2510.18433)|null|
|**2025-10-21**|**Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding**|Xian Wu Team|[2510.18321](http://arxiv.org/abs/2510.18321)|null|
|**2025-10-21**|**StreamingTOM: Streaming Token Compression for Efficient Video Understanding**|Huan Wang Team|[2510.18269](http://arxiv.org/abs/2510.18269)|null|
|**2025-10-21**|**UWBench: A Comprehensive Vision-Language Benchmark for Underwater Understanding**|Xuelong Li Team|[2510.18262](http://arxiv.org/abs/2510.18262)|null|
|**2025-10-21**|**RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology**|Bjoern Menze Team|[2510.18188](http://arxiv.org/abs/2510.18188)|null|
|**2025-10-20**|**Online In-Context Distillation for Low-Resource Vision Language Models**|Karteek Alahari Team|[2510.18117](http://arxiv.org/abs/2510.18117)|null|
|**2025-10-20**|**HouseTour: A Virtual Real Estate A(I)gent**|Iro Armeni Team|[2510.18054](http://arxiv.org/abs/2510.18054)|null|
|**2025-10-20**|**SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection**|Johannes Betz Team|[2510.18034](http://arxiv.org/abs/2510.18034)|null|
|**2025-10-21**|**Glyph: Scaling Context Windows via Visual-Text Compression**|Minlie Huang Team|[2510.17800](http://arxiv.org/abs/2510.17800)|null|
|**2025-10-20**|**SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference**|Zhijian Liu Team|[2510.17777](http://arxiv.org/abs/2510.17777)|null|
|**2025-10-20**|**Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs**|Hanghang Tong Team|[2510.17771](http://arxiv.org/abs/2510.17771)|null|
|**2025-10-20**|**VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models**|Ruqi Zhang Team|[2510.17759](http://arxiv.org/abs/2510.17759)|null|
|**2025-10-20**|**Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs**|Rachid Chelouah Team|[2510.17651](http://arxiv.org/abs/2510.17651)|null|
|**2025-10-20**|**SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering**|Li Liu Team|[2510.17633](http://arxiv.org/abs/2510.17633)|null|
|**2025-10-20**|**MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning**|Adiba Mahbub Proma Team|[2510.17590](http://arxiv.org/abs/2510.17590)|null|
|**2025-10-20**|**Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation**|Zhicheng Dou Team|[2510.17354](http://arxiv.org/abs/2510.17354)|null|
|**2025-10-20**|**Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations**|Omri Azencot Team|[2510.17313](http://arxiv.org/abs/2510.17313)|null|
|**2025-10-20**|**FineVision: Open Data Is All You Need**|Andrés Marafioti Team|[2510.17269](http://arxiv.org/abs/2510.17269)|null|
|**2025-10-20**|**ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models**|Guoming Tang Team|[2510.17197](http://arxiv.org/abs/2510.17197)|null|
|**2025-10-20**|**SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving**|Shaohua Wu Team|[2510.17191](http://arxiv.org/abs/2510.17191)|null|
|**2025-10-20**|**OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation**|Arash Ajoudani Team|[2510.17150](http://arxiv.org/abs/2510.17150)|**[link](https://sites.google.com/view/omni-vic})**|
|**2025-10-20**|**Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey**|Jian Cheng Team|[2510.17111](http://arxiv.org/abs/2510.17111)|null|
|**2025-10-19**|**Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding**|Yutong Zhong Team|[2510.17034](http://arxiv.org/abs/2510.17034)|null|
|**2025-10-19**|**Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?**|Renfen Hu Team|[2510.16924](http://arxiv.org/abs/2510.16924)|null|
|**2025-10-19**|**VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents**|Manling Li Team|[2510.16907](http://arxiv.org/abs/2510.16907)|null|
|**2025-10-19**|**Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding**|Xiaowei He Team|[2510.16870](http://arxiv.org/abs/2510.16870)|null|
|**2025-10-19**|**Region in Context: Text-condition Image editing with Human-like semantic reasoning**|Phan Xuan Tan Team|[2510.16772](http://arxiv.org/abs/2510.16772)|null|
|**2025-10-19**|**See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models**|Xike Xie Team|[2510.16769](http://arxiv.org/abs/2510.16769)|null|
|**2025-10-17**|**BiomedXPro: Prompt Optimization for Explainable Diagnosis with Biomedical Vision Language Models**|Damayanthi Herath Team|[2510.15866](http://arxiv.org/abs/2510.15866)|null|
|**2025-10-17**|**Neuro-Symbolic Spatial Reasoning in Segmentation**|Shaogang Gong Team|[2510.15841](http://arxiv.org/abs/2510.15841)|null|
|**2025-10-17**|**Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models**|Xiting Wang Team|[2510.15430](http://arxiv.org/abs/2510.15430)|null|
|**2025-10-17**|**Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs**|Goh Man Fye Team|[2510.15418](http://arxiv.org/abs/2510.15418)|null|
|**2025-10-17**|**Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing**|Yuan Qi Team|[2510.15349](http://arxiv.org/abs/2510.15349)|null|
|**2025-10-16**|**From Pixels to Words -- Towards Native Vision-Language Primitives at Scale**|Ziwei Liu Team|[2510.14979](http://arxiv.org/abs/2510.14979)|null|
|**2025-10-16**|**Learning an Image Editing Model without Image Editing Pairs**|Xun Huang Team|[2510.14978](http://arxiv.org/abs/2510.14978)|**[link](https://nupurkmr9.github.io/npedit/)**|
|**2025-10-16**|**RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks**|Jiachen Li Team|[2510.14968](http://arxiv.org/abs/2510.14968)|null|
|**2025-10-16**|**RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning**|Haoran Li Team|[2510.14828](http://arxiv.org/abs/2510.14828)|null|
|**2025-10-16**|**CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection**|Hyunjung Shim Team|[2510.14792](http://arxiv.org/abs/2510.14792)|null|
|**2025-10-16**|**Free-Grained Hierarchical Recognition**|Stella X. Yu Team|[2510.14737](http://arxiv.org/abs/2510.14737)|null|
|**2025-10-16**|**Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference**|Andrew Tao Team|[2510.14624](http://arxiv.org/abs/2510.14624)|null|
|**2025-10-16**|**Talking Points: Describing and Localizing Pixels**|Shai Avidan Team|[2510.14583](http://arxiv.org/abs/2510.14583)|null|
|**2025-10-16**|**Exploring Cross-Modal Flows for Few-Shot Learning**|Long Chen Team|[2510.14543](http://arxiv.org/abs/2510.14543)|null|
|**2025-10-17**|**PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model**|Yanjun Ma Team|[2510.14528](http://arxiv.org/abs/2510.14528)|**[link](https://github.com/PaddlePaddle/PaddleOCR)**|
|**2025-10-16**|**Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models**|Ziyu Zhao Team|[2510.14526](http://arxiv.org/abs/2510.14526)|null|
|**2025-10-16**|**Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control**|Yuanchun Shi Team|[2510.14388](http://arxiv.org/abs/2510.14388)|null|
|**2025-10-16**|**Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding**|Jinkyu Kim Team|[2510.14304](http://arxiv.org/abs/2510.14304)|**[link](https://github.com/KR-0822/TCD)**|
|**2025-10-15**|**Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models**|Miguel Arana-Catania Team|[2510.13993](http://arxiv.org/abs/2510.13993)|null|
|**2025-10-15**|**VisCoP: Visual Probing for Video Domain Adaptation of Vision Language Models**|Srijan Das Team|[2510.13808](http://arxiv.org/abs/2510.13808)|null|
|**2025-10-15**|**Generative Universal Verifier as Multimodal Meta-Reasoner**|Yujiu Yang Team|[2510.13804](http://arxiv.org/abs/2510.13804)|null|
|**2025-10-15**|**Spatial-DISE: A Unified Benchmark for Evaluating Spatial Reasoning in Vision-Language Models**|Xiaowei Huang Team|[2510.13394](http://arxiv.org/abs/2510.13394)|null|
|**2025-10-15**|**DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning**|Hang Zhao Team|[2510.13375](http://arxiv.org/abs/2510.13375)|null|
|**2025-10-15**|**Language as a Label: Zero-Shot Multimodal Classification of Everyday Postures under Data Scarcity**|Jubal Chandy Jacob Team|[2510.13364](http://arxiv.org/abs/2510.13364)|null|
|**2025-10-15**|**Improving Visual Recommendation on E-commerce Platforms Using Vision-Language Models**|Andre Rusli Team|[2510.13359](http://arxiv.org/abs/2510.13359)|null|
|**2025-10-15**|**Self-Augmented Visual Contrastive Decoding**|Vivek Gupta Team|[2510.13315](http://arxiv.org/abs/2510.13315)|null|
|**2025-10-15**|**MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models**|Min Zhang Team|[2510.13276](http://arxiv.org/abs/2510.13276)|null|
|**2025-10-15**|**Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs**|Bohyung Han Team|[2510.13251](http://arxiv.org/abs/2510.13251)|null|
|**2025-10-15**|**What "Not" to Detect: Negation-Aware VLMs via Structured Reasoning and Token Merging**|Hyunjung Shim Team|[2510.13232](http://arxiv.org/abs/2510.13232)|null|
|**2025-10-15**|**SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs**|Usman Naseem Team|[2510.13190](http://arxiv.org/abs/2510.13190)|null|
|**2025-10-15**|**DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models**|Jose M. Alvarez Team|[2510.13108](http://arxiv.org/abs/2510.13108)|null|
|**2025-10-15**|**VLA-0: Building State-of-the-Art VLAs with Zero Modification**|Fabio Ramos Team|[2510.13054](http://arxiv.org/abs/2510.13054)|null|
|**2025-10-14**|**SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding**|Thomas Seidl Team|[2510.13016](http://arxiv.org/abs/2510.13016)|null|
|**2025-10-14**|**UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles**|Ufuk Topcu Team|[2510.12992](http://arxiv.org/abs/2510.12992)|null|
|**2025-10-14**|**Scope: Selective Cross-modal Orchestration of Visual Perception Experts**|Perouz Taslakian Team|[2510.12974](http://arxiv.org/abs/2510.12974)|null|
|**2025-10-14**|**Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation**|Bo Du Team|[2510.12953](http://arxiv.org/abs/2510.12953)|null|
|**2025-10-14**|**Unifying Vision-Language Latents for Zero-label Image Caption Enhancement**|Woo Seong Chung Team|[2510.12931](http://arxiv.org/abs/2510.12931)|null|
|**2025-10-14**|**UniFusion: Vision-Language Model as Unified Encoder in Image Generation**|Ajinkya Kale Team|[2510.12789](http://arxiv.org/abs/2510.12789)|**[link](https://thekevinli.github.io/unifusion/)**|
|**2025-10-15**|**SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model**|Chao Feng Team|[2510.12709](http://arxiv.org/abs/2510.12709)|null|
|**2025-10-14**|**ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning**|Tong Zhang Team|[2510.12693](http://arxiv.org/abs/2510.12693)|null|
|**2025-10-14**|**VISaGE: Understanding Visual Generics and Exceptions**|Emily Allaway Team|[2510.12548](http://arxiv.org/abs/2510.12548)|null|
|**2025-10-14**|**A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation**|Luping Zhou Team|[2510.12444](http://arxiv.org/abs/2510.12444)|null|
|**2025-10-14**|**Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda**|Nuno F. Rodrigues Team|[2510.12400](http://arxiv.org/abs/2510.12400)|null|
|**2025-10-14**|**Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector**|Yiwei Wang Team|[2510.12287](http://arxiv.org/abs/2510.12287)|null|
|**2025-10-14**|**Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model**|Haoang Li Team|[2510.12276](http://arxiv.org/abs/2510.12276)|null|
|**2025-10-14**|**HoneyBee: Data Recipes for Vision-Language Reasoners**|Ramakanth Pasunuru Team|[2510.12225](http://arxiv.org/abs/2510.12225)|null|
|**2025-10-14**|**Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos**|Yu Yamaguchi Team|[2510.12190](http://arxiv.org/abs/2510.12190)|null|
|**2025-10-14**|**ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation**|Renjie Wan Team|[2510.12119](http://arxiv.org/abs/2510.12119)|null|
|**2025-10-13**|**Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval**|Vyas Raina Team|[2510.12014](http://arxiv.org/abs/2510.12014)|null|
|**2025-10-13**|**Learning Dynamics of VLM Finetuning**|Keze Wang Team|[2510.11978](http://arxiv.org/abs/2510.11978)|null|
|**2025-10-13**|**Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection**|Marcos Zampieri Team|[2510.11852](http://arxiv.org/abs/2510.11852)|**[link](https://icdmw25mmai.github.io/)**|
|**2025-10-13**|**Data or Language Supervision: What Makes CLIP Better than DINO?**|Serena Yeung-Levy Team|[2510.11835](http://arxiv.org/abs/2510.11835)|null|
|**2025-10-13**|**CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images**|Xihui Liu Team|[2510.11718](http://arxiv.org/abs/2510.11718)|null|
|**2025-10-13**|**Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation**|Mac Schwager Team|[2510.11689](http://arxiv.org/abs/2510.11689)|null|
|**2025-10-13**|**EvoCAD: Evolutionary CAD Code Generation with Vision Language Models**|Niki van Stein Team|[2510.11631](http://arxiv.org/abs/2510.11631)|null|
|**2025-10-13**|**mmWalk: Towards Multi-modal Multi-view Walking Assistance**|Rainer Stiefelhagen Team|[2510.11520](http://arxiv.org/abs/2510.11520)|**[link](https://github.com/KediYing/mmWalk)**|
|**2025-10-13**|**Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion**|Guangmang Cui Team|[2510.11456](http://arxiv.org/abs/2510.11456)|null|
|**2025-10-13**|**Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications**|Yingqiang Gao Team|[2510.11314](http://arxiv.org/abs/2510.11314)|null|
|**2025-10-13**|**When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models**|Samer Al-Hamadani Team|[2510.11302](http://arxiv.org/abs/2510.11302)|null|
|**2025-10-13**|**$Δ\mathrm{Energy}$ : Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization**|Nanyang Ye Team|[2510.11296](http://arxiv.org/abs/2510.11296)|null|
|**2025-10-13**|**Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering**|Thomas Seidl Team|[2510.11295](http://arxiv.org/abs/2510.11295)|null|
|**2025-10-13**|**Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations**|Keno K. Bressem Team|[2510.11196](http://arxiv.org/abs/2510.11196)|null|
|**2025-10-13**|**BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models**|Roy Ka-Wei Lee Team|[2510.11178](http://arxiv.org/abs/2510.11178)|null|
|**2025-10-13**|**Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning**|Zhi Hou Team|[2510.11027](http://arxiv.org/abs/2510.11027)|null|
|**2025-10-13**|**GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation**|Jing Zhang Team|[2510.11020](http://arxiv.org/abs/2510.11020)|null|
|**2025-10-13**|**COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models**|Aidong Zhang Team|[2510.11012](http://arxiv.org/abs/2510.11012)|null|
|**2025-10-13**|**Catch-Only-One: Non-Transferable Examples for Model-Specific Authorization**|Guangdong Bai Team|[2510.10982](http://arxiv.org/abs/2510.10982)|null|
|**2025-10-13**|**Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning**|Aidong Zhang Team|[2510.10973](http://arxiv.org/abs/2510.10973)|null|
|**2025-10-13**|**IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation**|Jing Tang Team|[2510.10969](http://arxiv.org/abs/2510.10969)|null|
|**2025-10-13**|**MC#: Mixture Compressor for Mixture-of-Experts Large Models**|Xiaojuan Qi Team|[2510.10962](http://arxiv.org/abs/2510.10962)|null|
|**2025-10-13**|**FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model**|Yuhui Yin Team|[2510.10921](http://arxiv.org/abs/2510.10921)|null|
|**2025-10-13**|**Topological Alignment of Shared Vision-Language Embedding Space**|Jae-Hun Jung Team|[2510.10889](http://arxiv.org/abs/2510.10889)|null|
|**2025-10-10**|**StreamingVLM: Real-Time Understanding for Infinite Video Streams**|Song Han Team|[2510.09608](http://arxiv.org/abs/2510.09608)|null|
|**2025-10-10**|**VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation**|Caifeng Shan Team|[2510.09607](http://arxiv.org/abs/2510.09607)|**[link](https://ltbai.github.io/VITA-VLA/)**|
|**2025-10-10**|**Vision Language Models: A Survey of 26K Papers**|Fengming Lin Team|[2510.09586](http://arxiv.org/abs/2510.09586)|null|
|**2025-10-10**|**D-TPT: Dimensional Entropy Maximization for Calibrating Test-Time Prompt Tuning in Vision-Language Models**|Wonjun Hwang Team|[2510.09473](http://arxiv.org/abs/2510.09473)|null|
|**2025-10-10**|**Boosting Multi-modal Keyphrase Prediction with Dynamic Chain-of-Thought in Vision-Language Models**|Jiao Ran Team|[2510.09358](http://arxiv.org/abs/2510.09358)|**[link](https://github.com/bytedance/DynamicCoT)**|
|**2025-10-10**|**Spotlight on Token Perception for Multimodal Reinforcement Learning**|Yu Cheng Team|[2510.09285](http://arxiv.org/abs/2510.09285)|**[link](https://github.com/huaixuheqing/VPPO-RL)**|
|**2025-10-10**|**Hallucination Filtering in Radiology Vision-Language Models Using Discrete Semantic Entropy**|Daniel Truhn Team|[2510.09256](http://arxiv.org/abs/2510.09256)|**[link](https://github.com/TruhnLab/VisionSemanticEntropy)**|
|**2025-10-10**|**Zero-shot image privacy classification with Vision-Language Models**|Andrea Cavallaro Team|[2510.09253](http://arxiv.org/abs/2510.09253)|null|
|**2025-10-10**|**Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation**|Subrahmanyam Murala Team|[2510.09228](http://arxiv.org/abs/2510.09228)|null|
|**2025-10-10**|**MCMC: Bridging Rendering, Optimization and Generative AI**|Wenzel Jakob Team|[2510.09078](http://arxiv.org/abs/2510.09078)|null|
|**2025-10-10**|**On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models**|Se Young Chun Team|[2510.09008](http://arxiv.org/abs/2510.09008)|null|
|**2025-10-10**|**Unleashing Perception-Time Scaling to Multimodal Reasoning Models**|Minghui Qiu Team|[2510.08964](http://arxiv.org/abs/2510.08964)|null|
|**2025-10-10**|**PHyCLIP: $\ell_1$ -Product of Hyperbolic Factors Unifies Hierarchy and Compositionality in Vision-Language Representation Learning**|Takashi Matsubara Team|[2510.08919](http://arxiv.org/abs/2510.08919)|null|
|**2025-10-09**|**CDE: Concept-Driven Exploration for Reinforcement Learning**|Joseph Campbell Team|[2510.08851](http://arxiv.org/abs/2510.08851)|null|
|**2025-10-09**|**FOLK: Fast Open-Vocabulary 3D Instance Segmentation via Label-guided Knowledge Distillation**|Zhihua Wei Team|[2510.08849](http://arxiv.org/abs/2510.08849)|null|
|**2025-10-09**|**D-CoDe: Scaling Image-Pretrained VLMs to Video via Dynamic Compression and Question Decomposition**|Yun Fu Team|[2510.08818](http://arxiv.org/abs/2510.08818)|null|
|**2025-10-09**|**Q-Router: Agentic Video Quality Assessment with Expert Model Routing and Artifact Localization**|Zhengzhong Tu Team|[2510.08789](http://arxiv.org/abs/2510.08789)|null|
|**2025-10-09**|**MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning**|Salman Khan Team|[2510.08567](http://arxiv.org/abs/2510.08567)|null|
|**2025-10-09**|**SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models**|Yueting Zhuang Team|[2510.08531](http://arxiv.org/abs/2510.08531)|**[link](https://zju-real.github.io/SpatialLadder/)**|
|**2025-10-09**|**To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models**|Leonid Sigal Team|[2510.08510](http://arxiv.org/abs/2510.08510)|**[link](https://davidhalladay.github.io/diysink_demo)**|
|**2025-10-09**|**MoA-VR: A Mixture-of-Agents System Towards All-in-One Video Restoration**|Guangtao Zhai Team|[2510.08508](http://arxiv.org/abs/2510.08508)|null|
|**2025-10-09**|**The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping**|Esam Ghaleb Team|[2510.08482](http://arxiv.org/abs/2510.08482)|null|
|**2025-10-09**|**Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling**|Paula Buttery Team|[2510.08470](http://arxiv.org/abs/2510.08470)|null|
|**2025-10-09**|**VideoVerse: How Far is Your T2V Generator from a World Model?**|Lei Zhang Team|[2510.08398](http://arxiv.org/abs/2510.08398)|null|
|**2025-10-09**|**Evaluating Small Vision-Language Models on Distance-Dependent Traffic Perception**|Ciaran Eising Team|[2510.08352](http://arxiv.org/abs/2510.08352)|null|
|**2025-10-09**|**Chain-of-Trigger: An Agentic Backdoor that Paradoxically Enhances Agentic Robustness**|Hai Zhao Team|[2510.08238](http://arxiv.org/abs/2510.08238)|null|
|**2025-10-09**|**Approximate Domain Unlearning for Vision-Language Models**|Go Irie Team|[2510.08132](http://arxiv.org/abs/2510.08132)|null|
|**2025-10-09**|**CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning**|Rongrong Ji Team|[2510.08003](http://arxiv.org/abs/2510.08003)|null|
|**2025-10-09**|**Executable Analytic Concepts as the Missing Link Between VLM Insight and Precise Manipulation**|Jianhua Sun Team|[2510.07975](http://arxiv.org/abs/2510.07975)|null|
|**2025-10-09**|**Effective and Stealthy One-Shot Jailbreaks on Deployed Mobile Vision-Language Agents**|Jun Zhu Team|[2510.07809](http://arxiv.org/abs/2510.07809)|null|
|**2025-10-09**|**GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models**|Long Zeng Team|[2510.07791](http://arxiv.org/abs/2510.07791)|null|
|**2025-10-09**|**IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction**|Liqiang Nie Team|[2510.07778](http://arxiv.org/abs/2510.07778)|null|
|**2025-10-09**|**Multimodal Safety Evaluation in Generative Agent Social Simulations**|Bernard Ghanem Team|[2510.07709](http://arxiv.org/abs/2510.07709)|null|
|**2025-10-09**|**Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models**|Fuzhi Tang Team|[2510.07632](http://arxiv.org/abs/2510.07632)|null|
|**2025-10-08**|**Cross-Modal Attention Guided Unlearning in Vision-Language Models**|Xintao Wu Team|[2510.07567](http://arxiv.org/abs/2510.07567)|null|
|**2025-10-08**|**Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices**|Jimmy Huang Team|[2510.07545](http://arxiv.org/abs/2510.07545)|null|
|**2025-10-09**|**TIGeR: Tool-Integrated Geometric Reasoning in Vision-Language Models for Robotics**|Shanghang Zhang Team|[2510.07181](http://arxiv.org/abs/2510.07181)|null|
|**2025-10-08**|**Few-Shot Adaptation Benchmark for Remote Sensing Vision-Language Models**|Benoit Macq Team|[2510.07135](http://arxiv.org/abs/2510.07135)|null|
|**2025-10-08**|**TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription**|Haoyu Wang Team|[2510.07098](http://arxiv.org/abs/2510.07098)|null|
|**2025-10-08**|**Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications**|Yuke Zhu Team|[2510.07077](http://arxiv.org/abs/2510.07077)|**[link](https://vla-survey.github.io)**|
|**2025-10-08**|**Unified Molecule Pre-training with Flexible 2D and 3D Modalities: Single and Paired Modality Integration**|Yuan Fang Team|[2510.07035](http://arxiv.org/abs/2510.07035)|null|
|**2025-10-08**|**Get RICH or Die Scaling: Profitably Trading Inference Compute for Robustness**|Brian Bartoldson Team|[2510.06790](http://arxiv.org/abs/2510.06790)|null|
|**2025-10-08**|**TTRV: Test-Time Reinforcement Learning for Vision Language Models**|M. Jehanzeb Mirza Team|[2510.06783](http://arxiv.org/abs/2510.06783)|null|
|**2025-10-08**|**ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory**|Zora Zhiruo Wang Team|[2510.06664](http://arxiv.org/abs/2510.06664)|null|
|**2025-10-08**|**VUGEN: Visual Understanding priors for GENeration**|Jakob Verbeek Team|[2510.06529](http://arxiv.org/abs/2510.06529)|null|
|**2025-10-07**|**ChainMPQ: Interleaved Text-Image Reasoning Chains for Mitigating Relation Hallucinations**|Yujun Cai Team|[2510.06292](http://arxiv.org/abs/2510.06292)|null|
|**2025-10-06**|**Surgeons Are Indian Males and Speech Therapists Are White Females: Auditing Biases in Vision-Language Models for Healthcare Professionals**|Beenish Moalla Chaudhry Team|[2510.06280](http://arxiv.org/abs/2510.06280)|null|
|**2025-10-07**|**Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA**|Junfeng Yang Team|[2510.06067](http://arxiv.org/abs/2510.06067)|null|
|**2025-10-07**|**Medical Vision Language Models as Policies for Robotic Surgery**|Martin Radfar Team|[2510.06064](http://arxiv.org/abs/2510.06064)|null|
|**2025-10-07**|**Data Factory with Minimal Human Effort Using VLMs**|Andrew Markham Team|[2510.05722](http://arxiv.org/abs/2510.05722)|null|
|**2025-10-07**|**Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM**|Zheng Zhang Team|[2510.05544](http://arxiv.org/abs/2510.05544)|null|
|**2025-10-06**|**Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization**|Ariel Gera Team|[2510.05038](http://arxiv.org/abs/2510.05038)|null|
|**2025-10-06**|**Efficient Navigation in Unknown Indoor Environments with Vision-Language Models**|J. P. How Team|[2510.04991](http://arxiv.org/abs/2510.04991)|null|
|**2025-10-06**|**ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts**|Dan Pei Team|[2510.04710](http://arxiv.org/abs/2510.04710)|null|
|**2025-10-06**|**Conditional Representation Learning for Customized Tasks**|Xi Peng Team|[2510.04564](http://arxiv.org/abs/2510.04564)|null|
|**2025-10-06**|**More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models**|Jun Luo Team|[2510.04532](http://arxiv.org/abs/2510.04532)|null|
|**2025-10-06**|**VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery**|Hao Tang Team|[2510.04479](http://arxiv.org/abs/2510.04479)|null|
|**2025-10-06**|**MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models**|Gyeongyeon Hwang Team|[2510.04477](http://arxiv.org/abs/2510.04477)|null|
|**2025-10-06**|**A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering**|Chen Chen Team|[2510.04428](http://arxiv.org/abs/2510.04428)|null|
|**2025-10-06**|**Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting**|Jiahao Zhang Team|[2510.04401](http://arxiv.org/abs/2510.04401)|null|
|**2025-10-05**|**AgentTypo: Adaptive Typographic Prompt Injection Attacks against Black-box Multimodal Agents**|Bin Xiao Team|[2510.04257](http://arxiv.org/abs/2510.04257)|null|
|**2025-10-05**|**ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context**|Jinwoo Shin Team|[2510.04246](http://arxiv.org/abs/2510.04246)|**[link](https://huiwon-jang.github.io/contextvla)**|
|**2025-10-05**|**Zoom-In to Sort AI-Generated Images Out**|Jianfu Zhang Team|[2510.04225](http://arxiv.org/abs/2510.04225)|null|
|**2025-10-05**|**Automating construction safety inspections using a multi-modal vision-language RAG framework**|Daniel Dias-da-Costa Team|[2510.04145](http://arxiv.org/abs/2510.04145)|null|
|**2025-10-07**|**AgriGPT-VL: Agricultural Vision-Language Understanding Suite**|Shijian Li Team|[2510.04002](http://arxiv.org/abs/2510.04002)|null|
|**2025-10-04**|**No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models**|Serena Yeung-Levy Team|[2510.03978](http://arxiv.org/abs/2510.03978)|null|
|**2025-10-04**|**Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models**|Chris Thomas Team|[2510.03903](http://arxiv.org/abs/2510.03903)|null|
|**2025-10-04**|**Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert**|Chunhua Shen Team|[2510.03896](http://arxiv.org/abs/2510.03896)|null|
|**2025-10-04**|**Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models**|Durga Toshniwal Team|[2510.03840](http://arxiv.org/abs/2510.03840)|null|
|**2025-10-04**|**Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models**|Zeynep Akata Team|[2510.03721](http://arxiv.org/abs/2510.03721)|null|
|**2025-10-04**|**MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations**|Jingliang Duan Team|[2510.03666](http://arxiv.org/abs/2510.03666)|null|
|**2025-10-03**|**Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning**|Yang Zhang Team|[2510.03182](http://arxiv.org/abs/2510.03182)|null|
|**2025-10-03**|**SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus**|Caifeng Shan Team|[2510.03160](http://arxiv.org/abs/2510.03160)|null|
|**2025-10-03**|**Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights**|Konstantina Nikita Team|[2510.02922](http://arxiv.org/abs/2510.02922)|null|
|**2025-10-03**|**Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting**|Mostafa Tavassolipour Team|[2510.02913](http://arxiv.org/abs/2510.02913)|null|
|**2025-10-03**|**Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis**|Xin Gao Team|[2510.02815](http://arxiv.org/abs/2510.02815)|null|
|**2025-10-03**|**MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding**|Yujiu Yang Team|[2510.02790](http://arxiv.org/abs/2510.02790)|null|
|**2025-10-03**|**OTR: Synthesizing Overlay Text Dataset for Text Removal**|Kota Yamaguchi Team|[2510.02787](http://arxiv.org/abs/2510.02787)|**[link](https://doi.org/10.1145/3746027.3758297)**|
|**2025-10-03**|**Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models**|Prahitha Movva Team|[2510.02780](http://arxiv.org/abs/2510.02780)|null|
|**2025-10-03**|**AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding**|Mohammed Bennamoun Team|[2510.02778](http://arxiv.org/abs/2510.02778)|null|
|**2025-10-03**|**Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models**|Zhen Lei Team|[2510.02750](http://arxiv.org/abs/2510.02750)|null|
|**2025-10-03**|**Team Xiaomi EV-AD VLA: Caption-Guided Retrieval System for Cross-Modal Drone Navigation -- Technical Report for IROS 2025 RoboSense Challenge Track 4**|Xiaoshuai Hao Team|[2510.02728](http://arxiv.org/abs/2510.02728)|null|
|**2025-10-03**|**ARMs: Adaptive Red-Teaming Agent against Multimodal Models with Plug-and-Play Attacks**|Bo Li Team|[2510.02677](http://arxiv.org/abs/2510.02677)|null|
|**2025-10-02**|**Exploring OCR-augmented Generation for Bilingual VQA**|Sunho Park Team|[2510.02543](http://arxiv.org/abs/2510.02543)|null|
|**2025-10-02**|**Multimodal Function Vectors for Spatial Relations**|Hongjing Lu Team|[2510.02528](http://arxiv.org/abs/2510.02528)|null|
|**2025-10-02**|**From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens**|Freda Shi Team|[2510.02292](http://arxiv.org/abs/2510.02292)|**[link](https://github.com/compling-wat/vlm-lens)**|
|**2025-10-02**|**microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification**|Muhammad Haris Khan Team|[2510.02270](http://arxiv.org/abs/2510.02270)|null|
|**2025-10-02**|**Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents**|Zhuosheng Zhang Team|[2510.02204](http://arxiv.org/abs/2510.02204)|null|
|**2025-10-02**|**GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation**|Heng Tao Shen Team|[2510.02186](http://arxiv.org/abs/2510.02186)|null|
|**2025-10-02**|**Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting**|Jing Zhang Team|[2510.02155](http://arxiv.org/abs/2510.02155)|null|
|**2025-10-02**|**Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving**|Chun Jason Xue Team|[2510.01795](http://arxiv.org/abs/2510.01795)|null|
|**2025-10-02**|**Accelerating Attention with Basis Decomposition**|Jialin Zhao Team|[2510.01718](http://arxiv.org/abs/2510.01718)|null|
|**2025-10-02**|**Contrastive Representation Regularization for Vision-Language-Action Models**|Jinwoo Shin Team|[2510.01711](http://arxiv.org/abs/2510.01711)|null|
|**2025-10-02**|**VaPR -- Vision-language Preference alignment for Reasoning**|Nanyun Peng Team|[2510.01700](http://arxiv.org/abs/2510.01700)|null|
|**2025-10-02**|**Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning**|Wentao Zhang Team|[2510.01681](http://arxiv.org/abs/2510.01681)|null|
|**2025-10-02**|**Source-Free Cross-Domain Continual Learning**|Kutluyil Dogancay Team|[2510.01649](http://arxiv.org/abs/2510.01649)|null|
|**2025-10-02**|**FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models**|Bihan Wen Team|[2510.01642](http://arxiv.org/abs/2510.01642)|**[link](https://jimntu.github.io/FailSafe/)**|
|**2025-10-02**|**ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models**|Murali Emani Team|[2510.01582](http://arxiv.org/abs/2510.01582)|null|
|**2025-10-03**|**Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed**|Sanmi Koyejo Team|[2510.01494](http://arxiv.org/abs/2510.01494)|null|
|**2025-10-01**|**VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs**|Gonzalo Ferrer Team|[2510.01483](http://arxiv.org/abs/2510.01483)|null|
|**2025-10-01**|**Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories**|Baharan Mirzasoleiman Team|[2510.01454](http://arxiv.org/abs/2510.01454)|**[link](https://bigml-cs-ucla.github.io/XMAS-project-page/)**|
|**2025-10-01**|**GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings**|Rakesh Kumar Team|[2510.01448](http://arxiv.org/abs/2510.01448)|null|
|**2025-10-01**|**VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation**|Amirreza Shaban Team|[2510.01388](http://arxiv.org/abs/2510.01388)|null|
|**2025-10-01**|**Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models**|Feng Zhao Team|[2510.01304](http://arxiv.org/abs/2510.01304)|null|
|**2025-10-01**|**Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs**|Paul Whatmough Team|[2510.01185](http://arxiv.org/abs/2510.01185)|null|
|**2025-09-30**|**MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation**|Shanghang Zhang Team|[2509.26642](http://arxiv.org/abs/2509.26642)|null|
|**2025-09-30**|**Query-Kontext: An Unified Multimodal Model for Image Generation and Editing**|Jingdong Wang Team|[2509.26641](http://arxiv.org/abs/2509.26641)|null|
|**2025-09-30**|**Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces**|Ivan Titov Team|[2509.26594](http://arxiv.org/abs/2509.26594)|null|
|**2025-09-30**|**The Invisible Mentor: Inferring User Actions from Screen Recordings to Recommend Better Workflows**|Emerson Murphy-Hill Team|[2509.26557](http://arxiv.org/abs/2509.26557)|null|
|**2025-09-30**|**Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation**|Varun Jampani Team|[2509.26555](http://arxiv.org/abs/2509.26555)|**[link](https://stable-cinemetrics.github.io/)**|
|**2025-09-30**|**Zero-Shot Decentralized Federated Learning**|Giovanni Bellitto Team|[2509.26462](http://arxiv.org/abs/2509.26462)|**[link](https://github.com/perceivelab/ZeroDFL)**|
|**2025-09-30**|**SQUARE: Semantic Query-Augmented Fusion and Efficient Batch Reranking for Training-free Zero-Shot Composed Image Retrieval**|Huei-Fang Yang Team|[2509.26330](http://arxiv.org/abs/2509.26330)|null|
|**2025-09-30**|**ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation**|Antonio Liotta Team|[2509.26278](http://arxiv.org/abs/2509.26278)|null|
|**2025-09-30**|**Interpret, prune and distill Donut : towards lightweight VLMs for VQA on document**|David Naccache Team|[2509.26235](http://arxiv.org/abs/2509.26235)|null|
|**2025-09-30**|**TSalV360: A Method and Dataset for Text-driven Saliency Detection in 360-Degrees Videos**|Vasileios Mezaris Team|[2509.26208](http://arxiv.org/abs/2509.26208)|**[link](https://ieeexplore.ieee.org/)**|
|**2025-09-30**|**SGS: Segmentation-Guided Scoring for Global Scene Inconsistencies**|Xue Li Team|[2509.26039](http://arxiv.org/abs/2509.26039)|null|
|**2025-10-01**|**AgenticIQA: An Agentic Framework for Adaptive and Interpretable Image Quality Assessment**|Weisi Lin Team|[2509.26006](http://arxiv.org/abs/2509.26006)|null|
|**2025-09-30**|**Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations**|Antonino Furnari Team|[2509.26004](http://arxiv.org/abs/2509.26004)|null|
|**2025-09-30**|**Towards Unified Multimodal Misinformation Detection in Social Media: A Benchmark Dataset and Baseline**|Zhun Zhong Team|[2509.25991](http://arxiv.org/abs/2509.25991)|null|
|**2025-09-30**|**NuRisk: A Visual Question Answering Dataset for Agent-Level Risk Assessment in Autonomous Driving**|Johannes Betz Team|[2509.25944](http://arxiv.org/abs/2509.25944)|null|
|**2025-09-30**|**VLM-FO1: Bridging the Gap Between High-Level Reasoning and Fine-Grained Perception in VLMs**|Tiancheng Zhao Team|[2509.25916](http://arxiv.org/abs/2509.25916)|null|
|**2025-10-01**|**LLaVAShield: Safeguarding Multimodal Multi-Turn Dialogues in Vision-Language Models**|Yongjun Shen Team|[2509.25896](http://arxiv.org/abs/2509.25896)|null|
|**2025-09-30**|**DeepSketcher: Internalizing Visual Manipulation for Multimodal Reasoning**|Jing Zhang Team|[2509.25866](http://arxiv.org/abs/2509.25866)|null|
|**2025-09-30**|**MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification**|Daoqiang Zhang Team|[2509.25863](http://arxiv.org/abs/2509.25863)|null|
|**2025-09-30**|**Reinforced Embodied Planning with Verifiable Reward for Real-World Robotic Manipulation**|Hao Chen Team|[2509.25852](http://arxiv.org/abs/2509.25852)|null|
|**2025-09-29**|**TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models**|Nanyun Peng Team|[2509.25143](http://arxiv.org/abs/2509.25143)|null|
|**2025-09-29**|**Visual serial processing deficits explain divergences in human and VLM reasoning**|Thomas L. Griffiths Team|[2509.25142](http://arxiv.org/abs/2509.25142)|null|
|**2025-09-29**|**GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning**|Salman Khan Team|[2509.25026](http://arxiv.org/abs/2509.25026)|**[link](https://mustansarfiaz.github.io/GeoVLM-R1/)**|
|**2025-09-29**|**World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training**|Qing Zhang Team|[2509.24948](http://arxiv.org/abs/2509.24948)|null|
|**2025-09-29**|**From Code to Action: Hierarchical Learning of Diffusion-VLM Policies**|Daniel Dijkman Team|[2509.24917](http://arxiv.org/abs/2509.24917)|null|
|**2025-09-29**|**Training-Free Token Pruning via Zeroth-Order Gradient Estimation in Vision-Language Models**|Sungeun Hong Team|[2509.24837](http://arxiv.org/abs/2509.24837)|null|
|**2025-09-29**|**IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks**|Ville Kyrki Team|[2509.24768](http://arxiv.org/abs/2509.24768)|null|
|**2025-09-29**|**IWR-Bench: Can LVLMs reconstruct interactive webpage from a user interaction video?**|Botian Shi Team|[2509.24709](http://arxiv.org/abs/2509.24709)|null|
|**2025-09-29**|**Can you SPLICE it together? A Human Curated Benchmark for Probing Visual Reasoning in VLMs**|Elia Bruni Team|[2509.24640](http://arxiv.org/abs/2509.24640)|null|
|**2025-09-30**|**Inducing Dyslexia in Vision Language Models**|Martin Schrimpf Team|[2509.24597](http://arxiv.org/abs/2509.24597)|null|
|**2025-09-29**|**TokenSwap: Backdoor Attack on the Compositional Understanding of Large Vision-Language Models**|Joey Tianyi Zhou Team|[2509.24566](http://arxiv.org/abs/2509.24566)|null|
|**2025-09-29**|**CORE-3D: Context-aware Open-vocabulary Retrieval by Embeddings in 3D**|Matin Mirzababaei Team|[2509.24528](http://arxiv.org/abs/2509.24528)|null|
|**2025-09-29**|**PhysiAgent: An Embodied Agent Framework in Physical World**|Xianyuan Zhan Team|[2509.24524](http://arxiv.org/abs/2509.24524)|null|
|**2025-09-29**|**GRPO-MA: Multi-Answer Generation in GRPO for Stable and Efficient Chain-of-Thought Training**|Hao Dong Team|[2509.24494](http://arxiv.org/abs/2509.24494)|null|
|**2025-09-29**|**Euclid's Gift: Enhancing Spatial Perception and Reasoning in Vision-Language Models via Geometric Surrogate Tasks**|Kai Chen Team|[2509.24473](http://arxiv.org/abs/2509.24473)|null|
|**2025-09-29**|**AXIS: Explainable Time Series Anomaly Detection with Large Language Models**|Chen Zhang Team|[2509.24378](http://arxiv.org/abs/2509.24378)|null|
|**2025-09-29**|**SONAR: Semantic-Object Navigation with Aggregated Reasoning through a Cross-Modal Inference Paradigm**|Jiankun Wang Team|[2509.24321](http://arxiv.org/abs/2509.24321)|null|
|**2025-09-30**|**FrameThinker: Learning to Think with Long Videos via Multi-Turn Frame Spotlighting**|Yu Cheng Team|[2509.24304](http://arxiv.org/abs/2509.24304)|null|
|**2025-09-29**|**ViReSkill: Vision-Grounded Replanning with Skill Memory for LLM-Based Planning in Lifelong Robot Learning**|Yang You Team|[2509.24219](http://arxiv.org/abs/2509.24219)|null|
|**2025-09-29**|**Talk in Pieces, See in Whole: Disentangling and Hierarchical Aggregating Representations for Language-based Object Detection**|Donghyun Kim Team|[2509.24192](http://arxiv.org/abs/2509.24192)|null|
|**2025-09-26**|**See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation**|Yu-Lun Liu Team|[2509.22653](http://arxiv.org/abs/2509.22653)|**[link](https://spf-web.pages.dev)**|
|**2025-09-26**|**CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning**|Dahua Lin Team|[2509.22647](http://arxiv.org/abs/2509.22647)|**[link](https://github.com/InternLM/CapRL)**|
|**2025-09-26**|**Hierarchical Representation Matching for CLIP-based Class-Incremental Learning**|Da-Wei Zhou Team|[2509.22645](http://arxiv.org/abs/2509.22645)|null|
|**2025-09-26**|**WoW: Towards a World omniscient World model Through Embodied Interaction**|Jian Tang Team|[2509.22642](http://arxiv.org/abs/2509.22642)|null|
|**2025-09-26**|**SPARK: Synergistic Policy And Reward Co-Evolving Framework**|Jiaqi Wang Team|[2509.22624](http://arxiv.org/abs/2509.22624)|**[link](https://github.com/InternLM/Spark)**|
|**2025-09-26**|**Color Names in Vision-Language Models**|Javier Vazquez-Corral Team|[2509.22524](http://arxiv.org/abs/2509.22524)|null|
|**2025-09-26**|**Guiding Evolution of Artificial Life Using Vision-Language Models**|Frederico Wieser Team|[2509.22447](http://arxiv.org/abs/2509.22447)|null|
|**2025-09-26**|**Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding**|Mrinmaya Sachan Team|[2509.22437](http://arxiv.org/abs/2509.22437)|**[link](https://github.com/CHIzhP/Chimera))**|
|**2025-09-26**|**RAU: Reference-based Anatomical Understanding with Vision Language Models**|Shanhui Sun Team|[2509.22404](http://arxiv.org/abs/2509.22404)|null|
|**2025-09-26**|**Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach**|Zijing Zhou Team|[2509.22378](http://arxiv.org/abs/2509.22378)|null|
|**2025-09-26**|**Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models**|Andreas Fischer Team|[2509.22283](http://arxiv.org/abs/2509.22283)|**[link](https://github.com/jungomi/vision-finetune)**|
|**2025-09-26**|**Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks**|Shangyang Li Team|[2509.22258](http://arxiv.org/abs/2509.22258)|null|
|**2025-09-26**|**A Tale of Two Experts: Cooperative Learning for Source-Free Unsupervised Domain Adaptation**|Cheng Deng Team|[2509.22229](http://arxiv.org/abs/2509.22229)|null|
|**2025-09-26**|**Polysemous Language Gaussian Splatting via Matching-based Mask Lifting**|Ge Li Team|[2509.22225](http://arxiv.org/abs/2509.22225)|null|
|**2025-09-26**|**Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models**|Bo Yang Team|[2509.22221](http://arxiv.org/abs/2509.22221)|null|
|**2025-09-26**|**Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting**|Anirudha Majumdar Team|[2509.22195](http://arxiv.org/abs/2509.22195)|null|
|**2025-09-26**|**MinerU2.5: A Decoupled Vision-Language Model for Efficient High-Resolution Document Parsing**|Conghui He Team|[2509.22186](http://arxiv.org/abs/2509.22186)|**[link](https://github.com/opendatalab/MinerU)**|
|**2025-09-26**|**Multilingual Vision-Language Models, A Survey**|Jindřich Libovický Team|[2509.22123](http://arxiv.org/abs/2509.22123)|null|
|**2025-09-26**|**Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics**|Stefan K. Ehrlich Team|[2509.22014](http://arxiv.org/abs/2509.22014)|null|
|**2025-09-26**|**CoFFT: Chain of Foresight-Focus Thought for Visual Language Models**|Mike Zheng Shou Team|[2509.22010](http://arxiv.org/abs/2509.22010)|null|
|**2025-09-25**|**Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive Cross-Stage Parallelization**|Guihai Chen Team|[2509.21301](http://arxiv.org/abs/2509.21301)|null|
|**2025-09-25**|**DisCoCLIP: A Distributional Compositional Tensor Network Encoder for Vision-Language Understanding**|Mehrnoosh Sadrzadeh Team|[2509.21287](http://arxiv.org/abs/2509.21287)|null|
|**2025-09-25**|**Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication**|Alexander Nagaev Team|[2509.21262](http://arxiv.org/abs/2509.21262)|null|
|**2025-09-25**|**Hallucination as an Upper Bound: A New Perspective on Text-to-Image Evaluation**|Mohammad Hossein Rohban Team|[2509.21257](http://arxiv.org/abs/2509.21257)|null|
|**2025-09-25**|**Learning to Look: Cognitive Attention Alignment with Vision-Language Models**|Nidhi Rastogi Team|[2509.21247](http://arxiv.org/abs/2509.21247)|null|
|**2025-09-25**|**TABLET: A Large-Scale Dataset for Robust Visual Table Understanding**|Mirella Lapata Team|[2509.21205](http://arxiv.org/abs/2509.21205)|null|
|**2025-09-25**|**Human-like Navigation in a World Built for Humans**|Shenlong Wang Team|[2509.21189](http://arxiv.org/abs/2509.21189)|**[link](https://reasonnav.github.io/)**|
|**2025-09-25**|**Can Less Precise Be More Reliable? A Systematic Evaluation of Quantization's Impact on CLIP Beyond Accuracy**|Chokri Mraidha Team|[2509.21173](http://arxiv.org/abs/2509.21173)|null|
|**2025-09-25**|**Teaching RL Agents to Act Better: VLM as Action Advisor for Online Reinforcement Learning**|Mingyu Hu Team|[2509.21126](http://arxiv.org/abs/2509.21126)|null|
|**2025-09-25**|**Cross-Modal Instructions for Robot Motion Generation**|Weiming Zhi Team|[2509.21107](http://arxiv.org/abs/2509.21107)|null|
|**2025-09-25**|**Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in Vision-Language Models**|Robert Jenssen Team|[2509.21102](http://arxiv.org/abs/2509.21102)|null|
|**2025-09-25**|**SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials**|Lu Cheng Team|[2509.21079](http://arxiv.org/abs/2509.21079)|null|
|**2025-09-25**|**Unlocking Financial Insights: An advanced Multimodal Summarization with Multimodal Output Framework for Financial Advisory Videos**|Alka Maurya Team|[2509.20961](http://arxiv.org/abs/2509.20961)|null|
|**2025-09-25**|**Decoding the Surgical Scene: A Scoping Review of Scene Graphs in Surgery**|M. Ali Nasseri Team|[2509.20941](http://arxiv.org/abs/2509.20941)|null|
|**2025-09-25**|**MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases**|Diange Yang Team|[2509.20843](http://arxiv.org/abs/2509.20843)|null|
|**2025-09-25**|**DAC-LoRA: Dynamic Adversarial Curriculum for Efficient and Robust Few-Shot Adaptation**|Ved Umrajkar Team|[2509.20792](http://arxiv.org/abs/2509.20792)|null|
|**2025-09-25**|**Provenance Analysis of Archaeological Artifacts via Multimodal RAG Systems**|Ruiliang Liu Team|[2509.20769](http://arxiv.org/abs/2509.20769)|null|
|**2025-09-25**|**Seeing Through Words, Speaking Through Pixels: Deep Representational Alignment Between Vision and Language Models**|Meenakshi Khosla Team|[2509.20751](http://arxiv.org/abs/2509.20751)|null|
|**2025-09-25**|**Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery**|Ali Mostafavi Team|[2509.20628](http://arxiv.org/abs/2509.20628)|null|
|**2025-09-24**|**InstructVTON: Optimal Auto-Masking and Natural-Language-Guided Interactive Style Control for Inpainting-Based Virtual Try-On**|Karim Bouyarmane Team|[2509.20524](http://arxiv.org/abs/2509.20524)|null|
|**2025-09-24**|**A co-evolving agentic AI system for medical imaging analysis**|Zhi Huang Team|[2509.20279](http://arxiv.org/abs/2509.20279)|null|
|**2025-09-24**|**Universal Camouflage Attack on Vision-Language Models for Autonomous Driving**|Wenqi Ren Team|[2509.20196](http://arxiv.org/abs/2509.20196)|null|
|**2025-09-24**|**EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models**|Dacheng Tao Team|[2509.20146](http://arxiv.org/abs/2509.20146)|null|
|**2025-09-24**|**A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA**|Yova Kementchedjhieva Team|[2509.20119](http://arxiv.org/abs/2509.20119)|null|
|**2025-09-24**|**Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving**|Xianpeng Lang Team|[2509.20109](http://arxiv.org/abs/2509.20109)|null|
|**2025-09-24**|**Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning**|Jiajun Liu Team|[2509.20077](http://arxiv.org/abs/2509.20077)|null|
|**2025-09-25**|**OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving**|Jun Ma Team|[2509.19973](http://arxiv.org/abs/2509.19973)|null|
|**2025-09-24**|**Generalist Robot Manipulation beyond Action Labeled Data**|Danda Pani Paudel Team|[2509.19958](http://arxiv.org/abs/2509.19958)|null|
|**2025-09-24**|**Benchmarking Gaslighting Attacks Against Speech Large Language Models**|Pan Zhou Team|[2509.19858](http://arxiv.org/abs/2509.19858)|null|
|**2025-09-24**|**CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition**|Monica S. Lam Team|[2509.19768](http://arxiv.org/abs/2509.19768)|null|
|**2025-09-24**|**Logics-Parsing Technical Report**|Minggang Wu Team|[2509.19760](http://arxiv.org/abs/2509.19760)|null|
|**2025-09-24**|**Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization**|Glen Chou Team|[2509.19688](http://arxiv.org/abs/2509.19688)|null|
|**2025-09-24**|**Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment**|Shaina Raza Team|[2509.19659](http://arxiv.org/abs/2509.19659)|null|
|**2025-09-23**|**Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models**|Tianyu Jiang Team|[2509.19595](http://arxiv.org/abs/2509.19595)|null|
|**2025-09-23**|**iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning**|Abhishek Aich Team|[2509.19552](http://arxiv.org/abs/2509.19552)|null|
|**2025-09-23**|**Score the Steps, Not Just the Goal: VLM-Based Subgoal Evaluation for Robotic Manipulation**|Chi-Guhn Lee Team|[2509.19524](http://arxiv.org/abs/2509.19524)|null|
|**2025-09-23**|**DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture**|Sriparna Saha Team|[2509.19274](http://arxiv.org/abs/2509.19274)|null|
|**2025-09-23**|**Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs**|Yova Kementchedjhieva Team|[2509.19207](http://arxiv.org/abs/2509.19207)|null|
|**2025-09-23**|**Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions**|Georgios Tzimiropoulos Team|[2509.19203](http://arxiv.org/abs/2509.19203)|null|
|**2025-09-23**|**Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models**|Xiaojie Wang Team|[2509.19191](http://arxiv.org/abs/2509.19191)|null|
|**2025-09-23**|**FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation**|Jianwei Zhang Team|[2509.19102](http://arxiv.org/abs/2509.19102)|**[link](https://sites.google.com/view/funcanon)**|
|**2025-09-23**|**ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?**|Jiahao Cui Team|[2509.19070](http://arxiv.org/abs/2509.19070)|null|
|**2025-09-23**|**Pure Vision Language Action (VLA) Models: A Comprehensive Survey**|Qingguo Zhou Team|[2509.19012](http://arxiv.org/abs/2509.19012)|null|
|**2025-09-23**|**Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards**|Xinlong Wang Team|[2509.19003](http://arxiv.org/abs/2509.19003)|null|
|**2025-09-23**|**No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning**|Joel Luís Carbonera Team|[2509.18938](http://arxiv.org/abs/2509.18938)|null|
|**2025-09-23**|**How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective**|Huchuan Lu Team|[2509.18905](http://arxiv.org/abs/2509.18905)|null|
|**2025-09-23**|**Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography**|Giovanni Colavizza Team|[2509.18839](http://arxiv.org/abs/2509.18839)|null|
|**2025-09-23**|**Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models**|Dinesh Manocha Team|[2509.18763](http://arxiv.org/abs/2509.18763)|null|
|**2025-09-23**|**Knowledge Transfer from Interaction Learning**|Shugong Xu Team|[2509.18733](http://arxiv.org/abs/2509.18733)|null|
|**2025-09-23**|**What Makes You Unique? Attribute Prompt Composition for Object Re-Identification**|Huchuan Lu Team|[2509.18715](http://arxiv.org/abs/2509.18715)|null|
|**2025-09-23**|**RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images**|Quan Wang Team|[2509.18711](http://arxiv.org/abs/2509.18711)|null|
|**2025-09-23**|**NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment**|Vijaykrishnan Narayanan Team|[2509.18672](http://arxiv.org/abs/2509.18672)|null|
|**2025-09-23**|**Learning neuroimaging models from health system-scale data**|Todd Hollon Team|[2509.18638](http://arxiv.org/abs/2509.18638)|null|
|**2025-09-23**|**SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones**|Mac Schwager Team|[2509.18610](http://arxiv.org/abs/2509.18610)|null|
|**2025-09-23**|**VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation**|Ufuk Topcu Team|[2509.18592](http://arxiv.org/abs/2509.18592)|**[link](https://vln-zero.github.io/)**|
|**2025-09-22**|**Losing the Plot: How VLM responses degrade on imperfect charts**|Mahantesh Halappanavar Team|[2509.18425](http://arxiv.org/abs/2509.18425)|null|
|**2025-09-22**|**NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning**|Sandeep Chinchali Team|[2509.18041](http://arxiv.org/abs/2509.18041)|null|
|**2025-09-22**|**Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery**|Yoshitaka Ushiku Team|[2509.17666](http://arxiv.org/abs/2509.17666)|null|
|**2025-09-22**|**SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models**|Jieping Ye Team|[2509.17664](http://arxiv.org/abs/2509.17664)|null|
|**2025-09-22**|**From Benchmarks to Reality: Advancing Visual Anomaly Detection by the VAND 3.0 Challenge**|Paula Ramos Team|[2509.17615](http://arxiv.org/abs/2509.17615)|null|
|**2025-09-22**|**COLA: Context-aware Language-driven Test-time Adaptation**|Zhihe Lu Team|[2509.17598](http://arxiv.org/abs/2509.17598)|null|
|**2025-09-22**|**Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models**|Seong Jae Hwang Team|[2509.17588](http://arxiv.org/abs/2509.17588)|null|
|**2025-09-23**|**Visual Instruction Pretraining for Domain-Specific Foundation Models**|Jian Yang Team|[2509.17562](http://arxiv.org/abs/2509.17562)|null|
|**2025-09-22**|**ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding**|Xiaoyu Qin Team|[2509.17481](http://arxiv.org/abs/2509.17481)|null|
|**2025-09-22**|**Training-Free Label Space Alignment for Universal Domain Adaptation**|Donghyun Kim Team|[2509.17452](http://arxiv.org/abs/2509.17452)|null|
|**2025-09-23**|**Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration**|Yueming Jin Team|[2509.17429](http://arxiv.org/abs/2509.17429)|null|
|**2025-09-22**|**Vision Language Models Are Not (Yet) Spelling Correctors**|Bojun Zhang Team|[2509.17418](http://arxiv.org/abs/2509.17418)|null|
|**2025-09-22**|**Mano Report**|Shuo Wang Team|[2509.17336](http://arxiv.org/abs/2509.17336)|null|
|**2025-09-22**|**UIPro: Unleashing Superior Interaction Capability For GUI Agents**|Zhaoxiang Zhang Team|[2509.17328](http://arxiv.org/abs/2509.17328)|null|
|**2025-09-22**|**OpenGVL - Benchmarking Visual Temporal Progress for Data Curation**|Krzysztof Walas Team|[2509.17321](http://arxiv.org/abs/2509.17321)|null|
|**2025-09-21**|**FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions**|Zhongyuan Wang Team|[2509.17177](http://arxiv.org/abs/2509.17177)|null|
|**2025-09-21**|**MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors**|Soumyabrata Dev Team|[2509.17084](http://arxiv.org/abs/2509.17084)|null|
|**2025-09-21**|**CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner**|Xiaomeng Li Team|[2509.17065](http://arxiv.org/abs/2509.17065)|null|
|**2025-09-21**|**AgriDoctor: A Multimodal Intelligent Assistant for Agriculture**|Liang Wang Team|[2509.17044](http://arxiv.org/abs/2509.17044)|null|
|**2025-09-21**|**Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning**|Jun Ma Team|[2509.17042](http://arxiv.org/abs/2509.17042)|null|
|**2025-09-21**|**When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration**|Jun Li Team|[2509.17024](http://arxiv.org/abs/2509.17024)|null|
|**2025-09-19**|**Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks**|Evangelos E. Papalexakis Team|[2509.16163](http://arxiv.org/abs/2509.16163)|null|
|**2025-09-19**|**Randomized Smoothing Meets Vision-Language Models**|Chih-Hong Cheng Team|[2509.16088](http://arxiv.org/abs/2509.16088)|null|
|**2025-09-19**|**I-FailSense: Towards General Robotic Failure Detection with Vision-Language Models**|Mohamed Chetouani Team|[2509.16072](http://arxiv.org/abs/2509.16072)|null|
|**2025-09-19**|**Compose by Focus: Scene Graph-based Atomic Skills**|Heng Yang Team|[2509.16053](http://arxiv.org/abs/2509.16053)|null|
|**2025-09-19**|**CIDER: A Causal Cure for Brand-Obsessed Text-to-Image Models**|Wushao Wen Team|[2509.15803](http://arxiv.org/abs/2509.15803)|null|
|**2025-09-19**|**Vision-Language Models as Differentiable Semantic and Spatial Rewards for Text-to-3D Generation**|He Sun Team|[2509.15772](http://arxiv.org/abs/2509.15772)|null|
|**2025-09-19**|**GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning**|Zhaojian Li Team|[2509.15738](http://arxiv.org/abs/2509.15738)|null|
|**2025-09-19**|**Training-Free Pyramid Token Pruning for Efficient Large Vision-Language Models via Region, Token, and Instruction-Guided Importance**|Xiangyang Xue Team|[2509.15704](http://arxiv.org/abs/2509.15704)|null|
|**2025-09-19**|**ORIC: Benchmarking Object Recognition in Incongruous Context for Large Vision-Language Models**|Hao Su Team|[2509.15695](http://arxiv.org/abs/2509.15695)|null|
|**2025-09-19**|**SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models**|Nima Mesgarani Team|[2509.15661](http://arxiv.org/abs/2509.15661)|null|
|**2025-09-19**|**PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models**|Byung-Cheol Min Team|[2509.15607](http://arxiv.org/abs/2509.15607)|null|
|**2025-09-18**|**SmolRGPT: Efficient Spatial Reasoning for Warehouse Environments with 600M Parameters**|Andy Couturier Team|[2509.15490](http://arxiv.org/abs/2509.15490)|null|
|**2025-09-18**|**Comparing Computational Pathology Foundation Models using Representational Similarity Analysis**|William Lotter Team|[2509.15482](http://arxiv.org/abs/2509.15482)|null|
|**2025-09-18**|**ORCA: Agentic Reasoning For Hallucination and Adversarial Robustness in Vision-Language Models**|Nathaniel D. Bastian Team|[2509.15435](http://arxiv.org/abs/2509.15435)|null|
|**2025-09-18**|**SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models**|Andrew Yates Team|[2509.15432](http://arxiv.org/abs/2509.15432)|null|
|**2025-09-18**|**CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization**|Xin Lin Team|[2509.15330](http://arxiv.org/abs/2509.15330)|null|
|**2025-09-18**|**Calibration-Aware Prompt Learning for Medical Vision-Language Models**|Muhammad Haris Khan Team|[2509.15226](http://arxiv.org/abs/2509.15226)|null|
|**2025-09-19**|**ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data**|Wenhai Wang Team|[2509.15221](http://arxiv.org/abs/2509.15221)|null|
|**2025-09-18**|**What's the Best Way to Retrieve Slides? A Comparative Study of Multimodal, Caption-Based, and Hybrid Retrieval Techniques**|Grigorios Tsoumakas Team|[2509.15211](http://arxiv.org/abs/2509.15211)|null|
|**2025-09-18**|**MedFact-R1: Towards Factual Medical Reasoning via Pseudo-Label Augmentation**|Guodong Ding Team|[2509.15154](http://arxiv.org/abs/2509.15154)|null|
|**2025-09-18**|**Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models**|Yanqing Zhang Team|[2509.15076](http://arxiv.org/abs/2509.15076)|null|
|**2025-09-18**|**QuizRank: Picking Images by Quizzing VLMs**|Eytan Adar Team|[2509.15059](http://arxiv.org/abs/2509.15059)|null|
|**2025-09-18**|**PRISM: Product Retrieval In Shopping Carts using Hybrid Matching**|Jiajing Chen Team|[2509.14985](http://arxiv.org/abs/2509.14985)|null|
|**2025-09-18**|**EchoVLM: Dynamic Mixture-of-Experts Vision-Language Model for Universal Ultrasound Intelligence**|Qinghua Huang Team|[2509.14977](http://arxiv.org/abs/2509.14977)|null|
|**2025-09-19**|**Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery**|Yasuhisa Hasegawa Team|[2509.14967](http://arxiv.org/abs/2509.14967)|null|
|**2025-09-18**|**MARIC: Multi-Agent Reasoning for Image Classification**|Seunghyun Lee Team|[2509.14860](http://arxiv.org/abs/2509.14860)|null|
|**2025-09-18**|**V-SEAM: Visual Semantic Editing and Attention Modulating for Causal Interpretability of Vision-Language Models**|Ming Jiang Team|[2509.14837](http://arxiv.org/abs/2509.14837)|null|
|**2025-09-18**|**Frame Sampling Strategies Matter: A Benchmark for small vision language models**|Mounim A. El Yacoubi Team|[2509.14769](http://arxiv.org/abs/2509.14769)|null|
|**2025-09-18**|**Do Vision-Language Models See Urban Scenes as People Do? An Urban Perception Benchmark**|Rashid Mushkani Team|[2509.14574](http://arxiv.org/abs/2509.14574)|null|
|**2025-09-18**|**VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models**|Yuxin Ma Team|[2509.14571](http://arxiv.org/abs/2509.14571)|null|
|**2025-09-17**|**CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks**|Negar Mehr Team|[2509.14380](http://arxiv.org/abs/2509.14380)|null|
|**2025-09-17**|**Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark**|Vishal M. Patel Team|[2509.14227](http://arxiv.org/abs/2509.14227)|null|
|**2025-09-19**|**TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning**|Guitao Cao Team|[2509.14172](http://arxiv.org/abs/2509.14172)|null|
|**2025-09-17**|**VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement**|Fei Richard Yu Team|[2509.14060](http://arxiv.org/abs/2509.14060)|null|
|**2025-09-17**|**Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation**|Minh Hoai Team|[2509.13939](http://arxiv.org/abs/2509.13939)|null|
|**2025-09-17**|**Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration**|Xiaoqiang Li Team|[2509.13919](http://arxiv.org/abs/2509.13919)|null|
|**2025-09-17**|**EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics**|Jielei Wang Team|[2509.13858](http://arxiv.org/abs/2509.13858)|null|
|**2025-09-17**|**Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models**|Longwen Gao Team|[2509.13836](http://arxiv.org/abs/2509.13836)|null|
|**2025-09-17**|**Iterative Prompt Refinement for Safer Text-to-Image Generation**|Byung-Jun Lee Team|[2509.13760](http://arxiv.org/abs/2509.13760)|null|
|**2025-09-17**|**Reinforcement Learning for Robotic Insertion of Flexible Cables in Industrial Settings**|Changjoo Nam Team|[2509.13731](http://arxiv.org/abs/2509.13731)|null|
|**2025-09-17**|**DREAM: Domain-aware Reasoning for Efficient Autonomous Underwater Monitoring**|Xiaomin Lin Team|[2509.13666](http://arxiv.org/abs/2509.13666)|null|
|**2025-09-16**|**Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation**|Samer Al-Hamadani Team|[2509.13590](http://arxiv.org/abs/2509.13590)|null|
|**2025-09-16**|**Using Visual Language Models to Control Bionic Hands: Assessment of Object Perception and Grasp Inference**|Cedomir Stefanovic Team|[2509.13572](http://arxiv.org/abs/2509.13572)|null|
|**2025-09-16**|**EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing**|Mingyuan Zhou Team|[2509.13399](http://arxiv.org/abs/2509.13399)|null|
|**2025-09-16**|**3D Aware Region Prompted Vision Language Model**|Sifei Liu Team|[2509.13317](http://arxiv.org/abs/2509.13317)|**[link](https://www.anjiecheng.me/sr3d)**|
|**2025-09-16**|**Image Realness Assessment and Localization with Multimodal Features**|Somdyuti Paul Team|[2509.13289](http://arxiv.org/abs/2509.13289)|null|
|**2025-09-16**|**ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement**|Giuseppe Carenini Team|[2509.13282](http://arxiv.org/abs/2509.13282)|null|
|**2025-09-16**|**RadGame: An AI-Powered Platform for Radiology Education**|Pranav Rajpurkar Team|[2509.13270](http://arxiv.org/abs/2509.13270)|null|
|**2025-09-16**|**HERO: Rethinking Visual Token Early Dropping in High-Resolution Large Vision-Language Models**|Xiangyang Xue Team|[2509.13067](http://arxiv.org/abs/2509.13067)|null|
|**2025-09-16**|**Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual Reasoning in Vision-Language Models**|Jingdong Wang Team|[2509.13031](http://arxiv.org/abs/2509.13031)|null|
|**2025-09-16**|**Cross-Layer Vision Smoothing: Enhancing Visual Understanding via Sustained Focus on Key Objects in Large Vision-Language Models**|Chong Feng Team|[2509.12897](http://arxiv.org/abs/2509.12897)|null|
|**2025-09-16**|**Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents**|Haiyang Zhang Team|[2509.12876](http://arxiv.org/abs/2509.12876)|null|
|**2025-09-16**|**Defense-to-Attack: Bypassing Weak Defenses Enables Stronger Jailbreaks in Vision-Language Models**|Xingjun Ma Team|[2509.12724](http://arxiv.org/abs/2509.12724)|null|
|**2025-09-16**|**AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models**|Jin Huang Team|[2509.12715](http://arxiv.org/abs/2509.12715)|null|
|**2025-09-15**|**Evaluating Robustness of Vision-Language Models Under Noisy Conditions**|Alireza Team|[2509.12492](http://arxiv.org/abs/2509.12492)|null|
|**2025-09-15**|**An integrated process for design and control of lunar robotics using AI and simulation**|Martin Servin Team|[2509.12367](http://arxiv.org/abs/2509.12367)|null|
|**2025-09-15**|**Open-ended Hierarchical Streaming Video Understanding with Vision Language Models**|Seon Joo Kim Team|[2509.12145](http://arxiv.org/abs/2509.12145)|null|
|**2025-09-15**|**Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models**|Jiajun Zhang Team|[2509.12132](http://arxiv.org/abs/2509.12132)|null|
|**2025-09-16**|**Embodied Navigation Foundation Model**|He Wang Team|[2509.12129](http://arxiv.org/abs/2509.12129)|**[link](https://pku-epic.github.io/NavFoM-Web/)**|
|**2025-09-15**|**Lost in Embeddings: Information Loss in Vision-Language Models**|Anders Søgaard Team|[2509.11986](http://arxiv.org/abs/2509.11986)|null|
|**2025-09-15**|**Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding**|Yijun Chen Team|[2509.11961](http://arxiv.org/abs/2509.11961)|null|
|**2025-09-15**|**Bridging Vision Language Models and Symbolic Grounding for Video Question Answering**|Daisy Zhe Wang Team|[2509.11862](http://arxiv.org/abs/2509.11862)|null|
|**2025-09-15**|**Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation**|Michael Louis Iuzzolino Team|[2509.11840](http://arxiv.org/abs/2509.11840)|null|
|**2025-09-15**|**SpecVLM: Fast Speculative Decoding in Vision-Language Models**|Emad Barsoum Team|[2509.11815](http://arxiv.org/abs/2509.11815)|null|
|**2025-09-15**|**Igniting VLMs toward the Embodied Space**|Zach Xu Team|[2509.11766](http://arxiv.org/abs/2509.11766)|null|
|**2025-09-15**|**EMeRALDS: Electronic Medical Record Driven Automated Lung Nodule Detection and Classification in Thoracic CT Images**|Syed Muhammad Anwar Team|[2509.11714](http://arxiv.org/abs/2509.11714)|null|
|**2025-09-15**|**How Auxiliary Reasoning Unleashes GUI Grounding in VLMs**|Manni Duan Team|[2509.11548](http://arxiv.org/abs/2509.11548)|null|
|**2025-09-15**|**LVLMs are Bad at Overhearing Human Referential Communication**|Susan E. Brennan Team|[2509.11514](http://arxiv.org/abs/2509.11514)|null|
|**2025-09-14**|**CEMTM: Contextual Embedding-based Multimodal Topic Modeling**|Giuseppe Carenini Team|[2509.11465](http://arxiv.org/abs/2509.11465)|null|
|**2025-09-14**|**Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations**|Xuanlin Li Team|[2509.11417](http://arxiv.org/abs/2509.11417)|**[link](https://gen-vla.github.io/)**|
|**2025-09-14**|**ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation**|Yizhao Wang Team|[2509.11364](http://arxiv.org/abs/2509.11364)|null|
|**2025-09-14**|**Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations**|Weiming Hu Team|[2509.11287](http://arxiv.org/abs/2509.11287)|null|
|**2025-09-14**|**The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge**|Dehui Du Team|[2509.11071](http://arxiv.org/abs/2509.11071)|null|
|**2025-09-14**|**ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch**|Jialu Zhang Team|[2509.11065](http://arxiv.org/abs/2509.11065)|null|
|**2025-09-13**|**Language-based Color ISP Tuning**|Jiro Takatori Team|[2509.10765](http://arxiv.org/abs/2509.10765)|null|
|**2025-09-12**|**TASC: Task-Aware Shared Control for Teleoperated Manipulation**|Renaud Detry Team|[2509.10416](http://arxiv.org/abs/2509.10416)|null|
|**2025-09-12**|**Towards Understanding Visual Grounding in Visual Language Models**|Eda B. Özyiğit Team|[2509.10345](http://arxiv.org/abs/2509.10345)|null|
|**2025-09-12**|**Detecting Text Manipulation in Images using Vision Language Models**|Sébastien Marcel Team|[2509.10278](http://arxiv.org/abs/2509.10278)|**[link](https://www.idiap.ch/paper/textvlmdet/)**|
|**2025-09-12**|**MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation**|Xiaoming Wei Team|[2509.10260](http://arxiv.org/abs/2509.10260)|null|
|**2025-09-12**|**Towards Reliable and Interpretable Document Question Answering via VLMs**|Simone Marinai Team|[2509.10129](http://arxiv.org/abs/2509.10129)|null|
|**2025-09-12**|**VARCO-VISION-2.0 Technical Report**|Youngjune Kim Team|[2509.10105](http://arxiv.org/abs/2509.10105)|null|
|**2025-09-12**|**Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration**|Wayne Zhang Team|[2509.10059](http://arxiv.org/abs/2509.10059)|null|
|**2025-09-12**|**LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA**|Jianshu Li Team|[2509.10026](http://arxiv.org/abs/2509.10026)|null|
|**2025-09-11**|**How well can LLMs provide planning feedback in grounded environments?**|Victor Zhong Team|[2509.09790](http://arxiv.org/abs/2509.09790)|null|
|**2025-09-11**|**FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark**|Hongsheng Li Team|[2509.09680](http://arxiv.org/abs/2509.09680)|**[link](https://flux-reason-6m.github.io/)**|
|**2025-09-11**|**Compositional Concept Generalization with Variational Quantum Circuits**|Mehrnoosh sadrzadeh Team|[2509.09541](http://arxiv.org/abs/2509.09541)|null|
|**2025-09-11**|**Decoupling Clinical and Class-Agnostic Features for Reliable Few-Shot Adaptation under Shift**|Dwarikanath Mahapatra Team|[2509.09397](http://arxiv.org/abs/2509.09397)|null|
|**2025-09-11**|**VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model**|Donglin Wang Team|[2509.09372](http://arxiv.org/abs/2509.09372)|null|
|**2025-09-11**|**Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning**|Abderrezzak Debilou Team|[2509.09356](http://arxiv.org/abs/2509.09356)|null|
|**2025-09-11**|**Image Recognition with Vision and Language Embeddings of VLMs**|Jiri Matas Team|[2509.09311](http://arxiv.org/abs/2509.09311)|null|
|**2025-09-11**|**Visual Programmability: A Guide for Code-as-Thought in Chart Understanding**|Ya Zhang Team|[2509.09286](http://arxiv.org/abs/2509.09286)|null|
|**2025-09-11**|**Towards Better Dental AI: A Multimodal Benchmark and Instruction Dataset for Panoramic X-ray Analysis**|Kuo Feng Hung Team|[2509.09254](http://arxiv.org/abs/2509.09254)|null|
|**2025-09-11**|**Bridging the Gap Between Ideal and Real-world Evaluation: Benchmarking AI-Generated Image Detection in Challenging Scenarios**|Yao Zhu Team|[2509.09172](http://arxiv.org/abs/2509.09172)|null|
|**2025-09-11**|**Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation Models and Text-to-image Attention**|Fumio Okura Team|[2509.09116](http://arxiv.org/abs/2509.09116)|null|
|**2025-09-10**|**COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation**|Umair Hassan Team|[2509.09014](http://arxiv.org/abs/2509.09014)|**[link](https://huggingface.co/datasets/umairhassan02/urdu-translated-coco-captions-subset.)**|
|**2025-09-10**|**Can Vision-Language Models Solve Visual Math Equations?**|Mrinmaya Sachan Team|[2509.09013](http://arxiv.org/abs/2509.09013)|null|
|**2025-09-10**|**Generalized User-Oriented Image Semantic Coding Empowered by Large Vision-Language Model**|Vincent W. S. Wong Team|[2509.08913](http://arxiv.org/abs/2509.08913)|null|
|**2025-09-10**|**Recurrence Meets Transformers for Universal Multimodal Retrieval**|Rita Cucchiara Team|[2509.08897](http://arxiv.org/abs/2509.08897)|null|
|**2025-09-10**|**RewardDance: Reward Scaling in Visual Generation**|Weilin Huang Team|[2509.08826](http://arxiv.org/abs/2509.08826)|null|
|**2025-09-10**|**RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation**|Hao Zhao Team|[2509.08820](http://arxiv.org/abs/2509.08820)|**[link](https://zzongzheng0918.github.io/RoboChemist.github.io/)**|
|**2025-09-10**|**SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation**|Peter Stone Team|[2509.08757](http://arxiv.org/abs/2509.08757)|**[link](https://larg.github.io/socialnav-sub)**|
|**2025-09-10**|**TCPO: Thought-Centric Preference Optimization for Effective Embodied Decision-making**|Xiu Li Team|[2509.08500](http://arxiv.org/abs/2509.08500)|null|
|**2025-09-10**|**A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models**|Zhou Ni Team|[2509.08490](http://arxiv.org/abs/2509.08490)|null|
|**2025-09-11**|**Adapting Vision-Language Models for Neutrino Event Classification in High-Energy Physics**|Pierre Baldi Team|[2509.08461](http://arxiv.org/abs/2509.08461)|null|
|**2025-09-10**|**Retrieval-Augmented VLMs for Multimodal Melanoma Diagnosis**|Charmgil Hong Team|[2509.08338](http://arxiv.org/abs/2509.08338)|null|
|**2025-09-10**|**Interpretable Physics Reasoning and Performance Taxonomy in Vision-Language Models**|Monali Deshmukh Team|[2509.08270](http://arxiv.org/abs/2509.08270)|null|
|**2025-09-10**|**Examining Vision Language Models through Multi-dimensional Experiments with Vision and Text Features**|Donald E. Brown Team|[2509.08266](http://arxiv.org/abs/2509.08266)|null|
|**2025-09-10**|**Vector embedding of multi-modal texts: a tool for discovery?**|Sachith Withana Team|[2509.08216](http://arxiv.org/abs/2509.08216)|null|
|**2025-09-09**|**Privacy Preserving Semantic Communications Using Vision Language Models: A Segmentation and Generation Approach**|Qianqian Zhang Team|[2509.08142](http://arxiv.org/abs/2509.08142)|null|
|**2025-09-09**|**Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images**|Marc Haraoui Team|[2509.07966](http://arxiv.org/abs/2509.07966)|null|
|**2025-09-09**|**Data-Efficient Fine-Tuning of Vision-Language Models for Diagnosis of Alzheimer's Disease**|Xiaochen Yang Team|[2509.07613](http://arxiv.org/abs/2509.07613)|null|
|**2025-09-09**|**Fine-Tuning Vision-Language Models for Visual Navigation Assistance**|Xi Wang Team|[2509.07488](http://arxiv.org/abs/2509.07488)|null|
|**2025-09-09**|**DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis**|Alois C. Knoll Team|[2509.07463](http://arxiv.org/abs/2509.07463)|null|
|**2025-09-09**|**SpecifyUI: Supporting Iterative UI Design Intent Expression through Structured Specifications and Generative AI**|Liuqing Chen Team|[2509.07334](http://arxiv.org/abs/2509.07334)|null|
|**2025-09-10**|**LLaDA-VLA: Vision Language Diffusion Action Models**|Xiaoyan Sun Team|[2509.06932](http://arxiv.org/abs/2509.06932)|null|
|**2025-09-08**|**D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning**|Nagendra Kumar Team|[2509.06771](http://arxiv.org/abs/2509.06771)|null|
|**2025-09-08**|**Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots**|Aliasghar Arab Team|[2509.06768](http://arxiv.org/abs/2509.06768)|null|
|**2025-09-08**|**Aligning Large Vision-Language Models by Deep Reinforcement Learning and Direct Preference Optimization**|Janis Dalins Team|[2509.06759](http://arxiv.org/abs/2509.06759)|null|
|**2025-09-08**|**Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning**|Xueqi Cheng Team|[2509.06461](http://arxiv.org/abs/2509.06461)|null|
|**2025-09-08**|**When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection**|Muhammad Ashad Kabir Team|[2509.06427](http://arxiv.org/abs/2509.06427)|null|
|**2025-09-08**|**Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models**|Inyong Yun Team|[2509.06415](http://arxiv.org/abs/2509.06415)|null|
|**2025-09-08**|**Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning**|Dong Liang Team|[2509.06409](http://arxiv.org/abs/2509.06409)|null|
|**2025-09-08**|**Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing**|Ha Young Kim Team|[2509.06336](http://arxiv.org/abs/2509.06336)|null|
|**2025-09-08**|**Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes**|Mohammad Akbari Team|[2509.06266](http://arxiv.org/abs/2509.06266)|null|
|**2025-09-07**|**PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology**|Hujun Yin Team|[2509.06105](http://arxiv.org/abs/2509.06105)|null|
|**2025-09-07**|**Analysis of Blood Report Images Using General Purpose Vision-Language Models**|Hamid Beigy Team|[2509.06033](http://arxiv.org/abs/2509.06033)|null|
|**2025-09-07**|**ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction**|Luis Figueredo Team|[2509.06031](http://arxiv.org/abs/2509.06031)|null|
|**2025-09-07**|**Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance**|Tal Arbel Team|[2509.05978](http://arxiv.org/abs/2509.05978)|null|
|**2025-09-06**|**Towards an Automated Framework to Audit Youth Safety on TikTok**|Francesco Pierri Team|[2509.05838](http://arxiv.org/abs/2509.05838)|null|
|**2025-09-06**|**Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization**|Torsten Möller Team|[2509.05718](http://arxiv.org/abs/2509.05718)|null|
|**2025-09-06**|**Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis**|Kazuhiro Nakadai Team|[2509.05703](http://arxiv.org/abs/2509.05703)|null|
|**2025-09-05**|**VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation**|Noel E. O'Connor Team|[2509.05154](http://arxiv.org/abs/2509.05154)|null|
|**2025-09-05**|**GenAI-based test case generation and execution in SDV platform**|Alois Knoll Team|[2509.05112](http://arxiv.org/abs/2509.05112)|null|
|**2025-09-05**|**MM-DREX: Multimodal-Driven Dynamic Routing of LLM Experts for Financial Trading**|Fei Wu Team|[2509.05080](http://arxiv.org/abs/2509.05080)|null|
|**2025-09-05**|**Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework**|Guangmang Cui Team|[2509.05000](http://arxiv.org/abs/2509.05000)|null|
|**2025-09-05**|**SynGen-Vision: Synthetic Data Generation for training industrial vision models**|Nitish Bhardwaj Team|[2509.04894](http://arxiv.org/abs/2509.04894)|null|
|**2025-09-05**|**TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution**|Guihua Shan Team|[2509.04834](http://arxiv.org/abs/2509.04834)|null|
|**2025-09-05**|**FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph**|John E. Taylor Team|[2509.04772](http://arxiv.org/abs/2509.04772)|null|
|**2025-09-05**|**Dynamic Group Detection using VLM-augmented Temporal Groupness Graph**|Norimichi Ukita Team|[2509.04758](http://arxiv.org/abs/2509.04758)|null|
|**2025-09-04**|**Guideline-Consistent Segmentation via Multi-Agent Refinement**|James Davis Team|[2509.04687](http://arxiv.org/abs/2509.04687)|null|
|**2025-09-04**|**TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection**|Mong Li Lee Team|[2509.04448](http://arxiv.org/abs/2509.04448)|**[link](https://yanzehong.github.io/trust-vl/)**|
|**2025-09-05**|**GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization**|Yixuan Li Team|[2509.04334](http://arxiv.org/abs/2509.04334)|null|
|**2025-09-04**|**Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding**|Juntao Li Team|[2509.04243](http://arxiv.org/abs/2509.04243)|null|
|**2025-09-04**|**An Automated, Scalable Machine Learning Model Inversion Assessment Pipeline**|Nathaniel D. Bastian Team|[2509.04214](http://arxiv.org/abs/2509.04214)|null|
|**2025-09-04**|**Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs and Optimizations**|Ashiyana Abdul Majeed Team|[2509.04162](http://arxiv.org/abs/2509.04162)|null|
|**2025-09-04**|**Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection**|C. L. Philip Chen Team|[2509.03961](http://arxiv.org/abs/2509.03961)|null|
|**2025-09-04**|**Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model**|Hyunseung Choo Team|[2509.03895](http://arxiv.org/abs/2509.03895)|null|
|**2025-09-04**|**Weakly-Supervised Learning of Dense Functional Correspondences**|Jiajun Wu Team|[2509.03893](http://arxiv.org/abs/2509.03893)|**[link](https://dense-functional-correspondence.github.io/)**|
|**2025-09-04**|**Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata**|Cédric Colas Team|[2509.03863](http://arxiv.org/abs/2509.03863)|null|
|**2025-09-04**|**Measuring How (Not Just Whether) VLMs Build Common Ground**|Malihe Alikhani Team|[2509.03805](http://arxiv.org/abs/2509.03805)|null|
|**2025-09-04**|**Causality-guided Prompt Learning for Vision-language Models via Visual Granulation**|Qiulei Dong Team|[2509.03803](http://arxiv.org/abs/2509.03803)|null|
|**2025-09-04**|**MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting**|Xiaofeng Yang Team|[2509.03800](http://arxiv.org/abs/2509.03800)|null|
|**2025-09-03**|**Singular Value Few-shot Adaptation of Vision-Language Models**|Yiming Xiao Team|[2509.03740](http://arxiv.org/abs/2509.03740)|null|
|**2025-09-03**|**E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition**|Anupam Purwar Team|[2509.03615](http://arxiv.org/abs/2509.03615)|null|
|**2025-09-05**|**Unveiling the Response of Large Vision-Language Models to Visually Absent Tokens**|Eunho Yang Team|[2509.03025](http://arxiv.org/abs/2509.03025)|null|
|**2025-09-03**|**KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models**|Hong Chen Team|[2509.02966](http://arxiv.org/abs/2509.02966)|null|
|**2025-09-02**|**A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation**|Pedro J. Moreno Team|[2509.02864](http://arxiv.org/abs/2509.02864)|null|
|**2025-09-02**|**Challenges in Understanding Modality Conflict in Vision-Language Models**|David Jensen Team|[2509.02805](http://arxiv.org/abs/2509.02805)|null|
|**2025-09-02**|**2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous Driving Using Vision Language Model**|Yi Yang Team|[2509.02659](http://arxiv.org/abs/2509.02659)|null|
|**2025-08-31**|**Radio Astronomy in the Era of Vision-Language Models: Prompt Sensitivity and Adaptation**|Slava Voloshynovskiy Team|[2509.02615](http://arxiv.org/abs/2509.02615)|null|
|**2025-09-02**|**Language-Guided Long Horizon Manipulation with LLM-based Planning and Visual Perception**|Bin He Team|[2509.02324](http://arxiv.org/abs/2509.02324)|null|
|**2025-09-02**|**RS-OOD: A Vision-Language Augmented Framework for Out-of-Distribution Detection in Remote Sensing**|Yao Zhu Team|[2509.02273](http://arxiv.org/abs/2509.02273)|null|
|**2025-09-02**|**E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI**|Syed Afaq Ali Shah Team|[2509.02100](http://arxiv.org/abs/2509.02100)|null|
|**2025-09-02**|**Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models**|Hiroshi Sasaki Team|[2509.01959](http://arxiv.org/abs/2509.01959)|null|
|**2025-09-02**|**RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events**|Feng Zhang Team|[2509.01907](http://arxiv.org/abs/2509.01907)|null|
|**2025-09-02**|**Automated Wildfire Damage Assessment from Multi view Ground level Imagery Via Vision Language Models**|Yiming Xiao Team|[2509.01895](http://arxiv.org/abs/2509.01895)|null|
|**2025-09-01**|**MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation**|Haibin Yan Team|[2509.01658](http://arxiv.org/abs/2509.01658)|**[link](https://gary3410.github.io/MoTo/)**|
|**2025-09-01**|**Unified Supervision For Vision-Language Modeling in 3D Computed Tomography**|Xueyan Mei Team|[2509.01554](http://arxiv.org/abs/2509.01554)|null|
|**2025-09-01**|**Variation-aware Vision Token Dropping for Faster Large Vision-Language Models**|Honggang Chen Team|[2509.01552](http://arxiv.org/abs/2509.01552)|**[link](https://github.com/xuyang-liu16/V2Drop})**|
|**2025-09-01**|**Error Notebook-Guided, Training-Free Part Retrieval in 3D CAD Assemblies via Vision-Language Models**|Zhiming Tan Team|[2509.01350](http://arxiv.org/abs/2509.01350)|null|
|**2025-09-03**|**Novel Category Discovery with X-Agent Attention for Open-Vocabulary Semantic Segmentation**|Yanyun Qu Team|[2509.01275](http://arxiv.org/abs/2509.01275)|null|
|**2025-09-01**|**ReCap: Event-Aware Image Captioning with Article Retrieval and Semantic Gaussian Normalization**|Trung-Nghia Le Team|[2509.01259](http://arxiv.org/abs/2509.01259)|null|
|**2025-09-01**|**POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion**|Jie Zhou Team|[2509.01215](http://arxiv.org/abs/2509.01215)|null|
|**2025-09-01**|**Measuring Image-Relation Alignment: Reference-Free Evaluation of VLMs and Synthetic Pre-training for Open-Vocabulary Scene Graph Generation**|Akihiro Sugimoto Team|[2509.01209](http://arxiv.org/abs/2509.01209)|null|
|**2025-08-29**|**VoCap: Video Object Captioning and Segmentation from Any Prompt**|Cordelia Schmid Team|[2508.21809](http://arxiv.org/abs/2508.21809)|null|
|**2025-08-29**|**CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models**|Rodrigo Ventura Team|[2508.21732](http://arxiv.org/abs/2508.21732)|null|
|**2025-08-29**|**How Well Do Vision--Language Models Understand Cities? A Comparative Study on Spatial Reasoning from Street-View Images**|Yoonjin Yoon Team|[2508.21565](http://arxiv.org/abs/2508.21565)|null|
|**2025-08-29**|**HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones**|Shaozi Li Team|[2508.21539](http://arxiv.org/abs/2508.21539)|null|
|**2025-08-28**|**OneReward: Unified Mask-Guided Image Generation via Multi-Task Human Preference Learning**|Xinglong Wu Team|[2508.21066](http://arxiv.org/abs/2508.21066)|**[link](https://one-reward.github.io)**|
|**2025-08-28**|**CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification**|Liqiang Nie Team|[2508.21046](http://arxiv.org/abs/2508.21046)|**[link](https://jiutian-vl.github.io/CogVLA-page)**|
|**2025-08-28**|**Learning Primitive Embodied World Models: Towards Scalable Robotic Learning**|Qinying Gu Team|[2508.20840](http://arxiv.org/abs/2508.20840)|null|
|**2025-08-28**|**Estimating 2D Keypoints of Surgical Tools Using Vision-Language Models with Low-Rank Adaptation**|Binod Bhattarai Team|[2508.20830](http://arxiv.org/abs/2508.20830)|null|
|**2025-08-28**|**Evaluating Compositional Generalisation in VLMs and Diffusion Models**|Martha Lewis Team|[2508.20783](http://arxiv.org/abs/2508.20783)|null|
|**2025-09-02**|**Occlusion Robustness of CLIP for Military Vehicle Classification**|Hugo J. Kuijf Team|[2508.20760](http://arxiv.org/abs/2508.20760)|null|
|**2025-08-28**|**"Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection**|Panagiotis C. Petrantonakis Team|[2508.20670](http://arxiv.org/abs/2508.20670)|null|
|**2025-08-28**|**Towards Mechanistic Defenses Against Typographic Attacks in CLIP**|Wojciech Samek Team|[2508.20570](http://arxiv.org/abs/2508.20570)|null|
|**2025-08-28**|**MedGR $^2$ : Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning**|Shangyang Li Team|[2508.20549](http://arxiv.org/abs/2508.20549)|null|
|**2025-08-28**|**MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models**|Yuankai Huo Team|[2508.20345](http://arxiv.org/abs/2508.20345)|null|
|**2025-08-28**|**GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs**|Haohan Wang Team|[2508.20325](http://arxiv.org/abs/2508.20325)|null|
|**2025-08-27**|**A Novel Framework for Automated Explain Vision Model Using Vision-Language Models**|Truong Son Hy Team|[2508.20227](http://arxiv.org/abs/2508.20227)|null|
|**2025-08-27**|**Segmentation Assisted Incremental Test Time Adaptation in an Open World**|Soma Biswas Team|[2508.20029](http://arxiv.org/abs/2508.20029)|null|
|**2025-08-27**|**SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control**|Ping Luo Team|[2508.20018](http://arxiv.org/abs/2508.20018)|null|
|**2025-08-27**|**GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity**|Yixuan Li Team|[2508.19972](http://arxiv.org/abs/2508.19972)|null|
|**2025-08-27**|**Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models**|Shoaib Ehsan Team|[2508.19967](http://arxiv.org/abs/2508.19967)|null|
|**2025-08-27**|**KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts**|Hyunjun Eun Team|[2508.19944](http://arxiv.org/abs/2508.19944)|null|
|**2025-08-28**|**NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks**|Somak Aditya Team|[2508.19724](http://arxiv.org/abs/2508.19724)|null|
|**2025-08-27**|**InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning**|Bo Zheng Team|[2508.19679](http://arxiv.org/abs/2508.19679)|null|
|**2025-08-27**|**Self-Rewarding Vision-Language Model via Reasoning Decomposition**|Dong Yu Team|[2508.19652](http://arxiv.org/abs/2508.19652)|null|
|**2025-08-27**|**FakeSV-VLM: Taming VLM for Detecting Fake Short-Video News via Progressive Mixture-Of-Experts Adapter**|Zhun Zhong Team|[2508.19639](http://arxiv.org/abs/2508.19639)|null|
|**2025-08-26**|**LaVA-Man: Learning Visual Action Representations for Robot Manipulation**|Changjae Oh Team|[2508.19391](http://arxiv.org/abs/2508.19391)|null|
|**2025-08-26**|**Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments**|Pierre Baldi Team|[2508.19376](http://arxiv.org/abs/2508.19376)|null|
|**2025-08-26**|**AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays**|Yiyu Shi Team|[2508.19322](http://arxiv.org/abs/2508.19322)|null|
|**2025-10-01**|**Object Detection with Multimodal Large Vision-Language Models: An In-depth Review**|Manoj Karkee Team|[2508.19294](http://arxiv.org/abs/2508.19294)|null|
|**2025-08-26**|**Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs**|Keping Bi Team|[2508.19111](http://arxiv.org/abs/2508.19111)|null|
|**2025-08-26**|**ProPy: Building Interactive Prompt Pyramids upon CLIP for Partially Relevant Video Retrieval**|Xiaoguang Zhao Team|[2508.19024](http://arxiv.org/abs/2508.19024)|null|
|**2025-08-26**|**Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone**|Amit Sheth Team|[2508.18989](http://arxiv.org/abs/2508.18989)|null|
|**2025-08-28**|**Enhancing Document VQA Models via Retrieval-Augmented Generation**|Ernest Valveny Team|[2508.18984](http://arxiv.org/abs/2508.18984)|null|
|**2025-08-26**|**Toward Robust Medical Fairness: Debiased Dual-Modal Alignment via Text-Guided Attribute-Disentangled Prompt Learning for Vision-Language Models**|Yong Xia Team|[2508.18886](http://arxiv.org/abs/2508.18886)|null|
|**2025-08-26**|**Hidden Tail: Adversarial Image Causing Stealthy Resource Consumption in Vision-Language Models**|Guowen Xu Team|[2508.18805](http://arxiv.org/abs/2508.18805)|null|
|**2025-08-26**|**Rethinking Human-Object Interaction Evaluation for both Vision-Language Models and HOI-Specific Methods**|Robby T. Tan Team|[2508.18753](http://arxiv.org/abs/2508.18753)|null|
|**2025-08-26**|**Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning**|Zuozhu Liu Team|[2508.18687](http://arxiv.org/abs/2508.18687)|null|
|**2025-08-26**|**PRISM: Robust VLM Alignment with Principled Reasoning for Integrated Safety in Multimodality**|Chaowei Xiao Team|[2508.18649](http://arxiv.org/abs/2508.18649)|null|
|**2025-08-25**|**CLARIFY: A Specialist-Generalist Framework for Accurate and Lightweight Dermatological Visual Question Answering**|Mohammad Ariful Haque Team|[2508.18430](http://arxiv.org/abs/2508.18430)|null|
|**2025-08-25**|**Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models**|Jingbo Zhu Team|[2508.18381](http://arxiv.org/abs/2508.18381)|null|
|**2025-08-25**|**SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation**|Ziwei Wang Team|[2508.18268](http://arxiv.org/abs/2508.18268)|**[link](https://denghaoyuan123.github.io/SafeBimanip/)**|
|**2025-08-25**|**MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs**|Qi Qian Team|[2508.18264](http://arxiv.org/abs/2508.18264)|**[link](https://project.ironieser.cc/mmtok)**|
|**2025-08-25**|**SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models**|Ashton Anderson Team|[2508.18179](http://arxiv.org/abs/2508.18179)|null|
|**2025-08-25**|**Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance**|Liyong Ren Team|[2508.18177](http://arxiv.org/abs/2508.18177)|null|
|**2025-08-25**|**ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation**|Ye Li Team|[2508.18050](http://arxiv.org/abs/2508.18050)|null|
|**2025-08-25**|**PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration**|Zhen Wang Team|[2508.18040](http://arxiv.org/abs/2508.18040)|null|
|**2025-08-25**|**Alternating Training-based Label Smoothing Enhances Prompt Generalization**|Yu Zhang Team|[2508.17846](http://arxiv.org/abs/2508.17846)|null|
|**2025-08-25**|**PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models**|Dan Zeng Team|[2508.17807](http://arxiv.org/abs/2508.17807)|null|
|**2025-08-25**|**F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model**|Jinchao Zhang Team|[2508.17714](http://arxiv.org/abs/2508.17714)|null|
|**2025-08-25**|**Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing**|Yogesh Kumar Team|[2508.17686](http://arxiv.org/abs/2508.17686)|null|
|**2025-08-25**|**Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection**|Ruixuan Wang Team|[2508.17667](http://arxiv.org/abs/2508.17667)|null|
|**2025-08-25**|**Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning**|Chunping Qiu Team|[2508.17638](http://arxiv.org/abs/2508.17638)|null|
|**2025-08-25**|**TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints**|Xuan-Huong Nguyen Team|[2508.17595](http://arxiv.org/abs/2508.17595)|null|
|**2025-08-25**|**MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation**|Wojciech Matusik Team|[2508.17568](http://arxiv.org/abs/2508.17568)|null|
|**2025-08-24**|**MoE-Inference-Bench: Performance Evaluation of Mixture of Expert Large Language and Vision Models**|Venkatram Vishwanath Team|[2508.17467](http://arxiv.org/abs/2508.17467)|null|
|**2025-08-24**|**Multi-Level LVLM Guidance for Untrimmed Video Action Recognition**|Yunjie Guo Team|[2508.17442](http://arxiv.org/abs/2508.17442)|null|
|**2025-08-24**|**Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models**|Qinghua Hu Team|[2508.17417](http://arxiv.org/abs/2508.17417)|null|
|**2025-08-24**|**Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis**|Tom Hope Team|[2508.17394](http://arxiv.org/abs/2508.17394)|null|
|**2025-08-26**|**Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs**|Gaurav Harit Team|[2508.17334](http://arxiv.org/abs/2508.17334)|null|
|**2025-08-24**|**Explain Before You Answer: A Survey on Compositional Visual Reasoning**|Hamid Rezatofighi Team|[2508.17298](http://arxiv.org/abs/2508.17298)|null|
|**2025-08-22**|**Modular Embedding Recomposition for Incremental Learning**|Simone Calderara Team|[2508.16463](http://arxiv.org/abs/2508.16463)|null|
|**2025-08-22**|**Structuring GUI Elements through Vision Language Models: Towards Action Space Generation**|Jingdong Chen Team|[2508.16271](http://arxiv.org/abs/2508.16271)|null|
|**2025-08-22**|**RAGSR: Regional Attention Guided Diffusion for Image Super-Resolution**|Gui-Song Xia Team|[2508.16158](http://arxiv.org/abs/2508.16158)|null|
|**2025-08-22**|**Beyond Human-prompting: Adaptive Prompt Tuning with Semantic Alignment for Anomaly Detection**|Chao-Chun Chen Team|[2508.16157](http://arxiv.org/abs/2508.16157)|null|
|**2025-08-22**|**Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation**|Hasan Mahmud Team|[2508.16076](http://arxiv.org/abs/2508.16076)|null|
|**2025-08-22**|**Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants**|Jinchao Zhang Team|[2508.16070](http://arxiv.org/abs/2508.16070)|null|
|**2025-08-21**|**Glo-VLMs: Leveraging Vision-Language Models for Fine-Grained Diseased Glomerulus Classification**|Ruining Deng Team|[2508.15960](http://arxiv.org/abs/2508.15960)|null|
|**2025-08-21**|**Semantic-Aware Ship Detection with Vision-Language Integration**|Xiaomeng Huang Team|[2508.15930](http://arxiv.org/abs/2508.15930)|null|
|**2025-08-21**|**VT-LVLM-AR: A Video-Temporal Large Vision-Language Model Adapter for Fine-Grained Action Recognition in Long-Term Videos**|Zihan Xu Team|[2508.15903](http://arxiv.org/abs/2508.15903)|null|
|**2025-08-21**|**LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions**|Yulong Bian Team|[2508.15688](http://arxiv.org/abs/2508.15688)|null|
|**2025-08-21**|**Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation**|Alexey K. Kovalev Team|[2508.15663](http://arxiv.org/abs/2508.15663)|null|
|**2025-08-21**|**DesignCLIP: Multimodal Learning with CLIP for Design Patent Understanding**|Sourav Medya Team|[2508.15297](http://arxiv.org/abs/2508.15297)|null|
|**2025-08-21**|**Normal and Abnormal Pathology Knowledge-Augmented Vision-Language Model for Anomaly Detection in Pathology Images**|Jin Tae Kwak Team|[2508.15256](http://arxiv.org/abs/2508.15256)|null|
|**2025-08-21**|**Pathology-Informed Latent Diffusion Model for Anomaly Detection in Lymph Node Metastasis**|Jin Tae Kwak Team|[2508.15236](http://arxiv.org/abs/2508.15236)|null|
|**2025-08-21**|**See it. Say it. Sorted: Agentic System for Compositional Diagram Generation**|Ed Li Team|[2508.15222](http://arxiv.org/abs/2508.15222)|null|
|**2025-08-21**|**ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following**|Taeyang Yoon Team|[2508.15164](http://arxiv.org/abs/2508.15164)|null|
|**2025-08-20**|**MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in Mixture-of-Experts LLMs**|Yunsi Fei Team|[2508.15036](http://arxiv.org/abs/2508.15036)|null|
|**2025-08-20**|**WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion**|Won-Ki Jeong Team|[2508.14537](http://arxiv.org/abs/2508.14537)|null|
|**2025-08-19**|**Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference**|Simon Gottschalk Team|[2508.14280](http://arxiv.org/abs/2508.14280)|null|
|**2025-08-19**|**CLIPSym: Delving into Symmetry Detection with CLIP**|Raymond A. Yeh Team|[2508.14197](http://arxiv.org/abs/2508.14197)|null|
|**2025-08-19**|**LENS: Learning to Segment Anything with Unified Reinforced Reasoning**|Xinggang Wang Team|[2508.14153](http://arxiv.org/abs/2508.14153)|**[link](https://github.com/hustvl/LENS)**|
|**2025-08-19**|**Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation**|Jianye Hao Team|[2508.13998](http://arxiv.org/abs/2508.13998)|null|
|**2025-08-19**|**Mitigating Cross-Image Information Leakage in LVLMs for Multi-Image Tasks**|Junsuk Choe Team|[2508.13744](http://arxiv.org/abs/2508.13744)|**[link](https://github.com/yejipark-m/FOCUS)**|
|**2025-08-19**|**Enhancing Targeted Adversarial Attacks on Large Vision-Language Models through Intermediate Projector Guidance**|Bin Xiao Team|[2508.13739](http://arxiv.org/abs/2508.13739)|null|
|**2025-08-19**|**Hierarchical Vision-Language Retrieval of Educational Metaverse Content in Agriculture**|Giuseppe Serra Team|[2508.13713](http://arxiv.org/abs/2508.13713)|null|
|**2025-08-19**|**ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?**|Daeyoung Kim Team|[2508.13680](http://arxiv.org/abs/2508.13680)|null|
|**2025-08-19**|**Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation**|Lin Ma Team|[2508.13587](http://arxiv.org/abs/2508.13587)|null|
|**2025-08-21**|**DictAS: A Framework for Class-Generalizable Few-Shot Anomaly Segmentation via Dictionary Lookup**|Guiguang Ding Team|[2508.13560](http://arxiv.org/abs/2508.13560)|**[link](https://github.com/xiaozhen228/DictAS)**|
|**2025-08-19**|**Evaluating Open-Source Vision Language Models for Facial Emotion Recognition against Traditional Deep Learning Models**|Sridevi Bonthu Team|[2508.13524](http://arxiv.org/abs/2508.13524)|null|
|**2025-08-19**|**STER-VLM: Spatio-Temporal With Enhanced Reference Vision-Language Models**|Tien-Huy Nguyen Team|[2508.13470](http://arxiv.org/abs/2508.13470)|null|
|**2025-08-19**|**CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models**|Sergey Levine Team|[2508.13446](http://arxiv.org/abs/2508.13446)|null|
|**2025-08-19**|**Structured Prompting and Multi-Agent Knowledge Distillation for Traffic Video Interpretation and Risk Inference**|Jidong J. Yang Team|[2508.13439](http://arxiv.org/abs/2508.13439)|null|
|**2025-08-19**|**Mitigating Easy Option Bias in Multiple-Choice Question Answering**|Basura Fernando Team|[2508.13428](http://arxiv.org/abs/2508.13428)|null|
|**2025-08-18**|**Prune2Drive: A Plug-and-Play Framework for Accelerating Vision-Language Models in Autonomous Driving**|Linfeng Zhang Team|[2508.13305](http://arxiv.org/abs/2508.13305)|null|
|**2025-08-18**|**CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support**|Jinming Duan Team|[2508.13256](http://arxiv.org/abs/2508.13256)|null|
|**2025-08-18**|**Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey**|Liqiang Nie Team|[2508.13073](http://arxiv.org/abs/2508.13073)|**[link](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)**|
|**2025-08-18**|**IntelliCap: Intelligent Guidance for Consistent View Sampling**|Shohei Mori Team|[2508.13043](http://arxiv.org/abs/2508.13043)|**[link](https://mediated-reality.github.io/projects/yasunaga_ismar25/)**|
|**2025-08-18**|**Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination**|Lihua Zhang Team|[2508.12957](http://arxiv.org/abs/2508.12957)|null|
|**2025-08-18**|**RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph**|Yunquan Sun Team|[2508.12916](http://arxiv.org/abs/2508.12916)|null|
|**2025-08-18**|**Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning**|Ruixuan Wang Team|[2508.12877](http://arxiv.org/abs/2508.12877)|null|
|**2025-08-18**|**Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models**|Ruixuan Wang Team|[2508.12861](http://arxiv.org/abs/2508.12861)|null|
|**2025-08-18**|**HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks**|Yu Wang Team|[2508.12778](http://arxiv.org/abs/2508.12778)|null|
|**2025-08-18**|**Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection**|Wei Zhou Team|[2508.12711](http://arxiv.org/abs/2508.12711)|null|
|**2025-08-18**|**WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art**|Feng Liu Team|[2508.12668](http://arxiv.org/abs/2508.12668)|**[link](https://github.com/abhijay9/wpclip)**|
|**2025-08-18**|**SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer**|Zheng Yang Team|[2508.12638](http://arxiv.org/abs/2508.12638)|null|
|**2025-08-18**|**ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving**|Ziran Wang Team|[2508.12603](http://arxiv.org/abs/2508.12603)|null|
|**2025-08-18**|**Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models**|Chris Ngo Team|[2508.12587](http://arxiv.org/abs/2508.12587)|null|
|**2025-08-18**|**REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language**|Yash Butala Team|[2508.12543](http://arxiv.org/abs/2508.12543)|null|
|**2025-08-17**|**LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models**|Venkatram Vishwanath Team|[2508.12512](http://arxiv.org/abs/2508.12512)|null|
|**2025-08-17**|**Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System**|Kasun De Zoysa Team|[2508.12473](http://arxiv.org/abs/2508.12473)|null|
|**2025-08-17**|**M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following**|Yanfei Qian Team|[2508.12458](http://arxiv.org/abs/2508.12458)|null|
|**2025-08-17**|**X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning**|Shaoqing Tang Team|[2508.12455](http://arxiv.org/abs/2508.12455)|null|
|**2025-08-17**|**LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving**|Li Zhang Team|[2508.12404](http://arxiv.org/abs/2508.12404)|null|
|**2025-08-17**|**MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models**|Xueying Huang Team|[2508.12400](http://arxiv.org/abs/2508.12400)|null|
|**2025-08-17**|**Federated Cross-Modal Style-Aware Prompt Generation**|Amit Sethi Team|[2508.12399](http://arxiv.org/abs/2508.12399)|null|
|**2025-08-15**|**Reinforcing Video Reasoning Segmentation to Think Before It Segments**|Huchuan Lu Team|[2508.11538](http://arxiv.org/abs/2508.11538)|null|
|**2025-08-15**|**OVSegDT: Segmenting Transformer for Open-Vocabulary Object Goal Navigation**|Aleksandr Panov Team|[2508.11479](http://arxiv.org/abs/2508.11479)|null|
|**2025-08-15**|**ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving**|Li Zhang Team|[2508.11428](http://arxiv.org/abs/2508.11428)|null|
|**2025-08-15**|**Semantically Guided Adversarial Testing of Vision Models Using Language Models**|Jorge M. Cruz-Duarte Team|[2508.11341](http://arxiv.org/abs/2508.11341)|null|
|**2025-08-15**|**Noise Matters: Optimizing Matching Noise for Diffusion Classifiers**|Long Chen Team|[2508.11330](http://arxiv.org/abs/2508.11330)|null|
|**2025-08-15**|**Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models**|Tat-Seng Chua Team|[2508.11317](http://arxiv.org/abs/2508.11317)|null|
|**2025-08-15**|**Vision-Language Models display a strong gender bias**|Sreedath Panat Team|[2508.11262](http://arxiv.org/abs/2508.11262)|null|
|**2025-08-15**|**Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception**|Zhuotao Tian Team|[2508.11256](http://arxiv.org/abs/2508.11256)|null|
|**2025-08-15**|**UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning**|Yue Zhang Team|[2508.11196](http://arxiv.org/abs/2508.11196)|null|
|**2025-08-15**|**Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning**|Bin Luo Team|[2508.11176](http://arxiv.org/abs/2508.11176)|null|
|**2025-08-15**|**Better Supervised Fine-tuning for VQA: Integer-Only Loss**|Junhui Cui Team|[2508.11170](http://arxiv.org/abs/2508.11170)|null|
|**2025-08-14**|**Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance**|Rustam Stolkin Team|[2508.11093](http://arxiv.org/abs/2508.11093)|null|
|**2025-08-14**|**Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?**|Zhengbo Zou Team|[2508.11011](http://arxiv.org/abs/2508.11011)|null|
|**2025-08-14**|**Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision**|Anhong Guo Team|[2508.10972](http://arxiv.org/abs/2508.10972)|null|
|**2025-08-14**|**AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences**|Joey Tianyi Zhou Team|[2508.10771](http://arxiv.org/abs/2508.10771)|null|
|**2025-08-14**|**From Diagnosis to Improvement: Probing Spatio-Physical Reasoning in Vision Language Models**|Wenqi Shao Team|[2508.10770](http://arxiv.org/abs/2508.10770)|null|
|**2025-08-14**|**IADGPT: Unified LVLM for Few-Shot Industrial Anomaly Detection, Localization, and Reasoning via In-Context Learning**|Bin Li Team|[2508.10681](http://arxiv.org/abs/2508.10681)|null|
|**2025-08-14**|**AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models**|Jieping Ye Team|[2508.10667](http://arxiv.org/abs/2508.10667)|null|
|**2025-08-14**|**SemPT: Semantic Prompt Tuning for Vision-Language Models**|Zhenzhong Chen Team|[2508.10645](http://arxiv.org/abs/2508.10645)|null|
|**2025-08-14**|**ChatENV: An Interactive Vision-Language Model for Sensor-Guided Environmental Monitoring and Scenario Simulation**|Mohsen Guizani Team|[2508.10635](http://arxiv.org/abs/2508.10635)|null|
|**2025-08-14**|**Retrieval-Augmented Prompt for OOD Detection**|Changqing Zhang Team|[2508.10556](http://arxiv.org/abs/2508.10556)|null|
|**2025-08-14**|**DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales**|Zhi Zeng Team|[2508.10444](http://arxiv.org/abs/2508.10444)|null|
|**2025-08-14**|**MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance**|Yi Zhang Team|[2508.10429](http://arxiv.org/abs/2508.10429)|**[link](https://huggingface.co/datasets/Codatta/MM-Food-100K)**|
|**2025-08-14**|**STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes**|Yu Yamaguchi Team|[2508.10427](http://arxiv.org/abs/2508.10427)|**[link](https://turingmotors.github.io/stride-qa/)**|
|**2025-08-14**|**PQ-DAF: Pose-driven Quality-controlled Data Augmentation for Data-scarce Driver Distraction Detection**|Xinghui Song Team|[2508.10397](http://arxiv.org/abs/2508.10397)|null|
|**2025-08-14**|**Contrast Sensitivity Function of Multimodal Vision-Language Models**|Valero Laparra Team|[2508.10367](http://arxiv.org/abs/2508.10367)|null|
|**2025-08-14**|**JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics**|Hamid Rezatofighi Team|[2508.10287](http://arxiv.org/abs/2508.10287)|null|
|**2025-08-14**|**MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs**|Yujun Cai Team|[2508.10264](http://arxiv.org/abs/2508.10264)|null|
|**2025-08-13**|**Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs**|Xiaoxiao Li Team|[2508.10180](http://arxiv.org/abs/2508.10180)|null|
|**2025-08-13**|**SynSpill: Improved Industrial Spill Detection With Synthetic Data**|Shruti Vyas Team|[2508.10171](http://arxiv.org/abs/2508.10171)|null|
|**2025-08-13**|**Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs**|Bin Li Team|[2508.10113](http://arxiv.org/abs/2508.10113)|null|
|**2025-08-13**|**LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit**|Wenya Wang Team|[2508.09981](http://arxiv.org/abs/2508.09981)|null|
|**2025-08-13**|**January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis**|Mark Woodward Team|[2508.09966](http://arxiv.org/abs/2508.09966)|null|
|**2025-08-14**|**Prototype-Guided Diffusion: Visual Conditioning without External Memory**|Mustapha Lebbah Team|[2508.09922](http://arxiv.org/abs/2508.09922)|null|
|**2025-08-12**|**OpenCUA: Open Foundations for Computer-Use Agents**|Tao Yu Team|[2508.09123](http://arxiv.org/abs/2508.09123)|null|
|**2025-08-12**|**Bridging Formal Language with Chain-of-Thought Reasoning to Geometry Problem Solving**|Tian Ding Team|[2508.09099](http://arxiv.org/abs/2508.09099)|null|
|**2025-08-12**|**Addressing Bias in VLMs for Glaucoma Detection Without Protected Attribute Supervision**|Prashnna Gyawali Team|[2508.09087](http://arxiv.org/abs/2508.09087)|null|
|**2025-08-13**|**GeoVLA: Empowering 3D Representations in Vision-Language-Action Models**|Jiale Cao Team|[2508.09071](http://arxiv.org/abs/2508.09071)|**[link](https://linsun449.github.io/GeoVLA/)**|
|**2025-08-12**|**VLM-3D:End-to-End Vision-Language Models for Open-World 3D Perception**|Lei He Team|[2508.09061](http://arxiv.org/abs/2508.09061)|null|
|**2025-08-12**|**MVISU-Bench: Benchmarking Mobile Agents for Real-World Tasks by Multi-App, Vague, Interactive, Single-App and Unethical Instructions**|Jin Xu Team|[2508.09057](http://arxiv.org/abs/2508.09057)|null|
|**2025-08-12**|**Rational Inverse Reasoning**|Leslie Pack Kaelbling Team|[2508.08983](http://arxiv.org/abs/2508.08983)|null|
|**2025-08-12**|**How Does a Virtual Agent Decide Where to Look? -- Symbolic Cognitive Reasoning for Embodied Head Rotation**|Hyeongyeop Kang Team|[2508.08930](http://arxiv.org/abs/2508.08930)|null|
|**2025-08-12**|**Safe Semantics, Unsafe Interpretations: Tackling Implicit Reasoning Safety in Large Vision-Language Models**|Xuelong Li Team|[2508.08926](http://arxiv.org/abs/2508.08926)|null|
|**2025-08-12**|**3DFroMLLM: 3D Prototype Generation only from Pretrained Multimodal LLMs**|Eddy Ilg Team|[2508.08821](http://arxiv.org/abs/2508.08821)|null|
|**2025-08-12**|**SafeFix: Targeted Model Repair via Controlled Image Generation**|Yunhui Guo Team|[2508.08701](http://arxiv.org/abs/2508.08701)|null|
|**2025-08-12**|**STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision**|Marios Savvides Team|[2508.08688](http://arxiv.org/abs/2508.08688)|null|
|**2025-08-12**|**AME: Aligned Manifold Entropy for Robust Vision-Language Distillation**|Yuming Ou Team|[2508.08644](http://arxiv.org/abs/2508.08644)|null|
|**2025-08-13**|**Transferable Model-agnostic Vision-Language Model Adaptation for Efficient Weak-to-Strong Generalization**|Hyunwoo J. Kim Team|[2508.08604](http://arxiv.org/abs/2508.08604)|null|
|**2025-08-12**|**Superclass-Guided Representation Disentanglement for Spurious Correlation Mitigation**|Qi Lei Team|[2508.08570](http://arxiv.org/abs/2508.08570)|null|
|**2025-08-11**|**VISOR: Visual Input-based Steering for Output Redirection in Vision-Language Models**|Ravikumar Balakrishnan Team|[2508.08521](http://arxiv.org/abs/2508.08521)|null|
|**2025-08-11**|**Re:Verse -- Can Your VLM Read a Manga?**|Shruti Vyas Team|[2508.08508](http://arxiv.org/abs/2508.08508)|null|
|**2025-08-11**|**ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks**|Chunhua Shen Team|[2508.08240](http://arxiv.org/abs/2508.08240)|null|
|**2025-08-11**|**Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model**|Shaoliang Peng Team|[2508.08199](http://arxiv.org/abs/2508.08199)|null|
|**2025-08-11**|**BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models**|Bo Wang Team|[2508.08040](http://arxiv.org/abs/2508.08040)|null|
|**2025-08-11**|**TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation**|Robert Wille Team|[2508.08038](http://arxiv.org/abs/2508.08038)|null|
|**2025-08-11**|**TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding**|Jee-Hyong Lee Team|[2508.07925](http://arxiv.org/abs/2508.07925)|null|
|**2025-08-11**|**RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering**|Mukesh Prasad Team|[2508.07918](http://arxiv.org/abs/2508.07918)|null|
|**2025-08-11**|**CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning**|Ruixiang Tang Team|[2508.07871](http://arxiv.org/abs/2508.07871)|null|
|**2025-08-11**|**Effortless Vision-Language Model Specialization in Histopathology without Annotation**|Katharina Breininger Team|[2508.07835](http://arxiv.org/abs/2508.07835)|null|
|**2025-08-11**|**MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization**|Alexandros Stergiou Team|[2508.07833](http://arxiv.org/abs/2508.07833)|**[link](https://anaekin.github.io/MIMIC)**|
|**2025-08-11**|**Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP**|Yueyi Luo Team|[2508.07819](http://arxiv.org/abs/2508.07819)|null|
|**2025-08-11**|**SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing**|Dzmitry Tsetserukou Team|[2508.07814](http://arxiv.org/abs/2508.07814)|null|
|**2025-08-11**|**Grasp-HGN: Grasping the Unexpected**|Gunar Schirner Team|[2508.07648](http://arxiv.org/abs/2508.07648)|null|
|**2025-08-11**|**Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents**|Parisa Kordjamshidi Team|[2508.07642](http://arxiv.org/abs/2508.07642)|null|
|**2025-08-11**|**InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information**|Vivek Gupta Team|[2508.07630](http://arxiv.org/abs/2508.07630)|null|
|**2025-08-11**|**AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning**|Yang Liu Team|[2508.07626](http://arxiv.org/abs/2508.07626)|null|
|**2025-08-11**|**Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models**|Duc Thanh Nguyen Team|[2508.07570](http://arxiv.org/abs/2508.07570)|null|
|**2025-08-10**|**FormCoach: Lift Smarter, Not Harder**|Lingjie Liu Team|[2508.07501](http://arxiv.org/abs/2508.07501)|null|
|**2025-08-10**|**Freeze and Reveal: Exposing Modality Bias in Vision-Language Models**|Ponnurangam Kumaraguru Team|[2508.07432](http://arxiv.org/abs/2508.07432)|null|
|**2025-08-10**|**AgriVLN: Vision-and-Language Navigation for Agricultural Robots**|Xiang Li Team|[2508.07406](http://arxiv.org/abs/2508.07406)|null|
|**2025-08-10**|**Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM**|Wentao Zhang Team|[2508.07260](http://arxiv.org/abs/2508.07260)|null|
|**2025-08-08**|**Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding**|Kun Shao Team|[2508.06317](http://arxiv.org/abs/2508.06317)|null|
|**2025-08-08**|**Real-Time 3D Vision-Language Embedding Mapping**|Elmar Rueckert Team|[2508.06291](http://arxiv.org/abs/2508.06291)|null|
|**2025-08-08**|**InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?**|Youngjae Yu Team|[2508.06220](http://arxiv.org/abs/2508.06220)|null|
|**2025-08-08**|**VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation**|ChengSheng Deng Team|[2508.06152](http://arxiv.org/abs/2508.06152)|null|
|**2025-08-08**|**Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation**|Shaohui Liu Team|[2508.06092](http://arxiv.org/abs/2508.06092)|null|
|**2025-08-08**|**AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance**|Yunhao Liu Team|[2508.06084](http://arxiv.org/abs/2508.06084)|null|
|**2025-08-08**|**Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models**|Zhouhan Lin Team|[2508.06038](http://arxiv.org/abs/2508.06038)|null|
|**2025-08-08**|**More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment**|Zhepeng Wang Team|[2508.06036](http://arxiv.org/abs/2508.06036)|null|
|**2025-08-08**|**Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making**|Xiaosong Wang Team|[2508.05996](http://arxiv.org/abs/2508.05996)|null|
|**2025-08-08**|**PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation**|Yao Mu Team|[2508.05976](http://arxiv.org/abs/2508.05976)|null|
|**2025-08-07**|**HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing**|Chris Callison-Burch Team|[2508.05899](http://arxiv.org/abs/2508.05899)|null|
|**2025-08-07**|**ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates**|Ali cheraghian Team|[2508.05898](http://arxiv.org/abs/2508.05898)|null|
|**2025-08-07**|**Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis**|Zeyu Wang Team|[2508.05580](http://arxiv.org/abs/2508.05580)|null|
|**2025-08-07**|**Adapting Vision-Language Models Without Labels: A Comprehensive Survey**|Olga Fink Team|[2508.05547](http://arxiv.org/abs/2508.05547)|**[link](https://github.com/tim-learn/Awesome-LabelFree-VLMs})**|
|**2025-08-07**|**Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions**|Przemyslaw Biecek Team|[2508.05430](http://arxiv.org/abs/2508.05430)|null|
|**2025-08-07**|**From Detection to Correction: Backdoor-Resilient Face Recognition via Vision-Language Trigger Detection and Noise-Based Neutralization**|Ibrahim Khalil Team|[2508.05409](http://arxiv.org/abs/2508.05409)|null|
|**2025-08-07**|**DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning**|Bo Zheng Team|[2508.05405](http://arxiv.org/abs/2508.05405)|null|
|**2025-08-07**|**StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models**|Youqiang Zhou Team|[2508.05383](http://arxiv.org/abs/2508.05383)|null|
|**2025-08-07**|**Textual Inversion for Efficient Adaptation of Open-Vocabulary Object Detectors Without Forgetting**|Hugo Kuijf Team|[2508.05323](http://arxiv.org/abs/2508.05323)|null|
|**2025-08-07**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Jorge Peña Queralta Team|[2508.05294](http://arxiv.org/abs/2508.05294)|null|
|**2025-08-07**|**RegionMed-CLIP: A Region-Aware Multimodal Contrastive Learning Pre-trained Model for Medical Image Understanding**|Guiru Liu Team|[2508.05244](http://arxiv.org/abs/2508.05244)|null|
|**2025-08-07**|**Navigating the Trade-off: A Synthesis of Defensive Strategies for Zero-Shot Adversarial Robustness in Vision-Language Models**|Jason Sun Team|[2508.05237](http://arxiv.org/abs/2508.05237)|null|
|**2025-08-07**|**ReasoningTrack: Chain-of-Thought Reasoning for Long-term Vision-Language Tracking**|Zhipeng Zhang Team|[2508.05221](http://arxiv.org/abs/2508.05221)|null|
|**2025-08-07**|**SPEX: A Vision-Language Model for Land Cover Extraction on Spectral Remote Sensing Images**|Liangpei Zhang Team|[2508.05202](http://arxiv.org/abs/2508.05202)|null|
|**2025-08-07**|**Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories**|Andrew I. Cooper Team|[2508.05148](http://arxiv.org/abs/2508.05148)|null|
|**2025-08-07**|**Multimodal Causal-Driven Representation Learning for Generalizable Medical Image Segmentation**|Zhen Lei Team|[2508.05008](http://arxiv.org/abs/2508.05008)|null|
|**2025-08-07**|**Attribute Guidance With Inherent Pseudo-label For Occluded Person Re-identification**|Haiyang Zhang Team|[2508.04998](http://arxiv.org/abs/2508.04998)|null|
|**2025-08-07**|**Unified modality separation: A vision-language framework for unsupervised domain adaptation**|Heng Tao Shen Team|[2508.04987](http://arxiv.org/abs/2508.04987)|null|
|**2025-08-07**|**Accelerating Conditional Prompt Learning via Masked Image Modeling for Vision-Language Models**|Hyunseung Choo Team|[2508.04942](http://arxiv.org/abs/2508.04942)|null|
|**2025-08-06**|**INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM**|Nikos Tsagarakis Team|[2508.04931](http://arxiv.org/abs/2508.04931)|**[link](https://robo-intention.github.io)**|
|**2025-08-06**|**Automated Bug Frame Retrieval from Gameplay Videos Using Vision-Language Models**|Cor-Paul Bezemer Team|[2508.04895](http://arxiv.org/abs/2508.04895)|null|
|**2025-08-06**|**Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications**|Wassim Bouachir Team|[2508.04868](http://arxiv.org/abs/2508.04868)|null|
|**2025-08-01**|**MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval**|Chengcheng Mai Team|[2508.00579](http://arxiv.org/abs/2508.00579)|null|
|**2025-08-01**|**Training-Free Class Purification for Open-Vocabulary Semantic Segmentation**|Xiaohua Xie Team|[2508.00557](http://arxiv.org/abs/2508.00557)|null|
|**2025-08-01**|**HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models**|Bin Chen Team|[2508.00553](http://arxiv.org/abs/2508.00553)|null|
|**2025-08-01**|**Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images**|Timo Ropinski Team|[2508.00549](http://arxiv.org/abs/2508.00549)|null|
|**2025-08-01**|**EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond**|Baochang Zhang Team|[2508.00522](http://arxiv.org/abs/2508.00522)|null|
|**2025-08-01**|**When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework**|Tony Q. S. Quek Team|[2508.00456](http://arxiv.org/abs/2508.00456)|null|
|**2025-08-01**|**CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text**|Petar Durdevic Team|[2508.00447](http://arxiv.org/abs/2508.00447)|null|
|**2025-08-01**|**AutoDebias: Automated Framework for Debiasing Text-to-Image Models**|Yang Liu Team|[2508.00445](http://arxiv.org/abs/2508.00445)|null|
|**2025-08-01**|**Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents**|Rowel O. Atienza Team|[2508.00400](http://arxiv.org/abs/2508.00400)|null|
|**2025-08-01**|**iSafetyBench: A video-language benchmark for safety in industrial environment**|Shruti Vyas Team|[2508.00399](http://arxiv.org/abs/2508.00399)|null|
|**2025-08-01**|**Decouple before Align: Visual Disentanglement Enhances Prompt Tuning**|Yanfeng Wang Team|[2508.00395](http://arxiv.org/abs/2508.00395)|null|
|**2025-08-01**|**SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation**|Renxin Zhong Team|[2508.00390](http://arxiv.org/abs/2508.00390)|null|
|**2025-08-01**|**CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding**|Lin Shang Team|[2508.00378](http://arxiv.org/abs/2508.00378)|null|
|**2025-08-01**|**Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning**|Athanasios Voulodimos Team|[2508.00356](http://arxiv.org/abs/2508.00356)|null|
|**2025-08-01**|**Evaluating the Efficacy of Large Language Models for Generating Fine-Grained Visual Privacy Policies in Homes**|Hewu Li Team|[2508.00321](http://arxiv.org/abs/2508.00321)|null|
|**2025-08-01**|**DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios**|Lin Ma Team|[2508.00311](http://arxiv.org/abs/2508.00311)|null|
|**2025-08-01**|**Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models**|Eunwoo Kim Team|[2508.00260](http://arxiv.org/abs/2508.00260)|null|
|**2025-08-01**|**Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product**|Ehsan Abbasnejad Team|[2508.00230](http://arxiv.org/abs/2508.00230)|null|
|**2025-07-31**|**On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI**|Enzo Ferrante Team|[2508.00171](http://arxiv.org/abs/2508.00171)|null|
|**2025-07-31**|**ART: Adaptive Relation Tuning for Generalized Relation Prediction**|Stefan Roth Team|[2507.23543](http://arxiv.org/abs/2507.23543)|null|
|**2025-07-23**|**BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems**|Christian Berger Team|[2507.17722](http://arxiv.org/abs/2507.17722)|null|
|**2025-07-23**|**InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation**|Jiangmiao Pang Team|[2507.17520](http://arxiv.org/abs/2507.17520)|null|
|**2025-07-23**|**Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection**|Elisa Ricci Team|[2507.17456](http://arxiv.org/abs/2507.17456)|null|
|**2025-07-23**|**VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization**|Shoaib Ehsan Team|[2507.17455](http://arxiv.org/abs/2507.17455)|null|
|**2025-07-23**|**Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection**|Xi Li Team|[2507.17436](http://arxiv.org/abs/2507.17436)|null|
|**2025-07-23**|**Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models**|Guanghui Sun Team|[2507.17379](http://arxiv.org/abs/2507.17379)|null|
|**2025-07-23**|**RoadBench: A Vision-Language Foundation Model and Benchmark for Road Damage Understanding**|Tianyang Wang Team|[2507.17353](http://arxiv.org/abs/2507.17353)|null|
|**2025-07-23**|**HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study**|Maria Spence Team|[2507.17118](http://arxiv.org/abs/2507.17118)|null|
|**2025-07-23**|**FedVLM: Scalable Personalized Vision-Language Models through Federated Learning**|Habeeb Olufowobi Team|[2507.17088](http://arxiv.org/abs/2507.17088)|null|
|**2025-07-22**|**VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings**|Kannan Achan Team|[2507.17080](http://arxiv.org/abs/2507.17080)|null|
|**2025-07-22**|**Controllable Hybrid Captioner for Improved Long-form Video Understanding**|Arun Reddy Team|[2507.17047](http://arxiv.org/abs/2507.17047)|null|
|**2025-07-22**|**Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning**|Kai Chen Team|[2507.16814](http://arxiv.org/abs/2507.16814)|null|
|**2025-07-22**|**Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems**|Arslan Munir Team|[2507.16781](http://arxiv.org/abs/2507.16781)|null|
|**2025-07-22**|**Enhancing Remote Sensing Vision-Language Models Through MLLM and LLM-Based High-Quality Image-Text Dataset Generation**|Ke Yang Team|[2507.16716](http://arxiv.org/abs/2507.16716)|null|
|**2025-07-22**|**Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory**|Marco Hutter Team|[2507.16713](http://arxiv.org/abs/2507.16713)|null|
|**2025-07-22**|**Spatial 3D-LLM: Exploring Spatial Awareness in 3D Vision-Language Models**|Chao Zhang Team|[2507.16524](http://arxiv.org/abs/2507.16524)|null|
|**2025-07-22**|**SceneLoom: Communicating Data with Scene Context**|Siming Chen Team|[2507.16466](http://arxiv.org/abs/2507.16466)|null|
|**2025-07-22**|**Quality Text, Robust Vision: The Role of Language in Enhancing Visual Robustness of Vision-Language Models**|Isao Echizen Team|[2507.16257](http://arxiv.org/abs/2507.16257)|null|
|**2025-07-22**|**SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction**|Jiaqi Wang Team|[2507.15852](http://arxiv.org/abs/2507.15852)|null|
|**2025-07-21**|**Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models**|Erkut Erdem Team|[2507.15824](http://arxiv.org/abs/2507.15824)|null|
|**2025-07-23**|**Visual-Language Model Knowledge Distillation Method for Image Quality Assessment**|Jiarun Song Team|[2507.15680](http://arxiv.org/abs/2507.15680)|null|
|**2025-07-21**|**Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging**|Margret Keuper Team|[2507.15576](http://arxiv.org/abs/2507.15576)|null|
|**2025-07-21**|**HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation**|Robby T. Tan Team|[2507.15542](http://arxiv.org/abs/2507.15542)|null|
|**2025-07-21**|**Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner**|Lin Ma Team|[2507.15509](http://arxiv.org/abs/2507.15509)|null|
|**2025-07-21**|**One Last Attention for Your Vision-Language Model**|Zhiqiang Shen Team|[2507.15480](http://arxiv.org/abs/2507.15480)|null|
|**2025-07-21**|**EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent**|Xinlei Chen Team|[2507.15428](http://arxiv.org/abs/2507.15428)|null|
|**2025-07-21**|**In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems**|Christoph Busch Team|[2507.15285](http://arxiv.org/abs/2507.15285)|null|
|**2025-07-21**|**VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving**|Tong Heng Lee Team|[2507.15266](http://arxiv.org/abs/2507.15266)|null|
|**2025-07-20**|**Survey of GenAI for Automotive Software Development: From Requirements to Executable Code**|Alois Knoll Team|[2507.15025](http://arxiv.org/abs/2507.15025)|null|
|**2025-07-20**|**Hierarchical Cross-modal Prompt Learning for Vision-Language Models**|Zhenhua Huang Team|[2507.14976](http://arxiv.org/abs/2507.14976)|null|
|**2025-07-20**|**FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models**|Mengnan Du Team|[2507.14823](http://arxiv.org/abs/2507.14823)|null|
|**2025-07-19**|**IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark**|Ruiheng Zhang Team|[2507.14449](http://arxiv.org/abs/2507.14449)|null|
|**2025-07-18**|**CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation**|Nicolas Thome Team|[2507.14312](http://arxiv.org/abs/2507.14312)|null|
|**2025-07-18**|**In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding**|Leonid Sigal Team|[2507.14298](http://arxiv.org/abs/2507.14298)|null|
|**2025-07-18**|**VLA-Mark: A cross modal watermark for large vision-language alignment model**|Xuming Hu Team|[2507.14067](http://arxiv.org/abs/2507.14067)|null|
|**2025-07-18**|**EdgeVLA: Efficient Vision-Language-Action Models**|Benjamin Bolte Team|[2507.14049](http://arxiv.org/abs/2507.14049)|null|
|**2025-07-18**|**Moodifier: MLLM-Enhanced Emotion-Driven Image Editing**|Sharon X. Huang Team|[2507.14024](http://arxiv.org/abs/2507.14024)|null|
|**2025-07-18**|**When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models**|Alberto Cazzaniga Team|[2507.13868](http://arxiv.org/abs/2507.13868)|null|
|**2025-07-18**|**Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions**|Jiajun Zhang Team|[2507.13773](http://arxiv.org/abs/2507.13773)|null|
|**2025-07-17**|**LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning**|Margrit Betke Team|[2507.13568](http://arxiv.org/abs/2507.13568)|null|
|**2025-07-17**|**COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark**|Vasu Sharma Team|[2507.13405](http://arxiv.org/abs/2507.13405)|null|
|**2025-07-17**|**VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning**|Jiaya Jia Team|[2507.13348](http://arxiv.org/abs/2507.13348)|null|
|**2025-07-17**|**Leveraging Language Prior for Infrared Small Target Detection**|Pravendra Singh Team|[2507.13113](http://arxiv.org/abs/2507.13113)|null|
|**2025-07-17**|**GLAD: Generalizable Tuning for Vision-Language Models**|Shifeng Chen Team|[2507.13089](http://arxiv.org/abs/2507.13089)|null|
|**2025-07-17**|**Advancing Complex Wide-Area Scene Understanding with Hierarchical Coresets Selection**|Changwen Zheng Team|[2507.13061](http://arxiv.org/abs/2507.13061)|null|
|**2025-07-21**|**LaViPlan : Language-Guided Visual Path Planning with RLVR**|Hayeon Oh Team|[2507.12911](http://arxiv.org/abs/2507.12911)|null|
|**2025-07-17**|**City-VLM: Towards Multidomain Perception Scene Understanding via Multimodal Incomplete Learning**|Xiaowen Chu Team|[2507.12795](http://arxiv.org/abs/2507.12795)|null|
|**2025-07-16**|**VLMgineer: Vision Language Models as Robotic Toolsmiths**|Dinesh Jayaraman Team|[2507.12644](http://arxiv.org/abs/2507.12644)|null|
|**2025-07-16**|**NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting**|Chaoli Wang Team|[2507.12621](http://arxiv.org/abs/2507.12621)|null|
|**2025-07-16**|**MindJourney: Test-Time Scaling with World Models for Spatial Reasoning**|Chuang Gan Team|[2507.12508](http://arxiv.org/abs/2507.12508)|null|
|**2025-07-16**|**ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving**|Xinge Zhu Team|[2507.12499](http://arxiv.org/abs/2507.12499)|null|
|**2025-07-15**|**Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering**|Dimosthenis Karatzas Team|[2507.12490](http://arxiv.org/abs/2507.12490)|null|
|**2025-07-20**|**PhysX-3D: Physical-Grounded 3D Asset Generation**|Ziwei Liu Team|[2507.12465](http://arxiv.org/abs/2507.12465)|null|
|**2025-07-16**|**Describe Anything Model for Visual Question Answering on Text-rich Images**|Min Xu Team|[2507.12441](http://arxiv.org/abs/2507.12441)|null|
|**2025-07-16**|**AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models**|Sihao Ding Team|[2507.12414](http://arxiv.org/abs/2507.12414)|null|
|**2025-07-16**|**Generate to Ground: Multimodal Text Conditioning Boosts Phrase Grounding in Medical Vision-Language Models**|Bernhard Kainz Team|[2507.12236](http://arxiv.org/abs/2507.12236)|null|
|**2025-07-16**|**InstructFLIP: Exploring Unified Vision-Language Model for Face Anti-spoofing**|Wen-Huang Cheng Team|[2507.12060](http://arxiv.org/abs/2507.12060)|null|
|**2025-07-16**|**GS-Bias: Global-Spatial Bias Learner for Single-Image Test-Time Adaptation of Vision-Language Models**|Rongrong Ji Team|[2507.11969](http://arxiv.org/abs/2507.11969)|null|
|**2025-07-16**|**POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering**|Qin Jin Team|[2507.11939](http://arxiv.org/abs/2507.11939)|null|
|**2025-07-15**|**Seeing the Signs: A Survey of Edge-Deployable OCR Models for Billboard Visibility Analysis**|Lihang Ying Team|[2507.11730](http://arxiv.org/abs/2507.11730)|null|
|**2025-07-18**|**How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study**|Rossella Arcucci Team|[2507.11200](http://arxiv.org/abs/2507.11200)|null|
|**2025-07-15**|**Bridging the Gap in Vision Language Models in Identifying Unsafe Concepts Across Modalities**|Yang Zhang Team|[2507.11155](http://arxiv.org/abs/2507.11155)|null|
|**2025-07-15**|**Assessing Color Vision Test in Large Vision-language Models**|Hongyang Chen Team|[2507.11153](http://arxiv.org/abs/2507.11153)|null|
|**2025-07-15**|**MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models**|Hamza Moustafa Team|[2507.11114](http://arxiv.org/abs/2507.11114)|null|
|**2025-07-15**|**Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander**|Lei Chen Team|[2507.11079](http://arxiv.org/abs/2507.11079)|null|
|**2025-07-15**|**Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection**|Guanzhong Tian Team|[2507.11003](http://arxiv.org/abs/2507.11003)|null|
|**2025-07-14**|**EmbRACE-3K: Embodied Reasoning and Action in Complex Environments**|Xiaojuan Qi Team|[2507.10548](http://arxiv.org/abs/2507.10548)|null|
|**2025-07-14**|**CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding**|Yi Wang Team|[2507.10449](http://arxiv.org/abs/2507.10449)|null|
|**2025-07-14**|**Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter**|Bin Luo Team|[2507.10355](http://arxiv.org/abs/2507.10355)|null|
|**2025-07-14**|**Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection**|Wenqiang Zhang Team|[2507.10225](http://arxiv.org/abs/2507.10225)|null|
|**2025-07-14**|**BlueGlass: A Framework for Composite AI Safety**|Kay-Ulrich Scholl Team|[2507.10106](http://arxiv.org/abs/2507.10106)|null|
|**2025-07-14**|**Foundation Model Driven Robotics: A Comprehensive Review**|Ammar Waheed Team|[2507.10087](http://arxiv.org/abs/2507.10087)|null|
|**2025-07-14**|**LayLens: Improving Deepfake Understanding through Simplified Explanations**|Abhinav Dhall Team|[2507.10066](http://arxiv.org/abs/2507.10066)|null|
|**2025-07-14**|**CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books**|Dimosthenis Karatzas Team|[2507.10053](http://arxiv.org/abs/2507.10053)|null|
|**2025-07-14**|**Text-Driven Causal Representation Learning for Source-Free Domain Generalization**|Zhen Lei Team|[2507.09961](http://arxiv.org/abs/2507.09961)|null|
|**2025-07-13**|**NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection**|Pulei Xiong Team|[2507.09795](http://arxiv.org/abs/2507.09795)|null|
|**2025-07-13**|**Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score**|Muhammad Haris Khan Team|[2507.09615](http://arxiv.org/abs/2507.09615)|null|
|**2025-07-13**|**Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations**|Guiguang Ding Team|[2507.09500](http://arxiv.org/abs/2507.09500)|null|
|**2025-07-13**|**GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?**|Huaxiu Yao Team|[2507.09491](http://arxiv.org/abs/2507.09491)|null|
|**2025-07-12**|**Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models**|Tat-Seng Chua Team|[2507.09209](http://arxiv.org/abs/2507.09209)|null|
|**2025-07-12**|**MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models**|Dahan Wang Team|[2507.09184](http://arxiv.org/abs/2507.09184)|null|
|**2025-07-12**|**OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering**|Niaz Abdolrahim Team|[2507.09155](http://arxiv.org/abs/2507.09155)|null|
|**2025-07-12**|**RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze**|Honghan Wu Team|[2507.09097](http://arxiv.org/abs/2507.09097)|null|
|**2025-07-11**|**BlindSight: Harnessing Sparsity for Efficient VLMs**|Steven K. Reinhardt Team|[2507.09071](http://arxiv.org/abs/2507.09071)|null|
|**2025-07-11**|**Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery**|Seana Coulson Team|[2507.09011](http://arxiv.org/abs/2507.09011)|null|
|**2025-07-11**|**VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models**|Olivier Déforges Team|[2507.08982](http://arxiv.org/abs/2507.08982)|null|
|**2025-07-11**|**ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way**|Subarna Tripathi Team|[2507.08679](http://arxiv.org/abs/2507.08679)|null|
|**2025-07-11**|**Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance**|András Lőrincz Team|[2507.08624](http://arxiv.org/abs/2507.08624)|null|
|**2025-07-11**|**Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data**|Ambedkar Dukkipati Team|[2507.08610](http://arxiv.org/abs/2507.08610)|null|
|**2025-07-11**|**BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis**|Hui Xiong Team|[2507.08607](http://arxiv.org/abs/2507.08607)|null|
|**2025-07-11**|**Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R**|Sanidhya Kashyap Team|[2507.08505](http://arxiv.org/abs/2507.08505)|null|
|**2025-07-11**|**LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning**|Lei Fan Team|[2507.08496](http://arxiv.org/abs/2507.08496)|null|
|**2025-07-11**|**Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models**|Jianping Fan Team|[2507.08410](http://arxiv.org/abs/2507.08410)|null|
|**2025-07-11**|**Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning**|Yejin Choi Team|[2507.08224](http://arxiv.org/abs/2507.08224)|null|
|**2025-07-10**|**CLIP Won't Learn Object-Attribute Binding from Natural Data and Here is Why**|Thomas Brox Team|[2507.07985](http://arxiv.org/abs/2507.07985)|null|
|**2025-07-10**|**Scaling RL to Long Videos**|Song Han Team|[2507.07966](http://arxiv.org/abs/2507.07966)|null|
|**2025-07-10**|**SAGE: A Visual Language Model for Anomaly Detection via Fact Enhancement and Entropy-aware Alignment**|Lei Fan Team|[2507.07939](http://arxiv.org/abs/2507.07939)|null|
|**2025-07-10**|**MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving**|Chao Zhang Team|[2507.07818](http://arxiv.org/abs/2507.07818)|null|
|**2025-07-10**|**Energy-Guided Decoding for Object Hallucination Mitigation**|Christopher Zach Team|[2507.07731](http://arxiv.org/abs/2507.07731)|null|
|**2025-07-10**|**One Object, Multiple Lies: A Benchmark for Cross-task Adversarial Attack on Unified Vision-Language Models**|Cairong Zhao Team|[2507.07709](http://arxiv.org/abs/2507.07709)|null|
|**2025-07-10**|**Rationale-Enhanced Decoding for Multi-modal Chain-of-Thought**|Daiki Chijiwa Team|[2507.07685](http://arxiv.org/abs/2507.07685)|null|
|**2025-07-11**|**ViLU: Learning Vision-Language Uncertainties for Failure Prediction**|Nicolas Thome Team|[2507.07620](http://arxiv.org/abs/2507.07620)|null|
|**2025-07-10**|**LOSC: LiDAR Open-voc Segmentation Consolidator**|Renaud Marlet Team|[2507.07605](http://arxiv.org/abs/2507.07605)|null|
|**2025-07-10**|**The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs**|Qun Liu Team|[2507.07562](http://arxiv.org/abs/2507.07562)|null|
|**2025-07-10**|**ArchiveGPT: A human-centered evaluation of using a vision language model for image cataloguing**|Markus Huff Team|[2507.07551](http://arxiv.org/abs/2507.07551)|null|
|**2025-07-11**|**Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning**|David Martins de Matos Team|[2507.07340](http://arxiv.org/abs/2507.07340)|null|
|**2025-07-09**|**ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation**|Suren Kumar Team|[2507.07317](http://arxiv.org/abs/2507.07317)|null|
|**2025-07-09**|**LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation**|Angel X. Chang Team|[2507.07299](http://arxiv.org/abs/2507.07299)|null|
|**2025-07-09**|**MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning**|Dan Goldwasser Team|[2507.07297](http://arxiv.org/abs/2507.07297)|null|
|**2025-07-09**|**4KAgent: Agentic Any Image to 4K Super-Resolution**|Zhengzhong Tu Team|[2507.07105](http://arxiv.org/abs/2507.07105)|null|
|**2025-07-14**|**Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models**|Junfei Xiao Team|[2507.07104](http://arxiv.org/abs/2507.07104)|**[link](https://lambert-x.github.io/Vision-Language-Vision/)**|
|**2025-07-09**|**Evaluating Attribute Confusion in Fashion Text-to-Image Generation**|Davide Talon Team|[2507.07079](http://arxiv.org/abs/2507.07079)|null|
|**2025-07-09**|**Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM**|Sibei Yang Team|[2507.06973](http://arxiv.org/abs/2507.06973)|null|
|**2025-07-09**|**CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale**|Quan Wang Team|[2507.06959](http://arxiv.org/abs/2507.06959)|null|
|**2025-07-09**|**VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation**|Tat-Seng Chua Team|[2507.06899](http://arxiv.org/abs/2507.06899)|null|
|**2025-07-09**|**HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement**|Yanning Zhang Team|[2507.06814](http://arxiv.org/abs/2507.06814)|null|
|**2025-07-09**|**Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu**|Donghyeok Choi Team|[2507.06761](http://arxiv.org/abs/2507.06761)|null|
|**2025-07-09**|**Text-promptable Object Counting via Quantity Awareness Enhancement**|Li Li Team|[2507.06679](http://arxiv.org/abs/2507.06679)|null|
|**2025-07-09**|**Cross-Modal Dual-Causal Learning for Long-Term Action Recognition**|Fan Chao Team|[2507.06603](http://arxiv.org/abs/2507.06603)|null|
|**2025-07-09**|**Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection**|Xiangmin Xu Team|[2507.06510](http://arxiv.org/abs/2507.06510)|null|
|**2025-07-09**|**3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds**|Nick Haber Team|[2507.06484](http://arxiv.org/abs/2507.06484)|null|
|**2025-07-08**|**VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic**|Andreas A. Malikopoulos Team|[2507.06441](http://arxiv.org/abs/2507.06441)|null|
|**2025-07-08**|**CultureCLIP: Empowering CLIP with Cultural Awareness through Synthetic Images and Contextualized Captions**|Yi R. Fung Team|[2507.06210](http://arxiv.org/abs/2507.06210)|null|
|**2025-07-08**|**Enhancing Scientific Visual Question Answering through Multimodal Reasoning and Ensemble Modeling**|Naga Harshita Marupaka Team|[2507.06183](http://arxiv.org/abs/2507.06183)|null|
|**2025-07-10**|**Skywork-R1V3 Technical Report**|Yahui Zhou Team|[2507.06167](http://arxiv.org/abs/2507.06167)|null|
|**2025-07-08**|**LangMamba: A Language-driven Mamba Framework for Low-dose CT Denoising with Vision-language Models**|Hongming Shan Team|[2507.06140](http://arxiv.org/abs/2507.06140)|null|
|**2025-07-08**|**GeoMag: A Vision-Language Model for Pixel-level Fine-Grained Remote Sensing Image Parsing**|Hao Liu Team|[2507.05887](http://arxiv.org/abs/2507.05887)|null|
|**2025-07-08**|**Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports**|Hitomi Yanaka Team|[2507.05799](http://arxiv.org/abs/2507.05799)|null|
|**2025-07-08**|**SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic Scene Graph Generation with Long- and Local-range Context Reasoning**|Tao He Team|[2507.05798](http://arxiv.org/abs/2507.05798)|null|
|**2025-07-08**|**A Satellite-Ground Synergistic Large Vision-Language Model System for Earth Observation**|Yue Gao Team|[2507.05731](http://arxiv.org/abs/2507.05731)|null|
|**2025-07-09**|**Integrated Structural Prompt Learning for Vision-Language Models**|Bin Luo Team|[2507.05677](http://arxiv.org/abs/2507.05677)|null|
|**2025-07-08**|**R-VLM: Region-Aware Vision Language Model for Precise GUI Grounding**|Shabnam Ghadar Team|[2507.05673](http://arxiv.org/abs/2507.05673)|null|
|**2025-07-08**|**Dynamic Rank Adaptation for Vision-Language Models**|Bin Luo Team|[2507.05668](http://arxiv.org/abs/2507.05668)|null|
|**2025-07-08**|**Structured Task Solving via Modular Embodied Intelligence: A Case Study on Rubik's Cube**|Shenghai Yuan Team|[2507.05607](http://arxiv.org/abs/2507.05607)|null|
|**2025-07-08**|**Rethinking Layered Graphic Design Generation with a Top-Down Approach**|Qifeng Chen Team|[2507.05601](http://arxiv.org/abs/2507.05601)|null|
|**2025-07-08**|**PaddleOCR 3.0 Technical Report**|Yanjun Ma Team|[2507.05595](http://arxiv.org/abs/2507.05595)|null|
|**2025-07-07**|**Fine-Grained Vision-Language Modeling for Multimodal Training Assistants in Augmented Reality**|Junxiao Wang Team|[2507.05515](http://arxiv.org/abs/2507.05515)|null|
|**2025-07-07**|**Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model**|Even Oldridge Team|[2507.05513](http://arxiv.org/abs/2507.05513)|null|
|**2025-07-07**|**OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts**|Priyadarshini Panda Team|[2507.05427](http://arxiv.org/abs/2507.05427)|null|
|**2025-07-07**|**pFedMMA: Personalized Federated Fine-Tuning with Multi-Modal Adapter for Vision-Language Models**|Ramtin Pedarsani Team|[2507.05394](http://arxiv.org/abs/2507.05394)|null|
|**2025-07-07**|**NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving**|Cheng Lu Team|[2507.05227](http://arxiv.org/abs/2507.05227)|null|
|**2025-07-07**|**All in One: Visual-Description-Guided Unified Point Cloud Segmentation**|Rao Muhammad Anwer Team|[2507.05211](http://arxiv.org/abs/2507.05211)|null|
|**2025-07-07**|**Differential Attention for Multimodal Crisis Event Analysis**|Abdullah-Al-Zubaer Imran Team|[2507.05165](http://arxiv.org/abs/2507.05165)|null|
|**2025-07-07**|**INTER: Mitigating Hallucination in Large Vision-Language Models by Interaction Guidance Sampling**|Bo Zheng Team|[2507.05056](http://arxiv.org/abs/2507.05056)|null|
|**2025-07-07**|**Adaptation of Multi-modal Representation Models for Multi-task Surgical Computer Vision**|Nicolas Padoy Team|[2507.05020](http://arxiv.org/abs/2507.05020)|null|
|**2025-07-07**|**Training-free Generation of Temporally Consistent Rewards from VLMs**|Jian Tang Team|[2507.04789](http://arxiv.org/abs/2507.04789)|null|
|**2025-07-07**|**Vision-Language Models Can't See the Obvious**|Sanath Narayan Team|[2507.04741](http://arxiv.org/abs/2507.04741)|null|
|**2025-07-07**|**An analysis of vision-language models for fabric retrieval**|Fabio Poiesi Team|[2507.04735](http://arxiv.org/abs/2507.04735)|null|
|**2025-07-07**|**A Visual Leap in CLIP Compositionality Reasoning through Generation of Counterfactual Sets**|Jie Zhou Team|[2507.04699](http://arxiv.org/abs/2507.04699)|null|
|**2025-07-07**|**MOSU: Autonomous Long-range Robot Navigation with Multi-modal Scene Understanding**|Dinesh Manocha Team|[2507.04686](http://arxiv.org/abs/2507.04686)|null|
|**2025-07-07**|**Identify, Isolate, and Purge: Mitigating Hallucinations in LVLMs via Self-Evolving Distillation**|Chang Xu Team|[2507.04680](http://arxiv.org/abs/2507.04680)|null|
|**2025-07-06**|**VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust Long-Horizon Manipulation**|Lei Han Team|[2507.04524](http://arxiv.org/abs/2507.04524)|null|
|**2025-07-08**|**FA: Forced Prompt Learning of Vision-Language Models for Out-of-Distribution Detection**|Ruixuan Wang Team|[2507.04511](http://arxiv.org/abs/2507.04511)|null|
|**2025-07-06**|**MVL-Loc: Leveraging Vision-Language Model for Generalizable Multi-Scene Camera Relocalization**|Changhao Chen Team|[2507.04509](http://arxiv.org/abs/2507.04509)|null|
|**2025-07-06**|**Think Twice Before You Judge: Mixture of Dual Reasoning Experts for Multimodal Sarcasm Detection**|Sanasam Ranbir Singh Team|[2507.04458](http://arxiv.org/abs/2507.04458)|null|
|**2025-07-06**|**Multi-Modal Semantic Parsing for the Interpretation of Tombstone Inscriptions**|Johan Bos Team|[2507.04377](http://arxiv.org/abs/2507.04377)|null|
|**2025-07-05**|**LVLM-Composer's Explicit Planning for Image Generation**|Amina Grant Team|[2507.04152](http://arxiv.org/abs/2507.04152)|null|
|**2025-07-05**|**Unlocking Compositional Control: Self-Supervision for LVLM-Based Image Generation**|Hunter Young Team|[2507.04151](http://arxiv.org/abs/2507.04151)|null|
|**2025-07-05**|**PresentAgent: Multimodal Agent for Presentation Video Generation**|Yang Zhao Team|[2507.04036](http://arxiv.org/abs/2507.04036)|null|
|**2025-07-05**|**A Comparative Study of Specialized LLMs as Dense Retrievers**|Jiafeng Guo Team|[2507.03958](http://arxiv.org/abs/2507.03958)|null|
|**2025-07-03**|**ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects**|Cewu Lu Team|[2507.02600](http://arxiv.org/abs/2507.02600)|null|
|**2025-07-02**|**cVLA: Towards Efficient Camera-Space VLAs**|Thomas Brox Team|[2507.02190](http://arxiv.org/abs/2507.02190)|null|
|**2025-07-02**|**Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges**|Anuj Sharma Team|[2507.02074](http://arxiv.org/abs/2507.02074)|null|
|**2025-07-01**|**Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames**|Cordelia Schmid Team|[2507.02001](http://arxiv.org/abs/2507.02001)|null|
|**2025-07-02**|**How Do Vision-Language Models Process Conflicting Information Across Modalities?**|Ellie Pavlick Team|[2507.01790](http://arxiv.org/abs/2507.01790)|null|
|**2025-07-02**|**Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition**|Muzammil Behzad Team|[2507.01673](http://arxiv.org/abs/2507.01673)|null|
|**2025-07-02**|**MARVIS: Modality Adaptive Reasoning over VISualizations**|Chinmay Hegde Team|[2507.01544](http://arxiv.org/abs/2507.01544)|null|
|**2025-07-02**|**Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence**|Martin Schramm Team|[2507.01504](http://arxiv.org/abs/2507.01504)|null|
|**2025-07-02**|**BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments**|Mingzhai Sun Team|[2507.01485](http://arxiv.org/abs/2507.01485)|null|
|**2025-07-03**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Yanwei Fu Team|[2507.01424](http://arxiv.org/abs/2507.01424)|null|
|**2025-07-02**|**CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning**|Yoshitaka Ushiku Team|[2507.01409](http://arxiv.org/abs/2507.01409)|null|
|**2025-07-02**|**Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model**|Xi Li Team|[2507.01351](http://arxiv.org/abs/2507.01351)|null|
|**2025-07-02**|**AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation**|Jiawei Zhang Team|[2507.01255](http://arxiv.org/abs/2507.01255)|null|
|**2025-07-02**|**GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning**|Jie Tang Team|[2507.01006](http://arxiv.org/abs/2507.01006)|null|
|**2025-07-04**|**Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations**|Yunzhu Li Team|[2507.00990](http://arxiv.org/abs/2507.00990)|null|
|**2025-07-01**|**Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact**|Seyedali Mirjalili Team|[2507.00951](http://arxiv.org/abs/2507.00951)|null|
|**2025-07-01**|**The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses**|Fabio Correa Xavier Team|[2507.00907](http://arxiv.org/abs/2507.00907)|null|
|**2025-07-01**|**ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models**|Yaqi Xie Team|[2507.00898](http://arxiv.org/abs/2507.00898)|null|
|**2025-07-01**|**GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond**|Luc Van Gool Team|[2507.00886](http://arxiv.org/abs/2507.00886)|null|
|**2025-07-01**|**UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement**|Xiangxiang Chu Team|[2507.00721](http://arxiv.org/abs/2507.00721)|null|
|**2025-07-01**|**Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English**|Rajesh Sharma Team|[2507.00700](http://arxiv.org/abs/2507.00700)|null|
|**2025-07-01**|**Context-Aware Academic Emotion Dataset and Benchmark**|Wenwu Yang Team|[2507.00586](http://arxiv.org/abs/2507.00586)|null|
|**2025-07-01**|**Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation**|Rong Xiao Team|[2507.00537](http://arxiv.org/abs/2507.00537)|null|
|**2025-07-01**|**Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving**|Yadan Luo Team|[2507.00525](http://arxiv.org/abs/2507.00525)|null|
|**2025-06-30**|**EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations**|Sungzoon Cho Team|[2506.24016](http://arxiv.org/abs/2506.24016)|null|
|**2025-06-30**|**The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models**|Tieniu Tan Team|[2506.24000](http://arxiv.org/abs/2506.24000)|null|
|**2025-06-30**|**GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models**|Hassan Rivaz Team|[2506.23903](http://arxiv.org/abs/2506.23903)|null|
|**2025-06-30**|**A Closer Look at Conditional Prompt Tuning for Vision-Language Models**|Heng Tao Shen Team|[2506.23856](http://arxiv.org/abs/2506.23856)|null|
|**2025-06-30**|**Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model**|Fahad Shahbaz Khan Team|[2506.23822](http://arxiv.org/abs/2506.23822)|null|
|**2025-06-30**|**Visual Textualization for Image Prompted Object Detection**|Yan Xu Team|[2506.23785](http://arxiv.org/abs/2506.23785)|null|
|**2025-06-30**|**PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?**|Ransalu Senanayake Team|[2506.23725](http://arxiv.org/abs/2506.23725)|null|
|**2025-06-30**|**On the Domain Robustness of Contrastive Vision-Language Models**|Erik Rodner Team|[2506.23663](http://arxiv.org/abs/2506.23663)|null|
|**2025-06-30**|**CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models**|Bing Qin Team|[2506.23590](http://arxiv.org/abs/2506.23590)|null|
|**2025-06-30**|**A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation**|Jie Xu Team|[2506.23584](http://arxiv.org/abs/2506.23584)|null|
|**2025-07-01**|**ZonUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding**|ShengJing Yang Team|[2506.23491](http://arxiv.org/abs/2506.23491)|null|
|**2025-06-30**|**Sanitizing Manufacturing Dataset Labels Using Vision-Language Models**|Vinh Nguyen Team|[2506.23465](http://arxiv.org/abs/2506.23465)|null|
|**2025-06-29**|**GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields**|Yutaka Matsuo Team|[2506.23352](http://arxiv.org/abs/2506.23352)|null|
|**2025-06-29**|**IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering**|Brandon Y. Feng Team|[2506.23329](http://arxiv.org/abs/2506.23329)|null|
|**2025-07-01**|**SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting**|Hongliang Ren Team|[2506.23309](http://arxiv.org/abs/2506.23309)|null|
|**2025-06-29**|**Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models**|Tanmoy Chakraborty Team|[2506.23122](http://arxiv.org/abs/2506.23122)|null|
|**2025-06-29**|**MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings**|Zhicheng Dou Team|[2506.23115](http://arxiv.org/abs/2506.23115)|null|
|**2025-06-29**|**Empowering Small VLMs to Think with Dynamic Memorization and Exploration**|Long Chen Team|[2506.23061](http://arxiv.org/abs/2506.23061)|null|
|**2025-06-29**|**SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions**|Maarten Sap Team|[2506.23046](http://arxiv.org/abs/2506.23046)|null|
|**2025-06-28**|**Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models**|Swadesh Swain Team|[2506.22982](http://arxiv.org/abs/2506.22982)|null|
|**2025-06-27**|**MiCo: Multi-image Contrast for Reinforcement Visual Reasoning**|Hengshuang Zhao Team|[2506.22434](http://arxiv.org/abs/2506.22434)|null|
|**2025-06-27**|**Test-Time Consistency in Vision Language Models**|Leonid Sigal Team|[2506.22395](http://arxiv.org/abs/2506.22395)|null|
|**2025-06-27**|**Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation**|Xun Xu Team|[2506.22375](http://arxiv.org/abs/2506.22375)|null|
|**2025-06-27**|**Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment**|Bo Du Team|[2506.22283](http://arxiv.org/abs/2506.22283)|null|
|**2025-06-27**|**COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication**|Albert Gatt Team|[2506.22274](http://arxiv.org/abs/2506.22274)|null|
|**2025-06-27**|**Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs**|Mahdieh Soleymani Baghshah Team|[2506.22146](http://arxiv.org/abs/2506.22146)|null|
|**2025-06-27**|**Universal Retrieval for Multimodal Trajectory Modeling**|Dehan Kong Team|[2506.22056](http://arxiv.org/abs/2506.22056)|null|
|**2025-06-27**|**Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation**|Daisuke Deguchi Team|[2506.22032](http://arxiv.org/abs/2506.22032)|null|
|**2025-06-27**|**SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation**|Xulei Yang Team|[2506.21892](http://arxiv.org/abs/2506.21892)|null|
|**2025-06-27**|**Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles**|Matthew J. Barth Team|[2506.21885](http://arxiv.org/abs/2506.21885)|null|
|**2025-06-27**|**Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation**|Zhiting Hu Team|[2506.21876](http://arxiv.org/abs/2506.21876)|null|
|**2025-06-27**|**On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling**|Ben Y. Zhao Team|[2506.21874](http://arxiv.org/abs/2506.21874)|null|
|**2025-06-27**|**Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling**|Yong Man Ro Team|[2506.21863](http://arxiv.org/abs/2506.21863)|null|
|**2025-06-27**|**Embodied Domain Adaptation for Object Detection**|Feras Dayoub Team|[2506.21860](http://arxiv.org/abs/2506.21860)|null|
|**2025-06-27**|**The Cost of Avoiding Backpropagation**|Hui Guan Team|[2506.21833](http://arxiv.org/abs/2506.21833)|null|
|**2025-06-26**|**ViStruct: Simulating Expert-Like Reasoning Through Task Decomposition and Visual Attention Cues**|Carolina Nobre Team|[2506.21762](http://arxiv.org/abs/2506.21762)|null|
|**2025-06-26**|**Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs**|Ismini Lourentzou Team|[2506.21656](http://arxiv.org/abs/2506.21656)|null|
|**2025-06-26**|**Mitigating Hallucination of Large Vision-Language Models via Dynamic Logits Calibration**|Jian Wu Team|[2506.21509](http://arxiv.org/abs/2506.21509)|null|
|**2025-06-26**|**Global and Local Entailment Learning for Natural World Imagery**|Nathan Jacobs Team|[2506.21476](http://arxiv.org/abs/2506.21476)|null|
|**2025-06-26**|**Spatial Mental Modeling from Limited Views**|Li Fei-Fei Team|[2506.21458](http://arxiv.org/abs/2506.21458)|null|
|**2025-06-27**|**ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models**|Ziwei Liu Team|[2506.21356](http://arxiv.org/abs/2506.21356)|null|
|**2025-06-26**|**LLaVA-Pose: Enhancing Human Pose and Action Understanding via Keypoint-Integrated Instruction Tuning**|Hayaru Shouno Team|[2506.21317](http://arxiv.org/abs/2506.21317)|null|
|**2025-06-26**|**DrishtiKon: Multi-Granular Visual Grounding for Text-Rich Document Images**|Ganesh Ramakrishnan Team|[2506.21316](http://arxiv.org/abs/2506.21316)|null|
|**2025-06-26**|**World-aware Planning Narratives Enhance Large Vision-Language Model Planner**|Xipeng QIu Team|[2506.21230](http://arxiv.org/abs/2506.21230)|null|
|**2025-06-26**|**Personalized Federated Learning via Dual-Prompt Optimization and Cross Fusion**|Jian Liang Team|[2506.21144](http://arxiv.org/abs/2506.21144)|null|
|**2025-06-26**|**V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling**|Bin Ran Team|[2506.21041](http://arxiv.org/abs/2506.21041)|null|
|**2025-06-26**|**Multimodal Prompt Alignment for Facial Expression Recognition**|Shutao Li Team|[2506.21017](http://arxiv.org/abs/2506.21017)|null|
|**2025-06-26**|**Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology**|S Kevin Zhou Team|[2506.21001](http://arxiv.org/abs/2506.21001)|null|
|**2025-06-26**|**TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation**|Yihong Wu Team|[2506.20991](http://arxiv.org/abs/2506.20991)|null|
|**2025-06-26**|**SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes**|Zheng Zhang Team|[2506.20990](http://arxiv.org/abs/2506.20990)|null|
|**2025-06-26**|**Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends**|Zeng-Guang Hou Team|[2506.20966](http://arxiv.org/abs/2506.20966)|null|
|**2025-06-26**|**E-FreeM2: Efficient Training-Free Multi-Scale and Cross-Modal News Verification via MLLMs**|Minh-Son Dao Team|[2506.20944](http://arxiv.org/abs/2506.20944)|null|
|**2025-06-25**|**Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models**|Zafer Dogan Team|[2506.20832](http://arxiv.org/abs/2506.20832)|null|
|**2025-06-25**|**How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?**|Bastian Leibe Team|[2506.20795](http://arxiv.org/abs/2506.20795)|null|
|**2025-06-27**|**Shape2Animal: Creative Animal Generation from Natural Silhouettes**|Trung-Nghia Le Team|[2506.20616](http://arxiv.org/abs/2506.20616)|null|
|**2025-06-25**|**HRIBench: Benchmarking Vision-Language Models for Real-Time Human Perception in Human-Robot Interaction**|Maja Matarić Team|[2506.20566](http://arxiv.org/abs/2506.20566)|null|
|**2025-06-25**|**Med-Art: Diffusion Transformer for 2D Medical Text-to-Image Generation**|Morten Rieger Hannemose Team|[2506.20449](http://arxiv.org/abs/2506.20449)|null|
|**2025-06-25**|**CARMA: Context-Aware Situational Grounding of Human-Robot Group Interactions by Combining Vision-Language Models with Object and Action Recognition**|Michael Gienger Team|[2506.20373](http://arxiv.org/abs/2506.20373)|null|
|**2025-06-25**|**Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards**|Bo Zheng Team|[2506.20332](http://arxiv.org/abs/2506.20332)|null|
|**2025-06-25**|**MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations**|Vikram S. Adve Team|[2506.20100](http://arxiv.org/abs/2506.20100)|null|
|**2025-06-24**|**Unified Vision-Language-Action Model**|Zhaoxiang Zhang Team|[2506.19850](http://arxiv.org/abs/2506.19850)|null|
|**2025-06-24**|**Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models**|Christoph M. Friedrich Team|[2506.19825](http://arxiv.org/abs/2506.19825)|null|
|**2025-06-24**|**CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation**|Jiangmiao Pang Team|[2506.19816](http://arxiv.org/abs/2506.19816)|null|
|**2025-06-24**|**UltraAD: Fine-Grained Ultrasound Anomaly Classification via Few-Shot CLIP Adaptation**|Zhongliang Jiang Team|[2506.19694](http://arxiv.org/abs/2506.19694)|null|
|**2025-06-24**|**PEVLM: Parallel Encoding for Vision-Language Models**|Yong Wu Team|[2506.19651](http://arxiv.org/abs/2506.19651)|null|
|**2025-06-24**|**V2T-CoT: From Vision to Text Chain-of-Thought for Medical Reasoning and Diagnosis**|Zuozhu Liu Team|[2506.19610](http://arxiv.org/abs/2506.19610)|null|
|**2025-06-24**|**ChordPrompt: Orchestrating Cross-Modal Prompt Synergy for Multi-Domain Incremental Learning in CLIP**|Bokui Chen Team|[2506.19608](http://arxiv.org/abs/2506.19608)|null|
|**2025-06-24**|**Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects**|Angelo Cangelosi Team|[2506.19579](http://arxiv.org/abs/2506.19579)|null|
|**2025-06-24**|**Visual hallucination detection in large vision-language models via evidential conflict**|Liping Jing Team|[2506.19513](http://arxiv.org/abs/2506.19513)|null|
|**2025-06-24**|**T-Rex: Task-Adaptive Spatial Representation Extraction for Robotic Manipulation with Vision-Language Models**|Qingyao Wu Team|[2506.19498](http://arxiv.org/abs/2506.19498)|null|
|**2025-06-24**|**Emergence of Text Readability in Vision Language Models**|Bohyung Han Team|[2506.19389](http://arxiv.org/abs/2506.19389)|null|
|**2025-06-24**|**Robotic Perception with a Large Tactile-Vision-Language Model for Physical Property Inference**|Nutan Chen Team|[2506.19303](http://arxiv.org/abs/2506.19303)|null|
|**2025-06-24**|**Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models**|Dan Zeng Team|[2506.19300](http://arxiv.org/abs/2506.19300)|null|
|**2025-06-24**|**Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding**|Hui Xiong Team|[2506.19288](http://arxiv.org/abs/2506.19288)|null|
|**2025-06-24**|**MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models**|Bo Zheng Team|[2506.19257](http://arxiv.org/abs/2506.19257)|null|
|**2025-06-24**|**Scaffolding Dexterous Manipulation with Vision-Language Models**|Dorsa Sadigh Team|[2506.19212](http://arxiv.org/abs/2506.19212)|null|
|**2025-06-23**|**Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition**|Bjoern W. Schuller Team|[2506.19079](http://arxiv.org/abs/2506.19079)|null|
|**2025-06-23**|**HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models**|Krzysztof Czarnecki Team|[2506.19072](http://arxiv.org/abs/2506.19072)|null|
|**2025-06-23**|**GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs**|Guanxi Shen Team|[2506.18985](http://arxiv.org/abs/2506.18985)|null|
|**2025-06-23**|**VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning**|Jian Zhang Team|[2506.18564](http://arxiv.org/abs/2506.18564)|null|
|**2025-06-23**|**Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey**|Heng Tao Shen Team|[2506.18504](http://arxiv.org/abs/2506.18504)|null|
|**2025-06-23**|**InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models**|Wenhai Wang Team|[2506.18385](http://arxiv.org/abs/2506.18385)|null|
|**2025-06-23**|**Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review**|Jing Qin Team|[2506.18378](http://arxiv.org/abs/2506.18378)|null|
|**2025-06-23**|**Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?**|Bill Howe Team|[2506.18322](http://arxiv.org/abs/2506.18322)|null|
|**2025-06-24**|**Referring Expression Instance Retrieval and A Strong End-to-End Baseline**|JinQiao Wang Team|[2506.18246](http://arxiv.org/abs/2506.18246)|null|
|**2025-06-23**|**Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning**|Xinhai Zhao Team|[2506.18234](http://arxiv.org/abs/2506.18234)|null|
|**2025-06-22**|**See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis**|Xiaoxiao Li Team|[2506.18140](http://arxiv.org/abs/2506.18140)|null|
|**2025-06-22**|**CLGRPO: Reasoning Ability Enhancement for Small VLMs**|Zhiwang Zhang Team|[2506.18048](http://arxiv.org/abs/2506.18048)|null|
|**2025-06-22**|**Adapting Vision-Language Models for Evaluating World Models**|Sarah Parisot Team|[2506.17967](http://arxiv.org/abs/2506.17967)|null|
|**2025-06-21**|**RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models**|Marco Pavone Team|[2506.17811](http://arxiv.org/abs/2506.17811)|null|
|**2025-06-21**|**MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation**|Xiaochuan Shi Team|[2506.17664](http://arxiv.org/abs/2506.17664)|null|
|**2025-06-21**|**Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning**|Yu-Chiang Frank Wang Team|[2506.17645](http://arxiv.org/abs/2506.17645)|null|
|**2025-06-21**|**CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning**|Xiaoling Wang Team|[2506.17629](http://arxiv.org/abs/2506.17629)|null|
|**2025-06-21**|**DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving**|Zhengzhong Tu Team|[2506.17590](http://arxiv.org/abs/2506.17590)|null|
|**2025-06-21**|**HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models**|Tao He Team|[2506.17587](http://arxiv.org/abs/2506.17587)|null|
|**2025-06-20**|**Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction**|Jose Dolz Team|[2506.17503](http://arxiv.org/abs/2506.17503)|null|
|**2025-06-20**|**Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation**|Ismail Ben Ayed Team|[2506.17500](http://arxiv.org/abs/2506.17500)|null|
|**2025-06-20**|**General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting**|Georgios Georgakis Team|[2506.17462](http://arxiv.org/abs/2506.17462)|null|
|**2025-06-20**|**Aha Moment Revisited: Are VLMs Truly Capable of Self Verification in Inference-time Scaling?**|Klara Nahrstedt Team|[2506.17417](http://arxiv.org/abs/2506.17417)|null|
|**2025-06-20**|**VLN-R1: Vision-Language Navigation via Reinforcement Fine-Tuning**|Hengshuang Zhao Team|[2506.17221](http://arxiv.org/abs/2506.17221)|null|
|**2025-06-20**|**Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens**|Chuang Gan Team|[2506.17218](http://arxiv.org/abs/2506.17218)|null|
|**2025-06-20**|**Do We Need Large VLMs for Spotting Soccer Actions?**|Sandeep Chaurasia Team|[2506.17144](http://arxiv.org/abs/2506.17144)|null|
|**2025-06-20**|**Prmpt2Adpt: Prompt-Based Zero-Shot Domain Adaptation for Resource-Constrained Environments**|Nathaniel D. Bastian Team|[2506.16994](http://arxiv.org/abs/2506.16994)|null|
|**2025-06-20**|**FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation**|Jinqiao Wang Team|[2506.16806](http://arxiv.org/abs/2506.16806)|null|
|**2025-06-20**|**Co-VisiON: Co-Visibility ReasONing on Sparse Image Sets of Indoor Scenes**|Chen Feng Team|[2506.16805](http://arxiv.org/abs/2506.16805)|null|
|**2025-06-20**|**Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models**|Xiaohua Xu Team|[2506.16760](http://arxiv.org/abs/2506.16760)|null|
|**2025-06-20**|**TeSG: Textual Semantic Guidance for Infrared and Visible Image Fusion**|Xinbo Gao Team|[2506.16730](http://arxiv.org/abs/2506.16730)|null|
|**2025-06-20**|**V-CASS: Vision-context-aware Expressive Speech Synthesis for Enhancing User Understanding of Videos**|Xiaoyu Qin Team|[2506.16716](http://arxiv.org/abs/2506.16716)|null|
|**2025-06-20**|**VLM-Empowered Multi-Mode System for Efficient and Safe Planetary Navigation**|Liang Ding Team|[2506.16703](http://arxiv.org/abs/2506.16703)|null|
|**2025-06-20**|**LaVi: Efficient Large Vision-Language Models via Internal Feature Modulation**|Jing Liu Team|[2506.16691](http://arxiv.org/abs/2506.16691)|null|
|**2025-06-19**|**CodeDiffuser: Attention-Enhanced Diffusion Policy via VLM-Generated Code for Instruction Ambiguity**|Yunzhu Li Team|[2506.16652](http://arxiv.org/abs/2506.16652)|null|
|**2025-06-19**|**History-Augmented Vision-Language Models for Frontier-Based Zero-Shot Object Navigation**|Fatemeh Afghah Team|[2506.16623](http://arxiv.org/abs/2506.16623)|null|
|**2025-06-19**|**GoalLadder: Incremental Goal Discovery with Vision-Language Models**|Shimon Whiteson Team|[2506.16396](http://arxiv.org/abs/2506.16396)|null|
|**2025-06-19**|**CLIP-MG: Guiding Semantic Attention with Skeletal Pose Features and RGB Data for Micro-Gesture Recognition on the iMiGUE Dataset**|Amith Adiraju Team|[2506.16385](http://arxiv.org/abs/2506.16385)|null|
|**2025-06-19**|**FOCoOp: Enhancing Out-of-Distribution Robustness in Federated Prompt Learning for Vision-Language Models**|Tat-Seng Chua Team|[2506.16218](http://arxiv.org/abs/2506.16218)|null|
|**2025-06-19**|**AutoV: Learning to Retrieve Visual Prompt for Large Vision-Language Models**|Shanghang Zhang Team|[2506.16112](http://arxiv.org/abs/2506.16112)|null|
|**2025-06-19**|**Stepping Out of Similar Semantic Space for Open-Vocabulary Segmentation**|Yansong Tang Team|[2506.16058](http://arxiv.org/abs/2506.16058)|null|
|**2025-06-19**|**DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning**|Zongqing Lu Team|[2506.16012](http://arxiv.org/abs/2506.16012)|null|
|**2025-06-18**|**VectorEdits: A Dataset and Benchmark for Instruction-Based Editing of Vector Graphics**|Michal Štefánik Team|[2506.15903](http://arxiv.org/abs/2506.15903)|null|
|**2025-06-18**|**GenRecal: Generation after Recalibration from Large to Small Vision-Language Models**|Yueh-Hua Wu Team|[2506.15681](http://arxiv.org/abs/2506.15681)|null|
|**2025-06-18**|**Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning**|Imran Razzak Team|[2506.15649](http://arxiv.org/abs/2506.15649)|null|
|**2025-06-18**|**FindingDory: A Benchmark to Evaluate Memory in Embodied Agents**|Zsolt Kira Team|[2506.15635](http://arxiv.org/abs/2506.15635)|null|
|**2025-06-18**|**WikiMixQA: A Multimodal Benchmark for Question Answering over Tables and Charts**|Rémi Lebret Team|[2506.15594](http://arxiv.org/abs/2506.15594)|**[link](https://github.com/negar-foroutan/wikimixqa)**|
|**2025-06-18**|**DiscoSG: Towards Discourse-Level Text Scene Graph Parsing through Iterative Graph Refinement**|Zhuang Li Team|[2506.15583](http://arxiv.org/abs/2506.15583)|**[link](https://github.com/shaoqlin/discosg)**|
|**2025-06-18**|**Context-Informed Grounding Supervision**|Minjoon Seo Team|[2506.15480](http://arxiv.org/abs/2506.15480)|**[link](https://github.com/kaistai/cings)**|
|**2025-06-19**|**OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models**|Guotai Wang Team|[2506.15318](http://arxiv.org/abs/2506.15318)|null|
|**2025-06-18**|**MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering**|Adrian K. Davision Team|[2506.15298](http://arxiv.org/abs/2506.15298)|null|
|**2025-06-18**|**ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections**|Shin'ichi Satoh Team|[2506.15180](http://arxiv.org/abs/2506.15180)|null|
|**2025-06-18**|**DyNaVLM: Zero-Shot Vision-Language Navigation System with Dynamic Viewpoints and Self-Refining Graph Memory**|Yue Gao Team|[2506.15096](http://arxiv.org/abs/2506.15096)|null|
|**2025-06-18**|**An Empirical Study of Bugs in Data Visualization Libraries**|Chengnian Sun Team|[2506.15084](http://arxiv.org/abs/2506.15084)|**[link](https://github.com/williamlus/dataviz-lib-bugs)**|
|**2025-06-17**|**PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning**|Yeyun Gong Team|[2506.14907](http://arxiv.org/abs/2506.14907)|**[link](https://github.com/alchemistyzz/perl)**|
|**2025-06-17**|**RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills**|Chuang Gan Team|[2506.14763](http://arxiv.org/abs/2506.14763)|null|
|**2025-06-17**|**Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models**|Yuke Zhu Team|[2506.14727](http://arxiv.org/abs/2506.14727)|null|
|**2025-06-17**|**AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions**|Dacheng Tao Team|[2506.14697](http://arxiv.org/abs/2506.14697)|null|
|**2025-06-17**|**Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models**|Jiaheng Wei Team|[2506.14674](http://arxiv.org/abs/2506.14674)|null|
|**2025-06-17**|**StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery**|Michelle Pasco Team|[2506.14670](http://arxiv.org/abs/2506.14670)|null|
|**2025-06-17**|**SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks**|Liang Lin Team|[2506.14512](http://arxiv.org/abs/2506.14512)|null|
|**2025-06-17**|**Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?**|Soumik Sarkar Team|[2506.14507](http://arxiv.org/abs/2506.14507)|**[link](https://github.com/oadamharoon/text2nav)**|
|**2025-06-17**|**Adapting Lightweight Vision Language Models for Radiological Visual Question Answering**|Chang Sun Team|[2506.14451](http://arxiv.org/abs/2506.14451)|null|
|**2025-06-17**|**Causally Steered Diffusion for Automated Video Counterfactual Generation**|Sotirios A. Tsaftaris Team|[2506.14404](http://arxiv.org/abs/2506.14404)|null|
|**2025-06-17**|**Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments**|Xuesu Xiao Team|[2506.14233](http://arxiv.org/abs/2506.14233)|null|
|**2025-06-17**|**Interpreting Biomedical VLMs on High-Imbalance Out-of-Distributions: An Insight into BiomedCLIP on Radiology**|Benjamin Kwan Team|[2506.14136](http://arxiv.org/abs/2506.14136)|null|
|**2025-06-17**|**A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving**|Ziran Wang Team|[2506.14100](http://arxiv.org/abs/2506.14100)|null|
|**2025-06-16**|**Disentangling 3D from Large Vision-Language Models for Controlled Portrait Generation**|Hyeongwoo Kim Team|[2506.14015](http://arxiv.org/abs/2506.14015)|null|
|**2025-06-16**|**GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics**|Mac Schwager Team|[2506.14009](http://arxiv.org/abs/2506.14009)|null|
|**2025-06-16**|**Comparison of ConvNeXt and Vision-Language Models for Breast Density Assessment in Screening Mammography**|Alejandro Santos-Díaz Team|[2506.13964](http://arxiv.org/abs/2506.13964)|null|
|**2025-06-16**|**HierVL: Semi-Supervised Segmentation leveraging Hierarchical Vision-Language Synergy with Dynamic Text-Spatial Query Alignment**|Abdul Bais Team|[2506.13925](http://arxiv.org/abs/2506.13925)|null|
|**2025-06-16**|**Touch begins where vision ends: Generalizable policies for contact-rich manipulation**|Raunaq Bhirangi Team|[2506.13762](http://arxiv.org/abs/2506.13762)|null|
|**2025-06-16**|**Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins**|Wei-Chiu Ma Team|[2506.13761](http://arxiv.org/abs/2506.13761)|null|
|**2025-06-16**|**OTFusion: Bridging Vision-only and Vision-Language Models via Optimal Transport for Transductive Zero-Shot Learning**|Yonghang Tai Team|[2506.13723](http://arxiv.org/abs/2506.13723)|null|
|**2025-06-16**|**ROSA: Harnessing Robot States for Vision-Language and Action Alignment**|Xiaoyan Sun Team|[2506.13679](http://arxiv.org/abs/2506.13679)|null|
|**2025-06-16**|**DualEdit: Dual Editing for Knowledge Updating in Vision-Language Models**|Hanspeter Pfister Team|[2506.13638](http://arxiv.org/abs/2506.13638)|null|
|**2025-06-16**|**VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation**|Wei Pan Team|[2506.13428](http://arxiv.org/abs/2506.13428)|null|
|**2025-06-16**|**Uncertainty-Informed Active Perception for Open Vocabulary Object Goal Navigation**|Marija Popović Team|[2506.13367](http://arxiv.org/abs/2506.13367)|null|
|**2025-06-16**|**Anomaly Object Segmentation with Vision-Language Models for Steel Scrap Recycling**|Rei Kawakami Team|[2506.13282](http://arxiv.org/abs/2506.13282)|null|
|**2025-06-16**|**Screen Hijack: Visual Poisoning of VLM Agents in Mobile Environments**|Ee-Chien Chang Team|[2506.13205](http://arxiv.org/abs/2506.13205)|null|
|**2025-06-16**|**Dynamic Context-oriented Decomposition for Task-aware Low-rank Adaptation with Less Forgetting and Faster Convergence**|Bernard Ghanem Team|[2506.13187](http://arxiv.org/abs/2506.13187)|null|
|**2025-06-16**|**GreedyPrune: Retenting Critical Visual Token Set for Large Vision Language Models**|Jun Wang Team|[2506.13166](http://arxiv.org/abs/2506.13166)|null|
|**2025-06-16**|**Rethinking Test-Time Scaling for Medical AI: Model and Task-Aware Strategies for LLMs and VLMs**|Byung-Hoon Kim Team|[2506.13102](http://arxiv.org/abs/2506.13102)|null|
|**2025-06-16**|**PRISM2: Unlocking Multi-Modal General Pathology AI with Clinical Dialogue**|Siqi Liu Team|[2506.13063](http://arxiv.org/abs/2506.13063)|null|
|**2025-06-17**|**HKD4VLM: A Progressive Hybrid Knowledge Distillation Framework for Robust Multimodal Hallucination and Factuality Detection in VLMs**|Xuezhi Cao Team|[2506.13038](http://arxiv.org/abs/2506.13038)|null|
|**2025-06-15**|**CAPO: Reinforcing Consistent Reasoning in Medical Decision-Making**|Zuozhu Liu Team|[2506.12849](http://arxiv.org/abs/2506.12849)|null|
|**2025-06-15**|**Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models**|Chang D. Yoo Team|[2506.12822](http://arxiv.org/abs/2506.12822)|null|
|**2025-06-15**|**Native Visual Understanding: Resolving Resolution Dilemmas in Vision-Language Models**|Wentao Zhang Team|[2506.12776](http://arxiv.org/abs/2506.12776)|null|
|**2025-06-15**|**NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models**|Jitao Sang Team|[2506.12706](http://arxiv.org/abs/2506.12706)|null|
|**2025-06-15**|**Evaluating Cell Type Inference in Vision Language Models Under Varying Visual Context**|Sandeep Singhal Team|[2506.12683](http://arxiv.org/abs/2506.12683)|null|
|**2025-06-14**|**Not All Tokens and Heads Are Equally Important: Dual-Level Attention Intervention for Hallucination Mitigation**|Yuexian Zou Team|[2506.12609](http://arxiv.org/abs/2506.12609)|null|
|**2025-06-13**|**Affogato: Learning Open-Vocabulary Affordance Grounding with Automated Data Generation at Scale**|Minsu Cho Team|[2506.12009](http://arxiv.org/abs/2506.12009)|null|
|**2025-06-13**|**How Visual Representations Map to Language Feature Space in Multimodal LLMs**|Neel Nanda Team|[2506.11976](http://arxiv.org/abs/2506.11976)|null|
|**2025-06-13**|**Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation**|Kaifu Zhang Team|[2506.11820](http://arxiv.org/abs/2506.11820)|null|
|**2025-06-13**|**MTabVQA: Evaluating Multi-Tabular Reasoning of Language Models in Visual Space**|Jan Strich Team|[2506.11684](http://arxiv.org/abs/2506.11684)|null|
|**2025-06-13**|**VLM@school -- Evaluation of AI image understanding on German middle school knowledge**|Vincent Tischler Team|[2506.11604](http://arxiv.org/abs/2506.11604)|null|
|**2025-06-16**|**EasyARC: Evaluating Vision Language Models on True Visual Reasoning**|Aylin Akkus Team|[2506.11595](http://arxiv.org/abs/2506.11595)|null|
|**2025-06-13**|**Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis**|Johannes Betz Team|[2506.11526](http://arxiv.org/abs/2506.11526)|null|
|**2025-06-13**|**Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs**|Min-Yen Kan Team|[2506.11515](http://arxiv.org/abs/2506.11515)|null|
|**2025-06-13**|**Taming Stable Diffusion for Computed Tomography Blind Super-Resolution**|Lichao Mou Team|[2506.11496](http://arxiv.org/abs/2506.11496)|null|
|**2025-06-13**|**On the Natural Robustness of Vision-Language Models Against Visual Perception Attacks in Autonomous Driving**|Mert D. Pesé Team|[2506.11472](http://arxiv.org/abs/2506.11472)|null|
|**2025-06-12**|**Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving**|Liam Paull Team|[2506.11234](http://arxiv.org/abs/2506.11234)|null|
|**2025-06-12**|**AIR: Zero-shot Generative Model Adaptation with Iterative Refinement**|Ngai-Man Cheung Team|[2506.10895](http://arxiv.org/abs/2506.10895)|**[link](https://github.com/guimeng-leo-liu/air)**|
|**2025-06-13**|**RationalVLA: A Rational Vision-Language-Action Model with Dual System**|Haoang Li Team|[2506.10826](http://arxiv.org/abs/2506.10826)|null|
|**2025-06-12**|**Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding**|Mir Feroskhan Team|[2506.10756](http://arxiv.org/abs/2506.10756)|null|
|**2025-06-13**|**IQE-CLIP: Instance-aware Query Embedding for Zero-/Few-shot Anomaly Detection in Medical Domain**|Yefeng Zheng Team|[2506.10730](http://arxiv.org/abs/2506.10730)|**[link](https://github.com/hongh0/iqe-clip)**|
|**2025-06-12**|**GigaVideo-1: Advancing Video Generation via Automatic Feedback with 4 GPU-Hours Fine-Tuning**|Guan Huang Team|[2506.10639](http://arxiv.org/abs/2506.10639)|null|
|**2025-06-12**|**Text to Image for Multi-Label Image Recognition with Joint Prompt-Adapter Learning**|Yong Liu Team|[2506.10575](http://arxiv.org/abs/2506.10575)|null|
|**2025-06-12**|**LLMs Are Not Yet Ready for Deepfake Image Detection**|Kristen Moore Team|[2506.10474](http://arxiv.org/abs/2506.10474)|null|
|**2025-06-12**|**UrbanSense:AFramework for Quantitative Analysis of Urban Streetscapes leveraging Vision Large Language Models**|Shuai Lu Team|[2506.10342](http://arxiv.org/abs/2506.10342)|null|
|**2025-06-12**|**Using Vision Language Models to Detect Students' Academic Emotion through Facial Expressions**|Gaowei Chen Team|[2506.10334](http://arxiv.org/abs/2506.10334)|null|
|**2025-06-12**|**HalLoc: Token-level Localization of Hallucinations for Vision Language Models**|Gunhee Kim Team|[2506.10286](http://arxiv.org/abs/2506.10286)|null|
|**2025-06-11**|**Q2E: Query-to-Event Decomposition for Zero-Shot Multilingual Text-to-Video Retrieval**|Francis Ferraro Team|[2506.10202](http://arxiv.org/abs/2506.10202)|null|
|**2025-06-11**|**Improving Personalized Search with Regularized Low-Rank Parameter Updates**|Bryan Russell Team|[2506.10182](http://arxiv.org/abs/2506.10182)|null|
|**2025-06-11**|**A Navigation Framework Utilizing Vision-Language Models**|Kaiyu tang Team|[2506.10172](http://arxiv.org/abs/2506.10172)|null|
|**2025-06-11**|**One Patient, Many Contexts: Scaling Medical AI Through Contextual Intelligence**|Marinka Zitnik Team|[2506.10157](http://arxiv.org/abs/2506.10157)|null|
|**2025-06-11**|**ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs**|Lijuan Wang Team|[2506.10128](http://arxiv.org/abs/2506.10128)|null|
|**2025-06-11**|**Test-Time Adaptation for Generalizable Task Progress Estimation**|Alessandra Russo Team|[2506.10085](http://arxiv.org/abs/2506.10085)|null|
|**2025-06-11**|**Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing**|Tieniu Tan Team|[2506.09965](http://arxiv.org/abs/2506.09965)|**[link](https://github.com/antresearchnlp/vilasr)**|
|**2025-06-11**|**From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models**|Chen Feng Team|[2506.09930](http://arxiv.org/abs/2506.09930)|null|
|**2025-06-11**|**3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation**|Hyunjung Shim Team|[2506.09883](http://arxiv.org/abs/2506.09883)|**[link](https://github.com/kaist-cvml/3d-vlm-gd)**|
|**2025-06-11**|**Adding simple structure at inference improves Vision-Language Compositionality**|Gorka Azkune Team|[2506.09691](http://arxiv.org/abs/2506.09691)|**[link](https://github.com/imirandam/structure-inference-compositionality)**|
|**2025-06-11**|**FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models**|Liangqiong Qu Team|[2506.09638](http://arxiv.org/abs/2506.09638)|null|
|**2025-06-11**|**Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs**|Jaehyung Kim Team|[2506.09522](http://arxiv.org/abs/2506.09522)|**[link](https://github.com/bscho333/ReVisiT)**|
|**2025-06-11**|**Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning**|Jia Li Team|[2506.09473](http://arxiv.org/abs/2506.09473)|null|
|**2025-06-11**|**TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision**|Susmit Jha Team|[2506.09445](http://arxiv.org/abs/2506.09445)|null|
|**2025-06-11**|**DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt**|Ge Li Team|[2506.09353](http://arxiv.org/abs/2506.09353)|null|
|**2025-06-10**|**UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation**|Li Fei-Fei Team|[2506.09284](http://arxiv.org/abs/2506.09284)|null|
|**2025-06-10**|**MultiNet: An Open-Source Software Toolkit \& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models**|Harshvardhan Sikka Team|[2506.09172](http://arxiv.org/abs/2506.09172)|null|
|**2025-06-10**|**VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning**|Zhenfei Yin Team|[2506.09049](http://arxiv.org/abs/2506.09049)|null|
|**2025-06-11**|**Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs**|Yonatan Belinkov Team|[2506.09047](http://arxiv.org/abs/2506.09047)|null|
|**2025-06-10**|**Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better**|Jiaqi Wang Team|[2506.09040](http://arxiv.org/abs/2506.09040)|null|
|**2025-06-10**|**Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models**|Liansheng Wang Team|[2506.08990](http://arxiv.org/abs/2506.08990)|null|
|**2025-06-10**|**Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions**|Yejin Choi Team|[2506.08927](http://arxiv.org/abs/2506.08927)|null|
|**2025-06-12**|**Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought**|Shanghang Zhang Team|[2506.08817](http://arxiv.org/abs/2506.08817)|null|
|**2025-06-10**|**Multimodal Representation Alignment for Cross-modal Information Retrieval**|Luis A. Leiva Team|[2506.08774](http://arxiv.org/abs/2506.08774)|null|
|**2025-06-10**|**PhyBlock: A Progressive Benchmark for Physical Understanding and Planning via 3D Block Assembly**|Xiaodan Liang Team|[2506.08708](http://arxiv.org/abs/2506.08708)|null|
|**2025-06-10**|**VReST: Enhancing Reasoning in Large Vision-Language Models through Tree Search and Self-Reward Mechanism**|Weijiang Yu Team|[2506.08691](http://arxiv.org/abs/2506.08691)|null|
|**2025-06-10**|**ATAS: Any-to-Any Self-Distillation for Enhanced Open-Vocabulary Dense Prediction**|Taesup Kim Team|[2506.08678](http://arxiv.org/abs/2506.08678)|null|
|**2025-06-10**|**Convergence of Spectral Principal Paths: How Deep Networks Distill Linear Representations from Noisy Inputs**|Ang Li Team|[2506.08543](http://arxiv.org/abs/2506.08543)|null|
|**2025-06-10**|**Better Reasoning with Less Data: Enhancing VLMs Through Unified Modality Scoring**|Jiaheng Wei Team|[2506.08429](http://arxiv.org/abs/2506.08429)|null|
|**2025-06-11**|**SafeCoT: Improving VLM Safety with Minimal Reasoning**|Chaochao Lu Team|[2506.08399](http://arxiv.org/abs/2506.08399)|null|
|**2025-06-10**|**SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding**|Jaeyoung Do Team|[2506.08391](http://arxiv.org/abs/2506.08391)|null|
|**2025-06-09**|**A Good CREPE needs more than just Sugar: Investigating Biases in Compositional Vision-Language Benchmarks**|Matthias Bethge Team|[2506.08227](http://arxiv.org/abs/2506.08227)|null|
|**2025-06-11**|**GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra**|Guha Balakrishnan Team|[2506.08194](http://arxiv.org/abs/2506.08194)|null|
|**2025-06-09**|**Open World Scene Graph Generation using Vision Language Models**|Anuj Karpatne Team|[2506.08189](http://arxiv.org/abs/2506.08189)|null|
|**2025-06-09**|**CuRe: Cultural Gaps in the Long Tail of Text-to-Image Systems**|Ramya Korlakai Vinayak Team|[2506.08071](http://arxiv.org/abs/2506.08071)|null|
|**2025-06-10**|**Vision Transformers Don't Need Trained Registers**|Yossi Gandelsman Team|[2506.08010](http://arxiv.org/abs/2506.08010)|null|
|**2025-06-09**|**Hidden in plain sight: VLMs overlook their visual representations**|Trevor Darrell Team|[2506.08008](http://arxiv.org/abs/2506.08008)|null|
|**2025-06-09**|**BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models**|Tieniu Tan Team|[2506.07961](http://arxiv.org/abs/2506.07961)|null|
|**2025-06-09**|**Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations**|Yiqing Shen Team|[2506.07943](http://arxiv.org/abs/2506.07943)|null|
|**2025-06-09**|**Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models**|Zsolt Kira Team|[2506.07936](http://arxiv.org/abs/2506.07936)|null|
|**2025-06-09**|**SAM2Auto: Auto Annotation Using FLASH**|Q. M. Jonathan Wu Team|[2506.07850](http://arxiv.org/abs/2506.07850)|null|
|**2025-06-09**|**Image Reconstruction as a Tool for Feature Analysis**|Andrey Kuznetsov Team|[2506.07803](http://arxiv.org/abs/2506.07803)|null|
|**2025-06-09**|**Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger**|Shiming Xiang Team|[2506.07785](http://arxiv.org/abs/2506.07785)|null|
|**2025-06-09**|**Language-Vision Planner and Executor for Text-to-Visual Reasoning**|Ling Liu Team|[2506.07778](http://arxiv.org/abs/2506.07778)|null|
|**2025-06-10**|**ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models**|Shuai Lu Team|[2506.07739](http://arxiv.org/abs/2506.07739)|null|
|**2025-06-09**|**OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting**|Bastian Leibe Team|[2506.07697](http://arxiv.org/abs/2506.07697)|null|
|**2025-06-09**|**Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline**|Idan Szpektor Team|[2506.07631](http://arxiv.org/abs/2506.07631)|null|
|**2025-06-09**|**Event-Priori-Based Vision-Language Model for Efficient Visual Understanding**|Michele Magno Team|[2506.07627](http://arxiv.org/abs/2506.07627)|null|
|**2025-06-10**|**SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems**|Zhengzhong Tu Team|[2506.07564](http://arxiv.org/abs/2506.07564)|null|
|**2025-06-10**|**GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition**|Conghui He Team|[2506.07553](http://arxiv.org/abs/2506.07553)|null|
|**2025-06-09**|**Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent**|Ting Yang Ling Team|[2506.07509](http://arxiv.org/abs/2506.07509)|null|
|**2025-06-09**|**Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency**|Xinggang Wang Team|[2506.07497](http://arxiv.org/abs/2506.07497)|null|
|**2025-06-09**|**CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization**|Hyun Myung Team|[2506.07484](http://arxiv.org/abs/2506.07484)|null|
|**2025-06-09**|**LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments**|Josh Park Team|[2506.07416](http://arxiv.org/abs/2506.07416)|null|
|**2025-06-09**|**MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems**|Tao Qi Team|[2506.07399](http://arxiv.org/abs/2506.07399)|null|
|**2025-06-06**|**CoMemo: LVLMs Need Image Context with Image Memory**|Jifeng Dai Team|[2506.06279](http://arxiv.org/abs/2506.06279)|null|
|**2025-06-06**|**Movie Facts and Fibs (MF $^2$ ): A Benchmark for Long Movie Understanding**|André F. T. Martins Team|[2506.06275](http://arxiv.org/abs/2506.06275)|null|
|**2025-06-06**|**Challenging Vision-Language Models with Surgical Data: A New Dataset and Broad Benchmarking Study**|Lena Maier-Hein Team|[2506.06232](http://arxiv.org/abs/2506.06232)|null|
|**2025-06-06**|**GenIR: Generative Visual Feedback for Mental Image Retrieval**|James Davis Team|[2506.06220](http://arxiv.org/abs/2506.06220)|null|
|**2025-06-06**|**STSBench: A Spatio-temporal Scenario Benchmark for Multi-modal Large Language Models in Autonomous Driving**|Horst Possegger Team|[2506.06218](http://arxiv.org/abs/2506.06218)|null|
|**2025-06-06**|**WisWheat: A Three-Tiered Vision-Language Dataset for Wheat Management**|Zijian Wang Team|[2506.06084](http://arxiv.org/abs/2506.06084)|null|
|**2025-06-06**|**Full Conformal Adaptation of Medical Vision-Language Models**|Jose Dolz Team|[2506.06076](http://arxiv.org/abs/2506.06076)|null|
|**2025-06-06**|**BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning**|Rudolf Lioutikov Team|[2506.06072](http://arxiv.org/abs/2506.06072)|null|
|**2025-06-06**|**MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness Against VLM-based Attacks**|Yiren Song Team|[2506.05982](http://arxiv.org/abs/2506.05982)|null|
|**2025-06-06**|**HMVLM: Multistage Reasoning-Enhanced Vision-Language Model for Long-Tailed Driving Scenarios**|Weihao Gu Team|[2506.05883](http://arxiv.org/abs/2506.05883)|null|
|**2025-06-06**|**Do Large Vision-Language Models Distinguish between the Actual and Apparent Features of Illusions?**|Hitomi Yanaka Team|[2506.05765](http://arxiv.org/abs/2506.05765)|null|
|**2025-06-06**|**MoralCLIP: Contrastive Alignment of Vision-and-Language Representations with Moral Foundations Theory**|João Magalhães Team|[2506.05696](http://arxiv.org/abs/2506.05696)|null|
|**2025-06-06**|**DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models**|Xianpeng Lang Team|[2506.05667](http://arxiv.org/abs/2506.05667)|null|
|**2025-06-05**|**MORSE-500: A Programmatically Controllable Video Benchmark to Stress-Test Multimodal Reasoning**|Furong Huang Team|[2506.05523](http://arxiv.org/abs/2506.05523)|null|
|**2025-06-05**|**Degradation-Aware Image Enhancement via Vision-Language Classification**|Zibo Meng Team|[2506.05450](http://arxiv.org/abs/2506.05450)|null|
|**2025-06-09**|**Coordinated Robustness Evaluation Framework for Vision-Language Models**|Soumyendu Sarkar Team|[2506.05429](http://arxiv.org/abs/2506.05429)|null|
|**2025-06-06**|**Does Your 3D Encoder Really Work? When Pretrain-SFT from 2D VLMs Meets 3D VLMs**|Xiaodan Liang Team|[2506.05318](http://arxiv.org/abs/2506.05318)|null|
|**2025-06-05**|**MonkeyOCR: Document Parsing with a Structure-Recognition-Relation Triplet Paradigm**|Xiang Bai Team|[2506.05218](http://arxiv.org/abs/2506.05218)|null|
|**2025-06-05**|**Quantifying Cross-Modality Memorization in Vision-Language Models**|Chiyuan Zhang Team|[2506.05198](http://arxiv.org/abs/2506.05198)|null|
|**2025-06-05**|**CIVET: Systematic Evaluation of Understanding in VLMs**|Giuseppe Riccardi Team|[2506.05146](http://arxiv.org/abs/2506.05146)|null|
|**2025-06-05**|**PixCell: A generative foundation model for digital histopathology images**|Dimitris Samaras Team|[2506.05127](http://arxiv.org/abs/2506.05127)|null|
|**2025-06-05**|**A Survey on Vietnamese Document Analysis and Recognition: Challenges and Future Directions**|Dung Nguyen Team|[2506.05061](http://arxiv.org/abs/2506.05061)|null|
|**2025-06-05**|**Hierarchical Language Models for Semantic Navigation and Manipulation in an Aerial-Ground Robotic System**|Moju Zhao Team|[2506.05020](http://arxiv.org/abs/2506.05020)|null|
|**2025-06-05**|**ConECT Dataset: Overcoming Data Scarcity in Context-Aware E-Commerce MT**|Mikołaj Koszowski Team|[2506.04929](http://arxiv.org/abs/2506.04929)|null|
|**2025-06-05**|**SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs**|Dacheng Tao Team|[2506.04743](http://arxiv.org/abs/2506.04743)|null|
|**2025-06-05**|**Robust Few-Shot Vision-Language Model Adaptation**|Shu Kong Team|[2506.04713](http://arxiv.org/abs/2506.04713)|null|
|**2025-06-05**|**HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model**|Sung Ju Hwang Team|[2506.04704](http://arxiv.org/abs/2506.04704)|null|
|**2025-06-05**|**SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents**|Yu-Wing Tai Team|[2506.04606](http://arxiv.org/abs/2506.04606)|null|
|**2025-06-05**|**MuSciClaims: Multimodal Scientific Claim Verification**|Niranjan Balasubramanian Team|[2506.04585](http://arxiv.org/abs/2506.04585)|null|
|**2025-06-05**|**Handle-based Mesh Deformation Guided By Vision Language Model**|Aniket Bera Team|[2506.04562](http://arxiv.org/abs/2506.04562)|null|
|**2025-06-04**|**RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics**|Shanghang Zhang Team|[2506.04308](http://arxiv.org/abs/2506.04308)|null|
|**2025-06-04**|**Image Editing As Programs with Diffusion Models**|Xinchao Wang Team|[2506.04158](http://arxiv.org/abs/2506.04158)|null|
|**2025-06-04**|**Recent Advances in Medical Image Classification**|Ngoc Quoc Ly Team|[2506.04129](http://arxiv.org/abs/2506.04129)|null|
|**2025-06-04**|**LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward**|Jing Li Team|[2506.04070](http://arxiv.org/abs/2506.04070)|null|
|**2025-06-04**|**Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization**|Min Zhang Team|[2506.04039](http://arxiv.org/abs/2506.04039)|null|
|**2025-06-04**|**Vocabulary-free few-shot learning for Vision-Language Models**|Christophe De Vleeschouwer Team|[2506.04005](http://arxiv.org/abs/2506.04005)|null|
|**2025-06-04**|**DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models**|Anders Holst Team|[2506.03933](http://arxiv.org/abs/2506.03933)|null|
|**2025-06-04**|**Zero-Shot Temporal Interaction Localization for Egocentric Videos**|Hesheng Wang Team|[2506.03662](http://arxiv.org/abs/2506.03662)|null|
|**2025-06-04**|**Spatial Understanding from Videos: Structured Prompts Meet Simulation Data**|Liqiang Nie Team|[2506.03642](http://arxiv.org/abs/2506.03642)|null|
|**2025-06-04**|**VLMs Can Aggregate Scattered Training Patches**|Chaochao Lu Team|[2506.03614](http://arxiv.org/abs/2506.03614)|null|
|**2025-06-04**|**BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance**|Ngan Le Team|[2506.03589](http://arxiv.org/abs/2506.03589)|null|
|**2025-06-04**|**MiMo-VL Technical Report**|Bingquan Xia Team|[2506.03569](http://arxiv.org/abs/2506.03569)|null|
|**2025-06-04**|**Target Semantics Clustering via Text Representations for Robust Universal Domain Adaptation**|Yixin Zhang Team|[2506.03521](http://arxiv.org/abs/2506.03521)|null|
|**2025-06-04**|**DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models**|Aliaksandr Siarohin Team|[2506.03517](http://arxiv.org/abs/2506.03517)|null|
|**2025-06-04**|**POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning**|Weixin Yao Team|[2506.03511](http://arxiv.org/abs/2506.03511)|**[link](https://github.com/astraeus999/POLARIS_img_analysis)**|
|**2025-06-03**|**Toward Reliable VLM: A Fine-Grained Benchmark and Framework for Exposure, Bias, and Inference in Korean Street Views**|Hansaem Kim Team|[2506.03371](http://arxiv.org/abs/2506.03371)|null|
|**2025-06-03**|**Robustness in Both Domains: CLIP Needs a Robust Text Encoder**|Volkan Cevher Team|[2506.03355](http://arxiv.org/abs/2506.03355)|null|
|**2025-06-03**|**Grounded Vision-Language Interpreter for Integrated Task and Motion Planning**|Atsushi Hashimoto Team|[2506.03270](http://arxiv.org/abs/2506.03270)|null|
|**2025-06-03**|**OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models**|Li Yi Team|[2506.03135](http://arxiv.org/abs/2506.03135)|null|
|**2025-06-03**|**EgoVLM: Policy Optimization for Egocentric Video Understanding**|Linshen Liu Team|[2506.03097](http://arxiv.org/abs/2506.03097)|null|
|**2025-06-03**|**DPO Learning with LLMs-Judge Signal for Computer Use Agents**|Phillip Howard Team|[2506.03095](http://arxiv.org/abs/2506.03095)|null|
|**2025-06-03**|**From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit**|Demba Ba Team|[2506.03093](http://arxiv.org/abs/2506.03093)|null|
|**2025-06-03**|**Text-guided Generation of Efficient Personalized Inspection Plans**|Aniket Bera Team|[2506.02917](http://arxiv.org/abs/2506.02917)|null|
|**2025-06-04**|**FlySearch: Exploring how vision-language models explore**|Maciej Wołczyk Team|[2506.02896](http://arxiv.org/abs/2506.02896)|null|
|**2025-06-03**|**Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights**|Tony Wu Team|[2506.02865](http://arxiv.org/abs/2506.02865)|null|
|**2025-06-03**|**SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking**|Yiwei Wang Team|[2506.02803](http://arxiv.org/abs/2506.02803)|null|
|**2025-06-04**|**Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning**|Arash Afkanpour Team|[2506.02738](http://arxiv.org/abs/2506.02738)|null|
|**2025-06-03**|**Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation**|Toshihiko Yamasaki Team|[2506.02708](http://arxiv.org/abs/2506.02708)|null|
|**2025-06-03**|**Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet**|Zhi Wang Team|[2506.02671](http://arxiv.org/abs/2506.02671)|null|
|**2025-06-03**|**Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models**|Dong Seog Han Team|[2506.02615](http://arxiv.org/abs/2506.02615)|null|
|**2025-06-03**|**Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models**|Farzan Farnia Team|[2506.02557](http://arxiv.org/abs/2506.02557)|null|
|**2025-06-03**|**Sign Language: Towards Sign Understanding for Robot Autonomy**|David Hsu Team|[2506.02556](http://arxiv.org/abs/2506.02556)|null|
|**2025-06-03**|**SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence**|Yueming Jin Team|[2506.02555](http://arxiv.org/abs/2506.02555)|null|
|**2025-06-03**|**Rethinking Post-Unlearning Behavior of Large Vision-Language Models**|Kyomin Jung Team|[2506.02541](http://arxiv.org/abs/2506.02541)|null|
|**2025-06-04**|**MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection**|Qingyao Wu Team|[2506.02535](http://arxiv.org/abs/2506.02535)|null|
|**2025-06-03**|**VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments**|Yu Wang Team|[2506.02387](http://arxiv.org/abs/2506.02387)|null|
|**2025-06-03**|**Auto-Labeling Data for Object Detection**|Jason J. Corso Team|[2506.02359](http://arxiv.org/abs/2506.02359)|null|
|**2025-06-03**|**RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models**|Jianzong Wang Team|[2506.02354](http://arxiv.org/abs/2506.02354)|null|
|**2025-05-30**|**ReasonGen-R1: CoT for Autoregressive Image generation models through SFT and RL**|Lili Qiu Team|[2505.24875](http://arxiv.org/abs/2505.24875)|null|
|**2025-05-30**|**ProxyThinker: Test-Time Guidance through Small Visual Reasoners**|Vicente Ordonez Team|[2505.24872](http://arxiv.org/abs/2505.24872)|null|
|**2025-05-30**|**GenSpace: Benchmarking Spatially-Aware Image Generation**|Zhou Zhao Team|[2505.24870](http://arxiv.org/abs/2505.24870)|null|
|**2025-05-30**|**Time Blindness: Why Video-Language Models Can't See What Humans Can?**|Mohamed Elhoseiny Team|[2505.24867](http://arxiv.org/abs/2505.24867)|null|
|**2025-05-30**|**Conformal Prediction for Zero-Shot Models**|Jose Dolz Team|[2505.24693](http://arxiv.org/abs/2505.24693)|null|
|**2025-05-30**|**BIMA: Bijective Maximum Likelihood Learning Approach to Hallucination Prediction and Mitigation in Large Vision-Language Models**|Khoa Luu Team|[2505.24649](http://arxiv.org/abs/2505.24649)|null|
|**2025-05-30**|**SARD: A Large-Scale Synthetic Arabic OCR Dataset for Book-Style Text Recognition**|Wadii Boulila Team|[2505.24600](http://arxiv.org/abs/2505.24600)|null|
|**2025-05-30**|**AMIA: Automatic Masking and Joint Intention Analysis Makes LVLMs Robust Jailbreak Defenders**|Liang Ding Team|[2505.24519](http://arxiv.org/abs/2505.24519)|null|
|**2025-05-30**|**CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation**|Thamar Solorio Team|[2505.24456](http://arxiv.org/abs/2505.24456)|null|
|**2025-05-30**|**Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning**|Matthias Hein Team|[2505.24424](http://arxiv.org/abs/2505.24424)|null|
|**2025-05-30**|**MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs**|Sophia Ananiadou Team|[2505.24423](http://arxiv.org/abs/2505.24423)|null|
|**2025-05-30**|**Grid-LOGAT: Grid Based Local and Global Area Transcription for Video Question Answering**|Fadoua Ghourabi Team|[2505.24371](http://arxiv.org/abs/2505.24371)|null|
|**2025-05-30**|**KEVER^2: Knowledge-Enhanced Visual Emotion Reasoning and Retrieval**|Yong Li Team|[2505.24342](http://arxiv.org/abs/2505.24342)|null|
|**2025-05-30**|**ROAD: Responsibility-Oriented Reward Design for Reinforcement Learning in Autonomous Driving**|Songan Zhang Team|[2505.24317](http://arxiv.org/abs/2505.24317)|null|
|**2025-05-30**|**Benchmarking Foundation Models for Zero-Shot Biometric Tasks**|Arun Ross Team|[2505.24214](http://arxiv.org/abs/2505.24214)|null|
|**2025-05-30**|**Bootstrapping LLM Robustness for VLM Safety via Reducing the Pretraining Modality Gap**|Baharan Mirzasoleiman Team|[2505.24208](http://arxiv.org/abs/2505.24208)|null|
|**2025-05-30**|**DrVD-Bench: Do Vision-Language Models Reason Like Human Doctors in Medical Image Diagnosis?**|Xuegong Zhang Team|[2505.24173](http://arxiv.org/abs/2505.24173)|null|
|**2025-05-30**|**CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning Capabilities of VLMs**|Xuchen Song Team|[2505.24120](http://arxiv.org/abs/2505.24120)|null|
|**2025-05-29**|**mRAG: Elucidating the Design Space of Multi-modal Retrieval-Augmented Generation**|Zhengzhong Tu Team|[2505.24073](http://arxiv.org/abs/2505.24073)|null|
|**2025-05-29**|**Multi-RAG: A Multimodal Retrieval-Augmented Generation System for Adaptive Video Understanding**|Tinoosh Mohsenin Team|[2505.23990](http://arxiv.org/abs/2505.23990)|null|
|**2025-05-29**|**ZeroGUI: Automating Online GUI Learning at Zero Human Cost**|Jifeng Dai Team|[2505.23762](http://arxiv.org/abs/2505.23762)|**[link](https://github.com/opengvlab/zerogui)**|
|**2025-05-29**|**Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint**|David M. Chan Team|[2505.23759](http://arxiv.org/abs/2505.23759)|**[link](https://github.com/kyunnilee/visual_puzzles)**|
|**2025-05-29**|**To Trust Or Not To Trust Your Vision-Language Model's Prediction**|Olga Fink Team|[2505.23745](http://arxiv.org/abs/2505.23745)|**[link](https://github.com/epfl-imos/trustvlm)**|
|**2025-05-29**|**LayerPeeler: Autoregressive Peeling for Layer-wise Image Vectorization**|Jing Liao Team|[2505.23740](http://arxiv.org/abs/2505.23740)|null|
|**2025-05-29**|**Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better**|Sergey Levine Team|[2505.23705](http://arxiv.org/abs/2505.23705)|null|
|**2025-05-29**|**Grounded Reinforcement Learning for Visual Reasoning**|Katerina Fragkiadaki Team|[2505.23678](http://arxiv.org/abs/2505.23678)|null|
|**2025-05-29**|**Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition**|Liangcai Gao Team|[2505.23566](http://arxiv.org/abs/2505.23566)|null|
|**2025-05-30**|**Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information**|Weiping Li Team|[2505.23558](http://arxiv.org/abs/2505.23558)|**[link](https://github.com/liar406/look_again)**|
|**2025-05-29**|**TRAP: Targeted Redirecting of Agentic Preferences**|Gagandeep Singh Team|[2505.23518](http://arxiv.org/abs/2505.23518)|null|
|**2025-05-29**|**VCapsBench: A Large-scale Fine-grained Benchmark for Video Caption Quality Evaluation**|Xu-Cheng Yin Team|[2505.23484](http://arxiv.org/abs/2505.23484)|**[link](https://github.com/gxym/vcapsbench)**|
|**2025-05-29**|**Beam-Guided Knowledge Replay for Knowledge-Rich Image Captioning using Vision-Language Model**|Muzammil Behzad Team|[2505.23358](http://arxiv.org/abs/2505.23358)|null|
|**2025-05-29**|**LADA: Scalable Label-Specific CLIP Adapter for Continual Learning**|Min-Ling Zhang Team|[2505.23271](http://arxiv.org/abs/2505.23271)|**[link](https://github.com/maolinluo/lada)**|
|**2025-05-29**|**VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation**|Panayiotis Kolios Team|[2505.23267](http://arxiv.org/abs/2505.23267)|null|
|**2025-05-29**|**Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion**|Tao Xiang Team|[2505.23266](http://arxiv.org/abs/2505.23266)|null|
|**2025-05-29**|**ChartMind: A Comprehensive Benchmark for Complex Real-world Multimodal Chart Question Answering**|Lei Wang Team|[2505.23242](http://arxiv.org/abs/2505.23242)|null|
|**2025-05-29**|**PhotoArtAgent: Intelligent Photo Retouching with Language Model-Based Artist Agents**|Jinjin Gu Team|[2505.23130](http://arxiv.org/abs/2505.23130)|null|
|**2025-05-29**|**Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation**|Yu Cheng Team|[2505.23043](http://arxiv.org/abs/2505.23043)|**[link](https://github.com/majordavidzhang/generalization_unified_vlm)**|
|**2025-05-29**|**An Empirical Study of Federated Prompt Learning for Vision Language Model**|Mang Ye Team|[2505.23024](http://arxiv.org/abs/2505.23024)|null|
|**2025-05-29**|**SeG-SR: Integrating Semantic Knowledge into Remote Sensing Image Super-Resolution via Vision-Language Model**|Zhenwei Shi Team|[2505.23010](http://arxiv.org/abs/2505.23010)|null|
|**2025-05-29**|**QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining**|Muhao Chen Team|[2505.23004](http://arxiv.org/abs/2505.23004)|**[link](https://github.com/kyrochi/qlip)**|
|**2025-05-28**|**Zero-Shot Vision Encoder Grafting via LLM Surrogates**|Tom Goldstein Team|[2505.22664](http://arxiv.org/abs/2505.22664)|**[link](https://github.com/facebookresearch/zero)**|
|**2025-05-28**|**Training Free Stylized Abstraction**|Vishal M. Patel Team|[2505.22663](http://arxiv.org/abs/2505.22663)|null|
|**2025-05-28**|**VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models**|Dong Yu Team|[2505.22654](http://arxiv.org/abs/2505.22654)|null|
|**2025-05-28**|**Sherlock: Self-Correcting Reasoning in Vision-Language Models**|Ruqi Zhang Team|[2505.22651](http://arxiv.org/abs/2505.22651)|null|
|**2025-05-28**|**Hypothesis Testing in Imaging Inverse Problems**|Marcelo Pereyra Team|[2505.22481](http://arxiv.org/abs/2505.22481)|null|
|**2025-05-28**|**Zero-Shot 3D Visual Grounding from Vision-Language Models**|Junwei Liang Team|[2505.22429](http://arxiv.org/abs/2505.22429)|null|
|**2025-05-28**|**IKIWISI: An Interactive Visual Pattern Generator for Evaluating the Reliability of Vision-Language Models Without Ground Truth**|Syed Masum Billah Team|[2505.22305](http://arxiv.org/abs/2505.22305)|null|
|**2025-05-28**|**Investigating Mechanisms for In-Context Vision Language Binding**|Vineet Gandhi Team|[2505.22200](http://arxiv.org/abs/2505.22200)|null|
|**2025-05-29**|**Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging**|Piji Li Team|[2505.22150](http://arxiv.org/abs/2505.22150)|null|
|**2025-05-28**|**3D Question Answering via only 2D Vision-Language Models**|Qianru Sun Team|[2505.22143](http://arxiv.org/abs/2505.22143)|null|
|**2025-05-28**|**Reinforced Reasoning for Embodied Planning**|Bo Jin Team|[2505.22050](http://arxiv.org/abs/2505.22050)|null|
|**2025-05-28**|**Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization**|Xinlei Chen Team|[2505.22038](http://arxiv.org/abs/2505.22038)|null|
|**2025-05-28**|**Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset**|Muhammad Abdul-Mageed Team|[2505.21979](http://arxiv.org/abs/2505.21979)|null|
|**2025-05-29**|**DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation**|Xin Tan Team|[2505.21969](http://arxiv.org/abs/2505.21969)|null|
|**2025-05-28**|**Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack**|Usman Naseem Team|[2505.21967](http://arxiv.org/abs/2505.21967)|null|
|**2025-05-28**|**Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs**|Byonghyo Shim Team|[2505.21955](http://arxiv.org/abs/2505.21955)|null|
|**2025-05-28**|**Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge**|Yi Xu Team|[2505.21906](http://arxiv.org/abs/2505.21906)|null|
|**2025-05-28**|**Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation**|Christian Desrosiers Team|[2505.21844](http://arxiv.org/abs/2505.21844)|null|
|**2025-05-27**|**MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning**|Vivek Gupta Team|[2505.21771](http://arxiv.org/abs/2505.21771)|null|
|**2025-05-27**|**MedBridge: Bridging Foundation Vision-Language Models to Medical Image Diagnosis**|Christian Wachinger Team|[2505.21698](http://arxiv.org/abs/2505.21698)|null|
|**2025-05-27**|**ViewSpatial-Bench: Evaluating Multi-perspective Spatial Localization in Vision-Language Models**|Yueting Zhuang Team|[2505.21500](http://arxiv.org/abs/2505.21500)|null|
|**2025-05-27**|**AdInject: Real-World Black-Box Attacks on Web Agents via Advertising Delivery**|Qing Wang Team|[2505.21499](http://arxiv.org/abs/2505.21499)|null|
|**2025-05-27**|**Mitigating Hallucination in Large Vision-Language Models via Adaptive Attention Calibration**|Ziwei Zhu Team|[2505.21472](http://arxiv.org/abs/2505.21472)|null|
|**2025-05-27**|**ID-Align: RoPE-Conscious Position Remapping for Dynamic High-Resolution Adaptation in Vision-Language Models**|Wentao Zhang Team|[2505.21465](http://arxiv.org/abs/2505.21465)|null|
|**2025-05-27**|**LazyVLM: Neuro-Symbolic Approach to Video Analytics**|M. Tamer Özsu Team|[2505.21459](http://arxiv.org/abs/2505.21459)|null|
|**2025-05-27**|**DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models**|Soumik Sarkar Team|[2505.21382](http://arxiv.org/abs/2505.21382)|null|
|**2025-05-27**|**XBOUND: Exploring the Capability Boundaries of Device-Control Agents through Trajectory Tree Exploration**|Min Zhang Team|[2505.21279](http://arxiv.org/abs/2505.21279)|null|
|**2025-05-27**|**Interpreting Social Bias in LVLMs via Information Flow Analysis and Multi-Round Dialogue Evaluation**|Yutao Yue Team|[2505.21106](http://arxiv.org/abs/2505.21106)|null|
|**2025-05-27**|**DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response**|Naoto Yokoya Team|[2505.21089](http://arxiv.org/abs/2505.21089)|null|
|**2025-05-27**|**LPOI: Listwise Preference Optimization for Vision Language Models**|Gunhee Kim Team|[2505.21061](http://arxiv.org/abs/2505.21061)|null|
|**2025-05-27**|**RefAV: Towards Planning-Centric Scenario Mining**|Neehar Peri Team|[2505.20981](http://arxiv.org/abs/2505.20981)|null|
|**2025-05-27**|**On VLMs for Diverse Tasks in Multimodal Meme Classification**|Jasabanta Patro Team|[2505.20937](http://arxiv.org/abs/2505.20937)|null|
|**2025-05-27**|**A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models**|Bugeun Kim Team|[2505.20901](http://arxiv.org/abs/2505.20901)|null|
|**2025-05-27**|**AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding**|Joon Son Chung Team|[2505.20862](http://arxiv.org/abs/2505.20862)|null|
|**2025-05-27**|**Rendering-Aware Reinforcement Learning for Vector Graphics Generation**|Marco Pedersoli Team|[2505.20793](http://arxiv.org/abs/2505.20793)|null|
|**2025-05-27**|**FM-Planner: Foundation Model Guided Path Planning for Autonomous Drone Navigation**|Mir Feroskhan Team|[2505.20783](http://arxiv.org/abs/2505.20783)|null|
|**2025-05-27**|**Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in Vision-Language Models**|Yao Yang Team|[2505.20728](http://arxiv.org/abs/2505.20728)|null|
|**2025-05-27**|**ManiTaskGen: A Comprehensive Task Generator for Benchmarking and Improving Vision-Language Agents on Embodied Decision-Making**|Hao Su Team|[2505.20726](http://arxiv.org/abs/2505.20726)|null|
|**2025-05-27**|**Automating eHMI Action Design with LLMs for Automated Vehicle Communication**|Takeo Igarashi Team|[2505.20711](http://arxiv.org/abs/2505.20711)|null|
|**2025-05-27**|**GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to Elevate AI Reasoning**|Sundong Kim Team|[2505.20672](http://arxiv.org/abs/2505.20672)|null|
|**2025-05-26**|**Seeing is Believing, but How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models**|Naoto Yokoya Team|[2505.20236](http://arxiv.org/abs/2505.20236)|null|
|**2025-05-26**|**Agentic 3D Scene Generation with Spatially Contextualized VLMs**|Chi-Keung Tang Team|[2505.20129](http://arxiv.org/abs/2505.20129)|null|
|**2025-05-26**|**MEBench: A Novel Benchmark for Understanding Mutual Exclusivity Bias in Vision-Language Models**|James M. Rehg Team|[2505.20122](http://arxiv.org/abs/2505.20122)|null|
|**2025-05-27**|**EmoNet-Face: An Expert-Annotated Benchmark for Synthetic Emotion Recognition**|Sören Auer Team|[2505.20033](http://arxiv.org/abs/2505.20033)|null|
|**2025-05-26**|**ViTaPEs: Visuotactile Position Encodings for Cross-Modal Alignment in Multimodal Transformers**|Elmar Rückert Team|[2505.20032](http://arxiv.org/abs/2505.20032)|null|
|**2025-05-26**|**Decomposing Complex Visual Comprehension into Atomic Visual Skills for Vision Language Models**|Ernest K. Ryu Team|[2505.20021](http://arxiv.org/abs/2505.20021)|null|
|**2025-05-26**|**Can Visual Encoder Learn to See Arrows?**|Hiroaki Ozaki Team|[2505.19944](http://arxiv.org/abs/2505.19944)|null|
|**2025-05-26**|**Attention! You Vision Language Model Could Be Maliciously Manipulated**|Shudong Zhang Team|[2505.19911](http://arxiv.org/abs/2505.19911)|null|
|**2025-05-26**|**Underwater Diffusion Attention Network with Contrastive Language-Image Joint Learning for Underwater Image Enhancement**|Muzammil Behzad Team|[2505.19895](http://arxiv.org/abs/2505.19895)|null|
|**2025-05-26**|**One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP**|Kehuan Zhang Team|[2505.19840](http://arxiv.org/abs/2505.19840)|null|
|**2025-05-26**|**TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning**|Dongbin Zhao Team|[2505.19769](http://arxiv.org/abs/2505.19769)|null|
|**2025-05-26**|**Modeling Beyond MOS: Quality Assessment Models Must Integrate Context, Reasoning, and Multimodality**|Alessandro Bruno Team|[2505.19696](http://arxiv.org/abs/2505.19696)|null|
|**2025-05-26**|**Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs**|Shu-Tao Xia Team|[2505.19678](http://arxiv.org/abs/2505.19678)|null|
|**2025-05-26**|**JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models**|Yingchun Wang Team|[2505.19610](http://arxiv.org/abs/2505.19610)|null|
|**2025-05-26**|**What You Perceive Is What You Conceive: A Cognition-Inspired Framework for Open Vocabulary Image Segmentation**|Rongrong Ji Team|[2505.19569](http://arxiv.org/abs/2505.19569)|null|
|**2025-05-26**|**FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models**|Ruixuan Li Team|[2505.19536](http://arxiv.org/abs/2505.19536)|null|
|**2025-05-26**|**Locality-Aware Zero-Shot Human-Object Interaction Detection**|Minsu Cho Team|[2505.19503](http://arxiv.org/abs/2505.19503)|null|
|**2025-05-26**|**Enhancing Visual Reliance in Text Generation: A Bayesian Perspective on Mitigating Hallucination in Large Vision-Language Models**|Guoliang Kang Team|[2505.19498](http://arxiv.org/abs/2505.19498)|null|
|**2025-05-26**|**Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model**|Yu Cheng Team|[2505.19406](http://arxiv.org/abs/2505.19406)|null|
|**2025-05-27**|**DiffVLA: Vision-Language Guided Diffusion Planning for Autonomous Driving**|Hao Zhao Team|[2505.19381](http://arxiv.org/abs/2505.19381)|null|
|**2025-05-26**|**DiSa: Directional Saliency-Aware Prompt Learning for Generalizable Vision-Language Models**|Fatemeh Afghah Team|[2505.19373](http://arxiv.org/abs/2505.19373)|null|
|**2025-05-23**|**VideoGameBench: Can Vision-Language Models complete popular video games?**|Ofir Press Team|[2505.18134](http://arxiv.org/abs/2505.18134)|null|
|**2025-05-23**|**One RL to See Them All: Visual Triple Unified Reinforcement Learning**|Junjie Yan Team|[2505.18129](http://arxiv.org/abs/2505.18129)|null|
|**2025-05-23**|**CXReasonBench: A Benchmark for Evaluating Structured Diagnostic Reasoning in Chest X-rays**|Edward Choi Team|[2505.18087](http://arxiv.org/abs/2505.18087)|null|
|**2025-05-23**|**FDBPL: Faster Distillation-Based Prompt Learning for Region-Aware Vision-Language Models Adaptation**|Shibiao Xu Team|[2505.18053](http://arxiv.org/abs/2505.18053)|null|
|**2025-05-23**|**Clip4Retrofit: Enabling Real-Time Image Labeling on Edge Devices via Cross-Architecture CLIP Distillation**|Bogdan Sorin Coseriu Team|[2505.18039](http://arxiv.org/abs/2505.18039)|null|
|**2025-05-23**|**Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling**|Mun Yong Yi Team|[2505.17982](http://arxiv.org/abs/2505.17982)|null|
|**2025-05-23**|**VLM Models and Automated Grading of Atopic Dermatitis**|Hamed Ghodrati Team|[2505.17835](http://arxiv.org/abs/2505.17835)|null|
|**2025-05-23**|**Seeing It or Not? Interpretable Vision-aware Latent Steering to Mitigate Object Hallucinations**|Chao Shen Team|[2505.17812](http://arxiv.org/abs/2505.17812)|null|
|**2025-05-23**|**U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding**|Hongcheng Guo Team|[2505.17779](http://arxiv.org/abs/2505.17779)|null|
|**2025-05-23**|**SafeMVDrive: Multi-view Safety-Critical Driving Video Synthesis in the Real World Domain**|Yu Li Team|[2505.17727](http://arxiv.org/abs/2505.17727)|null|
|**2025-05-23**|**Seek-CAD: A Self-refined Generative Modeling for 3D Parametric CAD Using Local Inference via DeepSeek**|Xiangdong Zhou Team|[2505.17702](http://arxiv.org/abs/2505.17702)|null|
|**2025-05-23**|**Towards General Continuous Memory for Vision-Language Models**|Biwei Huang Team|[2505.17670](http://arxiv.org/abs/2505.17670)|null|
|**2025-05-23**|**EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications**|Min Yang Team|[2505.17654](http://arxiv.org/abs/2505.17654)|null|
|**2025-05-23**|**HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning**|Jianfei Yang Team|[2505.17645](http://arxiv.org/abs/2505.17645)|null|
|**2025-05-23**|**Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports**|Takahiro Omi Team|[2505.17625](http://arxiv.org/abs/2505.17625)|null|
|**2025-05-23**|**CAS-IQA: Teaching Vision-Language Models for Synthetic Angiography Quality Assessment**|Zeng-Guang Hou Team|[2505.17619](http://arxiv.org/abs/2505.17619)|null|
|**2025-05-23**|**Decoupled Visual Interpretation and Linguistic Reasoning for Math Problem Solving**|Wangmeng Zuo Team|[2505.17609](http://arxiv.org/abs/2505.17609)|null|
|**2025-05-23**|**A Unified Multi-Scale Attention-Based Network for Automatic 3D Segmentation of Lung Parenchyma & Nodules In Thoracic CT Images**|Furqan Shaukat Team|[2505.17602](http://arxiv.org/abs/2505.17602)|null|
|**2025-05-23**|**Multimodal Conversation Structure Understanding**|David Bamman Team|[2505.17536](http://arxiv.org/abs/2505.17536)|null|
|**2025-05-23**|**Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding**|Sungzoon Cho Team|[2505.17529](http://arxiv.org/abs/2505.17529)|null|
|**2025-05-22**|**Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models**|Mike Zheng Shou Team|[2505.16854](http://arxiv.org/abs/2505.16854)|**[link](https://github.com/kokolerk/ton)**|
|**2025-05-23**|**LaViDa: A Large Diffusion Language Model for Multimodal Understanding**|Aditya Grover Team|[2505.16839](http://arxiv.org/abs/2505.16839)|**[link](https://github.com/jacklishufan/lavida)**|
|**2025-05-22**|**From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization**|Huaxiu Yao Team|[2505.16832](http://arxiv.org/abs/2505.16832)|**[link](https://github.com/aiming-lab/eduvisbench)**|
|**2025-05-22**|**Perceptual Quality Assessment for Embodied AI**|Guangtao Zhai Team|[2505.16815](http://arxiv.org/abs/2505.16815)|**[link](https://github.com/lcysyzxdxc/embodiediqa)**|
|**2025-05-22**|**SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving**|Hongsheng Li Team|[2505.16805](http://arxiv.org/abs/2505.16805)|null|
|**2025-05-22**|**REOBench: Benchmarking Robustness of Earth Observation Foundation Models**|Tianjin Huang Team|[2505.16793](http://arxiv.org/abs/2505.16793)|**[link](https://github.com/lx709/reobench)**|
|**2025-05-22**|**Single Domain Generalization for Few-Shot Counting via Universal Representation Matching**|Xinghao Chen Team|[2505.16778](http://arxiv.org/abs/2505.16778)|**[link](https://github.com/jbr97/urm)**|
|**2025-05-22**|**IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models**|AiTi Aw Team|[2505.16774](http://arxiv.org/abs/2505.16774)|**[link](https://github.com/audiollms/audiobench)**|
|**2025-05-22**|**Self-Rewarding Large Vision-Language Models for Optimizing Prompts in Text-to-Image Generation**|Jianbing Shen Team|[2505.16763](http://arxiv.org/abs/2505.16763)|null|
|**2025-05-22**|**SD-MAD: Sign-Driven Few-shot Multi-Anomaly Detection in Medical Images**|Mahsa Baktashmotlagh Team|[2505.16659](http://arxiv.org/abs/2505.16659)|null|
|**2025-05-22**|**Point, Detect, Count: Multi-Task Medical Image Understanding with Instruction-Tuned Vision-Language Models**|Pål Halvorsen Team|[2505.16647](http://arxiv.org/abs/2505.16647)|null|
|**2025-05-22**|**MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation**|Zongqing Lu Team|[2505.16602](http://arxiv.org/abs/2505.16602)|null|
|**2025-05-22**|**ManipLVM-R1: Reinforcement Learning for Reasoning in Embodied Manipulation with Large Vision-Language Models**|Xiuying Chen Team|[2505.16517](http://arxiv.org/abs/2505.16517)|null|
|**2025-05-22**|**Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models**|Yaochu Jin Team|[2505.16446](http://arxiv.org/abs/2505.16446)|null|
|**2025-05-22**|**Circle-RoPE: Cone-like Decoupled Rotary Positional Embedding for Large Vision-Language Models**|Kai Han Team|[2505.16416](http://arxiv.org/abs/2505.16416)|**[link](https://github.com/lose4578/circlerope)**|
|**2025-05-22**|**Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression**|Souvik Kundu Team|[2505.16411](http://arxiv.org/abs/2505.16411)|**[link](https://github.com/yueche77/spin)**|
|**2025-05-22**|**VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving**|Samuel Labi Team|[2505.16377](http://arxiv.org/abs/2505.16377)|null|
|**2025-05-22**|**MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing**|Xinhan Di Team|[2505.16279](http://arxiv.org/abs/2505.16279)|null|
|**2025-05-22**|**When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification**|Jiaheng Wei Team|[2505.16149](http://arxiv.org/abs/2505.16149)|null|
|**2025-05-22**|**Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation**|Junfeng Fang Team|[2505.16146](http://arxiv.org/abs/2505.16146)|null|
|**2025-05-21**|**InstructSAM: A Training-Free Framework for Instruction-Oriented Remote Sensing Object Recognition**|Xue Yang Team|[2505.15818](http://arxiv.org/abs/2505.15818)|null|
|**2025-05-21**|**From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems**|Soujanya Poria Team|[2505.15685](http://arxiv.org/abs/2505.15685)|null|
|**2025-05-21**|**FragFake: A Dataset for Fine-Grained Detection of Edited Images with Vision Language Models**|Qian Wang Team|[2505.15644](http://arxiv.org/abs/2505.15644)|null|
|**2025-05-21**|**Visual Perturbation and Adaptive Hard Negative Contrastive Learning for Compositional Reasoning in Vision-Language Models**|Ya Wang Team|[2505.15576](http://arxiv.org/abs/2505.15576)|**[link](https://github.com/nynu-bdai/ahnpl)**|
|**2025-05-21**|**TinyDrive: Multiscale Visual Question Answering with Selective Token Routing for Autonomous Driving**|Abdallah Shami Team|[2505.15564](http://arxiv.org/abs/2505.15564)|null|
|**2025-05-21**|**Clapper: Compact Learning and Video Representation in VLMs**|Fuzheng Zhang Team|[2505.15529](http://arxiv.org/abs/2505.15529)|null|
|**2025-05-21**|**Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets**|Ken Goldberg Team|[2505.15517](http://arxiv.org/abs/2505.15517)|null|
|**2025-05-21**|**Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought**|Libo Qin Team|[2505.15510](http://arxiv.org/abs/2505.15510)|null|
|**2025-05-21**|**Prompt Tuning Vision Language Models with Margin Regularizer for Few-Shot Learning under Distribution Shifts**|Soma Biswas Team|[2505.15506](http://arxiv.org/abs/2505.15506)|**[link](https://github.com/debarshigit/promptmargin)**|
|**2025-05-21**|**Beyond Linearity: Squeeze-and-Recalibrate Blocks for Few-Shot Whole Slide Image Classification**|Irwin King Team|[2505.15504](http://arxiv.org/abs/2505.15504)|null|
|**2025-05-21**|**Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models**|Bryan Hooi Team|[2505.15489](http://arxiv.org/abs/2505.15489)|null|
|**2025-05-21**|**Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL**|Qing Li Team|[2505.15436](http://arxiv.org/abs/2505.15436)|null|
|**2025-05-21**|**TimeCausality: Evaluating the Causal Ability in Time Dimension for Vision Language Models**|Keze Wang Team|[2505.15435](http://arxiv.org/abs/2505.15435)|null|
|**2025-05-21**|**On the Robustness of Medical Vision-Language Models: Are they Truly Generalizable?**|Mohammad Yaqub Team|[2505.15425](http://arxiv.org/abs/2505.15425)|null|
|**2025-05-21**|**Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study**|Hwanjo Yu Team|[2505.15389](http://arxiv.org/abs/2505.15389)|null|
|**2025-05-21**|**RAZER: Robust Accelerated Zero-Shot 3D Open-Vocabulary Panoptic Reconstruction with Spatio-Temporal Aggregation**|Farshad Khorrami Team|[2505.15373](http://arxiv.org/abs/2505.15373)|null|
|**2025-05-21**|**Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition**|Youngsook Song Team|[2505.15367](http://arxiv.org/abs/2505.15367)|null|
|**2025-05-21**|**AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving**|Diange Yang Team|[2505.15298](http://arxiv.org/abs/2505.15298)|null|
|**2025-05-21**|**Blind Spot Navigation: Evolutionary Discovery of Sensitive Semantic Concepts for LVLMs**|Zibin Zheng Team|[2505.15265](http://arxiv.org/abs/2505.15265)|null|
|**2025-05-21**|**Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation**|Kyomin Jung Team|[2505.15249](http://arxiv.org/abs/2505.15249)|null|
|**2025-05-20**|**UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens**|Wentao Zhang Team|[2505.14671](http://arxiv.org/abs/2505.14671)|null|
|**2025-05-20**|**CAD-Coder: An Open-Source Vision-Language Model for Computer-Aided Design Code Generation**|Faez Ahmed Team|[2505.14646](http://arxiv.org/abs/2505.14646)|null|
|**2025-05-20**|**Debating for Better Reasoning: An Unsupervised Multimodal Approach**|Mirella Lapata Team|[2505.14627](http://arxiv.org/abs/2505.14627)|null|
|**2025-05-21**|**PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models**|Wenjia Zhang Team|[2505.14481](http://arxiv.org/abs/2505.14481)|null|
|**2025-05-20**|**RAVENEA: A Benchmark for Multimodal Retrieval-Augmented Visual Culture Understanding**|Serge Belongie Team|[2505.14462](http://arxiv.org/abs/2505.14462)|**[link](https://github.com/yfyuan01/ravenea)**|
|**2025-05-20**|**SCAN: Semantic Document Layout Analysis for Textual and Visual Retrieval-Augmented Generation**|Masafumi Oyamada Team|[2505.14381](http://arxiv.org/abs/2505.14381)|null|
|**2025-05-20**|**Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds**|Agnieszka Wykowska Team|[2505.14366](http://arxiv.org/abs/2505.14366)|null|
|**2025-05-20**|**DeepEyes: Incentivizing "Thinking with Images" via Reinforcement Learning**|Xing Yu Team|[2505.14362](http://arxiv.org/abs/2505.14362)|**[link](https://github.com/visual-agent/deepeyes)**|
|**2025-05-20**|**Vision-Language Modeling Meets Remote Sensing: Models, Datasets and Perspectives**|Gui-Song Xia Team|[2505.14361](http://arxiv.org/abs/2505.14361)|null|
|**2025-05-20**|**Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey**|Dongwoo Kim Team|[2505.14340](http://arxiv.org/abs/2505.14340)|null|
|**2025-05-20**|**Aligning Attention Distribution to Information Flow for Hallucination Mitigation in Large Vision-Language Models**|Chong Feng Team|[2505.14257](http://arxiv.org/abs/2505.14257)|null|
|**2025-05-20**|**Visual Agentic Reinforcement Fine-Tuning**|Jiaqi Wang Team|[2505.14246](http://arxiv.org/abs/2505.14246)|**[link](https://github.com/liuziyu77/visual-rft)**|
|**2025-05-20**|**VoQA: Visual-only Question Answering**|Lei Huang Team|[2505.14227](http://arxiv.org/abs/2505.14227)|null|
|**2025-05-20**|**Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models**|Matthew Purver Team|[2505.14160](http://arxiv.org/abs/2505.14160)|null|
|**2025-05-20**|**Building a Stable Planner: An Extended Finite State Machine Based Planning Module for Mobile GUI Agent**|Xuming Hu Team|[2505.14141](http://arxiv.org/abs/2505.14141)|null|
|**2025-05-20**|**NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in Brain MRI**|Benedikt Wiestler Team|[2505.14064](http://arxiv.org/abs/2505.14064)|null|
|**2025-05-20**|**ShieldVLM: Safeguarding the Multimodal Implicit Toxicity via Deliberative Reasoning with LVLMs**|Minlie Huang Team|[2505.14035](http://arxiv.org/abs/2505.14035)|null|
|**2025-05-20**|**Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models**|Yalin Wang Team|[2505.13973](http://arxiv.org/abs/2505.13973)|null|
|**2025-05-20**|**APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight**|Ambuj Singh Team|[2505.13921](http://arxiv.org/abs/2505.13921)|**[link](https://github.com/hwj20/apex_exp)**|
|**2025-05-20**|**InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning**|Jingkuan Song Team|[2505.13888](http://arxiv.org/abs/2505.13888)|null|
|**2025-05-19**|**ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models**|Greg Durrett Team|[2505.13444](http://arxiv.org/abs/2505.13444)|null|
|**2025-05-19**|**G1: Bootstrapping Perception and Reasoning Abilities of Vision-Language Model via Reinforcement Learning**|Baobao Chang Team|[2505.13426](http://arxiv.org/abs/2505.13426)|**[link](https://github.com/chenllliang/g1)**|
|**2025-05-19**|**Seeing, Saying, Solving: An LLM-to-TL Framework for Cooperative Robots**|Shreyas Kousik Team|[2505.13376](http://arxiv.org/abs/2505.13376)|null|
|**2025-05-20**|**Unlabeled Data or Pre-trained Model: Rethinking Semi-Supervised Learning and Pretrain-Finetuning**|Lan-Zhe Guo Team|[2505.13317](http://arxiv.org/abs/2505.13317)|null|
|**2025-05-19**|**I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models**|R. Maria del Rio-Chanona Team|[2505.13302](http://arxiv.org/abs/2505.13302)|**[link](https://github.com/3lis/misinfo_vlm)**|
|**2025-05-19**|**Computer Vision Models Show Human-Like Sensitivity to Geometric and Topological Concepts**|Sashank Varma Team|[2505.13281](http://arxiv.org/abs/2505.13281)|null|
|**2025-05-19**|**From Local Details to Global Context: Advancing Vision-Language Models with Attention-Based Selection**|Jian Liang Team|[2505.13233](http://arxiv.org/abs/2505.13233)|**[link](https://github.com/bit-da/abs)**|
|**2025-05-19**|**ViPlan: A Benchmark for Visual Planning with Symbolic Predicates and Vision-Language Models**|Pekka Marttinen Team|[2505.13180](http://arxiv.org/abs/2505.13180)|**[link](https://github.com/merlerm/viplan)**|
|**2025-05-19**|**Hearing from Silence: Reasoning Audio Descriptions from Silent Videos via Vision-Language Model**|Dong Yu Team|[2505.13062](http://arxiv.org/abs/2505.13062)|null|
|**2025-05-20**|**3D Visual Illusion Depth Estimation**|Yunde Jia Team|[2505.13061](http://arxiv.org/abs/2505.13061)|**[link](https://github.com/yaochengtang/3d-visual-illusion-depth-estimation)**|
|**2025-05-19**|**MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO**|Ying Shan Team|[2505.13031](http://arxiv.org/abs/2505.13031)|**[link](https://github.com/easonxiao-888/mindomni)**|
|**2025-05-19**|**Uniformity First: Uniformity-aware Test-time Adaptation of Vision-language Models against Image Corruption**|Tomoki Hamagami Team|[2505.12912](http://arxiv.org/abs/2505.12912)|**[link](https://github.com/kzkadc/uninfo)**|
|**2025-05-19**|**TinyAlign: Boosting Lightweight Vision-Language Models by Mitigating Modal Alignment Bottlenecks**|Jin Dong Team|[2505.12884](http://arxiv.org/abs/2505.12884)|null|
|**2025-05-19**|**FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models**|Renxin Zhong Team|[2505.12835](http://arxiv.org/abs/2505.12835)|null|
|**2025-05-19**|**VLC Fusion: Vision-Language Conditioned Sensor Fusion for Robust Object Detection**|Ransalu Senanayake Team|[2505.12715](http://arxiv.org/abs/2505.12715)|null|
|**2025-05-19**|**TS-VLM: Text-Guided SoftSort Pooling for Vision-Language Models in Multi-View Driving Reasoning**|Soodeh Nikan Team|[2505.12670](http://arxiv.org/abs/2505.12670)|null|
|**2025-05-19**|**Predicting Reaction Time to Comprehend Scenes with Foveated Scene Understanding Maps**|Miguel P. Eckstein Team|[2505.12660](http://arxiv.org/abs/2505.12660)|null|
|**2025-05-19**|**AutoMat: Enabling Automated Crystal Structure Reconstruction from Microscopy via Agentic Tool Use**|Fei Wei Team|[2505.12650](http://arxiv.org/abs/2505.12650)|**[link](https://github.com/yyt-2378/automat)**|
|**2025-05-19**|**Use as Many Surrogates as You Want: Selective Ensemble Attack to Unleash Transferability without Sacrificing Resource Efficiency**|Zhengyu Zhao Team|[2505.12644](http://arxiv.org/abs/2505.12644)|null|
|**2025-05-19**|**Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents**|Honglak Lee Team|[2505.12632](http://arxiv.org/abs/2505.12632)|null|
|**2025-05-16**|**Patho-R1: A Multimodal Reinforcement Learning-Based Pathology Expert Reasoner**|Hong Bu Team|[2505.11404](http://arxiv.org/abs/2505.11404)|null|
|**2025-05-16**|**Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual Search in the Wild**|Guillaume Sartoretti Team|[2505.11350](http://arxiv.org/abs/2505.11350)|null|
|**2025-05-16**|**Temporally-Grounded Language Generation: A Benchmark for Real-Time Vision-Language Models**|Joyce Chai Team|[2505.11326](http://arxiv.org/abs/2505.11326)|null|
|**2025-05-16**|**Sample Efficient Reinforcement Learning via Large Vision Language Model Distillation**|Chang D. Yoo Team|[2505.11221](http://arxiv.org/abs/2505.11221)|null|
|**2025-05-16**|**Redundancy-Aware Pretraining of Vision-Language Foundation Models in Remote Sensing**|Begüm Demir Team|[2505.11121](http://arxiv.org/abs/2505.11121)|null|
|**2025-05-16**|**CUBIC: Concept Embeddings for Unsupervised Bias Identification using VLMs**|Natalia Díaz-Rodríguez Team|[2505.11060](http://arxiv.org/abs/2505.11060)|null|
|**2025-05-16**|**Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere**|Prashant Singh Team|[2505.11029](http://arxiv.org/abs/2505.11029)|null|
|**2025-05-16**|**On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating**|Alessandro Rinaldo Team|[2505.10860](http://arxiv.org/abs/2505.10860)|null|
|**2025-05-16**|**Benchmarking performance, explainability, and evaluation strategies of vision-language models for surgery: Challenges and opportunities**|Shan Lin Team|[2505.10764](http://arxiv.org/abs/2505.10764)|null|
|**2025-05-15**|**GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?**|Tanwi Mallick Team|[2505.10714](http://arxiv.org/abs/2505.10714)|null|
|**2025-05-15**|**MOSAIC: A Multi-View 2.5D Organ Slice Selector with Cross-Attentional Reasoning for Anatomically-Aware CT Localization in Medical Organ Segmentation**|Muzammil Behzad Team|[2505.10672](http://arxiv.org/abs/2505.10672)|null|
|**2025-05-15**|**CLIP Embeddings for AI-Generated Image Detection: A Few-Shot Study with Lightweight Classifier**|Ziyang Ou Team|[2505.10664](http://arxiv.org/abs/2505.10664)|null|
|**2025-05-15**|**Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding**|Chong Feng Team|[2505.10634](http://arxiv.org/abs/2505.10634)|null|
|**2025-05-15**|**MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly**|Mark Steedman Team|[2505.10610](http://arxiv.org/abs/2505.10610)|null|
|**2025-05-18**|**MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative Decoding of Vision-Language Models**|Vithursan Thangarasa Team|[2505.10526](http://arxiv.org/abs/2505.10526)|null|
|**2025-05-16**|**AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges**|Manoj Karkee Team|[2505.10468](http://arxiv.org/abs/2505.10468)|null|
|**2025-05-15**|**Vision language models have difficulty recognizing virtual objects**|J. G. Trafton Team|[2505.10453](http://arxiv.org/abs/2505.10453)|null|
|**2025-05-15**|**MMRL++: Parameter-Efficient and Interaction-Aware Representation Learning for Vision-Language Models**|Xiaodong Gu Team|[2505.10088](http://arxiv.org/abs/2505.10088)|**[link](https://github.com/yunncheng/MMRL)**|
|**2025-05-15**|**AdaptCLIP: Adapting CLIP for Universal Visual Anomaly Detection**|Chengjie Wang Team|[2505.09926](http://arxiv.org/abs/2505.09926)|**[link](https://github.com/gaobb/AdaptCLIP)**|
|**2025-05-14**|**Unfettered Forceful Skill Acquisition with Physical Reasoning and Coordinate Frame Labeling**|Nikolaus Correll Team|[2505.09731](http://arxiv.org/abs/2505.09731)|null|
|**2025-05-14**|**ManipBench: Benchmarking Vision-Language Models for Low-Level Robot Manipulation**|Daniel Seita Team|[2505.09698](http://arxiv.org/abs/2505.09698)|null|
|**2025-05-14**|**LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models**|Yanan Sun Team|[2505.09659](http://arxiv.org/abs/2505.09659)|**[link](https://github.com/lc783/las)**|
|**2025-05-14**|**Variational Visual Question Answering**|Marcus Rohrbach Team|[2505.09591](http://arxiv.org/abs/2505.09591)|null|
|**2025-05-14**|**VTLA: Vision-Tactile-Language-Action Model with Preference Learning for Insertion Manipulation**|Shuo Wang Team|[2505.09577](http://arxiv.org/abs/2505.09577)|null|
|**2025-05-14**|**Flash-VL 2B: Optimizing Vision-Language Model Performance for Ultra-Low Latency and High Throughput**|Lin Ma Team|[2505.09498](http://arxiv.org/abs/2505.09498)|null|
|**2025-05-14**|**Unsupervised Multiview Contrastive Language-Image Joint Learning with Pseudo-Labeled Prompts Via Vision-Language Model for 3D/4D Facial Expression Recognition**|Muzammil Behzad Team|[2505.09336](http://arxiv.org/abs/2505.09336)|null|
|**2025-05-14**|**MetaUAS: Universal Anomaly Segmentation with One-Prompt Meta-Learning**|Bin-Bin Gao Team|[2505.09265](http://arxiv.org/abs/2505.09265)|null|
|**2025-05-14**|**Beyond General Prompts: Automated Prompt Refinement using Contrastive Class Alignment Scores for Disambiguating Objects in Vision-Language Models**|Ross Greer Team|[2505.09139](http://arxiv.org/abs/2505.09139)|null|
|**2025-05-14**|**Seeing Beyond the Scene: Enhancing Vision-Language Models with Interactional Reasoning**|Qing Li Team|[2505.09118](http://arxiv.org/abs/2505.09118)|null|
|**2025-05-14**|**OpenLKA: An Open Dataset of Lane Keeping Assist from Recent Car Models under Real-world Driving Conditions**|Hao Zhou Team|[2505.09092](http://arxiv.org/abs/2505.09092)|**[link](https://github.com/openlka/openlka)**|
|**2025-05-13**|**Prioritizing Image-Related Tokens Enhances Vision-Language Pre-Training**|Heng Ji Team|[2505.08971](http://arxiv.org/abs/2505.08971)|**[link](https://github.com/yangyi-chen/prior)**|
|**2025-05-15**|**Behind Maya: Building a Multilingual Vision Language Model**|Alham Fikri Aji Team|[2505.08910](http://arxiv.org/abs/2505.08910)|**[link](https://github.com/nahidalam/maya)**|
|**2025-05-12**|**Position: Restructuring of Categories and Implementation of Guidelines Essential for VLM Adoption in Healthcare**|Imon Banerjee Team|[2505.08818](http://arxiv.org/abs/2505.08818)|null|
|**2025-05-13**|**Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving**|Xiang Bai Team|[2505.08725](http://arxiv.org/abs/2505.08725)|**[link](https://github.com/zc-zhao/drivemonkey)**|
|**2025-05-13**|**OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning**|Yu Cheng Team|[2505.08617](http://arxiv.org/abs/2505.08617)|**[link](https://github.com/zhaochen0110/openthinkimg)**|
|**2025-05-13**|**From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation**|Jianye Hao Team|[2505.08548](http://arxiv.org/abs/2505.08548)|null|
|**2025-05-13**|**Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?**|Jimmy Huang Team|[2505.08468](http://arxiv.org/abs/2505.08468)|**[link](https://github.com/tahmedge/chart_lvlm_judge)**|
|**2025-05-13**|**MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos**|Wei Zhang Team|[2505.08367](http://arxiv.org/abs/2505.08367)|null|
|**2025-05-13**|**Removing Watermarks with Partial Regeneration using Semantic Information**|Michael W. Mahoney Team|[2505.08234](http://arxiv.org/abs/2505.08234)|**[link](https://github.com/krtit/semanticregen)**|
|**2025-05-13**|**CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding**|Shuo Wang Team|[2505.08194](http://arxiv.org/abs/2505.08194)|null|
|**2025-05-13**|**DSADF: Thinking Fast and Slow for Decision Making**|Shufei Zhang Team|[2505.08189](http://arxiv.org/abs/2505.08189)|null|
|**2025-05-12**|**Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models**|Jia-Bin Huang Team|[2505.07815](http://arxiv.org/abs/2505.07815)|null|
|**2025-05-12**|**Reproducibility, Replicability, and Insights into Visual Document Retrieval with Late Interaction**|Andrew Yates Team|[2505.07730](http://arxiv.org/abs/2505.07730)|null|
|**2025-05-12**|**Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images**|Vasily Konovalov Team|[2505.07704](http://arxiv.org/abs/2505.07704)|null|
|**2025-05-12**|**Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models**|Yihong Gong Team|[2505.07690](http://arxiv.org/abs/2505.07690)|null|
|**2025-05-12**|**Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\mathbf{\texttt{D}}$ual-$\mathbf{\texttt{H}}$ead $\mathbf{\texttt{O}}$ ptimization**|Sung Ju Hwang Team|[2505.07675](http://arxiv.org/abs/2505.07675)|null|
|**2025-05-12**|**Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning**|Hanwang Zhang Team|[2505.07538](http://arxiv.org/abs/2505.07538)|null|
|**2025-05-12**|**AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography**|Xiaomeng Li Team|[2505.07347](http://arxiv.org/abs/2505.07347)|null|
|**2025-05-12**|**Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning**|Yahui Zhou Team|[2505.07263](http://arxiv.org/abs/2505.07263)|null|
|**2025-05-12**|**Incomplete In-context Learning**|Yangshijie Zhang Team|[2505.07251](http://arxiv.org/abs/2505.07251)|null|
|**2025-05-12**|**UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning**|Dzmitry Tsetserukou Team|[2505.07236](http://arxiv.org/abs/2505.07236)|null|
|**2025-05-12**|**Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection**|Ningjiang Chen Team|[2505.07219](http://arxiv.org/abs/2505.07219)|**[link](https://github.com/qinhongda8/ldds)**|
|**2025-05-12**|**Internet of Agents: Fundamentals, Applications, and Challenges**|Dusit Niyato Team|[2505.07176](http://arxiv.org/abs/2505.07176)|null|
|**2025-05-12**|**Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning**|Weiping Wang Team|[2505.07172](http://arxiv.org/abs/2505.07172)|null|
|**2025-05-12**|**EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis**|Eunil Park Team|[2505.07164](http://arxiv.org/abs/2505.07164)|null|
|**2025-05-11**|**A Vision-Language Foundation Model for Leaf Disease Identification**|Luyl-Da Quach Team|[2505.07019](http://arxiv.org/abs/2505.07019)|null|
|**2025-05-11**|**Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models**|Binod Bhattarai Team|[2505.07001](http://arxiv.org/abs/2505.07001)|null|
|**2025-05-11**|**UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms**|Zhenze Liu Team|[2505.06832](http://arxiv.org/abs/2505.06832)|null|
|**2025-05-10**|**STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation**|Jean Oh Team|[2505.06729](http://arxiv.org/abs/2505.06729)|null|
|**2025-05-10**|**METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection**|Shuo Yang Team|[2505.06663](http://arxiv.org/abs/2505.06663)|**[link](https://github.com/wangyongqi558/METOR)**|
|**2025-05-10**|**Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation**|Nancy F. Chen Team|[2505.06594](http://arxiv.org/abs/2505.06594)|null|
|**2025-05-09**|**MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks**|Bo Yan Team|[2505.06152](http://arxiv.org/abs/2505.06152)|**[link](https://github.com/zwq803/mm-skin)**|
|**2025-05-09**|**Leveraging Vision-Language Models for Visual Grounding and Analysis of Automotive UI**|Dominik Bollmann Team|[2505.05895](http://arxiv.org/abs/2505.05895)|null|
|**2025-05-09**|**Describe Anything in Medical Images**|Min Xu Team|[2505.05804](http://arxiv.org/abs/2505.05804)|null|
|**2025-05-09**|**3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks**|Farshad Khorrami Team|[2505.05800](http://arxiv.org/abs/2505.05800)|null|
|**2025-05-08**|**Fine-Tuning Video-Text Contrastive Model for Primate Behavior Retrieval from Unlabeled Raw Videos**|Nina S. T. Hirata Team|[2505.05681](http://arxiv.org/abs/2505.05681)|null|
|**2025-05-08**|**X-Transfer Attacks: Towards Super Transferable Adversarial Attacks on CLIP**|James Bailey Team|[2505.05528](http://arxiv.org/abs/2505.05528)|**[link](https://github.com/HanxunH/XTransferBench)**|
|**2025-05-08**|**Bring Reason to Vision: Understanding Perception and Reasoning through Model Merging**|Junxian He Team|[2505.05464](http://arxiv.org/abs/2505.05464)|**[link](https://github.com/shiqichen17/vlm_merging)**|
|**2025-05-08**|**SITE: towards Spatial Intelligence Thorough Evaluation**|Boqing Gong Team|[2505.05456](http://arxiv.org/abs/2505.05456)|null|
|**2025-05-08**|**DSDrive: Distilling Large Language Model for Lightweight End-to-End Autonomous Driving with Unified Reasoning and Planning**|Jun Ma Team|[2505.05360](http://arxiv.org/abs/2505.05360)|null|
|**2025-05-08**|**Hearing and Seeing Through CLIP: A Framework for Self-Supervised Sound Source Localization**|Joon Son Chung Team|[2505.05343](http://arxiv.org/abs/2505.05343)|**[link](https://github.com/swimmiing/ACL-SSL)**|
|**2025-05-08**|**Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects**|Matteo Matteucci Team|[2505.05318](http://arxiv.org/abs/2505.05318)|null|
|**2025-05-08**|**Biomed-DPT: Dual Modality Prompt Tuning for Biomedical Vision-Language Models**|Meng Zhang Team|[2505.05189](http://arxiv.org/abs/2505.05189)|null|
|**2025-05-08**|**OpenworldAUC: Towards Unified Evaluation and Optimization for Open-world Prompt Tuning**|Qingming Huang Team|[2505.05180](http://arxiv.org/abs/2505.05180)|**[link](https://github.com/huacong/openworldauc)**|
|**2025-05-08**|**Probabilistic Embeddings for Frozen Vision-Language Models: Uncertainty Quantification with Gaussian Process Latent Variable Models**|Joachim Denzler Team|[2505.05163](http://arxiv.org/abs/2505.05163)|null|
|**2025-05-08**|**CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language Models**|Furao Shen Team|[2505.05130](http://arxiv.org/abs/2505.05130)|null|
|**2025-05-08**|**X-Driver: Explainable Autonomous Driving with Vision-Language Models**|Zengfeng Zeng Team|[2505.05098](http://arxiv.org/abs/2505.05098)|null|
|**2025-05-08**|**Image-Text Relation Prediction for Multilingual Tweets**|Edison Marrese-Taylor Team|[2505.05040](http://arxiv.org/abs/2505.05040)|null|
|**2025-05-09**|**G-FOCUS: Towards a Robust Method for Assessing UI Design Persuasiveness**|Youngjae Yu Team|[2505.05026](http://arxiv.org/abs/2505.05026)|null|
|**2025-05-08**|**Split Matching for Inductive Zero-shot Semantic Segmentation**|Daisuke Deguchi Team|[2505.05023](http://arxiv.org/abs/2505.05023)|null|
|**2025-05-08**|**LVLM-MPC Collaboration for Autonomous Driving: A Safety-Aware and Task-Scalable Control Architecture**|Tatsuya Suzuki Team|[2505.04980](http://arxiv.org/abs/2505.04980)|null|
|**2025-05-07**|**Vision-Language-Action Models: Concepts, Progress, Applications and Challenges**|Manoj Karkee Team|[2505.04769](http://arxiv.org/abs/2505.04769)|null|
|**2025-05-07**|**"I Can See Forever!": Evaluating Real-time VideoLLMs for Assisting Individuals with Visual Impairments**|Xinlei He Team|[2505.04488](http://arxiv.org/abs/2505.04488)|null|
|**2025-05-07**|**DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception**|Zhuotao Tian Team|[2505.04410](http://arxiv.org/abs/2505.04410)|**[link](https://github.com/xiaomoguhz/declip)**|
|**2025-05-07**|**CM1 -- A Dataset for Evaluating Few-Shot Information Extraction with Large Vision Language Models**|Gernot A. Fink Team|[2505.04214](http://arxiv.org/abs/2505.04214)|null|
|**2025-05-07**|**R^3-VQA: "Read the Room" by Video Social Reasoning**|Lifeng Fan Team|[2505.04147](http://arxiv.org/abs/2505.04147)|null|
|**2025-05-06**|**X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains**|Hoifung Poon Team|[2505.03981](http://arxiv.org/abs/2505.03981)|null|
|**2025-05-06**|**Fill the Gap: Quantifying and Reducing the Modality Gap in Image-Text Representation Learning**|Victor Amblard Team|[2505.03703](http://arxiv.org/abs/2505.03703)|null|
|**2025-05-06**|**Distribution-Conditional Generation: From Class Distribution to Creative Generation**|Xin Geng Team|[2505.03667](http://arxiv.org/abs/2505.03667)|null|
|**2025-05-06**|**Learning Unknown Spoof Prompts for Generalized Face Anti-Spoofing Using Only Real Face Images**|Zhenan Sun Team|[2505.03611](http://arxiv.org/abs/2505.03611)|null|
|**2025-05-06**|**Learning Knowledge-based Prompts for Robust 3D Mask Presentation Attack Detection**|Ming-Hsuan Yang Team|[2505.03610](http://arxiv.org/abs/2505.03610)|null|
|**2025-05-06**|**Mitigating Image Captioning Hallucinations in Vision-Language Models**|Xi Li Team|[2505.03420](http://arxiv.org/abs/2505.03420)|null|
|**2025-05-07**|**Enhancing Target-unspecific Tasks through a Features Matrix**|Jun Yu Team|[2505.03414](http://arxiv.org/abs/2505.03414)|null|
|**2025-05-06**|**Reducing Annotation Burden in Physical Activity Research Using Vision-Language Models**|Aiden Doherty Team|[2505.03374](http://arxiv.org/abs/2505.03374)|null|
|**2025-05-06**|**A Vision-Language Model for Focal Liver Lesion Classification**|Chen Yen-Wei Team|[2505.03350](http://arxiv.org/abs/2505.03350)|null|
|**2025-05-06**|**From Word to Sentence: A Large-Scale Multi-Instance Dataset for Open-Set Aerial Detection**|Rong Xiao Team|[2505.03334](http://arxiv.org/abs/2505.03334)|null|
|**2025-05-06**|**Seeing the Abstract: Translating the Abstract Language for Vision Language Models**|Yiming Wang Team|[2505.03242](http://arxiv.org/abs/2505.03242)|**[link](https://github.com/davidetalon/fashionact)**|
|**2025-05-06**|**VLM Q-Learning: Aligning Vision-Language Models for Interactive Decision-Making**|Juan Carlos Niebles Team|[2505.03181](http://arxiv.org/abs/2505.03181)|null|
|**2025-05-06**|**Robust Fairness Vision-Language Learning for Medical Image Analysis**|Shu Hu Team|[2505.03153](http://arxiv.org/abs/2505.03153)|**[link](https://github.com/purdue-m2/robust_fairness_for_medical_image)**|
|**2025-05-05**|**Adversarial Robustness Analysis of Vision-Language Models in Medical Image Segmentation**|Manish Dhakal Team|[2505.02971](http://arxiv.org/abs/2505.02971)|null|
|**2025-05-05**|**LISAT: Language-Instructed Segmentation Assistant for Satellite Imagery**|David M. Chan Team|[2505.02829](http://arxiv.org/abs/2505.02829)|null|
|**2025-05-05**|**HapticVLM: VLM-Driven Texture Recognition Aimed at Intelligent Haptic Interaction**|Dzmitry Tsetserukou Team|[2505.02569](http://arxiv.org/abs/2505.02569)|null|
|**2025-05-05**|**Tevatron 2.0: Unified Document Retrieval Toolkit across Scale, Language, and Modality**|Jimmy Lin Team|[2505.02466](http://arxiv.org/abs/2505.02466)|null|
|**2025-05-05**|**Recent Advances in Out-of-Distribution Detection with CLIP-Like Models: A Survey**|Songcan Chen Team|[2505.02448](http://arxiv.org/abs/2505.02448)|null|
|**2025-05-05**|**SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing**|Sijie Zhu Team|[2505.02370](http://arxiv.org/abs/2505.02370)|**[link](https://github.com/bytedance/superedit)**|
|**2025-05-05**|**TeDA: Boosting Vision-Lanuage Models for Zero-Shot 3D Object Retrieval via Testing-time Distribution Alignment**|Xinwei He Team|[2505.02325](http://arxiv.org/abs/2505.02325)|null|
|**2025-05-04**|**Compositional Image-Text Matching and Retrieval by Grounding Entities**|Jana Košecká Team|[2505.02278](http://arxiv.org/abs/2505.02278)|null|
|**2025-05-04**|**Handling Imbalanced Pseudolabels for Vision-Language Models with Concept Alignment and Confusion-Aware Calibrated Margin**|Xinyang Chen Team|[2505.02056](http://arxiv.org/abs/2505.02056)|null|
|**2025-05-04**|**A Comprehensive Analysis for Visual Object Hallucination in Large Vision-Language Models**|Xinya Du Team|[2505.01958](http://arxiv.org/abs/2505.01958)|null|
|**2025-05-03**|**PhysNav-DG: A Novel Adaptive Framework for Robust VLM-Sensor Fusion in Navigation Applications**|Santosh Patapati Team|[2505.01881](http://arxiv.org/abs/2505.01881)|null|
|**2025-05-03**|**Enhancing the Learning Experience: Using Vision-Language Models to Generate Questions for Educational Videos**|Anett Hoppe Team|[2505.01790](http://arxiv.org/abs/2505.01790)|null|
|**2025-05-03**|**An LLM-Empowered Low-Resolution Vision System for On-Device Human Behavior Understanding**|Guoliang Xing Team|[2505.01743](http://arxiv.org/abs/2505.01743)|null|
|**2025-05-03**|**Vision and Intention Boost Large Language Model in Long-Term Action Anticipation**|Yanning Zhang Team|[2505.01713](http://arxiv.org/abs/2505.01713)|null|
|**2025-05-03**|**RoBridge: A Hierarchical Architecture Bridging Cognition and Execution for General Robotic Manipulation**|Xiaodan Liang Team|[2505.01709](http://arxiv.org/abs/2505.01709)|null|
|**2025-05-03**|**Topology-Aware CLIP Few-Shot Learning**|Dazhi Huang Team|[2505.01694](http://arxiv.org/abs/2505.01694)|null|
|**2025-05-02**|**TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action**|Jenq-Neng Hwang Team|[2505.01583](http://arxiv.org/abs/2505.01583)|null|
|**2025-05-02**|**Grounding Task Assistance with Multimodal Cues from a Single Demonstration**|Andrew D. Wilson Team|[2505.01578](http://arxiv.org/abs/2505.01578)|null|
|**2025-05-02**|**Dynamic Robot Tool Use with Vision Language Models**|Ahmed H. Qureshi Team|[2505.01399](http://arxiv.org/abs/2505.01399)|null|
|**2025-05-02**|**Evaluating Vision Language Model Adaptations for Radiology Report Generation in Low-Resource Languages**|Valerio Guarrasi Team|[2505.01096](http://arxiv.org/abs/2505.01096)|null|
|**2025-05-02**|**Any-to-Any Vision-Language Model for Multimodal X-ray Imaging and Radiological Report Generation**|Valerio Guarrasi Team|[2505.01091](http://arxiv.org/abs/2505.01091)|null|
|**2025-05-02**|**Transferable Adversarial Attacks on Black-Box Vision-Language Models**|Matt Fredrikson Team|[2505.01050](http://arxiv.org/abs/2505.01050)|null|
|**2025-04-30**|**Entropy Heat-Mapping: Localizing GPT-Based OCR Errors with Sliding-Window Shannon Analysis**|Alexei Kaltchenko Team|[2505.00746](http://arxiv.org/abs/2505.00746)|null|
|**2025-05-01**|**Robotic Visual Instruction**|Xianzheng Ma Team|[2505.00693](http://arxiv.org/abs/2505.00693)|null|
|**2025-05-01**|**Visual Test-time Scaling for GUI Agent Grounding**|Honglak Lee Team|[2505.00684](http://arxiv.org/abs/2505.00684)|null|
|**2025-05-01**|**DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation**|Yang Gao Team|[2505.00527](http://arxiv.org/abs/2505.00527)|null|
|**2025-05-01**|**LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving**|Henry X. Liu Team|[2505.00284](http://arxiv.org/abs/2505.00284)|null|
|**2025-05-01**|**AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care**|Tianming Liu Team|[2505.00275](http://arxiv.org/abs/2505.00275)|null|
|**2025-04-30**|**V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving**|Markus Lienkamp Team|[2505.00156](http://arxiv.org/abs/2505.00156)|null|
|**2025-04-30**|**Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models**|Xintao Wu Team|[2505.00150](http://arxiv.org/abs/2505.00150)|null|
|**2025-04-30**|**Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design**|Mahdi S. Hosseini Team|[2505.00134](http://arxiv.org/abs/2505.00134)|null|
|**2025-04-30**|**Early Exit and Multi Stage Knowledge Distillation in VLMs for Video Summarization**|Ganesh Ramakrishnan Team|[2504.21831](http://arxiv.org/abs/2504.21831)|null|
|**2025-04-30**|**Black-Box Visual Prompt Engineering for Mitigating Object Hallucination in Large Vision Language Models**|Lin Lee Cheong Team|[2504.21559](http://arxiv.org/abs/2504.21559)|null|
|**2025-04-30**|**RoboGround: Robotic Manipulation with Grounded Vision-Language Priors**|Zhou Zhao Team|[2504.21530](http://arxiv.org/abs/2504.21530)|null|
|**2025-04-30**|**Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection**|William Hsu Team|[2504.21344](http://arxiv.org/abs/2504.21344)|null|
|**2025-04-29**|**MemeBLIP2: A novel lightweight multimodal system to detect harmful memes**|Lisha Xu Team|[2504.21226](http://arxiv.org/abs/2504.21226)|null|
|**2025-04-29**|**GLIP-OOD: Zero-Shot Graph OOD Detection with Foundation Model**|Yue Zhao Team|[2504.21186](http://arxiv.org/abs/2504.21186)|null|
|**2025-04-29**|**Token-Level Prompt Mixture with Parameter-Free Routing for Federated Domain Generalization**|Xiaojun Chang Team|[2504.21063](http://arxiv.org/abs/2504.21063)|null|
|**2025-04-29**|**Real-Time Wayfinding Assistant for Blind and Low-Vision Users**|Farhan Sadaf Team|[2504.20976](http://arxiv.org/abs/2504.20976)|null|
|**2025-04-29**|**FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models**|Elisa Ricci Team|[2504.20860](http://arxiv.org/abs/2504.20860)|null|
|**2025-04-29**|**In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer**|Yi Yang Team|[2504.20690](http://arxiv.org/abs/2504.20690)|null|
|**2025-04-29**|**SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data**|Freda Shi Team|[2504.20648](http://arxiv.org/abs/2504.20648)|null|
|**2025-04-29**|**PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations**|Xuguang Lan Team|[2504.20520](http://arxiv.org/abs/2504.20520)|null|
|**2025-04-29**|**Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception**|Xiaoqiang Li Team|[2504.20468](http://arxiv.org/abs/2504.20468)|null|
|**2025-04-29**|**Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks**|Dimitrios K. Nasiopoulos Team|[2504.20419](http://arxiv.org/abs/2504.20419)|null|
|**2025-04-29**|**FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding**|Bo Zheng Team|[2504.20384](http://arxiv.org/abs/2504.20384)|null|
|**2025-04-28**|**A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports**|Christoph M. Friedrich Team|[2504.20220](http://arxiv.org/abs/2504.20220)|null|
|**2025-04-28**|**Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains**|Rui Yan Team|[2504.20199](http://arxiv.org/abs/2504.20199)|null|
|**2025-04-28**|**SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning**|Alan Yuille Team|[2504.20024](http://arxiv.org/abs/2504.20024)|null|
|**2025-04-28**|**EcoWikiRS: Learning Ecological Representation of Satellite Images from Weak Supervision with Species Observations and Wikipedia**|Diego Marcos Team|[2504.19742](http://arxiv.org/abs/2504.19742)|null|
|**2025-04-28**|**Contrastive Language-Image Learning with Augmented Textual Prompts for 3D/4D FER Using Vision-Language Model**|Guoying Zhao Team|[2504.19739](http://arxiv.org/abs/2504.19739)|null|
|**2025-04-28**|**VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning**|Xiaobo Xia Team|[2504.19627](http://arxiv.org/abs/2504.19627)|null|
|**2025-04-28**|**LR-IAD:Mask-Free Industrial Anomaly Detection with Logical Reasoning**|Aimin Yang Team|[2504.19524](http://arxiv.org/abs/2504.19524)|null|
|**2025-04-27**|**DeepSPG: Exploring Deep Semantic Prior Guidance for Low-light Image Enhancement with Multimodal Learning**|Shini Han Team|[2504.19127](http://arxiv.org/abs/2504.19127)|null|
|**2025-04-27**|**Boosting Single-domain Generalized Object Detection via Vision-Language Knowledge Interaction**|Jian Liu Team|[2504.19086](http://arxiv.org/abs/2504.19086)|null|
|**2025-04-26**|**Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation**|Arif Mahmood Team|[2504.18856](http://arxiv.org/abs/2504.18856)|null|
|**2025-04-26**|**Video CLIP Model for Multi-View Echocardiography Interpretation**|Norihiko Takeda Team|[2504.18800](http://arxiv.org/abs/2504.18800)|null|
|**2025-04-25**|**A Review of 3D Object Detection with Vision-Language Models**|Manoj Karkee Team|[2504.18738](http://arxiv.org/abs/2504.18738)|null|
|**2025-04-25**|**Proof-of-TBI -- Fine-Tuned Vision Language Model Consortium and OpenAI-o3 Reasoning LLM-Based Medical Diagnosis Support System for Mild Traumatic Brain Injury (TBI) Prediction**|Donna Broshek Team|[2504.18671](http://arxiv.org/abs/2504.18671)|null|
|**2025-04-25**|**Generalization Capability for Imitation Learning**|Yixiao Wang Team|[2504.18538](http://arxiv.org/abs/2504.18538)|null|
|**2025-04-25**|**Fast-Slow Thinking for Large Vision-Language Model Reasoning**|Fei Wu Team|[2504.18458](http://arxiv.org/abs/2504.18458)|null|
|**2025-04-25**|**Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation**|Guang Yang Team|[2504.18453](http://arxiv.org/abs/2504.18453)|null|
|**2025-04-25**|**Revisiting Data Auditing in Large Vision-Language Models**|Zhuosheng Zhang Team|[2504.18349](http://arxiv.org/abs/2504.18349)|null|
|**2025-04-25**|**A Large Vision-Language Model based Environment Perception System for Visually Impaired People**|Shiguo Lian Team|[2504.18027](http://arxiv.org/abs/2504.18027)|null|
|**2025-04-24**|**CAMU: Context Augmentation for Meme Understanding**|Aditya Joshi Team|[2504.17902](http://arxiv.org/abs/2504.17902)|null|
|**2025-04-24**|**FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model**|Waikeung Wong Team|[2504.17826](http://arxiv.org/abs/2504.17826)|null|
|**2025-04-25**|**Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction**|Weiyan Wen Team|[2504.17671](http://arxiv.org/abs/2504.17671)|null|
|**2025-04-24**|**SDVPT: Semantic-Driven Visual Prompt Tuning for Open-World Object Counting**|Qingming Huang Team|[2504.17395](http://arxiv.org/abs/2504.17395)|null|
|**2025-04-24**|**M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction**|Tatsunori Mori Team|[2504.17353](http://arxiv.org/abs/2504.17353)|null|
|**2025-04-24**|**DIMT25@ICDAR2025: HW-TSC's End-to-End Document Image Machine Translation System Leveraging Large Vision-Language Model**|Hao Yang Team|[2504.17315](http://arxiv.org/abs/2504.17315)|null|
|**2025-04-24**|**Cracking the Code of Action: a Generative Approach to Affordances for Reinforcement Learning**|Khimya Khetarpal Team|[2504.17282](http://arxiv.org/abs/2504.17282)|null|
|**2025-04-24**|**Perspective-Aware Reasoning in Vision-Language Models via Mental Imagery Simulation**|Minhyuk Sung Team|[2504.17207](http://arxiv.org/abs/2504.17207)|null|
|**2025-04-23**|**Distilling semantically aware orders for autoregressive image generation**|Marco Pedersoli Team|[2504.17069](http://arxiv.org/abs/2504.17069)|null|
|**2025-04-23**|**DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs**|Ran Xu Team|[2504.17040](http://arxiv.org/abs/2504.17040)|null|
|**2025-04-24**|**V $^2$ R-Bench: Holistically Evaluating LVLM Robustness to Fundamental Visual Variations**|Yi R. Fung Team|[2504.16727](http://arxiv.org/abs/2504.16727)|null|
|**2025-04-23**|**Streetscape Analysis with Generative AI (SAGAI): Vision-Language Assessment and Mapping of Urban Scenes**|Giovanni Fusco Team|[2504.16538](http://arxiv.org/abs/2504.16538)|null|
|**2025-04-23**|**TraveLLaMA: Facilitating Multi-modal Large Language Models to Understand Urban Scenes and Provide Travel Assistance**|Jiaya Jia Team|[2504.16505](http://arxiv.org/abs/2504.16505)|null|
|**2025-04-23**|**FrogDogNet: Fourier frequency Retained visual prompt Output Guidance for Domain Generalization of CLIP in Remote Sensing**|Biplab Banerjee Team|[2504.16433](http://arxiv.org/abs/2504.16433)|null|
|**2025-04-22**|**CLIP-IT: CLIP-based Pairing for Histology Images Classification**|Eric Granger Team|[2504.16181](http://arxiv.org/abs/2504.16181)|null|
|**2025-04-22**|**MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention**|Lili Qiu Team|[2504.16083](http://arxiv.org/abs/2504.16083)|null|
|**2025-04-22**|**MR. Video: "MapReduce" is the Principle for Long Video Understanding**|Yu-Xiong Wang Team|[2504.16082](http://arxiv.org/abs/2504.16082)|null|
|**2025-04-22**|**Describe Anything: Detailed Localized Image and Video Captioning**|Yin Cui Team|[2504.16072](http://arxiv.org/abs/2504.16072)|null|
|**2025-04-22**|**Vision language models are unreliable at trivial spatial cognition**|J. Gregory Trafton Team|[2504.16061](http://arxiv.org/abs/2504.16061)|null|
|**2025-04-22**|**Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation**|Joyce Chai Team|[2504.16060](http://arxiv.org/abs/2504.16060)|null|
|**2025-04-22**|**Evaluating Vision Language Models (VLMs) for Radiology: A Comprehensive Analysis**|Judy Gichoya Team|[2504.16047](http://arxiv.org/abs/2504.16047)|null|
|**2025-04-22**|**LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale**|Mike Zheng Shou Team|[2504.16030](http://arxiv.org/abs/2504.16030)|null|
|**2025-04-24**|**Meta-Entity Driven Triplet Mining for Aligning Medical Vision-Language Models**|Tolga Çukur Team|[2504.15929](http://arxiv.org/abs/2504.15929)|null|
|**2025-04-21**|**CAPTURe: Evaluating Spatial Reasoning in Vision Language Models via Occluded Object Counting**|Mohit Bansal Team|[2504.15485](http://arxiv.org/abs/2504.15485)|null|
|**2025-04-21**|**Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models**|Guilin Liu Team|[2504.15271](http://arxiv.org/abs/2504.15271)|null|
|**2025-04-21**|**KGMEL: Knowledge Graph-Enhanced Multimodal Entity Linking**|Kijung Shin Team|[2504.15135](http://arxiv.org/abs/2504.15135)|**[link](https://github.com/juyeonnn/kgmel)**|
|**2025-04-21**|**Benchmarking Large Vision-Language Models on Fine-Grained Image Tasks: A Comprehensive Evaluation**|Serge Belongie Team|[2504.14988](http://arxiv.org/abs/2504.14988)|**[link](https://github.com/seu-vipgroup/fg-bmk)**|
|**2025-04-21**|**VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform**|Kun Gai Team|[2504.14904](http://arxiv.org/abs/2504.14904)|null|
|**2025-04-21**|**Object-Level Verbalized Confidence Calibration in Vision-Language Models via Semantic Perturbation**|Yunji Chen Team|[2504.14848](http://arxiv.org/abs/2504.14848)|null|
|**2025-04-20**|**OmniV-Med: Scaling Medical Vision-Language Model for Universal Visual Understanding**|Zuozhu Liu Team|[2504.14692](http://arxiv.org/abs/2504.14692)|null|
|**2025-04-20**|**NVSMask3D: Hard Visual Prompting with Camera Pose Interpolation for 3D Open Vocabulary Instance Segmentation**|Juho Kannala Team|[2504.14638](http://arxiv.org/abs/2504.14638)|null|
|**2025-04-20**|**LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation**|Yongsheng Gao Team|[2504.14467](http://arxiv.org/abs/2504.14467)|null|
|**2025-04-20**|**Neglected Risks: The Disturbing Reality of Children's Images in Datasets and the Urgent Call for Accountability**|Sandra Avila Team|[2504.14446](http://arxiv.org/abs/2504.14446)|null|
|**2025-04-19**|**Hydra: An Agentic Reasoning Approach for Enhancing Adversarial Robustness and Mitigating Hallucinations in Vision-Language Models**|Nathaniel D. Bastian Team|[2504.14395](http://arxiv.org/abs/2504.14395)|null|
|**2025-04-19**|**How Well Can General Vision-Language Models Learn Medicine By Watching Public Educational Videos?**|James Zou Team|[2504.14391](http://arxiv.org/abs/2504.14391)|null|
|**2025-04-19**|**A Multimodal Recaptioning Framework to Account for Perceptual Diversity in Multilingual Vision-Language Modeling**|Adriana Kovashka Team|[2504.14359](http://arxiv.org/abs/2504.14359)|null|
|**2025-04-19**|**Diffusion-based Dynamic Contract for Federated AI Agent Construction in Mobile Metaverses**|Chau Yuen Team|[2504.14326](http://arxiv.org/abs/2504.14326)|null|
|**2025-04-19**|**Enhancing Multimodal In-Context Learning for Image Classification through Coreset Optimization**|Xu Yang Team|[2504.14200](http://arxiv.org/abs/2504.14200)|null|
|**2025-04-19**|**Bayesian Principles Improve Prompt Learning In Vision-Language Models**|Mijung Park Team|[2504.14123](http://arxiv.org/abs/2504.14123)|null|
|**2025-04-19**|**PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models**|Ozlem Ozmen Garibay Team|[2504.14117](http://arxiv.org/abs/2504.14117)|null|
|**2025-04-21**|**Analysing the Robustness of Vision-Language-Models to Common Corruptions**|Umair Bin Mansoor Team|[2504.13690](http://arxiv.org/abs/2504.13690)|null|
|**2025-04-18**|**EyecareGPT: Boosting Comprehensive Ophthalmology Understanding with Tailored Dataset, Benchmark and Model**|Beng Chin Ooi Team|[2504.13650](http://arxiv.org/abs/2504.13650)|**[link](https://github.com/dcdmllm/eyecaregpt)**|
|**2025-04-18**|**PV-VLM: A Multimodal Vision-Language Approach Incorporating Sky Images for Intra-Hour Photovoltaic Power Forecasting**|Miao Yu Team|[2504.13624](http://arxiv.org/abs/2504.13624)|null|
|**2025-04-18**|**Chain-of-Thought Textual Reasoning for Few-shot Temporal Action Localization**|Huadong Ma Team|[2504.13460](http://arxiv.org/abs/2504.13460)|null|
|**2025-04-18**|**Towards a Multi-Agent Vision-Language System for Zero-Shot Novel Hazardous Object Detection for Autonomous Driving Safety**|Ross Greer Team|[2504.13399](http://arxiv.org/abs/2504.13399)|null|
|**2025-04-17**|**VLLFL: A Vision-Language Model Based Lightweight Federated Learning Framework for Smart Agriculture**|Yanbo Huang Team|[2504.13365](http://arxiv.org/abs/2504.13365)|null|
|**2025-04-17**|**Chain-of-Modality: Learning Manipulation Programs from Multimodal Human Videos with Vision-Language-Models**|Jacky Liang Team|[2504.13351](http://arxiv.org/abs/2504.13351)|null|
|**2025-04-17**|**WildFireCan-MMD: A Multimodal dataset for Classification of User-generated Content During Wildfires in Canada**|Marzieh Amini Team|[2504.13231](http://arxiv.org/abs/2504.13231)|null|
|**2025-04-17**|**PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding**|Christoph Feichtenhofer Team|[2504.13180](http://arxiv.org/abs/2504.13180)|null|
|**2025-04-17**|**Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling**|David M. Chan Team|[2504.13169](http://arxiv.org/abs/2504.13169)|**[link](https://github.com/tsunghan-wu/reverse_vlm)**|
|**2025-04-17**|**Low-hallucination Synthetic Captions for Large-Scale Vision-Language Model Pre-training**|Zhanhui Kang Team|[2504.13123](http://arxiv.org/abs/2504.13123)|null|
|**2025-04-17**|**Probing and Inducing Combinational Creativity in Vision-Language Models**|Zilong Zheng Team|[2504.13120](http://arxiv.org/abs/2504.13120)|null|
|**2025-04-17**|**Object-Driven Narrative in AR: A Scenario-Metaphor Framework with VLM Integration**|Yong Hong Kuo Team|[2504.13119](http://arxiv.org/abs/2504.13119)|null|
|**2025-04-17**|**Early Accessibility: Automating Alt-Text Generation for UI Icons During App Development**|Christoph Csallner Team|[2504.13069](http://arxiv.org/abs/2504.13069)|null|
|**2025-04-17**|**NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation**|Michael Qizhe Shieh Team|[2504.13055](http://arxiv.org/abs/2504.13055)|null|
|**2025-04-17**|**Embodied-R: Collaborative Framework for Activating Embodied Spatial Reasoning in Foundation Models via Reinforcement Learning**|Wenwu Zhu Team|[2504.12680](http://arxiv.org/abs/2504.12680)|**[link](https://github.com/embodiedcity/embodied-r.code)**|
|**2025-04-17**|**VLMGuard-R1: Proactive Safety Alignment for VLMs via Reasoning-Driven Prompt Optimization**|Siheng Chen Team|[2504.12661](http://arxiv.org/abs/2504.12661)|null|
|**2025-04-16**|**Sparsity Outperforms Low-Rank Projections in Few-Shot Adaptation**|Éric Granger Team|[2504.12436](http://arxiv.org/abs/2504.12436)|**[link](https://github.com/nairouz/SO)**|
|**2025-04-16**|**FLIP Reasoning Challenge**|Roger Wattenhofer Team|[2504.12256](http://arxiv.org/abs/2504.12256)|null|
|**2025-04-16**|**Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -**|Hanno Gottschalk Team|[2504.12137](http://arxiv.org/abs/2504.12137)|null|
|**2025-04-17**|**Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions**|Zhi-Qi Cheng Team|[2504.11967](http://arxiv.org/abs/2504.11967)|null|
|**2025-04-16**|**Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning**|Yi Chang Team|[2504.11930](http://arxiv.org/abs/2504.11930)|null|
|**2025-04-16**|**A Visual RAG Pipeline for Few-Shot Fine-Grained Product Classification**|Janis Keuper Team|[2504.11838](http://arxiv.org/abs/2504.11838)|null|
|**2025-04-17**|**DVLTA-VQA: Decoupled Vision-Language Modeling with Text-Guided Adaptation for Blind Video Quality Assessment**|Moncef Gabbouj Team|[2504.11733](http://arxiv.org/abs/2504.11733)|null|
|**2025-04-16**|**Interpreting the Linear Structure of Vision-language Model Embedding Spaces**|Stephanie Gil Team|[2504.11695](http://arxiv.org/abs/2504.11695)|null|
|**2025-04-16**|**VLM-Fuzz: Vision Language Model Assisted Recursive Depth-first Search Exploration for Effective UI Testing of Android Apps**|Mariano Ceccato Team|[2504.11675](http://arxiv.org/abs/2504.11675)|null|
|**2025-04-15**|**Co-STAR: Collaborative Curriculum Self-Training with Adaptive Regularization for Source-Free Video Domain Adaptation**|Majid Mirmehdi Team|[2504.11669](http://arxiv.org/abs/2504.11669)|null|
|**2025-04-17**|**PATFinger: Prompt-Adapted Transferable Fingerprinting against Unauthorized Multimodal Dataset Usage**|Lina Wang Team|[2504.11509](http://arxiv.org/abs/2504.11509)|null|
|**2025-04-15**|**From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation**|Jungong Han Team|[2504.11368](http://arxiv.org/abs/2504.11368)|null|
|**2025-04-17**|**UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis**|Yan Lu Team|[2504.11257](http://arxiv.org/abs/2504.11257)|null|
|**2025-04-15**|**R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning**|Ran He Team|[2504.11195](http://arxiv.org/abs/2504.11195)|null|
|**2025-06-30**|**Benchmarking Vision Language Models on German Factual Data**|Vincent Tischler Team|[2504.11108](http://arxiv.org/abs/2504.11108)|null|
|**2025-04-16**|**Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR**|Gongshen Liu Team|[2504.11101](http://arxiv.org/abs/2504.11101)|null|
|**2025-04-15**|**QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models**|Yu Wang Team|[2504.11038](http://arxiv.org/abs/2504.11038)|null|
|**2025-04-15**|**Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles**|Ross Greer Team|[2504.10873](http://arxiv.org/abs/2504.10873)|null|
|**2025-04-15**|**LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation**|Mohsen Imani Team|[2504.10854](http://arxiv.org/abs/2504.10854)|null|
|**2025-04-15**|**Enhancing Features in Long-tailed Data Using Large Vision Mode**|Xuesong Li Team|[2504.10852](http://arxiv.org/abs/2504.10852)|null|
|**2025-04-14**|**ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models**|Lifeng Zhou Team|[2504.10757](http://arxiv.org/abs/2504.10757)|null|
|**2025-04-14**|**AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark**|Yu-Xiong Wang Team|[2504.10568](http://arxiv.org/abs/2504.10568)|null|
|**2025-04-14**|**Pixel-SAIL: Single Transformer For Pixel-Grounded Understanding**|Jiashi Feng Team|[2504.10465](http://arxiv.org/abs/2504.10465)|null|
|**2025-04-15**|**GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents**|Run Luo Team|[2504.10458](http://arxiv.org/abs/2504.10458)|null|
|**2025-04-14**|**SlowFastVAD: Video Anomaly Detection via Integrating Simple Detector and RAG-Enhanced Vision-Language Model**|Yanning Zhang Team|[2504.10320](http://arxiv.org/abs/2504.10320)|null|
|**2025-04-15**|**Breaking the Data Barrier -- Building GUI Agents Through Task Generalization**|Junxian He Team|[2504.10127](http://arxiv.org/abs/2504.10127)|null|
|**2025-04-14**|**AGO: Adaptive Grounding for Open World 3D Occupancy Prediction**|Andreas Zell Team|[2504.10117](http://arxiv.org/abs/2504.10117)|null|
|**2025-04-14**|**CameraBench: Benchmarking Visual Reasoning in MLLMs via Photography**|Jun-Cheng Chen Team|[2504.10090](http://arxiv.org/abs/2504.10090)|null|
|**2025-04-14**|**Summarization of Multimodal Presentations with Vision-Language Models: Study of the Effect of Modalities and Structure**|Frédéric Dufaux Team|[2504.10049](http://arxiv.org/abs/2504.10049)|null|
|**2025-04-14**|**Aligning Anime Video Generation with Human Feedback**|Zuxuan Wu Team|[2504.10044](http://arxiv.org/abs/2504.10044)|null|
|**2025-04-14**|**KeyMPs: One-Shot Vision-Language Guided Motion Generation by Sequencing DMPs for Occlusion-Rich Tasks**|Takamitsu Matsubara Team|[2504.10011](http://arxiv.org/abs/2504.10011)|null|
|**2025-04-14**|**GenTe: Generative Real-world Terrains for General Legged Robot Locomotion Control**|Xiaoqiang Ji Team|[2504.09997](http://arxiv.org/abs/2504.09997)|null|
|**2025-04-14**|**Resampling Benchmark for Efficient Comprehensive Evaluation of Large Vision-Language Models**|Keisuke Ozawa Team|[2504.09979](http://arxiv.org/abs/2504.09979)|null|
|**2025-04-14**|**Can VLMs Assess Similarity Between Graph Visualizations?**|Jinwook Seo Team|[2504.09859](http://arxiv.org/abs/2504.09859)|null|
|**2025-04-14**|**VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents**|Jun Suzuki Team|[2504.09795](http://arxiv.org/abs/2504.09795)|null|
|**2025-04-13**|**A Survey on Efficient Vision-Language Models**|Nirmalya Roy Team|[2504.09724](http://arxiv.org/abs/2504.09724)|null|
|**2025-04-13**|**Metropolis-Hastings Captioning Game: Knowledge Fusion of Vision Language Models via Decentralized Bayesian Inference**|Tadahiro Taniguchi Team|[2504.09620](http://arxiv.org/abs/2504.09620)|null|
|**2025-04-13**|**DualPrompt-MedCap: A Dual-Prompt Enhanced Approach for Medical Image Captioning**|Mukesh Prasad Team|[2504.09598](http://arxiv.org/abs/2504.09598)|null|
|**2025-04-15**|**Vision-Language Model for Object Detection and Segmentation: A Review and Evaluation**|Yunhong Wang Team|[2504.09480](http://arxiv.org/abs/2504.09480)|null|
|**2025-04-13**|**Identity-Aware Vision-Language Model for Explainable Face Forgery Detection**|Yu-Gang Jiang Team|[2504.09439](http://arxiv.org/abs/2504.09439)|null|
|**2025-04-13**|**BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning**|Boqing Gong Team|[2504.09426](http://arxiv.org/abs/2504.09426)|null|
|**2025-04-12**|**PathVLM-R1: A Reinforcement Learning-Driven Reasoning Model for Pathology Visual-Language Tasks**|Yang Liu Team|[2504.09258](http://arxiv.org/abs/2504.09258)|null|
|**2025-04-11**|**AstroLLaVA: towards the unification of astronomical data and natural language**|Dimitrios Tanoglidis Team|[2504.08583](http://arxiv.org/abs/2504.08583)|null|
|**2025-04-11**|**EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models**|Jinwoo Kim Team|[2504.08205](http://arxiv.org/abs/2504.08205)|null|
|**2025-04-10**|**Investigating Vision-Language Model for Point Cloud-based Vehicle Classification**|Camille Kamga Team|[2504.08154](http://arxiv.org/abs/2504.08154)|null|
|**2025-04-10**|**The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search**|David Ha Team|[2504.08066](http://arxiv.org/abs/2504.08066)|null|
|**2025-04-10**|**VCR-Bench: A Comprehensive Evaluation Framework for Video Chain-of-Thought Reasoning**|Feng Zhao Team|[2504.07956](http://arxiv.org/abs/2504.07956)|null|
|**2025-04-10**|**SAMJAM: Zero-Shot Video Scene Graph Generation for Egocentric Kitchen Videos**|Yuhao Chen Team|[2504.07867](http://arxiv.org/abs/2504.07867)|null|
|**2025-04-10**|**CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections**|Chris Biemann Team|[2504.07643](http://arxiv.org/abs/2504.07643)|null|
|**2025-04-10**|**VLM-R1: A Stable and Generalizable R1-style Large Vision-Language Model**|Tiancheng Zhao Team|[2504.07615](http://arxiv.org/abs/2504.07615)|**[link](https://github.com/om-ai-lab/vlm-r1)**|
|**2025-04-10**|**TokenFocus-VQA: Enhancing Text-to-Image Alignment with Position-Aware Focus and Multi-Perspective Aggregations on LVLMs**|Xuezhi Cao Team|[2504.07556](http://arxiv.org/abs/2504.07556)|null|
|**2025-04-10**|**Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models**|Xian-Sheng Hua Team|[2504.07521](http://arxiv.org/abs/2504.07521)|**[link](https://github.com/lum1104/eibench)**|
|**2025-04-10**|**Kimi-VL Technical Report**|Ziwei Chen Team|[2504.07491](http://arxiv.org/abs/2504.07491)|**[link](https://github.com/moonshotai/kimi-vl)**|
|**2025-04-09**|**Perception in Reflection**|Vishal M. Patel Team|[2504.07165](http://arxiv.org/abs/2504.07165)|null|
|**2025-04-09**|**Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation**|Marzieh Fadaee Team|[2504.07072](http://arxiv.org/abs/2504.07072)|null|
|**2025-04-09**|**Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition**|Aythami Morales Team|[2504.06925](http://arxiv.org/abs/2504.06925)|null|
|**2025-04-09**|**MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking**|Hesheng Wang Team|[2504.06863](http://arxiv.org/abs/2504.06863)|null|
|**2025-04-09**|**ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models**|Namhoon Lee Team|[2504.06838](http://arxiv.org/abs/2504.06838)|null|
|**2025-04-09**|**LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding**|Bo XU Team|[2504.06835](http://arxiv.org/abs/2504.06835)|null|
|**2025-04-08**|**PromptHMR: Promptable Human Mesh Recovery**|Muhammed Kocabas Team|[2504.06397](http://arxiv.org/abs/2504.06397)|null|
|**2025-04-08**|**SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation**|Zhaozheng Yin Team|[2504.06389](http://arxiv.org/abs/2504.06389)|null|
|**2025-04-08**|**OmniSVG: A Unified Scalable Vector Graphics Generation Model**|Yu-Gang Jiang Team|[2504.06263](http://arxiv.org/abs/2504.06263)|null|
|**2025-04-08**|**Latent Multimodal Reconstruction for Misinformation Detection**|Panagiotis C. Petrantonakis Team|[2504.06010](http://arxiv.org/abs/2504.06010)|**[link](https://github.com/stevejpapad/miscaptioned-image-reconstruction)**|
|**2025-04-08**|**Measuring Déjà vu Memorization Efficiently**|Kamalika Chaudhuri Team|[2504.05651](http://arxiv.org/abs/2504.05651)|null|
|**2025-04-08**|**A Lightweight Large Vision-language Model for Multimodal Medical Images**|Navid Toosy Saidy Team|[2504.05575](http://arxiv.org/abs/2504.05575)|null|
|**2025-04-10**|**ChartQAPro: A More Diverse and Challenging Benchmark for Chart Question Answering**|Shafiq Joty Team|[2504.05506](http://arxiv.org/abs/2504.05506)|null|
|**2025-04-07**|**Trust Through Transparency: Explainable Social Navigation for Autonomous Mobile Robots via Vision-Language Models**|Aliasghar Arab Team|[2504.05477](http://arxiv.org/abs/2504.05477)|null|
|**2025-04-07**|**Taxonomy-Aware Evaluation of Vision-Language Models**|Stella Frank Team|[2504.05457](http://arxiv.org/abs/2504.05457)|null|
|**2025-04-07**|**Probing the Visualization Literacy of Vision Language Models: the Good, the Bad, and the Ugly**|Anamaria Crisan Team|[2504.05445](http://arxiv.org/abs/2504.05445)|null|
|**2025-04-07**|**InteractVLM: 3D Interaction Reasoning from 2D Foundational Models**|Dimitrios Tzionas Team|[2504.05303](http://arxiv.org/abs/2504.05303)|null|
|**2025-04-07**|**SmolVLM: Redefining small and efficient multimodal models**|Thomas Wolf Team|[2504.05299](http://arxiv.org/abs/2504.05299)|null|
|**2025-04-07**|**A Reality Check of Vision-Language Pre-training in Radiology: Have We Progressed Using Text?**|Ismail Ben Ayed Team|[2504.05227](http://arxiv.org/abs/2504.05227)|null|
|**2025-04-07**|**Vision-Language Model Predictive Control for Manipulation Planning and Trajectory Generation**|Wei Zhang Team|[2504.05225](http://arxiv.org/abs/2504.05225)|null|
|**2025-04-08**|**A Taxonomy of Self-Handover**|Katsushi Ikeuchi Team|[2504.04939](http://arxiv.org/abs/2504.04939)|null|
|**2025-04-07**|**SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models**|Lorenz Hufe Team|[2504.04893](http://arxiv.org/abs/2504.04893)|null|
|**2025-04-07**|**Don't Lag, RAG: Training-Free Adversarial Detection Using RAG**|Ofer Hadar Team|[2504.04858](http://arxiv.org/abs/2504.04858)|null|
|**2025-04-07**|**OCC-MLLM-CoT-Alpha: Towards Multi-stage Occlusion Recognition Based on Large Language Models via 3D-Aware Supervision and Chain-of-Thoughts Guidance**|Xinhan Di Team|[2504.04781](http://arxiv.org/abs/2504.04781)|null|
|**2025-04-07**|**Feedback-Enhanced Hallucination-Resistant Vision-Language Model for Real-Time Scene Understanding**|Zahir Alsulaimawi Team|[2504.04772](http://arxiv.org/abs/2504.04772)|null|
|**2025-04-07**|**Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions**|Yue Wang Team|[2504.04744](http://arxiv.org/abs/2504.04744)|null|
|**2025-04-07**|**Enhancing Compositional Reasoning in Vision-Language Models with Synthetic Preference Data**|Venkatesh Saligrama Team|[2504.04740](http://arxiv.org/abs/2504.04740)|null|
|**2025-04-06**|**M2IV: Towards Efficient and Fine-grained Multimodal In-Context Learning in Large Vision-Language Models**|Ruixiang Tang Team|[2504.04633](http://arxiv.org/abs/2504.04633)|null|
|**2025-04-06**|**Foundation Models for Software Engineering of Cyber-Physical Systems: the Road Ahead**|Shaukat Ali Team|[2504.04630](http://arxiv.org/abs/2504.04630)|null|
|**2025-04-06**|**Enhance Then Search: An Augmentation-Search Strategy with Foundation Models for Cross-Domain Few-Shot Object Detection**|Xiaomeng Huang Team|[2504.04517](http://arxiv.org/abs/2504.04517)|**[link](https://github.com/jaychempan/ETS)**|
|**2025-04-06**|**OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning**|Jose M. Alvarez Team|[2504.04348](http://arxiv.org/abs/2504.04348)|null|
|**2025-04-06**|**MedM-VL: What Makes a Good Medical LVLM?**|Ji Wu Team|[2504.04323](http://arxiv.org/abs/2504.04323)|null|
|**2025-04-05**|**GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill**|Siyuan Huang Team|[2504.04191](http://arxiv.org/abs/2504.04191)|null|
|**2025-04-05**|**LATTE: Lightweight Attention-based Traffic Accident Anticipation Engine**|Zhenning Li Team|[2504.04103](http://arxiv.org/abs/2504.04103)|null|
|**2025-04-05**|**TARAC: Mitigating Hallucination in LVLMs via Temporal Attention Real-time Accumulative Connection**|Xiaohua Xu Team|[2504.04099](http://arxiv.org/abs/2504.04099)|null|
|**2025-04-04**|**VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models**|Anelia Angelova Team|[2504.03970](http://arxiv.org/abs/2504.03970)|null|
|**2025-04-04**|**Know What You do Not Know: Verbalized Uncertainty Estimation Robustness on Corrupted Images in Vision-Language Models**|Matias Valdenegro-Toro Team|[2504.03440](http://arxiv.org/abs/2504.03440)|null|
|**2025-04-04**|**SARLANG-1M: A Benchmark for Vision-Language Modeling in SAR Image Understanding**|Naoto Yokoya Team|[2504.03254](http://arxiv.org/abs/2504.03254)|null|
|**2025-04-04**|**Seeing is Believing: Belief-Space Planning with Foundation Models as Uncertainty Estimators**|Lawson L. S. Wong Team|[2504.03245](http://arxiv.org/abs/2504.03245)|null|
|**2025-04-04**|**Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation**|Robby T. Tan Team|[2504.03193](http://arxiv.org/abs/2504.03193)|null|
|**2025-04-04**|**NuScenes-SpatialQA: A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving**|Zhengzhong Tu Team|[2504.03164](http://arxiv.org/abs/2504.03164)|null|
|**2025-04-04**|**TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference**|Xianpeng Lang Team|[2504.03154](http://arxiv.org/abs/2504.03154)|null|
|**2025-04-04**|**MORAL: A Multimodal Reinforcement Learning Framework for Decision Making in Autonomous Laboratories**|Arvind Ramanathan Team|[2504.03153](http://arxiv.org/abs/2504.03153)|null|
|**2025-04-03**|**QID: Efficient Query-Informed ViTs in Data-Scarce Regimes for OCR-free Visual Document Understanding**|Bryan Wang Team|[2504.02971](http://arxiv.org/abs/2504.02971)|null|
|**2025-04-03**|**STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection**|Naoufel Werghi Team|[2504.02823](http://arxiv.org/abs/2504.02823)|null|
|**2025-04-03**|**Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models**|Zeynep Akata Team|[2504.02821](http://arxiv.org/abs/2504.02821)|null|
|**2025-04-03**|**Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence**|Serena Yeung-Levy Team|[2504.02799](http://arxiv.org/abs/2504.02799)|null|
|**2025-04-03**|**Robot-Led Vision Language Model Wellbeing Assessment of Children**|Hatice Gunes Team|[2504.02765](http://arxiv.org/abs/2504.02765)|null|
|**2025-04-04**|**Rethinking RL Scaling for Vision Language Models: A Transparent, From-Scratch Framework and Comprehensive Evaluation Scheme**|Pengfei Liu Team|[2504.02587](http://arxiv.org/abs/2504.02587)|null|
|**2025-04-03**|**Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision**|Shibiao Xu Team|[2504.02477](http://arxiv.org/abs/2504.02477)|null|
|**2025-04-03**|**Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation**|Rui Yan Team|[2504.02438](http://arxiv.org/abs/2504.02438)|null|
|**2025-04-03**|**ReuseDroid: A VLM-empowered Android UI Test Migrator Boosted by Active Feedback**|Hailong Wang Team|[2504.02357](http://arxiv.org/abs/2504.02357)|null|
|**2025-04-03**|**Large (Vision) Language Models are Unsupervised In-Context Learners**|Maria Brbic Team|[2504.02349](http://arxiv.org/abs/2504.02349)|**[link](https://github.com/mlbio-epfl/joint-inference)**|
|**2025-04-03**|**Re-thinking Temporal Search for Long-Form Video Understanding**|Manling Li Team|[2504.02259](http://arxiv.org/abs/2504.02259)|null|
|**2025-04-03**|**SocialGesture: Delving into Multi-person Gesture Understanding**|James M. Rehg Team|[2504.02244](http://arxiv.org/abs/2504.02244)|null|
|**2025-04-02**|**FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs**|Fatima Albreiki Team|[2504.01916](http://arxiv.org/abs/2504.01916)|**[link](https://github.com/tiiuae/FineLIP)**|
|**2025-04-02**|**Is Temporal Prompting All We Need For Limited Labeled Action Recognition?**|Xiaobo Jin Team|[2504.01890](http://arxiv.org/abs/2504.01890)|null|
|**2025-04-02**|**Prompting Medical Vision-Language Models to Mitigate Diagnosis Bias by Generating Realistic Dermoscopic Images**|Abdullah-Al-Zubaer Imran Team|[2504.01838](http://arxiv.org/abs/2504.01838)|**[link](https://github.com/munia03/dermdit)**|
|**2025-04-02**|**BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing**|Leonidas Guibas Team|[2504.01786](http://arxiv.org/abs/2504.01786)|null|
|**2025-04-02**|**AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization**|Linli Xu Team|[2504.01735](http://arxiv.org/abs/2504.01735)|null|
|**2025-04-02**|**Reasoning LLMs for User-Aware Multimodal Conversational Agents**|Mohamed Chetouani Team|[2504.01700](http://arxiv.org/abs/2504.01700)|null|
|**2025-04-02**|**CLIP-SLA: Parameter-Efficient CLIP Adaptation for Continuous Sign Language Recognition**|Hamzah Luqman Team|[2504.01666](http://arxiv.org/abs/2504.01666)|**[link](https://github.com/snalyami/CLIP-SLA)**|
|**2025-04-02**|**BioAtt: Anatomical Prior Driven Low-Dose CT Denoising**|UiHyun Cho Team|[2504.01662](http://arxiv.org/abs/2504.01662)|null|
|**2025-04-02**|**Text Speaks Louder than Vision: ASCII Art Reveals Textual Biases in Vision-Language Models**|Ming-Hsuan Yang Team|[2504.01589](http://arxiv.org/abs/2504.01589)|null|
|**2025-03-25**|**Vision-R1: Evolving Human-Free Alignment in Large Vision-Language Models via Vision-Guided Reinforcement Learning**|Jinqiao Wang Team|[2503.18013](http://arxiv.org/abs/2503.18013)|**[link](https://github.com/jefferyZhan/Griffon/tree/master/Vision-R1)**|
|**2024-12-02**|**VARCO-VISION: Expanding Frontiers in Korean Vision-Language Models**|Youngjune Kim Team|[2411.19103](http://arxiv.org/abs/2411.19103)|**[link](https://huggingface.co/NCSOFT/VARCO-VISION-14B.)**|
|**2025-05-19**|**Evaluating Vision-Language Models as Evaluators in Path Planning**|Ziyu Yao Team|[2411.18711](http://arxiv.org/abs/2411.18711)|null|
|**2025-03-11**|**ScVLM: Enhancing Vision-Language Model for Safety-Critical Event Understanding**|Feng Guo Team|[2410.00982](http://arxiv.org/abs/2410.00982)|null|
|**2024-09-24**|**Behavioral Bias of Vision-Language Models: A Behavioral Finance View**|Ming-Chang Chiu Team|[2409.15256](http://arxiv.org/abs/2409.15256)|null|
|**2024-08-01**|**Vision-Language Model Based Handwriting Verification**|Sargur Srihari Team|[2407.21788](http://arxiv.org/abs/2407.21788)|null|
|**2025-10-15**|**Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions**|Aman Chadha Team|[2404.07214](http://arxiv.org/abs/2404.07214)|null|
|**2024-05-10**|**Multi-Frame, Lightweight & Efficient Vision-Language Models for Question Answering in Autonomous Driving**|Mohan Trivedi Team|[2403.19838](http://arxiv.org/abs/2403.19838)|null|
|**2024-01-17**|**Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models**|Jie Yang Team|[2309.04041](http://arxiv.org/abs/2309.04041)|null|
|**2023-10-13**|**Distilling Large Vision-Language Model with Out-of-Distribution Generalizability**|Hao Su Team|[2307.03135](http://arxiv.org/abs/2307.03135)|**[link](https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf)**|
|**2023-06-16**|**LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models**|Ping Luo Team|[2306.09265](http://arxiv.org/abs/2306.09265)|null|
|**2023-11-08**|**Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection**|Adriana Kovashka Team|[2303.10093](http://arxiv.org/abs/2303.10093)|null|
|**2024-04-19**|**VLP: A Survey on Vision-Language Pre-training**|Bo Xu Team|[2202.09061](http://arxiv.org/abs/2202.09061)|null|
|**2022-10-07**|**Learning to Prompt for Vision-Language Models**|Ziwei Liu Team|[2109.01134](http://arxiv.org/abs/2109.01134)|null|

## VLA

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning**|Wei Zhan Team|[2602.21172](http://arxiv.org/abs/2602.21172)|null|
|**2026-02-24**|**ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking**|Brian Sheil Team|[2602.21161](http://arxiv.org/abs/2602.21161)|null|
|**2026-02-24**|**HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning**|Song Guo Team|[2602.21157](http://arxiv.org/abs/2602.21157)|null|
|**2026-02-24**|**Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks**|Roland Memisevic Team|[2602.21013](http://arxiv.org/abs/2602.21013)|null|
|**2026-02-24**|**IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation**|Huixu Dong Team|[2602.20715](http://arxiv.org/abs/2602.20715)|null|
|**2026-02-24**|**Recursive Belief Vision Language Model**|Nirav Patel Team|[2602.20659](http://arxiv.org/abs/2602.20659)|null|
|**2026-02-24**|**Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion**|Ziran Wang Team|[2602.20577](http://arxiv.org/abs/2602.20577)|null|
|**2026-02-24**|**An interactive enhanced driving dataset for autonomous driving**|Lu Xiong Team|[2602.20575](http://arxiv.org/abs/2602.20575)|null|
|**2026-02-24**|**BFA++: Hierarchical Best-Feature-Aware Token Prune for Multi-View Vision Language Action Model**|Hua Chen Team|[2602.20566](http://arxiv.org/abs/2602.20566)|null|
|**2026-02-23**|**QuantVLA: Scale-Calibrated Post-Training Quantization for Vision-Language-Action Models**|Mi Zhang Team|[2602.20309](http://arxiv.org/abs/2602.20309)|null|
|**2026-02-23**|**UniLACT: Depth-Aware RGB Latent Action Learning for Vision-Language-Action Models**|Srijan Das Team|[2602.20231](http://arxiv.org/abs/2602.20231)|**[link](https://manishgovind.github.io/unilact-vla/)**|
|**2026-02-22**|**Global Prior Meets Local Consistency: Dual-Memory Augmented Vision-Language-Action Model for Efficient Robotic Manipulation**|Liqiang Nie Team|[2602.20200](http://arxiv.org/abs/2602.20200)|null|
|**2026-02-23**|**Universal Pose Pretraining for Generalizable Vision-Language-Action Policies**|Yanwei Fu Team|[2602.19710](http://arxiv.org/abs/2602.19710)|null|
|**2026-02-22**|**TOPReward: Token Probabilities as Hidden Zero-Shot Rewards for Robotics**|Ranjay Krishna Team|[2602.19313](http://arxiv.org/abs/2602.19313)|null|
|**2026-02-22**|**The Price Is Not Right: Neuro-Symbolic Methods Outperform VLAs on Structured Long-Horizon Manipulation Tasks with Significantly Lower Energy Consumption**|Matthias Scheutz Team|[2602.19260](http://arxiv.org/abs/2602.19260)|null|
|**2026-02-21**|**Habilis- $β$ : A Fast-Motion and Long-Lasting On-Device Vision-Language-Action Model**|Theo Taeyeong Kim Team|[2602.18813](http://arxiv.org/abs/2602.18813)|null|
|**2026-02-20**|**VLANeXt: Recipes for Building Strong VLA Models**|Chen Change Loy Team|[2602.18532](http://arxiv.org/abs/2602.18532)|**[link](https://dravenalg.github.io/VLANeXt/)**|
|**2026-02-20**|**How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf**|Christos Kozyrakis Team|[2602.18397](http://arxiv.org/abs/2602.18397)|null|
|**2026-02-20**|**SimVLA: A Simple VLA Baseline for Robotic Manipulation**|Zhenguo Li Team|[2602.18224](http://arxiv.org/abs/2602.18224)|null|
|**2026-02-20**|**UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models**|Liang Wang Team|[2602.18020](http://arxiv.org/abs/2602.18020)|null|
|**2026-02-20**|**ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models**|Ang Li Team|[2602.17951](http://arxiv.org/abs/2602.17951)|null|
|**2026-02-19**|**When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs**|Mingyu Ding Team|[2602.17659](http://arxiv.org/abs/2602.17659)|**[link](https://vla-va.github.io/)**|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-17**|**World Action Models are Zero-shot Policies**|Joel Jang Team|[2602.15922](http://arxiv.org/abs/2602.15922)|**[link](https://dreamzero0.github.io/)**|
|**2026-02-17**|**Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**|Soo-Chul Lim Team|[2602.15543](http://arxiv.org/abs/2602.15543)|null|
|**2026-02-17**|**ActionCodec: What Makes for Good Action Tokenizers**|Jianye Hao Team|[2602.15397](http://arxiv.org/abs/2602.15397)|null|
|**2026-02-16**|**DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**|Tiancai Wang Team|[2602.14974](http://arxiv.org/abs/2602.14974)|**[link](https://github.com/Dexmal/dexbotic)**|
|**2026-02-16**|**DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**|Yan Wang Team|[2602.14577](http://arxiv.org/abs/2602.14577)|null|
|**2026-02-15**|**WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**|Dongbin Zhao Team|[2602.13977](http://arxiv.org/abs/2602.13977)|null|
|**2026-02-14**|**Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**|Yan Wu Team|[2602.13833](http://arxiv.org/abs/2602.13833)|null|
|**2026-02-14**|**MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**|Heng Tao Shen Team|[2602.13764](http://arxiv.org/abs/2602.13764)|null|
|**2026-02-14**|**HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**|Ivor Tsang Team|[2602.13710](http://arxiv.org/abs/2602.13710)|null|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-13**|**Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**|Sergey Levine Team|[2602.13193](http://arxiv.org/abs/2602.13193)|null|
|**2026-02-13**|**UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**|Ziwei Wang Team|[2602.13086](http://arxiv.org/abs/2602.13086)|**[link](https://henryhcliu.github.io/unimanip)**|
|**2026-02-13**|**Learning Native Continuation for Action Chunking Flow Policies**|Yang Gao Team|[2602.12978](http://arxiv.org/abs/2602.12978)|**[link](https://lyfeng001.github.io/Legato/)**|
|**2026-02-13**|**ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**|Maoqing Yao Team|[2602.12691](http://arxiv.org/abs/2602.12691)|null|
|**2026-02-13**|**Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**|Quanyun Zhou Team|[2602.12684](http://arxiv.org/abs/2602.12684)|**[link](https://xiaomi-robotics-0.github.io)**|
|**2026-02-16**|**Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**|Yu Wang Team|[2602.12628](http://arxiv.org/abs/2602.12628)|null|
|**2026-02-13**|**CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**|Jingtao Sun Team|[2602.12532](http://arxiv.org/abs/2602.12532)|null|
|**2026-02-12**|**ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**|Song Han Team|[2602.12322](http://arxiv.org/abs/2602.12322)|null|
|**2026-02-18**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|null|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|**[link](https://gigabrain05m.github.io/)**|
|**2026-02-15**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|**[link](https://sites.google.com/view/vlaw-arxiv)**|
|**2026-02-12**|**HoloBrain-0 Technical Report**|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|null|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?**|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|null|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-02-11**|**H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**|Yingxue Zhang Team|[2602.11291](http://arxiv.org/abs/2602.11291)|null|
|**2026-02-11**|**RISE: Self-Improving Robot Policy with Compositional World Model**|Hongyang Li Team|[2602.11075](http://arxiv.org/abs/2602.11075)|**[link](https://opendrivelab.com/kai0-rl/)**|
|**2026-02-12**|**Scaling World Model for Hierarchical Manipulation Policies**|Xinghang Li Team|[2602.10983](http://arxiv.org/abs/2602.10983)|null|
|**2026-02-11**|**RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**|Guangrun Wang Team|[2602.10980](http://arxiv.org/abs/2602.10980)|null|
|**2026-02-11**|**From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**|Yan Wang Team|[2602.10719](http://arxiv.org/abs/2602.10719)|null|
|**2026-02-11**|**AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**|F. Richard Yu Team|[2602.10698](http://arxiv.org/abs/2602.10698)|null|
|**2026-02-11**|**LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**|Anirudha Majumdar Team|[2602.10556](http://arxiv.org/abs/2602.10556)|**[link](https://lap-vla.github.io)**|
|**2026-02-10**|**Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**|Cheng Deng Team|[2602.10377](http://arxiv.org/abs/2602.10377)|null|
|**2026-02-10**|**ST4VLA: Spatially Guided Training for Vision-Language-Action Models**|Jiangmiao Pang Team|[2602.10109](http://arxiv.org/abs/2602.10109)|null|
|**2026-02-10**|**EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**|Li Chen Team|[2602.10106](http://arxiv.org/abs/2602.10106)|**[link](https://opendrivelab.com/EgoHumanoid)**|
|**2026-02-10**|**VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model**|Zhibo Chen Team|[2602.10098](http://arxiv.org/abs/2602.10098)|null|
|**2026-02-10**|**UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking**|Yao Mu Team|[2602.10093](http://arxiv.org/abs/2602.10093)|**[link](https://univtac.github.io/)**|
|**2026-02-10**|**RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**|Jiangmiao Pang Team|[2602.09973](http://arxiv.org/abs/2602.09973)|null|
|**2026-02-11**|**BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation**|Jianyu Chen Team|[2602.09849](http://arxiv.org/abs/2602.09849)|null|
|**2026-02-10**|**NavDreamer: Video Models as Zero-Shot 3D Navigators**|Fei Gao Team|[2602.09765](http://arxiv.org/abs/2602.09765)|null|
|**2026-02-10**|**Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization**|Qin Jin Team|[2602.09722](http://arxiv.org/abs/2602.09722)|null|
|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|null|
|**2026-02-10**|**Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments**|Shimin Di Team|[2602.09430](http://arxiv.org/abs/2602.09430)|null|
|**2026-02-10**|**CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments**|Yifan Wu Team|[2602.09367](http://arxiv.org/abs/2602.09367)|null|
|**2026-02-09**|**TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation**|Shanghang Zhang Team|[2602.09023](http://arxiv.org/abs/2602.09023)|null|
|**2026-02-09**|**Mimic Intent, Not Just Trajectories**|Panpan Cai Team|[2602.08602](http://arxiv.org/abs/2602.08602)|null|
|**2026-02-09**|**SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios**|Chelsea Finn Team|[2602.08440](http://arxiv.org/abs/2602.08440)|null|
|**2026-02-09**|**Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning**|Marco Pavone Team|[2602.08167](http://arxiv.org/abs/2602.08167)|null|
|**2026-02-08**|**Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning**|Ranjay Krishna Team|[2602.07845](http://arxiv.org/abs/2602.07845)|**[link](https://rd-vla.github.io/)**|
|**2026-02-10**|**RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI**|Yu Wang Team|[2602.07837](http://arxiv.org/abs/2602.07837)|null|
|**2026-02-07**|**VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation**|Angel X Chang Team|[2602.07555](http://arxiv.org/abs/2602.07555)|null|
|**2026-02-07**|**Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning**|Wei He Team|[2602.07541](http://arxiv.org/abs/2602.07541)|null|
|**2026-02-07**|**VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation**|En Yu Team|[2602.07399](http://arxiv.org/abs/2602.07399)|null|
|**2026-02-06**|**Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique**|Toshiaki Tsuji Team|[2602.06620](http://arxiv.org/abs/2602.06620)|null|
|**2026-02-06**|**Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation**|Guodong Guo Team|[2602.06575](http://arxiv.org/abs/2602.06575)|null|
|**2026-02-06**|**LIBERO-X: Robustness Litmus for Vision-Language-Action Models**|Xinmin Liu Team|[2602.06556](http://arxiv.org/abs/2602.06556)|null|
|**2026-02-06**|**DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving**|Long Chen Team|[2602.06521](http://arxiv.org/abs/2602.06521)|null|
|**2026-02-06**|**World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy**|Mike Zheng Shou Team|[2602.06508](http://arxiv.org/abs/2602.06508)|null|
|**2026-02-06**|**Action Hallucination in Generative Visual-Language-Action Models**|Eugene Lim Team|[2602.06339](http://arxiv.org/abs/2602.06339)|null|
|**2026-02-05**|**RL-VLA $^3$ : Reinforcement Learning VLA Accelerating via Full Asynchronism**|Junwu Xiong Team|[2602.05765](http://arxiv.org/abs/2602.05765)|null|
|**2026-02-05**|**Benchmarking Affordance Generalization with BusyBox**|Galen Mullins Team|[2602.05441](http://arxiv.org/abs/2602.05441)|null|
|**2026-02-07**|**RoboPaint: From Human Demonstration to Any Robot and Any View**|Zhengxue Cheng Team|[2602.05325](http://arxiv.org/abs/2602.05325)|null|
|**2026-02-05**|**MobileManiBench: Simplifying Model Verification for Mobile Manipulation**|Baining Guo Team|[2602.05233](http://arxiv.org/abs/2602.05233)|null|
|**2026-02-04**|**VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**|Dongdong Chen Team|[2602.05049](http://arxiv.org/abs/2602.05049)|**[link](https://vista-vla.github.io/)**|
|**2026-02-04**|**Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data**|Wenzhao Lian Team|[2602.04600](http://arxiv.org/abs/2602.04600)|null|
|**2026-02-04**|**GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning**|Hao Tang Team|[2602.04315](http://arxiv.org/abs/2602.04315)|null|
|**2026-02-04**|**Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**|Badong Chen Team|[2602.04228](http://arxiv.org/abs/2602.04228)|null|
|**2026-02-04**|**SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models**|Jonghyun Choi Team|[2602.04208](http://arxiv.org/abs/2602.04208)|null|
|**2026-02-04**|**Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models**|Ross Greer Team|[2602.04184](http://arxiv.org/abs/2602.04184)|null|
|**2026-02-03**|**Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**|Rex Ying Team|[2602.03983](http://arxiv.org/abs/2602.03983)|null|
|**2026-02-03**|**QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization**|Zhipeng Zhang Team|[2602.03782](http://arxiv.org/abs/2602.03782)|null|
|**2026-02-03**|**MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**|Jungwoo Lee Team|[2602.03668](http://arxiv.org/abs/2602.03668)|null|
|**2026-02-03**|**CRL-VLA: Continual Vision-Language-Action Learning**|Chao Huang Team|[2602.03445](http://arxiv.org/abs/2602.03445)|null|
|**2026-02-03**|**RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization**|Jun Zhu Team|[2602.03310](http://arxiv.org/abs/2602.03310)|null|
|**2026-02-03**|**When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens**|Miao Li Team|[2602.03153](http://arxiv.org/abs/2602.03153)|null|
|**2026-02-02**|**Accelerating Structured Chain-of-Thought in Autonomous Vehicles**|Marco Pavone Team|[2602.02864](http://arxiv.org/abs/2602.02864)|null|
|**2026-02-02**|**TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**|Jiaqi Ma Team|[2602.02459](http://arxiv.org/abs/2602.02459)|null|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|**[link](https://world-gymnast.github.io/)**|
|**2026-02-02**|**MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models**|Lemiao Qiu Team|[2602.02212](http://arxiv.org/abs/2602.02212)|null|
|**2026-02-02**|**FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**|Haiyue Zhu Team|[2602.02142](http://arxiv.org/abs/2602.02142)|null|
|**2026-02-02**|**Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models**|Di Wang Team|[2602.01834](http://arxiv.org/abs/2602.01834)|null|
|**2026-02-02**|**From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models**|Jianzong Wang Team|[2602.01811](http://arxiv.org/abs/2602.01811)|null|
|**2026-02-01**|**Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**|Shanghang Zhang Team|[2602.01166](http://arxiv.org/abs/2602.01166)|null|
|**2026-02-01**|**Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**|Matteo Matteucci Team|[2602.01158](http://arxiv.org/abs/2602.01158)|null|
|**2026-02-01**|**StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**|Lu Fang Team|[2602.01100](http://arxiv.org/abs/2602.01100)|null|
|**2026-02-01**|**A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**|Jose Barreiros Team|[2602.01067](http://arxiv.org/abs/2602.01067)|null|
|**2026-01-31**|**Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**|A. Postnikov Team|[2602.00919](http://arxiv.org/abs/2602.00919)|null|
|**2026-01-31**|**Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds**|Hengshuang Zhao Team|[2602.00807](http://arxiv.org/abs/2602.00807)|null|
|**2026-01-31**|**Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models**|Yanyong Zhang Team|[2602.00780](http://arxiv.org/abs/2602.00780)|null|
|**2026-01-31**|**SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning**|Ivor Tsang Team|[2602.00743](http://arxiv.org/abs/2602.00743)|null|
|**2026-01-31**|**Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching**|Shuo Yang Team|[2602.00686](http://arxiv.org/abs/2602.00686)|null|
|**2026-01-31**|**ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation**|Shuo Yang Team|[2602.00557](http://arxiv.org/abs/2602.00557)|null|
|**2026-01-31**|**Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning**|Shuo Yang Team|[2602.00500](http://arxiv.org/abs/2602.00500)|null|
|**2026-01-30**|**Vision-Language Models Unlock Task-Centric Latent Actions**|Vladislav Kurenkov Team|[2601.22714](http://arxiv.org/abs/2601.22714)|null|
|**2026-01-30**|**CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**|Jianzong Wang Team|[2601.22467](http://arxiv.org/abs/2601.22467)|null|
|**2026-01-29**|**DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**|Ziwei Liu Team|[2601.22153](http://arxiv.org/abs/2601.22153)|**[link](https://www.infinitescript.com/project/dynamic-vla/)**|
|**2026-01-29**|**MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**|Stefanie Speidel Team|[2601.21971](http://arxiv.org/abs/2601.21971)|null|
|**2026-01-29**|**CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**|Yaohua Liu Team|[2601.21712](http://arxiv.org/abs/2601.21712)|null|
|**2026-01-29**|**AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**|Yonglin Tian Team|[2601.21602](http://arxiv.org/abs/2601.21602)|null|
|**2026-01-29**|**IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation**|Jeonggil Ko Team|[2601.21506](http://arxiv.org/abs/2601.21506)|null|
|**2026-01-28**|**Demonstration-Free Robotic Control via LLM Agents**|Tiffany J. Hwu Team|[2601.20334](http://arxiv.org/abs/2601.20334)|null|
|**2026-01-30**|**TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation**|Ziyuan Jiao Team|[2601.20321](http://arxiv.org/abs/2601.20321)|null|
|**2026-01-28**|**Shallow-π: Knowledge Distillation for Flow-based VLAs**|Taehan Kim Team|[2601.20262](http://arxiv.org/abs/2601.20262)|null|
|**2026-01-27**|**AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation**|Lei Zhu Team|[2601.19634](http://arxiv.org/abs/2601.19634)|null|
|**2026-01-26**|**Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods**|Hong Liu Team|[2601.18723](http://arxiv.org/abs/2601.18723)|null|
|**2026-01-26**|**A Pragmatic VLA Foundation Model**|Kecheng Zheng Team|[2601.18692](http://arxiv.org/abs/2601.18692)|**[link](https://technology.robbyant.com/lingbot-vla/)**|
|**2026-01-26**|**TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion**|Jian Tang Team|[2601.18323](http://arxiv.org/abs/2601.18323)|null|
|**2026-01-25**|**PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation**|Xun Cao Team|[2601.17885](http://arxiv.org/abs/2601.17885)|null|
|**2026-01-25**|**SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation**|Andrew Jaeyong Choi Team|[2601.17657](http://arxiv.org/abs/2601.17657)|null|
|**2026-01-23**|**ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance**|Wei-Shi Zheng Team|[2601.16667](http://arxiv.org/abs/2601.16667)|null|
|**2026-01-23**|**Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)**|Walid G. Aref Team|[2601.16409](http://arxiv.org/abs/2601.16409)|null|
|**2026-01-22**|**IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance**|Michael S Ryoo Team|[2601.16207](http://arxiv.org/abs/2601.16207)|null|
|**2026-01-22**|**Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning**|Jinwei Gu Team|[2601.16163](http://arxiv.org/abs/2601.16163)|null|
|**2026-01-22**|**DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models**|Jingqun Tang Team|[2601.16065](http://arxiv.org/abs/2601.16065)|null|
|**2026-01-21**|**CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation**|Arash Ajoudani Team|[2601.15541](http://arxiv.org/abs/2601.15541)|null|
|**2026-01-27**|**LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries**|Kai Chen Team|[2601.15197](http://arxiv.org/abs/2601.15197)|null|
|**2026-01-21**|**TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control**|Wei Yun Yau Team|[2601.14945](http://arxiv.org/abs/2601.14945)|null|
|**2026-01-21**|**A Brain-inspired Embodied Intelligence for Fluid and Fast Reflexive Robotics Control**|Hui Xiong Team|[2601.14628](http://arxiv.org/abs/2601.14628)|null|
|**2026-01-20**|**SilentDrift: Exploiting Action Chunking for Stealthy Backdoor Attacks on Vision-Language-Action Models**|Emilio Ferrara Team|[2601.14323](http://arxiv.org/abs/2601.14323)|null|
|**2026-01-20**|**TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers**|Kai Chen Team|[2601.14133](http://arxiv.org/abs/2601.14133)|**[link](https://github.com/ZGC-EmbodyAI/TwinBrainVLA)**|
|**2026-01-20**|**Pedagogical Alignment for Vision-Language-Action Models: A Comprehensive Framework for Data, Architecture, and Evaluation in Education**|Gyeonggeon Lee Team|[2601.13876](http://arxiv.org/abs/2601.13876)|null|
|**2026-01-21**|**DroneVLA: VLA based Aerial Manipulation**|Dzmitry Tsetserukou Team|[2601.13809](http://arxiv.org/abs/2601.13809)|null|
|**2026-01-19**|**Being-H0.5: Scaling Human-Centric Robot Learning for Cross-Embodiment Generalization**|Zongqing Lu Team|[2601.12993](http://arxiv.org/abs/2601.12993)|null|
|**2026-01-17**|**Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving**|Sifa Zheng Team|[2601.12142](http://arxiv.org/abs/2601.12142)|null|
|**2026-01-16**|**Generative Scenario Rollouts for End-to-End Autonomous Driving**|Hong Cai Team|[2601.11475](http://arxiv.org/abs/2601.11475)|null|
|**2026-01-16**|**ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models**|Guanghui Ren Team|[2601.11404](http://arxiv.org/abs/2601.11404)|null|
|**2026-01-16**|**VLAgents: A Policy Server for Efficient VLA Inference**|Wolfram Burgard Team|[2601.11250](http://arxiv.org/abs/2601.11250)|null|
|**2026-01-14**|**Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning**|Fu-En Yang Team|[2601.09708](http://arxiv.org/abs/2601.09708)|**[link](https://jasper0314-huang.github.io/fast-thinkact/)**|
|**2026-01-14**|**CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion**|Angela P. Schoellig Team|[2601.09512](http://arxiv.org/abs/2601.09512)|**[link](https://tum-lsy.github.io/clare.)**|
|**2026-01-13**|**ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation**|Yanwei Fu Team|[2601.08325](http://arxiv.org/abs/2601.08325)|null|
|**2026-01-12**|**Motion Focus Recognition in Fast-Moving Egocentric Video**|Abolfazl Razi Team|[2601.07154](http://arxiv.org/abs/2601.07154)|null|
|**2026-01-11**|**PALM: Progress-Aware Policy Learning via Affordance Reasoning for Long-Horizon Robotic Manipulation**|Ismini Lourentzou Team|[2601.07060](http://arxiv.org/abs/2601.07060)|null|
|**2026-01-13**|**On-the-Fly VLA Adaptation via Test-Time Reinforcement Learning**|Cheng Han Team|[2601.06748](http://arxiv.org/abs/2601.06748)|null|
|**2026-01-17**|**SparseOccVLA: Bridging Occupancy and Vision-Language Models via Sparse Queries for Unified 4D Scene Understanding and Planning**|Yan Wang Team|[2601.06474](http://arxiv.org/abs/2601.06474)|null|
|**2026-01-10**|**CulinaryCut-VLAP: A Vision-Language-Action-Physics Framework for Food Cutting via a Force-Aware Material Point Method**|Heewon Kim Team|[2601.06451](http://arxiv.org/abs/2601.06451)|null|
|**2026-01-09**|**LatentVLA: Efficient Vision-Language Models for Autonomous Driving via Latent Action Prediction**|Hongyang Li Team|[2601.05611](http://arxiv.org/abs/2601.05611)|null|
|**2026-01-08**|**LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model**|Shanghang Zhang Team|[2601.05248](http://arxiv.org/abs/2601.05248)|null|
|**2026-01-08**|**RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation**|Jiangmiao Pang Team|[2601.05241](http://arxiv.org/abs/2601.05241)|null|
|**2026-01-07**|**State Backdoor: Towards Stealthy Real-world Poisoning Attack on Vision-Language-Action Model in State Space**|Dusit Niyato Team|[2601.04266](http://arxiv.org/abs/2601.04266)|null|
|**2026-01-07**|**CLAP: Contrastive Latent Action Pretraining for Learning Vision-Language-Action Models from Human Videos**|Yansong Tang Team|[2601.04061](http://arxiv.org/abs/2601.04061)|**[link](https://lin-shan.com/CLAP/)**|
|**2026-01-07**|**Stable Language Guidance for Vision-Language-Action Models**|Guangrun Wang Team|[2601.04052](http://arxiv.org/abs/2601.04052)|null|
|**2026-01-07**|**I2E: From Image Pixels to Actionable Interactive Environments for Text-Guided Image Editing**|Bowen Zhou Team|[2601.03741](http://arxiv.org/abs/2601.03741)|null|
|**2026-01-12**|**A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving**|Liang Xiao Team|[2601.03519](http://arxiv.org/abs/2601.03519)|null|
|**2026-01-06**|**VLM4VLA: Revisiting Vision-Language-Models in Vision-Language-Action Models**|Jianyu Chen Team|[2601.03309](http://arxiv.org/abs/2601.03309)|null|
|**2026-01-06**|**Limited Linguistic Diversity in Embodied AI Datasets**|Mitch Pryor Team|[2601.03136](http://arxiv.org/abs/2601.03136)|null|
|**2026-01-06**|**SOP: A Scalable Online Post-Training System for Vision-Language-Action Models**|Jianlan Luo Team|[2601.03044](http://arxiv.org/abs/2601.03044)|null|
|**2026-01-05**|**InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation**|Yuchen Zhu Team|[2601.02456](http://arxiv.org/abs/2601.02456)|**[link](https://internrobotics.github.io/internvla-a1.github.io/)**|
|**2026-01-05**|**CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding**|Andrew Markham Team|[2601.02295](http://arxiv.org/abs/2601.02295)|**[link](https://dannymcy.github.io/cyclevla/)**|
|**2026-01-04**|**Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation**|Shanghang Zhang Team|[2601.01618](http://arxiv.org/abs/2601.01618)|null|
|**2026-01-02**|**Value Vision-Language-Action Planning & Search**|Cyrus Neary Team|[2601.00969](http://arxiv.org/abs/2601.00969)|null|
|**2025-12-31**|**Dichotomous Diffusion Policy Optimization**|Xianyuan Zhan Team|[2601.00898](http://arxiv.org/abs/2601.00898)|null|
|**2025-12-31**|**VLA-RAIL: A Real-Time Asynchronous Inference Linker for VLA Models and Robots**|Han Gao Team|[2512.24673](http://arxiv.org/abs/2512.24673)|null|
|**2026-01-06**|**RoboMIND 2.0: A Multimodal, Bimanual Mobile Manipulation Dataset for Generalizable Embodied Intelligence**|Jian Tang Team|[2512.24653](http://arxiv.org/abs/2512.24653)|null|
|**2025-12-30**|**Counterfactual VLA: Self-Reflective Vision-Language-Action Model with Adaptive Reasoning**|Marco Pavone Team|[2512.24426](http://arxiv.org/abs/2512.24426)|null|
|**2026-01-09**|**GR-Dexter Technical Report**|Hang Li Team|[2512.24210](http://arxiv.org/abs/2512.24210)|null|
|**2026-01-01**|**Unified Embodied VLM Reasoning with Robotic Action via Autoregressive Discretized Pre-training**|Jianlan Luo Team|[2512.24125](http://arxiv.org/abs/2512.24125)|null|
|**2025-12-29**|**Learning to Feel the Future: DreamTacVLA for Contact-Rich Manipulation**|Han Liu Team|[2512.23864](http://arxiv.org/abs/2512.23864)|null|
|**2026-01-05**|**SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling**|Daguang Xu Team|[2512.23162](http://arxiv.org/abs/2512.23162)|null|
|**2025-12-31**|**ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving**|Hongsheng Li Team|[2512.22939](http://arxiv.org/abs/2512.22939)|**[link](https://pqh22.github.io/projects/ColaVLA/index.html)**|
|**2026-01-04**|**Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone**|Lingpeng Kong Team|[2512.22615](http://arxiv.org/abs/2512.22615)|null|
|**2025-12-27**|**VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models**|Yaodong Yang Team|[2512.22539](http://arxiv.org/abs/2512.22539)|null|
|**2025-12-27**|**Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding**|Ngan Le Team|[2512.22519](http://arxiv.org/abs/2512.22519)|**[link](https://uark-aicv.github.io/OBEYED_VLA)**|
|**2025-12-27**|**Emergence of Human to Robot Transfer in Vision-Language-Action Models**|Suraj Nair Team|[2512.22414](http://arxiv.org/abs/2512.22414)|null|
|**2025-12-22**|**Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA**|Yanzhi Wang Team|[2512.22208](http://arxiv.org/abs/2512.22208)|null|
|**2025-12-26**|**StereoVLA: Enhancing Vision-Language-Action Models with Stereo Vision**|He Wang Team|[2512.21970](http://arxiv.org/abs/2512.21970)|null|
|**2025-12-23**|**ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge**|Xuehai Zhou Team|[2512.20276](http://arxiv.org/abs/2512.20276)|null|
|**2025-12-23**|**Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation**|Lianyang Ma Team|[2512.20188](http://arxiv.org/abs/2512.20188)|null|
|**2025-12-23**|**LoLA: Long Horizon Latent Action Learning for General Robot Manipulation**|Baining Guo Team|[2512.20166](http://arxiv.org/abs/2512.20166)|null|
|**2025-12-23**|**Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting**|Wook-Shin Han Team|[2512.20014](http://arxiv.org/abs/2512.20014)|null|
|**2025-12-22**|**REALM: A Real-to-Sim Validated Benchmark for Generalization in Robotic Manipulation**|Vladimir Petrik Team|[2512.19562](http://arxiv.org/abs/2512.19562)|null|
|**2025-12-22**|**Point What You Mean: Visually Grounded Instruction Policy**|Yang Gao Team|[2512.18933](http://arxiv.org/abs/2512.18933)|null|
|**2025-12-20**|**STORM: Search-Guided Generative World Models for Robotic Manipulation**|Keze Wang Team|[2512.18477](http://arxiv.org/abs/2512.18477)|null|
|**2025-12-20**|**AOMGen: Photoreal, Physics-Consistent Demonstration Generation for Articulated Object Manipulation**|Yakun Huang Team|[2512.18396](http://arxiv.org/abs/2512.18396)|null|
|**2025-12-19**|**Robotic VLA Benefits from Joint Learning with Motion Image Diffusion**|Juan Carlos Niebles Team|[2512.18007](http://arxiv.org/abs/2512.18007)|**[link](https://vla-motion.github.io/)**|
|**2025-12-18**|**GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation**|Li Jiang Team|[2512.16811](http://arxiv.org/abs/2512.16811)|null|
|**2025-12-18**|**Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future**|Junwei Liang Team|[2512.16760](http://arxiv.org/abs/2512.16760)|**[link](https://github.com/worldbench/awesome-vla-for-ad)**|
|**2025-12-17**|**Large Video Planner Enables Generalizable Robot Control**|Yilun Du Team|[2512.15840](http://arxiv.org/abs/2512.15840)|null|
|**2025-12-19**|**mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs**|Elvis Nava Team|[2512.15692](http://arxiv.org/abs/2512.15692)|null|
|**2025-12-19**|**MiVLA: Towards Generalizable Vision-Language-Action Model with Human-Robot Mutual Imitation Pre-training**|Heng Tao Shen Team|[2512.15411](http://arxiv.org/abs/2512.15411)|null|
|**2025-12-19**|**VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments**|Fei Gao Team|[2512.15258](http://arxiv.org/abs/2512.15258)|null|
|**2025-12-16**|**EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models**|Mike Zheng Shou Team|[2512.14666](http://arxiv.org/abs/2512.14666)|null|
|**2025-12-16**|**Sample-Efficient Robot Skill Learning for Construction Tasks: Benchmarking Hierarchical Reinforcement Learning and Vision-Language-Action VLA Model**|Ci-Jyun Liang Team|[2512.14031](http://arxiv.org/abs/2512.14031)|null|
|**2025-12-16**|**MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning**|Xiang Bai Team|[2512.13636](http://arxiv.org/abs/2512.13636)|**[link](https://xiaomi-mlab.github.io/MindDrive/)**|
|**2025-12-15**|**Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos**|Zongqing Lu Team|[2512.13080](http://arxiv.org/abs/2512.13080)|null|
|**2025-12-15**|**Motus: A Unified Latent Action World Model**|Jun Zhu Team|[2512.13030](http://arxiv.org/abs/2512.13030)|null|
|**2025-12-14**|**DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning**|Hengshuang Zhao Team|[2512.12799](http://arxiv.org/abs/2512.12799)|null|
|**2025-12-11**|**Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control**|Ibrahim Sheikh Mohamed Team|[2512.11921](http://arxiv.org/abs/2512.11921)|null|
|**2025-12-10**|**Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models**|Arash Ajoudani Team|[2512.11908](http://arxiv.org/abs/2512.11908)|null|
|**2025-12-09**|**VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer**|Xiao He Team|[2512.11891](http://arxiv.org/abs/2512.11891)|null|
|**2025-12-12**|**BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models**|Yanfang Ye Team|[2512.11769](http://arxiv.org/abs/2512.11769)|**[link](https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model)**|
|**2025-12-12**|**Embodied Image Compression**|Guangtao Zhai Team|[2512.11612](http://arxiv.org/abs/2512.11612)|null|
|**2025-12-12**|**Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents**|Boris Kraychev Team|[2512.11584](http://arxiv.org/abs/2512.11584)|null|
|**2025-12-19**|**An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges**|Jiankang Deng Team|[2512.11362](http://arxiv.org/abs/2512.11362)|**[link](https://suyuz1.github.io/VLA-Survey-Anatomy/)**|
|**2025-12-12**|**Benchmarking the Generality of Vision-Language-Action Models**|Yangyue Wang Team|[2512.11315](http://arxiv.org/abs/2512.11315)|null|
|**2025-12-12**|**Seeing to Act, Prompting to Specify: A Bayesian Factorization of Vision Language Action Policy**|Yue Wang Team|[2512.11218](http://arxiv.org/abs/2512.11218)|null|
|**2025-12-15**|**WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control**|Hongyang Li Team|[2512.11047](http://arxiv.org/abs/2512.11047)|null|
|**2025-12-11**|**RoboNeuron: A Modular Framework Linking Foundation Models and ROS for Embodied AI**|Jian Cheng Team|[2512.10394](http://arxiv.org/abs/2512.10394)|null|
|**2025-12-11**|**Latent Chain-of-Thought World Modeling for End-to-End Driving**|Boris Ivanovic Team|[2512.10226](http://arxiv.org/abs/2512.10226)|null|
|**2025-12-10**|**HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models**|Donglin Wang Team|[2512.09928](http://arxiv.org/abs/2512.09928)|**[link](https://hifvla.github.io)**|
|**2025-12-10**|**Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models**|Zhihe Lu Team|[2512.09927](http://arxiv.org/abs/2512.09927)|null|
|**2025-12-10**|**UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving**|Ying-Cong Chen Team|[2512.09864](http://arxiv.org/abs/2512.09864)|**[link](https://seed-uniugp.github.io/)**|
|**2025-12-10**|**GLaD: Geometric Latent Distillation for Vision-Language-Action Models**|Xiaojun Chang Team|[2512.09619](http://arxiv.org/abs/2512.09619)|null|
|**2025-12-10**|**Mind to Hand: Purposeful Robotic Control via Embodied Reasoning**|Jianan Wang Team|[2512.08580](http://arxiv.org/abs/2512.08580)|null|
|**2025-12-09**|**Robust Finetuning of Vision-Language-Action Robot Policies via Parameter Merging**|Sergey Levine Team|[2512.08333](http://arxiv.org/abs/2512.08333)|null|
|**2025-12-08**|**See Once, Then Act: Vision-Language-Action Model with Task Learning from One-Shot Video Demonstrations**|Yufeng Yue Team|[2512.07582](http://arxiv.org/abs/2512.07582)|null|
|**2025-12-08**|**Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation**|Chang Xu Team|[2512.07472](http://arxiv.org/abs/2512.07472)|null|
|**2025-12-07**|**VideoVLA: Video Generators Can Be Generalizable Robot Manipulators**|Baining Guo Team|[2512.06963](http://arxiv.org/abs/2512.06963)|**[link](https://videovla-nips2025.github.io)**|
|**2025-12-07**|**Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge**|Akash Karnatak Team|[2512.06951](http://arxiv.org/abs/2512.06951)|null|
|**2025-12-11**|**WAM-Flow: Parallel Coarse-to-Fine Motion Planning via Discrete Flow Matching for Autonomous Driving**|Siyu Zhu Team|[2512.06112](http://arxiv.org/abs/2512.06112)|**[link](https://github.com/fudan-generative-vision/WAM-Flow)**|
|**2025-12-09**|**Training-Time Action Conditioning for Efficient Real-Time Chunking**|Sergey Levine Team|[2512.05964](http://arxiv.org/abs/2512.05964)|null|
|**2025-12-05**|**SIMPACT: Simulation-Enabled Action Planning using Vision-Language Models**|Yilun Du Team|[2512.05955](http://arxiv.org/abs/2512.05955)|null|
|**2025-12-05**|**TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models**|Babak Damavandi Team|[2512.05943](http://arxiv.org/abs/2512.05943)|null|
|**2025-12-05**|**Probing the effectiveness of World Models for Spatial Reasoning through Test-time Scaling**|Sarath Chandar Team|[2512.05809](http://arxiv.org/abs/2512.05809)|null|
|**2025-12-05**|**HiMoE-VLA: Hierarchical Mixture-of-Experts for Generalist Vision-Language-Action Policies**|Yu-Gang Jiang Team|[2512.05693](http://arxiv.org/abs/2512.05693)|null|
|**2025-12-05**|**Conscious Gaze: Adaptive Attention Mechanisms for Hallucination Mitigation in Vision-Language Models**|Guixian Zhang Team|[2512.05546](http://arxiv.org/abs/2512.05546)|null|
|**2025-12-05**|**VOST-SGG: VLM-Aided One-Stage Spatio-Temporal Scene Graph Generation**|Basura Fernando Team|[2512.05524](http://arxiv.org/abs/2512.05524)|null|
|**2025-12-04**|**From Segments to Scenes: Temporal Understanding in Autonomous Driving via Vision-Language Model**|Mohammad Akbari Team|[2512.05277](http://arxiv.org/abs/2512.05277)|null|
|**2025-12-04**|**STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models**|Benjamin Busam Team|[2512.05107](http://arxiv.org/abs/2512.05107)|null|
|**2025-12-04**|**TV2TV: A Unified Framework for Interleaved Language and Video Generation**|Emily Dinan Team|[2512.05103](http://arxiv.org/abs/2512.05103)|null|
|**2025-12-08**|**FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via Neural Action Tokenization**|Hang Zhao Team|[2512.04952](http://arxiv.org/abs/2512.04952)|null|
|**2025-12-04**|**E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving**|Chengzhong Xu Team|[2512.04733](http://arxiv.org/abs/2512.04733)|null|
|**2025-12-04**|**Towards Cross-View Point Correspondence in Vision-Language Models**|Xiaolong Zheng Team|[2512.04686](http://arxiv.org/abs/2512.04686)|null|
|**2025-12-04**|**SAM3-I: Segment Anything with Instructions**|Li Cheng Team|[2512.04585](http://arxiv.org/abs/2512.04585)|null|
|**2025-12-04**|**X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale**|Mike Zheng Shou Team|[2512.04537](http://arxiv.org/abs/2512.04537)|null|
|**2025-12-04**|**dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning**|Chaowei Xiao Team|[2512.04459](http://arxiv.org/abs/2512.04459)|null|
|**2025-12-04**|**Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops**|Minghui Zheng Team|[2512.04446](http://arxiv.org/abs/2512.04446)|null|
|**2025-12-04**|**MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving**|Ziying Song Team|[2512.04441](http://arxiv.org/abs/2512.04441)|null|
|**2025-12-04**|**FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination**|Guillaume Sartoretti Team|[2512.04381](http://arxiv.org/abs/2512.04381)|null|
|**2025-12-04**|**Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment**|Yu-Chiang Frank Wang Team|[2512.04356](http://arxiv.org/abs/2512.04356)|**[link](https://kpc0810.github.io/santa/)**|
|**2025-12-04**|**TempR1: Improving Temporal Understanding of MLLMs via Temporal-Aware Multi-Task Reinforcement Learning**|Limin Wang Team|[2512.03963](http://arxiv.org/abs/2512.03963)|null|
|**2025-12-03**|**Hierarchical Vision Language Action Model Using Success and Failure Demonstrations**|Sungjoon Choi Team|[2512.03913](http://arxiv.org/abs/2512.03913)|**[link](https://vine-vla.github.io/)**|
|**2025-12-08**|**PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention**|Mingming Gong Team|[2512.03724](http://arxiv.org/abs/2512.03724)|null|
|**2025-12-03**|**Towards Object-centric Understanding for Instructional Videos**|Yu Kong Team|[2512.03479](http://arxiv.org/abs/2512.03479)|null|
|**2025-12-02**|**VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling**|Guangrun Wang Team|[2512.02902](http://arxiv.org/abs/2512.02902)|null|
|**2025-12-02**|**Action Anticipation at a Glimpse: To What Extent Can Multimodal Cues Replace Video?**|Jose Garcia-Rodriguez Team|[2512.02846](http://arxiv.org/abs/2512.02846)|null|
|**2025-12-02**|**Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach**|Xuelong Li Team|[2512.02834](http://arxiv.org/abs/2512.02834)|null|
|**2025-12-02**|**Radiologist Copilot: An Agentic Assistant with Orchestrated Tools for Radiology Reporting with Quality Control**|Xiaofan Zhang Team|[2512.02814](http://arxiv.org/abs/2512.02814)|null|
|**2025-12-03**|**Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols**|Yong-Lu Li Team|[2512.02787](http://arxiv.org/abs/2512.02787)|null|
|**2025-12-02**|**RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning**|Haoqian Wang Team|[2512.02729](http://arxiv.org/abs/2512.02729)|null|
|**2025-12-01**|**ManualVLA: A Unified VLA Model for Chain-of-Thought Manual Generation and Robotic Manipulation**|Shanghang Zhang Team|[2512.02013](http://arxiv.org/abs/2512.02013)|null|
|**2025-12-02**|**Generative Action Tell-Tales: Assessing Human Motion in Synthesized Videos**|Deepti Ghadiyaram Team|[2512.01803](http://arxiv.org/abs/2512.01803)|null|
|**2025-12-02**|**GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation**|Yonghui Wu Team|[2512.01801](http://arxiv.org/abs/2512.01801)|null|
|**2025-12-01**|**IGen: Scalable Data Generation for Robot Learning from Open-World Images**|Zhi Wang Team|[2512.01773](http://arxiv.org/abs/2512.01773)|null|
|**2025-12-01**|**DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models**|Zongqing Lu Team|[2512.01715](http://arxiv.org/abs/2512.01715)|null|
|**2025-12-01**|**NavForesee: A Unified Vision-Language World Model for Hierarchical Planning and Dual-Horizon Navigation Prediction**|Mu Xu Team|[2512.01550](http://arxiv.org/abs/2512.01550)|null|
|**2025-11-30**|**VLASH: Real-Time VLAs via Future-State-Aware Asynchronous Inference**|Song Han Team|[2512.01031](http://arxiv.org/abs/2512.01031)|null|
|**2025-11-30**|**CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding**|Wei-Shi Zheng Team|[2512.01022](http://arxiv.org/abs/2512.01022)|**[link](https://isee-laboratory.github.io/OmniDexGrasp/)**|
|**2025-11-30**|**MM-ACT: Learn from Multimodal Parallel Generation to Act**|Ping Luo Team|[2512.00975](http://arxiv.org/abs/2512.00975)|null|
|**2025-11-30**|**SwiftVLA: Unlocking Spatiotemporal Dynamics for Lightweight VLA Models at Minimal Overhead**|Wenjun Mei Team|[2512.00903](http://arxiv.org/abs/2512.00903)|null|
|**2025-11-30**|**AFRAgent : An Adaptive Feature Renormalization Based High Resolution Aware GUI agent**|Mausoom Sarkar Team|[2512.00846](http://arxiv.org/abs/2512.00846)|null|
|**2025-11-30**|**Transforming Monolithic Foundation Models into Embodied Multi-Agent Architectures for Human-Robot Collaboration**|Huaping Liu Team|[2512.00797](http://arxiv.org/abs/2512.00797)|null|
|**2025-12-02**|**Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment**|Libo Wang Team|[2512.00783](http://arxiv.org/abs/2512.00783)|**[link](https://huggingface.co/Veltraxor/Sigma)**|
|**2025-11-29**|**Image Generation as a Visual Planner for Robotic Manipulation**|Ye Pang Team|[2512.00532](http://arxiv.org/abs/2512.00532)|null|
|**2025-11-28**|**AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI**|Ernest Pedapati Team|[2512.00194](http://arxiv.org/abs/2512.00194)|null|
|**2025-11-28**|**Hunyuan-GameCraft-2: Instruction-following Interactive Game World Model**|Qinglin Lu Team|[2511.23429](http://arxiv.org/abs/2511.23429)|**[link](https://hunyuan-gamecraft-2.github.io/)**|
|**2025-11-28**|**Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning**|Jun Wang Team|[2511.23262](http://arxiv.org/abs/2511.23262)|null|
|**2025-11-28**|**Obstruction reasoning for robotic grasping**|Fabio Poiesi Team|[2511.23186](http://arxiv.org/abs/2511.23186)|null|
|**2025-11-28**|**MindPower: Enabling Theory-of-Mind Reasoning in VLM-based Embodied Agents**|Wen-Huang Cheng Team|[2511.23055](http://arxiv.org/abs/2511.23055)|null|
|**2025-11-28**|**LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models**|Jianlong Fu Team|[2511.23034](http://arxiv.org/abs/2511.23034)|**[link](https://mm-robot.github.io/distill_latent_action/)**|
|**2025-11-28**|**From Illusion to Intention: Visual Rationale Learning for Vision-Language Reasoning**|Yunfeng Yan Team|[2511.23031](http://arxiv.org/abs/2511.23031)|null|
|**2025-11-27**|**Distracted Robot: How Visual Clutter Undermine Robotic Manipulation**|Xuan Zhao Team|[2511.22780](http://arxiv.org/abs/2511.22780)|null|
|**2025-11-27**|**Improving Robotic Manipulation Robustness via NICE Scene Surgery**|Amir Rasouli Team|[2511.22777](http://arxiv.org/abs/2511.22777)|null|
|**2025-11-27**|**Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations**|Roei Herzig Team|[2511.22697](http://arxiv.org/abs/2511.22697)|null|
|**2025-11-27**|**Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention**|Meibao Yao Team|[2511.22555](http://arxiv.org/abs/2511.22555)|null|
|**2025-11-27**|**CoT4AD: A Vision-Language-Action Model with Explicit Chain-of-Thought Reasoning for Autonomous Driving**|Hao Tang Team|[2511.22532](http://arxiv.org/abs/2511.22532)|null|
|**2025-11-27**|**SkeletonAgent: An Agentic Interaction Framework for Skeleton-based Action Recognition**|Zhenan Sun Team|[2511.22433](http://arxiv.org/abs/2511.22433)|null|
|**2025-11-27**|**DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action**|Feng Zhao Team|[2511.22134](http://arxiv.org/abs/2511.22134)|null|
|**2025-11-26**|**Attention-Guided Patch-Wise Sparse Adversarial Attacks on Vision-Language-Action Models**|Nan Zhang Team|[2511.21663](http://arxiv.org/abs/2511.21663)|null|
|**2025-11-26**|**VacuumVLA: Boosting VLA Capabilities via a Unified Suction and Gripping Tool for Complex Robotic Manipulation**|Shaoshuai Shi Team|[2511.21557](http://arxiv.org/abs/2511.21557)|null|
|**2025-11-26**|**$\mathcal{E}_0$ : Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion**|Guangrun Wang Team|[2511.21542](http://arxiv.org/abs/2511.21542)|null|
|**2025-11-26**|**From Observation to Action: Latent Action-based Primitive Segmentation for VLA Pre-training in Industrial Settings**|Alexander Kleiner Team|[2511.21428](http://arxiv.org/abs/2511.21428)|null|
|**2025-11-26**|**Towards an Effective Action-Region Tracking Framework for Fine-grained Video Action Recognition**|Zhiyong Wang Team|[2511.21202](http://arxiv.org/abs/2511.21202)|null|
|**2025-11-26**|**When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models**|Xudong Jiang Team|[2511.21192](http://arxiv.org/abs/2511.21192)|null|
|**2025-11-26**|**OVOD-Agent: A Markov-Bandit Framework for Proactive Visual Reasoning and Self-Evolving Detection**|Chu He Team|[2511.21064](http://arxiv.org/abs/2511.21064)|null|
|**2025-11-26**|**ENACT: Evaluating Embodied Cognition with World Modeling of Egocentric Interaction**|Manling Li Team|[2511.20937](http://arxiv.org/abs/2511.20937)|null|
|**2025-11-25**|**OVAL-Grasp: Open-Vocabulary Affordance Localization for Task Oriented Grasping**|Odest Chadwicke Jenkins Team|[2511.20841](http://arxiv.org/abs/2511.20841)|null|
|**2025-11-25**|**DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving**|Chun Jason Xue Team|[2511.20720](http://arxiv.org/abs/2511.20720)|null|
|**2025-11-25**|**Reinforcing Action Policies by Prophesying**|Li Zhang Team|[2511.20633](http://arxiv.org/abs/2511.20633)|**[link](https://LogosRoboticsGroup.github.io/ProphRL)**|
|**2025-11-25**|**ArtiBench and ArtiBrain: Benchmarking Generalizable Vision-Language Articulated Object Manipulation**|Yan Xia Team|[2511.20330](http://arxiv.org/abs/2511.20330)|null|
|**2025-11-25**|**ScenarioCLIP: Pretrained Transferable Visual Language Models and Action-Genome Dataset for Natural Scene Analysis**|Abhijit Das Team|[2511.20274](http://arxiv.org/abs/2511.20274)|null|
|**2025-11-25**|**Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving**|Tat-Seng Chua Team|[2511.19912](http://arxiv.org/abs/2511.19912)|null|
|**2025-11-25**|**MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization**|Zsolt Kira Team|[2511.19878](http://arxiv.org/abs/2511.19878)|null|
|**2025-11-25**|**GigaWorld-0: World Models as Data Engine to Empower Embodied AI**|Zheng Zhu Team|[2511.19861](http://arxiv.org/abs/2511.19861)|**[link](https://gigaworld0.github.io/)**|
|**2025-11-25**|**Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation**|Sanglu Lu Team|[2511.19859](http://arxiv.org/abs/2511.19859)|null|
|**2025-11-24**|**Prune-Then-Plan: Step-Level Calibration for Stable Frontier Exploration in Embodied Question Answering**|Roni Sengupta Team|[2511.19768](http://arxiv.org/abs/2511.19768)|**[link](https://noahfrahm.github.io/Prune-Then-Plan-project-page/)**|
|**2025-11-24**|**Learning Massively Multitask World Models for Continuous Control**|Xiaolong Wang Team|[2511.19584](http://arxiv.org/abs/2511.19584)|**[link](https://www.nicklashansen.com/NewtWM)**|
|**2025-11-24**|**Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories**|Jiang Bian Team|[2511.19528](http://arxiv.org/abs/2511.19528)|null|
|**2025-11-24**|**Mixture of Horizons in Action Chunking**|Mingyu Ding Team|[2511.19433](http://arxiv.org/abs/2511.19433)|null|
|**2025-11-24**|**Rethinking Intermediate Representation for VLM-based Robot Manipulation**|Chi-Wing Fu Team|[2511.19315](http://arxiv.org/abs/2511.19315)|null|
|**2025-11-24**|**Percept-WAM: Perception-Enhanced World-Awareness-Action Model for Robust End-to-End Autonomous Driving**|Hang Xu Team|[2511.19221](http://arxiv.org/abs/2511.19221)|null|
|**2025-11-24**|**AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention**|Xiaoyuan Yu Team|[2511.18960](http://arxiv.org/abs/2511.18960)|null|
|**2025-11-24**|**Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation**|Wenjing Qian Team|[2511.18950](http://arxiv.org/abs/2511.18950)|null|
|**2025-11-24**|**UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model**|Jianqiang Li Team|[2511.18845](http://arxiv.org/abs/2511.18845)|null|
|**2025-11-24**|**VideoPerceiver: Enhancing Fine-Grained Temporal Perception in Video Multimodal Large Language Models**|Danfeng Yan Team|[2511.18823](http://arxiv.org/abs/2511.18823)|null|
|**2025-11-24**|**MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent**|Yadan Luo Team|[2511.18810](http://arxiv.org/abs/2511.18810)|null|
|**2025-11-24**|**Understanding Task Transfer in Vision-Language Models**|Vineeth N. Balasubramanian Team|[2511.18787](http://arxiv.org/abs/2511.18787)|null|
|**2025-11-24**|**Any4D: Open-Prompt 4D Generation from Natural Language and Images**|Qiao Sun Team|[2511.18746](http://arxiv.org/abs/2511.18746)|null|
|**2025-11-24**|**Beyond Description: Cognitively Benchmarking Fine-Grained Action for Embodied Agents**|Yang Liu Team|[2511.18685](http://arxiv.org/abs/2511.18685)|null|
|**2025-11-23**|**TRANSPORTER: Transferring Visual Semantics from VLM Manifolds**|Alexandros Stergiou Team|[2511.18359](http://arxiv.org/abs/2511.18359)|**[link](https://alexandrosstergiou.github.io/TRANSPORTER)**|
|**2025-11-22**|**EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation**|Xiaodan Liang Team|[2511.18112](http://arxiv.org/abs/2511.18112)|null|
|**2025-11-22**|**Continually Evolving Skill Knowledge in Vision Language Action Model**|Hesheng Wang Team|[2511.18085](http://arxiv.org/abs/2511.18085)|null|
|**2025-11-22**|**ActDistill: General Action-Guided Self-Derived Distillation for Efficient Vision-Language-Action Models**|Guoli Yang Team|[2511.18082](http://arxiv.org/abs/2511.18082)|null|
|**2025-11-22**|**MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots**|Hao Tang Team|[2511.17889](http://arxiv.org/abs/2511.17889)|null|
|**2025-11-24**|**RynnVLA-002: A Unified Vision-Language-Action and World Model**|Hao Chen Team|[2511.17502](http://arxiv.org/abs/2511.17502)|null|
|**2025-11-21**|**METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model**|Shanghang Zhang Team|[2511.17366](http://arxiv.org/abs/2511.17366)|null|
|**2025-11-21**|**Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM**|Jonathan Le Roux Team|[2511.17335](http://arxiv.org/abs/2511.17335)|null|
|**2025-11-21**|**VLA-4D: Embedding 4D Awareness into Vision-Language-Action Models for SpatioTemporally Coherent Robotic Manipulation**|Gim Hee Lee Team|[2511.17199](http://arxiv.org/abs/2511.17199)|null|
|**2025-11-21**|**Progress-Think: Semantic Progress Reasoning for Vision-Language Navigation**|Zhaoxin Fan Team|[2511.17097](http://arxiv.org/abs/2511.17097)|null|
|**2025-11-20**|**InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy**|Jiangmiao Pang Team|[2511.16651](http://arxiv.org/abs/2511.16651)|null|
|**2025-11-21**|**VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference**|Bo Zhao Team|[2511.16449](http://arxiv.org/abs/2511.16449)|null|
|**2025-11-20**|**FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models**|Mingsheng Shang Team|[2511.16233](http://arxiv.org/abs/2511.16233)|null|
|**2025-11-20**|**When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models**|Yaochu Jin Team|[2511.16203](http://arxiv.org/abs/2511.16203)|null|
|**2025-11-20**|**Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight**|Zhijie Deng Team|[2511.16175](http://arxiv.org/abs/2511.16175)|null|
|**2025-11-20**|**EvoVLA: Self-Evolving Vision-Language-Action Model**|Hao Tang Team|[2511.16166](http://arxiv.org/abs/2511.16166)|null|
|**2025-11-19**|**SRPO: Self-Referential Policy Optimization for Vision-Language-Action Models**|Xipeng Qiu Team|[2511.15605](http://arxiv.org/abs/2511.15605)|null|
|**2025-11-19**|**Zero-Shot Open-Vocabulary Human Motion Grounding with Test-Time Training**|Jianfei Yang Team|[2511.15379](http://arxiv.org/abs/2511.15379)|null|
|**2025-11-19**|**Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception**|Wenzhao Lian Team|[2511.15279](http://arxiv.org/abs/2511.15279)|null|
|**2025-11-19**|**Generating Natural-Language Surgical Feedback: From Structured Representation to Domain-Grounded Evaluation**|Andrew J. Hung Team|[2511.15159](http://arxiv.org/abs/2511.15159)|null|
|**2025-11-19**|**$π^{*}_{0.6}$ : a VLA That Learns From Experience**|Zhiyuan Zhou Team|[2511.14759](http://arxiv.org/abs/2511.14759)|null|
|**2025-11-18**|**NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards**|Soujanya Poria Team|[2511.14659](http://arxiv.org/abs/2511.14659)|**[link](https://declare-lab.github.io/nora-1.5)**|
|**2025-11-18**|**Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM**|Siyuan Cheng Team|[2511.14499](http://arxiv.org/abs/2511.14499)|null|
|**2025-11-18**|**Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning**|Hongpeng Wang Team|[2511.14396](http://arxiv.org/abs/2511.14396)|**[link](https://qhemu.github.io/CCoL/)**|
|**2025-11-18**|**Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion**|Fei Chen Team|[2511.14178](http://arxiv.org/abs/2511.14178)|null|
|**2025-11-19**|**RoboTidy : A 3D Gaussian Splatting Household Tidying Benchmark for Embodied Navigation and Action**|Jiayu Chen Team|[2511.14161](http://arxiv.org/abs/2511.14161)|null|
|**2025-11-18**|**MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs**|Lu Cheng Team|[2511.14159](http://arxiv.org/abs/2511.14159)|null|
|**2025-11-18**|**AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models**|Biqing Qi Team|[2511.14148](http://arxiv.org/abs/2511.14148)|null|
|**2025-11-18**|**Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models**|Jidong J. Yang Team|[2511.14120](http://arxiv.org/abs/2511.14120)|null|
|**2025-11-19**|**Searching in Space and Time: Unified Memory-Action Loops for Open-World Object Retrieval**|Roberto Martín-Martín Team|[2511.14004](http://arxiv.org/abs/2511.14004)|null|
|**2025-11-17**|**TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models**|Ying-Cong Chen Team|[2511.13704](http://arxiv.org/abs/2511.13704)|**[link](https://haroldchen19.github.io/TiViBench-Page/)**|
|**2025-11-17**|**Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models**|Yuxiang Sun Team|[2511.12937](http://arxiv.org/abs/2511.12937)|null|
|**2025-11-18**|**Uni-Hand: Universal Hand Motion Forecasting in Egocentric Views**|Hesheng Wang Team|[2511.12878](http://arxiv.org/abs/2511.12878)|null|
|**2025-11-16**|**BridgeEQA: Virtual Embodied Agents for Real Bridge Inspections**|Vedhus Hoskere Team|[2511.12676](http://arxiv.org/abs/2511.12676)|null|
|**2025-11-16**|**RoboAfford++: A Generative AI-Enhanced Dataset for Multimodal Affordance Learning in Robotic Manipulation and Navigation**|Long Chen Team|[2511.12436](http://arxiv.org/abs/2511.12436)|null|
|**2025-11-16**|**VLA-R: Vision-Language Action Retrieval toward Open-World End-to-End Autonomous Driving**|David Hyunchul Shim Team|[2511.12405](http://arxiv.org/abs/2511.12405)|null|
|**2025-11-15**|**AttackVLA: Benchmarking Adversarial and Backdoor Attacks on Vision-Language-Action Models**|Yu-Gang Jiang Team|[2511.12149](http://arxiv.org/abs/2511.12149)|null|
|**2025-11-15**|**Decoupled Action Head: Confining Task Knowledge to Conditioning Layers**|Qi WU Team|[2511.12101](http://arxiv.org/abs/2511.12101)|null|
|**2025-11-18**|**Rethinking Progression of Memory State in Robotic Manipulation: An Object-Centric Perspective**|Ngan Le Team|[2511.11478](http://arxiv.org/abs/2511.11478)|null|
|**2025-11-14**|**EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment**|Hongyi Zhang Team|[2511.11301](http://arxiv.org/abs/2511.11301)|null|
|**2025-11-14**|**Experiences from Benchmarking Vision-Language-Action Models for Robotic Manipulation**|Xi Zheng Team|[2511.11298](http://arxiv.org/abs/2511.11298)|null|
|**2025-11-14**|**AdaptPNP: Integrating Prehensile and Non-Prehensile Skills for Adaptive Robotic Manipulation**|Lin Shao Team|[2511.11052](http://arxiv.org/abs/2511.11052)|null|
|**2025-11-14**|**DEFT-LLM: Disentangled Expert Feature Tuning for Micro-Expression Recognition**|Jianqin Yin Team|[2511.10948](http://arxiv.org/abs/2511.10948)|null|
|**2025-11-13**|**Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals**|Pawan Goyal Team|[2511.10615](http://arxiv.org/abs/2511.10615)|null|
|**2025-11-14**|**OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer**|Ziwei Liu Team|[2511.10560](http://arxiv.org/abs/2511.10560)|**[link](https://livioni.github.io/OmniVGGT-official/)**|
|**2025-11-13**|**SemanticVLA: Semantic-Aligned Sparsification and Enhancement for Efficient Robotic Manipulation**|Liqiang Nie Team|[2511.10518](http://arxiv.org/abs/2511.10518)|**[link](https://github.com/JiuTian-VL/SemanticVLA)**|
|**2025-11-13**|**Facial-R1: Aligning Reasoning and Recognition for Facial Emotion Analysis**|Min Cao Team|[2511.10254](http://arxiv.org/abs/2511.10254)|null|
|**2025-11-13**|**SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition**|Zitong Yu Team|[2511.10091](http://arxiv.org/abs/2511.10091)|null|
|**2025-11-13**|**Phantom Menace: Exploring and Enhancing the Robustness of VLA Models against Physical Sensor Attacks**|Wenyuan Xu Team|[2511.10008](http://arxiv.org/abs/2511.10008)|null|
|**2025-11-13**|**Audio-VLA: Adding Contact Audio Perception to Vision-Language-Action Model for Robotic Manipulation**|Changbo Wang Team|[2511.09958](http://arxiv.org/abs/2511.09958)|null|
|**2025-11-12**|**MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation**|Ziwei Wang Team|[2511.09516](http://arxiv.org/abs/2511.09516)|null|
|**2025-11-12**|**WMPO: World Model-based Policy Optimization for Vision-Language-Action Models**|Song Guo Team|[2511.09515](http://arxiv.org/abs/2511.09515)|**[link](https://wm-po.github.io)**|
|**2025-11-12**|**Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning**|Fatemeh Afghah Team|[2511.08942](http://arxiv.org/abs/2511.08942)|null|
|**2025-11-12**|**Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds**|Guang Shi Team|[2511.08892](http://arxiv.org/abs/2511.08892)|null|
|**2025-11-12**|**MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror**|Tao Shen Team|[2511.08865](http://arxiv.org/abs/2511.08865)|null|
|**2025-11-11**|**SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control**|Yuke Zhu Team|[2511.07820](http://arxiv.org/abs/2511.07820)|**[link](https://nvlabs.github.io/SONIC/)**|
|**2025-11-11**|**ViPRA: Video Prediction for Robot Actions**|Deepak Pathak Team|[2511.07732](http://arxiv.org/abs/2511.07732)|**[link](https://vipra-project.github.io)**|
|**2025-11-11**|**LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models**|Shiqi Zhang Team|[2511.07727](http://arxiv.org/abs/2511.07727)|null|
|**2025-11-10**|**How Do VLAs Effectively Inherit from VLMs?**|Jiang Bian Team|[2511.06619](http://arxiv.org/abs/2511.06619)|null|
|**2025-11-09**|**ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval**|Jeff Ichnowski Team|[2511.06202](http://arxiv.org/abs/2511.06202)|null|
|**2025-11-09**|**OpenVLN: Open-world aerial Vision-Language Navigation**|Yang Cong Team|[2511.06182](http://arxiv.org/abs/2511.06182)|null|
|**2025-11-08**|**10 Open Challenges Steering the Future of Vision-Language-Action Models**|David Hsu Team|[2511.05936](http://arxiv.org/abs/2511.05936)|null|
|**2025-11-11**|**Causal Tracing of Object Representations in Large Vision Language Models: Mechanistic Interpretability and Hallucination Mitigation**|Xiachong Feng Team|[2511.05923](http://arxiv.org/abs/2511.05923)|null|
|**2025-11-07**|**Lite VLA: Efficient Vision-Language-Action Control on CPU-Bound Edge Robots**|Mrinmoy Sarkar Team|[2511.05642](http://arxiv.org/abs/2511.05642)|null|
|**2025-11-07**|**Visual Spatial Tuning**|Hengshuang Zhao Team|[2511.05491](http://arxiv.org/abs/2511.05491)|null|
|**2025-11-07**|**EveryDayVLA: A Vision-Language-Action Model for Affordable Robotic Manipulation**|Samuel Dickerson Team|[2511.05397](http://arxiv.org/abs/2511.05397)|null|
|**2025-11-07**|**TwinVLA: Data-Efficient Bimanual Manipulation with Twin Single-Arm Vision-Language-Action Models**|Youngwoon Lee Team|[2511.05275](http://arxiv.org/abs/2511.05275)|**[link](https://jellyho.github.io/TwinVLA/)**|
|**2025-11-06**|**Evo-1: Lightweight Vision-Language-Action Model with Preserved Semantic Alignment**|Bo Zhao Team|[2511.04555](http://arxiv.org/abs/2511.04555)|**[link](https://github.com/MINT-SJTU/Evo-1)**|
|**2025-11-06**|**GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies**|Cédric Buche Team|[2511.04357](http://arxiv.org/abs/2511.04357)|null|
|**2025-11-04**|**TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System**|C. Karen Liu Team|[2511.02832](http://arxiv.org/abs/2511.02832)|**[link](https://yanjieze.com/TWIST2)**|
|**2025-11-04**|**XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations**|Jian Tang Team|[2511.02776](http://arxiv.org/abs/2511.02776)|null|
|**2025-11-01**|**iFlyBot-VLA Technical Report**|Jia Pan Team|[2511.01914](http://arxiv.org/abs/2511.01914)|null|
|**2025-11-03**|**Unified Diffusion VLA: Vision-Language-Action Model via Joint Discrete Denoising Diffusion Process**|Haoang Li Team|[2511.01718](http://arxiv.org/abs/2511.01718)|null|
|**2025-11-03**|**PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model**|Yang Cong Team|[2511.01571](http://arxiv.org/abs/2511.01571)|null|
|**2025-11-03**|**RobustVLA: Robustness-Aware Reinforcement Post-Training for Vision-Language-Action Models**|Donglin Wang Team|[2511.01331](http://arxiv.org/abs/2511.01331)|null|
|**2025-11-03**|**Embodiment Transfer Learning for Vision-Language-Action Models**|Yaxin Peng Team|[2511.01224](http://arxiv.org/abs/2511.01224)|null|
|**2025-11-06**|**OmniVLA: Physically-Grounded Multimodal VLA with Unified Multi-Sensor Perception for Robotic Manipulation**|Lili Qiu Team|[2511.01210](http://arxiv.org/abs/2511.01210)|null|
|**2025-10-31**|**End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection**|Zhibin Li Team|[2511.00139](http://arxiv.org/abs/2511.00139)|null|
|**2025-10-30**|**Self-Improving Vision-Language-Action Models with Data Generation via Residual RL**|Yuke Zhu Team|[2511.00091](http://arxiv.org/abs/2511.00091)|null|
|**2025-10-30**|**Alpamayo-R1: Bridging Reasoning and Action Prediction for Generalizable Autonomous Driving in the Long Tail**|Marco Pavone Team|[2511.00088](http://arxiv.org/abs/2511.00088)|null|
|**2025-11-04**|**Dual-Stream Diffusion for World-Model Augmented Vision-Language-Action Model**|Jinwoo Shin Team|[2510.27607](http://arxiv.org/abs/2510.27607)|null|
|**2025-10-31**|**EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities**|Luhui Hu Team|[2510.27545](http://arxiv.org/abs/2510.27545)|null|
|**2025-10-30**|**RoboOS-NeXT: A Unified Memory-based Framework for Lifelong, Scalable, and Robust Multi-Robot Collaboration**|Shanghang Zhang Team|[2510.26536](http://arxiv.org/abs/2510.26536)|null|
|**2025-10-30**|**Human-in-the-loop Online Rejection Sampling for Robotic Manipulation**|Yansong Tang Team|[2510.26406](http://arxiv.org/abs/2510.26406)|null|
|**2025-10-29**|**$π_\texttt{RL}$ : Online RL Fine-tuning for Flow-based Vision-Language-Action Models**|Chao Yu Team|[2510.25889](http://arxiv.org/abs/2510.25889)|null|
|**2025-10-29**|**Robotic Assistant: Completing Collaborative Tasks with Dexterous Vision-Language-Action Models**|Robert Katzschmann Team|[2510.25713](http://arxiv.org/abs/2510.25713)|null|
|**2025-10-29**|**Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization**|Aleksandr I. Panov Team|[2510.25616](http://arxiv.org/abs/2510.25616)|null|
|**2025-10-29**|**NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized Generalist Robotic Policies**|Jinghui Lu Team|[2510.25122](http://arxiv.org/abs/2510.25122)|null|
|**2025-10-27**|**A Survey on Efficient Vision-Language-Action Models**|Heng Tao Shen Team|[2510.24795](http://arxiv.org/abs/2510.24795)|null|
|**2025-10-28**|**BLM $_1$ : A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning**|Heng Tao Shen Team|[2510.24161](http://arxiv.org/abs/2510.24161)|null|
|**2025-11-01**|**RoboOmni: Proactive Robot Manipulation in Omni-modal Context**|Xipeng Qiu Team|[2510.23763](http://arxiv.org/abs/2510.23763)|null|
|**2025-10-27**|**UrbanVLA: A Vision-Language-Action Model for Urban Micromobility**|He Wang Team|[2510.23576](http://arxiv.org/abs/2510.23576)|null|
|**2025-10-27**|**Dexbotic: Open-Source Vision-Language-Action Toolbox**|Ziyu Zhang Team|[2510.23511](http://arxiv.org/abs/2510.23511)|**[link](https://dexbotic.com/.)**|
|**2025-10-28**|**Evaluation of Vision-LLMs in Surveillance Video**|Jelte P. Mense Team|[2510.23190](http://arxiv.org/abs/2510.23190)|null|
|**2025-10-25**|**ACG: Action Coherence Guidance for Flow-based VLA models**|Jaegul Choo Team|[2510.22201](http://arxiv.org/abs/2510.22201)|null|
|**2025-10-23**|**Butter-Bench: Evaluating LLM Controlled Robots for Practical Intelligence**|Elias Aronsson Team|[2510.21860](http://arxiv.org/abs/2510.21860)|null|
|**2025-10-21**|**VITA-E: Natural Embodied Interaction with Concurrent Seeing, Hearing, Speaking, and Acting**|Ran He Team|[2510.21817](http://arxiv.org/abs/2510.21817)|**[link](https://lxysl.github.io/VITA-E/)**|
|**2025-10-24**|**Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos**|Baining Guo Team|[2510.21571](http://arxiv.org/abs/2510.21571)|**[link](https://microsoft.github.io/VITRA/)**|
|**2025-10-23**|**SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing**|Axel Krieger Team|[2510.20965](http://arxiv.org/abs/2510.20965)|null|
|**2025-10-23**|**VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation**|Abhishek Gupta Team|[2510.20818](http://arxiv.org/abs/2510.20818)|null|
|**2025-10-23**|**MemER: Scaling Up Memory for Robot Control via Experience Retrieval**|Chelsea Finn Team|[2510.20328](http://arxiv.org/abs/2510.20328)|**[link](https://jen-pan.github.io/memer/)**|
|**2025-10-22**|**Learning Affordances at Inference-Time for Vision-Language-Action Models**|Sergey Levine Team|[2510.19752](http://arxiv.org/abs/2510.19752)|null|
|**2025-10-22**|**GigaBrain-0: A World Model-Powered Vision-Language-Action Model**|Zheng Zhu Team|[2510.19430](http://arxiv.org/abs/2510.19430)|**[link](https://gigabrain0.github.io/)**|
|**2025-10-22**|**Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes**|Baining Guo Team|[2510.19400](http://arxiv.org/abs/2510.19400)|**[link](https://github.com/microsoft/MV-RoboBench)**|
|**2025-10-23**|**MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning**|Heng Yang Team|[2510.18337](http://arxiv.org/abs/2510.18337)|null|
|**2025-10-24**|**RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation**|Ziwei Wang Team|[2510.17640](http://arxiv.org/abs/2510.17640)|null|
|**2025-10-20**|**From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors**|Pan Zhou Team|[2510.17439](http://arxiv.org/abs/2510.17439)|**[link](https://falcon-vla.github.io/)**|
|**2025-10-20**|**Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots**|Josie Hughes Team|[2510.17369](http://arxiv.org/abs/2510.17369)|null|
|**2025-10-21**|**DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment**|Wang Jijun Team|[2510.17148](http://arxiv.org/abs/2510.17148)|null|
|**2025-10-23**|**Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey**|Jian Cheng Team|[2510.17111](http://arxiv.org/abs/2510.17111)|null|
|**2025-10-18**|**MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation**|Ufuk Topcu Team|[2510.16617](http://arxiv.org/abs/2510.16617)|null|
|**2025-10-18**|**Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification**|Claudia P'erez-D'Arpino Team|[2510.16281](http://arxiv.org/abs/2510.16281)|null|
|**2025-10-21**|**NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?**|Yu Yin Team|[2510.16263](http://arxiv.org/abs/2510.16263)|**[link](https://vulab-ai.github.io/NEBULA-Alpha/)**|
|**2025-10-17**|**Cosmos-Surg-dVRK: World Foundation Model-based Automated Online Evaluation of Surgical Robot Policy Learning**|Sean Huver Team|[2510.16240](http://arxiv.org/abs/2510.16240)|null|
|**2025-10-17**|**VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving**|Zufeng Zhang Team|[2510.15446](http://arxiv.org/abs/2510.15446)|null|
|**2025-10-16**|**RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks**|Jiachen Li Team|[2510.14968](http://arxiv.org/abs/2510.14968)|null|
|**2025-10-17**|**From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance**|Chang Xu Team|[2510.14952](http://arxiv.org/abs/2510.14952)|null|
|**2025-10-16**|**VLA^2: Empowering Vision-Language-Action Models with an Agentic Framework for Unseen Concept Manipulation**|Donglin Wang Team|[2510.14902](http://arxiv.org/abs/2510.14902)|null|
|**2025-10-16**|**QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models**|Haoran Li Team|[2510.14836](http://arxiv.org/abs/2510.14836)|null|
|**2025-10-16**|**Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning**|Yao Mu Team|[2510.14300](http://arxiv.org/abs/2510.14300)|null|
|**2025-10-15**|**InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy**|Yangkun Zhu Team|[2510.13778](http://arxiv.org/abs/2510.13778)|null|
|**2025-10-15**|**LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models**|Xipeng Qiu Team|[2510.13626](http://arxiv.org/abs/2510.13626)|null|
|**2025-10-15**|**DepthVLA: Enhancing Vision-Language-Action Models with Depth-Aware Spatial Reasoning**|Hang Zhao Team|[2510.13375](http://arxiv.org/abs/2510.13375)|null|
|**2025-10-15**|**Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models**|Jingfeng Zhang Team|[2510.13237](http://arxiv.org/abs/2510.13237)|null|
|**2025-10-15**|**VLA-0: Building State-of-the-Art VLAs with Zero Modification**|Fabio Ramos Team|[2510.13054](http://arxiv.org/abs/2510.13054)|null|
|**2025-10-14**|**DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving**|Zhaoxiang Zhang Team|[2510.12796](http://arxiv.org/abs/2510.12796)|null|
|**2025-10-14**|**Reflection-Based Task Adaptation for Self-Improving VLA**|Hongbin Zha Team|[2510.12710](http://arxiv.org/abs/2510.12710)|null|
|**2025-10-17**|**Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model**|Haoang Li Team|[2510.12276](http://arxiv.org/abs/2510.12276)|null|
|**2025-10-14**|**ManiAgent: An Agentic Framework for General Robotic Manipulation**|Xudong Liu Team|[2510.11660](http://arxiv.org/abs/2510.11660)|null|
|**2025-10-13**|**Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning**|Zhi Hou Team|[2510.11027](http://arxiv.org/abs/2510.11027)|null|
|**2025-10-14**|**RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model**|Xinyu Wu Team|[2510.10975](http://arxiv.org/abs/2510.10975)|null|
|**2025-10-13**|**TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models**|Yu-Gang Jiang Team|[2510.10932](http://arxiv.org/abs/2510.10932)|null|
|**2025-10-11**|**X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model**|Xianyuan Zhan Team|[2510.10274](http://arxiv.org/abs/2510.10274)|null|
|**2025-10-11**|**Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback**|Hongtao Lu Team|[2510.10181](http://arxiv.org/abs/2510.10181)|null|
|**2025-10-11**|**Reinforcement Fine-Tuning of Flow-Matching Policies for Vision-Language-Action Models**|Yi Zeng Team|[2510.09976](http://arxiv.org/abs/2510.09976)|null|
|**2025-10-08**|**OmniSAT: Compact Action Token, Faster Auto Regression**|Changsheng Xu Team|[2510.09667](http://arxiv.org/abs/2510.09667)|null|
|**2025-10-10**|**VITA-VLA: Efficiently Teaching Vision-Language Models to Act via Action Expert Distillation**|Caifeng Shan Team|[2510.09607](http://arxiv.org/abs/2510.09607)|**[link](https://ltbai.github.io/VITA-VLA/)**|
|**2025-10-10**|**PhysToolBench: Benchmarking Physical Tool Understanding for MLLMs**|Ying-Cong Chen Team|[2510.09507](http://arxiv.org/abs/2510.09507)|null|
|**2025-10-10**|**Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects**|Jingfeng Zhang Team|[2510.09269](http://arxiv.org/abs/2510.09269)|null|
|**2025-10-09**|**Don't Run with Scissors: Pruning Breaks VLA Models but They Can Be Recovered**|Shayegan Omidshafiei Team|[2510.08464](http://arxiv.org/abs/2510.08464)|null|
|**2025-10-15**|**USIM and U0: A Vision-Language-Action Dataset and Model for General Underwater Robots**|Zhengxing Wu Team|[2510.07869](http://arxiv.org/abs/2510.07869)|**[link](https://vincentgu2000.github.io/u0project/)**|
|**2025-10-09**|**IntentionVLA: Generalizable and Efficient Embodied Intention Reasoning for Human-Robot Interaction**|Liqiang Nie Team|[2510.07778](http://arxiv.org/abs/2510.07778)|null|
|**2025-10-09**|**DEAS: DEtached value learning with Action Sequence for Scalable Offline RL**|Yuke Zhu Team|[2510.07730](http://arxiv.org/abs/2510.07730)|**[link](https://changyeon.site/deas)**|
|**2025-10-08**|**TrackVLA++: Unleashing Reasoning and Memory Capabilities in VLA Models for Embodied Visual Tracking**|He Wang Team|[2510.07134](http://arxiv.org/abs/2510.07134)|**[link](https://pku-epic.github.io/TrackVLA-plus-plus-Web/)**|
|**2025-10-08**|**Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications**|Yuke Zhu Team|[2510.07077](http://arxiv.org/abs/2510.07077)|**[link](https://vla-survey.github.io)**|
|**2025-10-08**|**Bring the Apple, Not the Sofa: Impact of Irrelevant Context in Embodied AI Commands on VLA Models**|Elena Tutubalina Team|[2510.07067](http://arxiv.org/abs/2510.07067)|null|
|**2025-10-08**|**RLinf-VLA: A Unified and Efficient Framework for VLA+RL Training**|Yu Wang Team|[2510.06710](http://arxiv.org/abs/2510.06710)|**[link](https://github.com/RLinf/RLinf)**|
|**2025-10-07**|**EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model**|Zhaoxiang Zhang Team|[2510.06207](http://arxiv.org/abs/2510.06207)|**[link](https://anonymous.4open.science/w/Embodied-Coder/)**|
|**2025-10-07**|**Verifier-free Test-Time Sampling for Vision Language Action Models**|Jinwoo Shin Team|[2510.05681](http://arxiv.org/abs/2510.05681)|null|
|**2025-10-07**|**MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption**|Marios Savvides Team|[2510.05580](http://arxiv.org/abs/2510.05580)|null|
|**2025-10-06**|**HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks**|Shimon Whiteson Team|[2510.04898](http://arxiv.org/abs/2510.04898)|null|
|**2025-10-05**|**ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context**|Jinwoo Shin Team|[2510.04246](http://arxiv.org/abs/2510.04246)|**[link](https://huiwon-jang.github.io/contextvla)**|
|**2025-10-05**|**SITCOM: Scaling Inference-Time COMpute for VLAs**|Esha Pahwa Team|[2510.04041](http://arxiv.org/abs/2510.04041)|null|
|**2025-10-04**|**Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert**|Chunhua Shen Team|[2510.03896](http://arxiv.org/abs/2510.03896)|null|
|**2025-10-04**|**NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation**|Chunhua Shen Team|[2510.03895](http://arxiv.org/abs/2510.03895)|null|
|**2025-10-04**|**LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization**|Lichao Sun Team|[2510.03827](http://arxiv.org/abs/2510.03827)|null|
|**2025-10-02**|**Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer**|Yuxiang Zhou Team|[2510.03342](http://arxiv.org/abs/2510.03342)|null|
|**2025-10-03**|**MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning**|He Wang Team|[2510.03142](http://arxiv.org/abs/2510.03142)|**[link](https://pku-epic.github.io/MM-Nav-Web/)**|
|**2025-10-02**|**Contrastive Representation Regularization for Vision-Language-Action Models**|Jinwoo Shin Team|[2510.01711](http://arxiv.org/abs/2510.01711)|null|
|**2025-10-02**|**FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models**|Bihan Wen Team|[2510.01642](http://arxiv.org/abs/2510.01642)|**[link](https://jimntu.github.io/FailSafe/)**|
|**2025-10-02**|**VLA-R1: Enhancing Reasoning in Vision-Language-Action Models**|Zheng Zhu Team|[2510.01623](http://arxiv.org/abs/2510.01623)|null|
|**2025-10-01**|**INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models**|Tesca FItzgerald Team|[2510.01389](http://arxiv.org/abs/2510.01389)|null|
|**2025-10-01**|**Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition**|Andrew F. Luo Team|[2510.01068](http://arxiv.org/abs/2510.01068)|**[link](https://sagecao1125.github.io/GPC-Site/)**|
|**2025-10-02**|**HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy**|Jinwoo Shin Team|[2510.00695](http://arxiv.org/abs/2510.00695)|**[link](https://myungkyukoo.github.io/hamlet/)**|
|**2025-10-01**|**Hybrid Training for Vision-Language-Action Models**|Daniel Dijkman Team|[2510.00600](http://arxiv.org/abs/2510.00600)|null|
|**2025-10-01**|**VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators**|Weihua Su Team|[2510.00406](http://arxiv.org/abs/2510.00406)|null|
|**2025-09-30**|**MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation**|Shanghang Zhang Team|[2509.26642](http://arxiv.org/abs/2509.26642)|null|
|**2025-09-30**|**Seeing Space and Motion: Enhancing Latent Actions with Spatial and Dynamic Awareness for VLA**|Ruqi Huang Team|[2509.26251](http://arxiv.org/abs/2509.26251)|null|
|**2025-09-30**|**MUVLA: Learning to Explore Object Navigation via Map Understanding**|Jianye Hao Team|[2509.25966](http://arxiv.org/abs/2509.25966)|null|
|**2025-09-30**|**TacRefineNet: Tactile-Only Grasp Refinement Between Arbitrary In-Hand Object Poses**|Yangwei You Team|[2509.25746](http://arxiv.org/abs/2509.25746)|null|
|**2025-09-30**|**VLA Model Post-Training via Action-Chunked PPO and Self Behavior Cloning**|Zeng-Guang Hou Team|[2509.25718](http://arxiv.org/abs/2509.25718)|null|
|**2025-09-30**|**dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought**|Yi Xu Team|[2509.25681](http://arxiv.org/abs/2509.25681)|null|
|**2025-09-29**|**AIRoA MoMa Dataset: A Large-Scale Hierarchical Dataset for Mobile Manipulation**|Tetsuya Ogata Team|[2509.25032](http://arxiv.org/abs/2509.25032)|null|
|**2025-09-29**|**World-Env: Leveraging World Model as a Virtual Environment for VLA Post-Training**|Qing Zhang Team|[2509.24948](http://arxiv.org/abs/2509.24948)|null|
|**2025-09-29**|**IA-VLA: Input Augmentation for Vision-Language-Action models in settings with semantically complex tasks**|Ville Kyrki Team|[2509.24768](http://arxiv.org/abs/2509.24768)|null|
|**2025-09-29**|**Emergent World Representations in OpenVLA**|Omar G. Younis Team|[2509.24559](http://arxiv.org/abs/2509.24559)|null|
|**2025-09-29**|**PhysiAgent: An Embodied Agent Framework in Physical World**|Xianyuan Zhan Team|[2509.24524](http://arxiv.org/abs/2509.24524)|null|
|**2025-09-28**|**AutoPrune: Each Complexity Deserves a Pruning Policy**|Zhipeng Zhang Team|[2509.23931](http://arxiv.org/abs/2509.23931)|null|
|**2025-09-28**|**Control Your Robot: A Unified System for Robot Control and Policy Deployment**|Bingshan Hu Team|[2509.23823](http://arxiv.org/abs/2509.23823)|**[link](https://github.com/Tian-Nian/control_your_robot)**|
|**2025-09-28**|**Focusing on What Matters: Object-Agent-centric Tokenization for Vision Language Action models**|Pietro Mazzaglia Team|[2509.23655](http://arxiv.org/abs/2509.23655)|null|
|**2025-09-27**|**Leave No Observation Behind: Real-time Correction for VLA Action Chunks**|Yusuke Iwasawa Team|[2509.23224](http://arxiv.org/abs/2509.23224)|null|
|**2025-09-27**|**Transferring Vision-Language-Action Models to Industry Applications: Architectures, Performance, and Challenges**|Zhibo Pang Team|[2509.23121](http://arxiv.org/abs/2509.23121)|null|
|**2025-09-26**|**VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search**|Ziwei Wang Team|[2509.22643](http://arxiv.org/abs/2509.22643)|null|
|**2025-09-26**|**UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation**|Dixia Fan Team|[2509.22441](http://arxiv.org/abs/2509.22441)|null|
|**2025-09-26**|**EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer**|Guan Huang Team|[2509.22407](http://arxiv.org/abs/2509.22407)|null|
|**2025-09-29**|**MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training**|Xingang Wang Team|[2509.22199](http://arxiv.org/abs/2509.22199)|null|
|**2025-09-26**|**Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting**|Anirudha Majumdar Team|[2509.22195](http://arxiv.org/abs/2509.22195)|null|
|**2025-09-26**|**Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation**|Chang Xu Team|[2509.22093](http://arxiv.org/abs/2509.22093)|null|
|**2025-09-26**|**Developing Vision-Language-Action Model from Egocentric Videos**|Shinsuke Mori Team|[2509.21986](http://arxiv.org/abs/2509.21986)|null|
|**2025-09-20**|**KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache**|Long Zhuang Team|[2509.21354](http://arxiv.org/abs/2509.21354)|null|
|**2025-09-25**|**RetoVLA: Reusing Register Tokens for Spatial Reasoning in Vision-Language-Action Models**|Andrew Jaeyong Choi Team|[2509.21243](http://arxiv.org/abs/2509.21243)|null|
|**2025-09-24**|**Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving**|Xianpeng Lang Team|[2509.20109](http://arxiv.org/abs/2509.20109)|null|
|**2025-09-24**|**FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models**|Yu-Gang Jiang Team|[2509.19870](http://arxiv.org/abs/2509.19870)|null|
|**2025-09-24**|**Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training**|Yi Chen Team|[2509.19752](http://arxiv.org/abs/2509.19752)|null|
|**2025-09-23**|**Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action**|Liam Paull Team|[2509.19571](http://arxiv.org/abs/2509.19571)|**[link](https://montrealrobotics.ca/agentic-scene-policies.github.io/)**|
|**2025-09-23**|**OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation**|Sergey Levine Team|[2509.19480](http://arxiv.org/abs/2509.19480)|null|
|**2025-09-25**|**Pure Vision Language Action (VLA) Models: A Comprehensive Survey**|Qingguo Zhou Team|[2509.19012](http://arxiv.org/abs/2509.19012)|null|
|**2025-09-23**|**Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations**|Wen Yao Team|[2509.18953](http://arxiv.org/abs/2509.18953)|null|
|**2025-09-22**|**Latent Action Pretraining Through World Modeling**|Ian Reid Team|[2509.18428](http://arxiv.org/abs/2509.18428)|null|
|**2025-09-18**|**VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation**|Anzhou Hou Team|[2509.18183](http://arxiv.org/abs/2509.18183)|null|
|**2025-09-19**|**CoReVLA: A Dual-Stage End-to-End Autonomous Driving Framework for Long-Tail Scenarios via Collect-and-Refine**|Jian Sun Team|[2509.15968](http://arxiv.org/abs/2509.15968)|null|
|**2025-09-19**|**A Vision-Language-Action-Critic Model for Robotic Real-World Reinforcement Learning**|Jiangmiao Pang Team|[2509.15937](http://arxiv.org/abs/2509.15937)|null|
|**2025-09-18**|**RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation**|Xin Li Team|[2509.15212](http://arxiv.org/abs/2509.15212)|**[link](https://github.com/alibaba-damo-academy/RynnVLA-001)**|
|**2025-09-18**|**Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale**|Florian Walter Team|[2509.14932](http://arxiv.org/abs/2509.14932)|null|
|**2025-09-18**|**CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human**|Huaping Liu Team|[2509.14889](http://arxiv.org/abs/2509.14889)|null|
|**2025-09-18**|**RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI**|Tao Shen Team|[2509.14687](http://arxiv.org/abs/2509.14687)|null|
|**2025-09-18**|**Toward Embodiment Equivariant Vision-Language-Action Policy**|Yue Wang Team|[2509.14630](http://arxiv.org/abs/2509.14630)|null|
|**2025-09-17**|**CLAW: A Vision-Language-Action Framework for Weight-Aware Robotic Grasping**|Lifeng Zhou Team|[2509.14143](http://arxiv.org/abs/2509.14143)|null|
|**2025-09-17**|**SeqVLA: Sequential Task Execution for Long-Horizon Manipulation with Completion-Aware Vision-Language-Action Model**|Yiming Feng Team|[2509.14138](http://arxiv.org/abs/2509.14138)|null|
|**2025-09-22**|**GeoAware-VLA: Implicit Geometry Aware Vision-Language-Action Model**|Dezhen Song Team|[2509.14117](http://arxiv.org/abs/2509.14117)|null|
|**2025-09-17**|**Dual-Actor Fine-Tuning of VLA Models: A Talk-and-Tweak Human-in-the-Loop Approach**|Yangwei You Team|[2509.13774](http://arxiv.org/abs/2509.13774)|null|
|**2025-09-17**|**AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving**|Zhi-xin Yang Team|[2509.13769](http://arxiv.org/abs/2509.13769)|null|
|**2025-09-13**|**OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft**|Yitao Liang Team|[2509.13347](http://arxiv.org/abs/2509.13347)|null|
|**2025-09-21**|**The Better You Learn, The Smarter You Prune: Towards Efficient Vision-language-action Models via Differentiable Token Pruning**|Xianpeng Lang Team|[2509.12594](http://arxiv.org/abs/2509.12594)|**[link](https://liauto-research.github.io/LightVLA)**|
|**2025-09-17**|**TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning**|Donglin Wang Team|[2509.11839](http://arxiv.org/abs/2509.11839)|null|
|**2025-09-15**|**Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs**|Yanzhi Wang Team|[2509.11480](http://arxiv.org/abs/2509.11480)|null|
|**2025-09-17**|**Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations**|Xuanlin Li Team|[2509.11417](http://arxiv.org/abs/2509.11417)|**[link](https://gen-vla.github.io/)**|
|**2025-09-11**|**SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning**|Ning Ding Team|[2509.09674](http://arxiv.org/abs/2509.09674)|null|
|**2025-09-22**|**VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model**|Donglin Wang Team|[2509.09372](http://arxiv.org/abs/2509.09372)|**[link](https://vla-adapter.github.io/)**|
|**2025-09-11**|**SQAP-VLA: A Synergistic Quantization-Aware Pruning Framework for High-Performance Vision-Language-Action Models**|Huanrui Yang Team|[2509.09090](http://arxiv.org/abs/2509.09090)|null|
|**2025-09-10**|**RoboChemist: Long-Horizon and Safety-Compliant Robotic Chemical Experimentation**|Hao Zhao Team|[2509.08820](http://arxiv.org/abs/2509.08820)|**[link](https://zzongzheng0918.github.io/RoboChemist.github.io/)**|
|**2025-09-09**|**TA-VLA: Elucidating the Design Space of Torque-aware Vision-Language-Action Models**|Hao Zhao Team|[2509.07962](http://arxiv.org/abs/2509.07962)|**[link](https://zzongzheng0918.github.io/Torque-Aware-VLA.github.io/})**|
|**2025-09-09**|**Graph-Fused Vision-Language-Action for Policy Reasoning in Multi-Arm Robotic Manipulation**|Yingbai Hu Team|[2509.07957](http://arxiv.org/abs/2509.07957)|null|
|**2025-09-09**|**F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions**|Jiangmiao Pang Team|[2509.06951](http://arxiv.org/abs/2509.06951)|**[link](https://aopolin-lv.github.io/F1-VLA/)**|
|**2025-09-11**|**LLaDA-VLA: Vision Language Diffusion Action Models**|Xiaoyan Sun Team|[2509.06932](http://arxiv.org/abs/2509.06932)|null|
|**2025-09-08**|**CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation**|Angela P. Schöllig Team|[2509.06819](http://arxiv.org/abs/2509.06819)|null|
|**2025-09-09**|**Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization**|Surasakdi Siripong Team|[2509.05695](http://arxiv.org/abs/2509.05695)|null|
|**2025-09-06**|**SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning**|Guohao Dai Team|[2509.05614](http://arxiv.org/abs/2509.05614)|null|
|**2025-09-06**|**OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision**|Hang Zhao Team|[2509.05578](http://arxiv.org/abs/2509.05578)|null|
|**2025-09-05**|**OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation**|Yu Xiang Team|[2509.05513](http://arxiv.org/abs/2509.05513)|null|
|**2025-09-05**|**FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies**|Rudolf Lioutikov Team|[2509.04996](http://arxiv.org/abs/2509.04996)|null|
|**2025-09-04**|**Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models**|Donglin Wang Team|[2509.04063](http://arxiv.org/abs/2509.04063)|null|
|**2025-09-04**|**FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction**|Jingtai Liu Team|[2509.04018](http://arxiv.org/abs/2509.04018)|null|
|**2025-09-03**|**ANNIE: Be Careful of Your Robots**|Yiming Gan Team|[2509.03383](http://arxiv.org/abs/2509.03383)|null|
|**2025-09-05**|**Align-Then-stEer: Adapting the Vision-Language Action Models through Unified Latent Guidance**|Xuelong Li Team|[2509.02055](http://arxiv.org/abs/2509.02055)|null|
|**2025-09-02**|**AutoDrive-R $^2$ : Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving**|Shuo Li Team|[2509.01944](http://arxiv.org/abs/2509.01944)|null|
|**2025-08-31**|**OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving**|Jun Ma Team|[2509.00789](http://arxiv.org/abs/2509.00789)|null|
|**2025-08-30**|**Galaxea Open-World Dataset and G0 Dual-System VLA Model**|Hang Zhao Team|[2509.00576](http://arxiv.org/abs/2509.00576)|**[link](https://opengalaxea.github.io/G0/)**|
|**2025-08-30**|**Mechanistic interpretability for steering vision-language-action models**|Claire Tomlin Team|[2509.00328](http://arxiv.org/abs/2509.00328)|**[link](https://vla-mech-interp.github.io/)**|
|**2025-09-09**|**EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control**|Dong Wang Team|[2508.21112](http://arxiv.org/abs/2508.21112)|null|
|**2025-10-02**|**CogVLA: Cognition-Aligned Vision-Language-Action Model via Instruction-Driven Routing & Sparsification**|Liqiang Nie Team|[2508.21046](http://arxiv.org/abs/2508.21046)|**[link](https://jiutian-vl.github.io/CogVLA-page)**|
|**2025-08-27**|**Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies**|Ping Luo Team|[2508.20072](http://arxiv.org/abs/2508.20072)|null|
|**2025-08-28**|**Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation**|Donglin Wang Team|[2508.19958](http://arxiv.org/abs/2508.19958)|**[link](https://long-vla.github.io)**|
|**2025-08-28**|**Ego-centric Predictive Model Conditioned on Hand Trajectories**|Mike Zheng Shou Team|[2508.19852](http://arxiv.org/abs/2508.19852)|null|
|**2025-08-15**|**TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models**|Huiling Duan Team|[2508.19257](http://arxiv.org/abs/2508.19257)|null|
|**2025-08-26**|**MemoryVLA: Perceptual-Cognitive Memory in Vision-Language-Action Models for Robotic Manipulation**|Gao Huang Team|[2508.19236](http://arxiv.org/abs/2508.19236)|**[link](https://shihao1895.github.io/MemoryVLA)**|
|**2025-08-26**|**FlowVLA: Thinking in Motion with a Visual Chain of Thought**|Haoang Li Team|[2508.18269](http://arxiv.org/abs/2508.18269)|null|
|**2025-09-06**|**4D Visual Pre-training for Robot Learning**|Huazhe Xu Team|[2508.17230](http://arxiv.org/abs/2508.17230)|null|
|**2025-08-23**|**NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows**|Vladislav Kurenkov Team|[2508.16845](http://arxiv.org/abs/2508.16845)|null|
|**2025-08-22**|**Do What? Teaching Vision-Language-Action Models to Reject the Impossible**|David M. Chan Team|[2508.16292](http://arxiv.org/abs/2508.16292)|null|
|**2025-11-13**|**Survey of Vision-Language-Action Models for Embodied Manipulation**|Dongbin Zhao Team|[2508.15201](http://arxiv.org/abs/2508.15201)|null|
|**2025-08-19**|**CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models**|Sergey Levine Team|[2508.13446](http://arxiv.org/abs/2508.13446)|null|
|**2025-08-18**|**Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy**|Zhi Hou Team|[2508.13103](http://arxiv.org/abs/2508.13103)|null|
|**2025-09-01**|**Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey**|Liqiang Nie Team|[2508.13073](http://arxiv.org/abs/2508.13073)|**[link](https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation)**|
|**2025-08-17**|**Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search**|Glen Berseth Team|[2508.12211](http://arxiv.org/abs/2508.12211)|null|
|**2025-08-16**|**Toward General Physical Intelligence for Resilient Agile Manufacturing Automation**|Sunny Katyara Team|[2508.11960](http://arxiv.org/abs/2508.11960)|null|
|**2025-08-14**|**CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model**|Hao Dong Team|[2508.10416](http://arxiv.org/abs/2508.10416)|null|
|**2025-08-14**|**Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning**|Ping Kuang Team|[2508.10399](http://arxiv.org/abs/2508.10399)|null|
|**2025-08-14**|**ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver**|Haoang Li Team|[2508.10333](http://arxiv.org/abs/2508.10333)|null|
|**2025-08-13**|**GeoVLA: Empowering 3D Representations in Vision-Language-Action Models**|Jiale Cao Team|[2508.09071](http://arxiv.org/abs/2508.09071)|**[link](https://linsun449.github.io/GeoVLA/)**|
|**2025-08-12**|**Spatial Traces: Enhancing VLA Models with Spatial-Temporal Understanding**|Aleksandr I. Panov Team|[2508.09032](http://arxiv.org/abs/2508.09032)|null|
|**2025-08-22**|**OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing**|Hengdi Zhang Team|[2508.08706](http://arxiv.org/abs/2508.08706)|**[link](https://readerek.github.io/Objtac.github.io)**|
|**2025-08-14**|**Reinforcement Learning in Vision: A Survey**|Mike Zheng Shou Team|[2508.08189](http://arxiv.org/abs/2508.08189)|null|
|**2025-08-12**|**MolmoAct: Action Reasoning Models that can Reason in Space**|Ranjay Krishna Team|[2508.07917](http://arxiv.org/abs/2508.07917)|**[link](https://allenai.org/blog/molmoact)**|
|**2025-08-13**|**AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation**|Lei Han Team|[2508.07770](http://arxiv.org/abs/2508.07770)|null|
|**2025-08-23**|**GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions**|Hong Zhang Team|[2508.07650](http://arxiv.org/abs/2508.07650)|null|
|**2025-08-15**|**IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model**|Li Sun Team|[2508.06571](http://arxiv.org/abs/2508.06571)|null|
|**2025-08-06**|**Static and Plugged: Make Embodied Evaluation Simple**|Guangtao Zhai Team|[2508.06553](http://arxiv.org/abs/2508.06553)|null|
|**2025-08-06**|**A tutorial note on collecting simulated data for vision-language-action models**|Jingfeng Zhang Team|[2508.06547](http://arxiv.org/abs/2508.06547)|null|
|**2025-08-07**|**Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control**|Hamid Reza Karimi Team|[2508.05342](http://arxiv.org/abs/2508.05342)|null|
|**2025-08-14**|**Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction**|Jorge Peña Queralta Team|[2508.05294](http://arxiv.org/abs/2508.05294)|null|
|**2025-08-07**|**Learning to See and Act: Task-Aware View Planning for Robotic Manipulation**|Liang Lin Team|[2508.05186](http://arxiv.org/abs/2508.05186)|**[link](https://hcplab-sysu.github.io/TAVP)**|
|**2025-08-06**|**Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions**|Xiaokang Yang Team|[2508.04681](http://arxiv.org/abs/2508.04681)|**[link](https://liangxuy.github.io/InterVLA/)**|
|**2025-08-06**|**Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces**|Pierre Lison Team|[2508.02917](http://arxiv.org/abs/2508.02917)|null|
|**2025-08-04**|**MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming**|Zhaoxin Fan Team|[2508.02549](http://arxiv.org/abs/2508.02549)|null|
|**2025-08-04**|**CO-RFT: Efficient Fine-Tuning of Vision-Language-Action Models through Chunked Offline Reinforcement Learning**|Chunhe Xia Team|[2508.02219](http://arxiv.org/abs/2508.02219)|null|
|**2025-08-04**|**FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation**|Xiaodong Wang Team|[2508.02190](http://arxiv.org/abs/2508.02190)|null|
|**2025-08-04**|**RICL: Adding In-Context Adaptability to Pre-Trained Vision-Language-Action Models**|Insup Lee Team|[2508.02062](http://arxiv.org/abs/2508.02062)|null|
|**2025-07-31**|**XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation**|Ning Yang Team|[2508.00097](http://arxiv.org/abs/2508.00097)|**[link](https://github.com/XR-Robotics)**|
|**2025-07-31**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Jiang Bian Team|[2507.23682](http://arxiv.org/abs/2507.23682)|**[link](https://aka.ms/villa-x)**|
|**2025-07-31**|**A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving**|Alois Knoll Team|[2507.23540](http://arxiv.org/abs/2507.23540)|null|
|**2025-08-02**|**FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play Reconstruction-based Token Pruning**|Shanghang Zhang Team|[2507.23318](http://arxiv.org/abs/2507.23318)|null|
|**2025-07-30**|**Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance**|Derek F. Wong Team|[2507.22424](http://arxiv.org/abs/2507.22424)|null|
|**2025-07-23**|**InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation**|Jiangmiao Pang Team|[2507.17520](http://arxiv.org/abs/2507.17520)|null|
|**2025-07-23**|**ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents**|Hesheng Wang Team|[2507.17462](http://arxiv.org/abs/2507.17462)|null|
|**2025-07-23**|**Confidence Calibration in Vision-Language-Action Models**|Richard Zemel Team|[2507.17383](http://arxiv.org/abs/2507.17383)|null|
|**2025-07-29**|**VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback**|Harold Soh Team|[2507.17294](http://arxiv.org/abs/2507.17294)|null|
|**2025-07-22**|**ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning**|Fu-En Yang Team|[2507.16815](http://arxiv.org/abs/2507.16815)|**[link](https://jasper0314-huang.github.io/thinkact-vla/)**|
|**2025-07-21**|**Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos**|Zongqing Lu Team|[2507.15597](http://arxiv.org/abs/2507.15597)|null|
|**2025-07-22**|**GR-3 Technical Report**|Yichu Yang Team|[2507.15493](http://arxiv.org/abs/2507.15493)|**[link](https://seed.bytedance.com/GR3/)**|
|**2025-07-18**|**EdgeVLA: Efficient Vision-Language-Action Models**|Benjamin Bolte Team|[2507.14049](http://arxiv.org/abs/2507.14049)|null|
|**2025-07-23**|**LaViPlan : Language-Guided Visual Path Planning with RLVR**|Hayeon Oh Team|[2507.12911](http://arxiv.org/abs/2507.12911)|null|
|**2025-07-17**|**AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation**|Jun Zhu Team|[2507.12768](http://arxiv.org/abs/2507.12768)|null|
|**2025-07-18**|**EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos**|Xiaolong Wang Team|[2507.12440](http://arxiv.org/abs/2507.12440)|**[link](https://rchalyang.github.io/EgoVLA)**|
|**2025-07-14**|**Vision Language Action Models in Robotic Manipulation: A Systematic Review**|Irfan Hussain Team|[2507.10672](http://arxiv.org/abs/2507.10672)|null|
|**2025-07-12**|**Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization**|Yang Gao Team|[2507.09160](http://arxiv.org/abs/2507.09160)|null|
|**2025-07-09**|**3D-Generalist: Self-Improving Vision-Language-Action Models for Crafting 3D Worlds**|Nick Haber Team|[2507.06484](http://arxiv.org/abs/2507.06484)|**[link](https://ai.stanford.edu/~sunfanyun/3d-generalist/)**|
|**2025-07-07**|**NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving**|Cheng Lu Team|[2507.05227](http://arxiv.org/abs/2507.05227)|null|
|**2025-10-06**|**VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting**|Yanzhi Wang Team|[2507.05116](http://arxiv.org/abs/2507.05116)|null|
|**2025-07-17**|**DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge**|Xin Jin Team|[2507.04447](http://arxiv.org/abs/2507.04447)|null|
|**2025-07-06**|**Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties**|Yunxin Liu Team|[2507.04227](http://arxiv.org/abs/2507.04227)|null|
|**2025-07-03**|**DexVLG: Dexterous Vision-Language-Grasp Model at Scale**|He Wang Team|[2507.02747](http://arxiv.org/abs/2507.02747)|null|
|**2025-07-02**|**cVLA: Towards Efficient Camera-Space VLAs**|Thomas Brox Team|[2507.02190](http://arxiv.org/abs/2507.02190)|null|
|**2025-07-03**|**A Survey on Vision-Language-Action Models: An Action Tokenization Perspective**|Yaodong Yang Team|[2507.01925](http://arxiv.org/abs/2507.01925)|null|
|**2025-07-02**|**MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics**|Nadiya Shvai Team|[2507.01843](http://arxiv.org/abs/2507.01843)|null|
|**2025-07-03**|**TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control**|Yanwei Fu Team|[2507.01424](http://arxiv.org/abs/2507.01424)|null|
|**2025-07-01**|**VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers**|Tong He Team|[2507.01016](http://arxiv.org/abs/2507.01016)|null|
|**2025-07-01**|**Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding**|Bo Zhao Team|[2507.00416](http://arxiv.org/abs/2507.00416)|null|
|**2025-06-30**|**A Survey on Vision-Language-Action Models for Autonomous Driving**|Lijun Sun Team|[2506.24044](http://arxiv.org/abs/2506.24044)|null|
|**2025-06-27**|**4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration**|Li Zhang Team|[2506.22242](http://arxiv.org/abs/2506.22242)|null|
|**2025-08-08**|**Can Vision Language Models Understand Mimed Actions?**|Jonathan May Team|[2506.21586](http://arxiv.org/abs/2506.21586)|null|
|**2025-06-26**|**WorldVLA: Towards Autoregressive Action World Model**|Hao Chen Team|[2506.21539](http://arxiv.org/abs/2506.21539)|null|
|**2025-06-26**|**Parallels Between VLA Model Post-Training and Human Motor Learning: Progress, Challenges, and Trends**|Zeng-Guang Hou Team|[2506.20966](http://arxiv.org/abs/2506.20966)|null|
|**2025-06-25**|**Unified Vision-Language-Action Model**|Zhaoxiang Zhang Team|[2506.19850](http://arxiv.org/abs/2506.19850)|null|
|**2025-06-24**|**CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation**|Jiangmiao Pang Team|[2506.19816](http://arxiv.org/abs/2506.19816)|null|
|**2025-07-07**|**RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models**|Marco Pavone Team|[2506.17811](http://arxiv.org/abs/2506.17811)|null|
|**2025-06-21**|**RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models**|Xiao Li Team|[2506.17639](http://arxiv.org/abs/2506.17639)|null|
|**2025-06-21**|**VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models**|Lin Shao Team|[2506.17561](http://arxiv.org/abs/2506.17561)|null|
|**2025-06-19**|**CapsDT: Diffusion-Transformer for Capsule Robot Manipulation**|Hongliang Ren Team|[2506.16263](http://arxiv.org/abs/2506.16263)|null|
|**2025-06-19**|**ControlVLA: Few-shot Object-centric Adaptation for Pre-trained Vision-Language-Action Models**|Siyuan Huang Team|[2506.16211](http://arxiv.org/abs/2506.16211)|null|
|**2025-06-19**|**ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in Cluttered Scenes**|Hao Dong Team|[2506.14317](http://arxiv.org/abs/2506.14317)|null|
|**2025-06-16**|**GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics**|Mac Schwager Team|[2506.14009](http://arxiv.org/abs/2506.14009)|null|
|**2025-06-16**|**AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning**|Jiaqi Ma Team|[2506.13757](http://arxiv.org/abs/2506.13757)|**[link](https://github.com/ucla-mobility/AutoVLA)**|
|**2025-06-19**|**LeVERB: Humanoid Whole-Body Control with Latent Vision-Language Instruction**|Shankar Sastry Team|[2506.13751](http://arxiv.org/abs/2506.13751)|null|
|**2025-06-16**|**CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding**|Haoang Li Team|[2506.13725](http://arxiv.org/abs/2506.13725)|null|
|**2025-06-16**|**ROSA: Harnessing Robot States for Vision-Language and Action Alignment**|Xiaoyan Sun Team|[2506.13679](http://arxiv.org/abs/2506.13679)|null|
|**2025-06-16**|**Block-wise Adaptive Caching for Accelerating Diffusion Policy**|Zhi Wang Team|[2506.13456](http://arxiv.org/abs/2506.13456)|null|
|**2025-06-19**|**A Comprehensive Survey on Continual Learning in Generative Models**|Cheng-Lin Liu Team|[2506.13045](http://arxiv.org/abs/2506.13045)|**[link](https://github.com/ghy0501/awesome-continual-learning-in-generative-models)**|
|**2025-06-19**|**SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration**|Wenwu Zhu Team|[2506.12723](http://arxiv.org/abs/2506.12723)|null|
|**2025-06-13**|**RationalVLA: A Rational Vision-Language-Action Model with Dual System**|Haoang Li Team|[2506.10826](http://arxiv.org/abs/2506.10826)|null|
|**2025-06-11**|**EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models**|Linfeng Zhang Team|[2506.10100](http://arxiv.org/abs/2506.10100)|null|
|**2025-06-11**|**SAFE: Multitask Failure Detection for Vision-Language-Action Models**|Florian Shkurti Team|[2506.09937](http://arxiv.org/abs/2506.09937)|null|
|**2025-06-11**|**From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models**|Chen Feng Team|[2506.09930](http://arxiv.org/abs/2506.09930)|null|
|**2025-06-17**|**An Open-Source Software Toolkit & Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models**|Harshvardhan Sikka Team|[2506.09172](http://arxiv.org/abs/2506.09172)|null|
|**2025-06-10**|**FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency**|Jian Tang Team|[2506.08822](http://arxiv.org/abs/2506.08822)|null|
|**2025-06-10**|**Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing**|Sebastian W. Pattinson Team|[2506.08462](http://arxiv.org/abs/2506.08462)|null|
|**2025-06-11**|**TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization**|Qi Wang Team|[2506.08440](http://arxiv.org/abs/2506.08440)|null|
|**2025-06-11**|**HiBerNAC: Hierarchical Brain-emulated Robotic Neural Agent Collective for Disentangling Complex Manipulation**|Cong Wang Team|[2506.08296](http://arxiv.org/abs/2506.08296)|null|
|**2025-06-14**|**Agentic Surgical AI: Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion in a Vision-Language-Action Framework**|Jason H. Moore Team|[2506.08185](http://arxiv.org/abs/2506.08185)|**[link](https://github.com/huixin-zhan-ai/surgeon_style_fingerprinting)**|
|**2025-06-09**|**BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models**|Tieniu Tan Team|[2506.07961](http://arxiv.org/abs/2506.07961)|null|
|**2025-06-09**|**Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse**|Chris Xiaoxuan Lu Team|[2506.07639](http://arxiv.org/abs/2506.07639)|null|
|**2025-06-09**|**BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation**|Xilin Chen Team|[2506.07530](http://arxiv.org/abs/2506.07530)|**[link](https://github.com/ustcwhy/bitvla)**|
|**2025-06-09**|**Real-Time Execution of Action Chunking Flow Policies**|Sergey Levine Team|[2506.07339](http://arxiv.org/abs/2506.07339)|null|
|**2025-06-12**|**Robotic Policy Learning via Human-assisted Action Preference Optimization**|Di Hu Team|[2506.07127](http://arxiv.org/abs/2506.07127)|null|
|**2025-06-07**|**RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation**|Si Liu Team|[2506.06677](http://arxiv.org/abs/2506.06677)|null|
|**2025-06-06**|**MapleGrasp: Mask-guided Feature Pooling for Language-driven Efficient Robotic Grasping**|Farshad Khorrami Team|[2506.06535](http://arxiv.org/abs/2506.06535)|null|
|**2025-06-06**|**DriveAction: A Benchmark for Exploring Human-like Driving Decisions in VLA Models**|Xianpeng Lang Team|[2506.05667](http://arxiv.org/abs/2506.05667)|null|
|**2025-06-04**|**SwitchVLA: Execution-Aware Task Switching for Vision-Language-Action Models**|Jian Tang Team|[2506.03574](http://arxiv.org/abs/2506.03574)|null|
|**2025-06-03**|**Adversarial Attacks on Robotic Vision Language Action Models**|J. Zico Kolter Team|[2506.03350](http://arxiv.org/abs/2506.03350)|**[link](https://github.com/eliotjones1/robogcg)**|
|**2025-06-02**|**Fast-in-Slow: A Dual-System Foundation Model Unifying Fast Manipulation within Slow Reasoning**|Pheng-Ann Heng Team|[2506.01953](http://arxiv.org/abs/2506.01953)|null|
|**2025-06-02**|**SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics**|Remi Cadene Team|[2506.01844](http://arxiv.org/abs/2506.01844)|**[link](https://github.com/huggingface/lerobot)**|
|**2025-06-02**|**MLA-Trust: Benchmarking Trustworthiness of Multimodal LLM Agents in GUI Environments**|Jun Zhu Team|[2506.01616](http://arxiv.org/abs/2506.01616)|null|
|**2025-06-02**|**ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding**|Huaxiu Yao Team|[2506.01300](http://arxiv.org/abs/2506.01300)|null|
|**2025-06-01**|**OG-VLA: 3D-Aware Vision Language Action Model via Orthographic Image Generation**|Valts Blukis Team|[2506.01196](http://arxiv.org/abs/2506.01196)|null|
|**2025-05-31**|**LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon Embodied Tasks**|Zhijie Deng Team|[2506.00411](http://arxiv.org/abs/2506.00411)|null|
|**2025-05-30**|**Towards a Generalizable Bimanual Foundation Policy via Flow-based Video Prediction**|Xuelong Li Team|[2505.24156](http://arxiv.org/abs/2505.24156)|null|
|**2025-05-29**|**Impromptu VLA: Open Weights and Open Data for Driving Vision-Language-Action Models**|Hao Zhao Team|[2505.23757](http://arxiv.org/abs/2505.23757)|**[link](https://github.com/ahydchh/impromptu-vla)**|
|**2025-05-29**|**Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better**|Sergey Levine Team|[2505.23705](http://arxiv.org/abs/2505.23705)|null|
|**2025-05-29**|**Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents**|Lichao Sun Team|[2505.23450](http://arxiv.org/abs/2505.23450)|null|
|**2025-05-29**|**TrackVLA: Embodied Visual Tracking in the Wild**|He Wang Team|[2505.23189](http://arxiv.org/abs/2505.23189)|null|
|**2025-05-28**|**ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation**|Wenqiang Zhang Team|[2505.22159](http://arxiv.org/abs/2505.22159)|null|
|**2025-05-29**|**ChatVLA-2: Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge**|Yi Xu Team|[2505.21906](http://arxiv.org/abs/2505.21906)|null|
|**2025-05-27**|**EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models**|Xiang Chen Team|[2505.21567](http://arxiv.org/abs/2505.21567)|null|
|**2025-06-02**|**Hume: Introducing System-2 Thinking in Visual-Language-Action Model**|Xuelong Li Team|[2505.21432](http://arxiv.org/abs/2505.21432)|null|
|**2025-05-27**|**Think Twice, Act Once: Token-Aware Compression and Action Reuse for Efficient Inference in Vision-Language-Action Models**|Tao Chen Team|[2505.21200](http://arxiv.org/abs/2505.21200)|null|
|**2025-05-26**|**Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review**|Goldie Nejat Team|[2505.20503](http://arxiv.org/abs/2505.20503)|null|
|**2025-05-26**|**What Can RL Bring to VLA Generalization? An Empirical Study**|Yu Wang Team|[2505.19789](http://arxiv.org/abs/2505.19789)|null|
|**2025-05-26**|**RFTF: Reinforcement Fine-tuning for Embodied Agents with Temporal Feedback**|Yongtao Wang Team|[2505.19767](http://arxiv.org/abs/2505.19767)|null|
|**2025-05-25**|**ReFineVLA: Reasoning-Aware Teacher-Guided Transfer Fine-Tuning**|Minh Nhat Vu Team|[2505.19080](http://arxiv.org/abs/2505.19080)|null|
|**2025-05-24**|**Genie Centurion: Accelerating Scalable Real-World Robot Training with Human Rewind-and-Refine Guidance**|Maoqing Yao Team|[2505.18793](http://arxiv.org/abs/2505.18793)|null|
|**2025-05-24**|**VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning**|Ziwei Wang Team|[2505.18719](http://arxiv.org/abs/2505.18719)|**[link](https://github.com/guanxinglu/vlarl)**|
|**2025-05-22**|**ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems**|Farhad Imani Team|[2505.17295](http://arxiv.org/abs/2505.17295)|null|
|**2025-05-22**|**Interactive Post-Training for Vision-Language-Action Models**|Philipp Krähenbühl Team|[2505.17016](http://arxiv.org/abs/2505.17016)|null|
|**2025-05-22**|**Perceptual Quality Assessment for Embodied AI**|Guangtao Zhai Team|[2505.16815](http://arxiv.org/abs/2505.16815)|**[link](https://github.com/lcysyzxdxc/embodiediqa)**|
|**2025-05-22**|**BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization**|Lichao Sun Team|[2505.16640](http://arxiv.org/abs/2505.16640)|null|
|**2025-05-22**|**DriveMoE: Mixture-of-Experts for Vision-Language-Action Model in End-to-End Autonomous Driving**|Junchi Yan Team|[2505.16278](http://arxiv.org/abs/2505.16278)|null|
|**2025-05-21**|**From Grounding to Manipulation: Case Studies of Foundation Model Integration in Embodied Robotic Systems**|Soujanya Poria Team|[2505.15685](http://arxiv.org/abs/2505.15685)|**[link](https://github.com/hritdy/claw_machine)**|
|**2025-05-24**|**Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization**|Junwei Liang Team|[2505.15660](http://arxiv.org/abs/2505.15660)|**[link](https://github.com/jiaming-zhou/X-ICM)**|
|**2025-05-21**|**FLARE: Robot Learning with Implicit World Modeling**|Linxi Fan Team|[2505.15659](http://arxiv.org/abs/2505.15659)|null|
|**2025-05-21**|**Saliency-Aware Quantized Imitation Learning for Efficient Robotic Control**|Jungwook Choi Team|[2505.15304](http://arxiv.org/abs/2505.15304)|null|
|**2025-05-21**|**EndoVLA: Dual-Phase Vision-Language-Action Model for Autonomous Tracking in Endoscopy**|Hongliang Ren Team|[2505.15206](http://arxiv.org/abs/2505.15206)|null|
|**2025-05-21**|**Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation**|Xiaodong He Team|[2505.15098](http://arxiv.org/abs/2505.15098)|null|
|**2025-05-20**|**AutoBio: A Simulation and Benchmark for Robotic Automation in Digital Biology Laboratory**|Ping Luo Team|[2505.14030](http://arxiv.org/abs/2505.14030)|null|
|**2025-05-22**|**InSpire: Vision-Language-Action Models with Intrinsic Spatial Reasoning**|Jingkuan Song Team|[2505.13888](http://arxiv.org/abs/2505.13888)|**[link](https://github.com/inspirevla/inspire)**|
|**2025-05-25**|**RoboFAC: A Comprehensive Framework for Robotic Failure Analysis and Correction**|Bo Zhao Team|[2505.12224](http://arxiv.org/abs/2505.12224)|null|
|**2025-05-17**|**OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning**|Yang Gao Team|[2505.11917](http://arxiv.org/abs/2505.11917)|null|
|**2025-05-16**|**Unveiling the Potential of Vision-Language-Action Models with Open-Ended Multimodal Instructions**|Donglin Wang Team|[2505.11214](http://arxiv.org/abs/2505.11214)|null|
|**2025-05-16**|**Conditioning Matters: Training Diffusion Policies is Faster Than You Think**|Jianye Hao Team|[2505.11123](http://arxiv.org/abs/2505.11123)|null|
|**2025-05-14**|**Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware**|Ken Goldberg Team|[2505.09601](http://arxiv.org/abs/2505.09601)|null|
|**2025-05-14**|**RT-cache: Efficient Robot Trajectory Retrieval System**|Amir Barati Farimani Team|[2505.09040](http://arxiv.org/abs/2505.09040)|null|
|**2025-05-13**|**From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation**|Jianye Hao Team|[2505.08548](http://arxiv.org/abs/2505.08548)|null|
|**2025-05-17**|**Training Strategies for Efficient Embodied Reasoning**|Sergey Levine Team|[2505.08243](http://arxiv.org/abs/2505.08243)|null|
|**2025-05-12**|**Pixel Motion as Universal Representation for Robot Control**|Michael S Ryoo Team|[2505.07817](http://arxiv.org/abs/2505.07817)|null|
|**2025-05-12**|**ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning**|Donglin Wang Team|[2505.07395](http://arxiv.org/abs/2505.07395)|null|
|**2025-05-15**|**UniVLA: Learning to Act Anywhere with Task-centric Latent Actions**|Hongyang Li Team|[2505.06111](http://arxiv.org/abs/2505.06111)|**[link](https://github.com/opendrivelab/univla)**|
|**2025-05-09**|**3D CAVLA: Leveraging Depth and 3D Context to Generalize Vision Language Action Models for Unseen Tasks**|Farshad Khorrami Team|[2505.05800](http://arxiv.org/abs/2505.05800)|null|
|**2025-05-08**|**Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments**|Harshvardhan Sikka Team|[2505.05540](http://arxiv.org/abs/2505.05540)|**[link](https://github.com/ManifoldRG/MultiNet)**|
|**2025-05-09**|**Vision-Language-Action Models: Concepts, Progress, Applications and Challenges**|Manoj Karkee Team|[2505.04769](http://arxiv.org/abs/2505.04769)|null|
|**2025-05-06**|**OpenHelix: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation**|Donglin Wang Team|[2505.03912](http://arxiv.org/abs/2505.03912)|**[link](https://github.com/OpenHelix-robot/OpenHelix)**|
|**2025-05-16**|**Task Reconstruction and Extrapolation for $π_0$ using Text Latent**|Quanyi Li Team|[2505.03500](http://arxiv.org/abs/2505.03500)|null|
|**2025-05-06**|**GraspVLA: a Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data**|He Wang Team|[2505.03233](http://arxiv.org/abs/2505.03233)|null|
|**2025-05-06**|**Automated Data Curation Using GPS & NLP to Generate Instruction-Action Pairs for Autonomous Vehicle Vision-Language Navigation Datasets**|Ross Greer Team|[2505.03174](http://arxiv.org/abs/2505.03174)|null|
|**2025-05-04**|**CrayonRobo: Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation**|Hao Dong Team|[2505.02166](http://arxiv.org/abs/2505.02166)|null|
|**2025-05-04**|**Interleave-VLA: Enhancing Robot Manipulation with Interleaved Image-Text Instructions**|Mingyu Ding Team|[2505.02152](http://arxiv.org/abs/2505.02152)|null|
|**2025-04-28**|**NORA: A Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks**|Soujanya Poria Team|[2504.19854](http://arxiv.org/abs/2504.19854)|null|
|**2025-04-22**|**$π_{0.5}$ : a Vision-Language-Action Model with Open-World Generalization**|Ury Zhilinsky Team|[2504.16054](http://arxiv.org/abs/2504.16054)|null|
|**2025-04-22**|**Few-Shot Vision-Language Action-Incremental Policy Learning**|Weili Guan Team|[2504.15517](http://arxiv.org/abs/2504.15517)|null|
|**2025-04-18**|**GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents**|Xiaobo Xia Team|[2504.10458](http://arxiv.org/abs/2504.10458)|null|
|**2025-04-09**|**OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning**|Tyler Fenstermaker Team|[2504.06538](http://arxiv.org/abs/2504.06538)|null|
|**2025-04-02**|**Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning**|Roozbeh Mottaghi Team|[2504.00907](http://arxiv.org/abs/2504.00907)|null|
|**2025-03-30**|**OpenDriveVLA: Towards End-to-end Autonomous Driving with Large Vision Language Action Model**|Alois C. Knoll Team|[2503.23463](http://arxiv.org/abs/2503.23463)|**[link](https://github.com/DriveVLA/OpenDriveVLA)**|
|**2025-03-27**|**CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models**|Tsung-Yi Lin Team|[2503.22020](http://arxiv.org/abs/2503.22020)|null|
|**2025-04-14**|**MoLe-VLA: Dynamic Layer-skipping Vision Language Action Model via Mixture-of-Layers for Efficient Robot Manipulation**|Shanghang Zhang Team|[2503.20384](http://arxiv.org/abs/2503.20384)|null|
|**2025-03-25**|**Gemini Robotics: Bringing AI into the Physical World**|Yuxiang Zhou Team|[2503.20020](http://arxiv.org/abs/2503.20020)|null|
|**2025-03-25**|**Dita: Scaling Diffusion Transformer for Generalist Vision-Language-Action Policy**|Yuntao Chen Team|[2503.19757](http://arxiv.org/abs/2503.19757)|null|
|**2025-03-25**|**DataPlatter: Boosting Robotic Manipulation Generalization with Minimal Costly Data**|Lin Ma Team|[2503.19516](http://arxiv.org/abs/2503.19516)|null|
|**2025-03-27**|**GR00T N1: An Open Foundation Model for Generalist Humanoid Robots**|Yuke Zhu Team|[2503.14734](http://arxiv.org/abs/2503.14734)|null|
|**2025-03-15**|**ReBot: Scaling Robot Learning with Real-to-Sim-to-Real Robotic Video Synthesis**|Mingyu Ding Team|[2503.14526](http://arxiv.org/abs/2503.14526)|null|
|**2025-03-17**|**MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation**|Haibin Yan Team|[2503.13446](http://arxiv.org/abs/2503.13446)|null|
|**2025-03-17**|**HybridVLA: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model**|Shanghang Zhang Team|[2503.10631](http://arxiv.org/abs/2503.10631)|null|
|**2025-03-13**|**SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment**|Oleg Sinavski Team|[2503.09594](http://arxiv.org/abs/2503.09594)|null|
|**2025-03-12**|**CombatVLA: An Efficient Vision-Language-Action Model for Combat Tasks in 3D Action Role-Playing Games**|Bo Zheng Team|[2503.09527](http://arxiv.org/abs/2503.09527)|null|
|**2025-03-11**|**MoRE: Unlocking Scalability in Reinforcement Learning for Quadruped Vision-Language-Action Models**|Zongyuan Ge Team|[2503.08007](http://arxiv.org/abs/2503.08007)|null|
|**2025-03-10**|**PointVLA: Injecting the 3D World into Vision-Language-Action Models**|Yichen Zhu Team|[2503.07511](http://arxiv.org/abs/2503.07511)|null|
|**2025-03-11**|**CLAD: Constrained Latent Action Diffusion for Vision-Language Procedure Planning**|Andreas Bulling Team|[2503.06637](http://arxiv.org/abs/2503.06637)|null|
|**2025-03-06**|**Refined Policy Distillation: From VLA Generalists to RL Experts**|Florian Walter Team|[2503.05833](http://arxiv.org/abs/2503.05833)|null|
|**2025-03-06**|**VLA Model-Expert Collaboration for Bi-directional Manipulation Learning**|Zeng-Guang Hou Team|[2503.04163](http://arxiv.org/abs/2503.04163)|null|
|**2025-03-26**|**OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction**|Pieter Abbeel Team|[2503.03734](http://arxiv.org/abs/2503.03734)|null|
|**2025-03-05**|**SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning**|Yaodong Yang Team|[2503.03480](http://arxiv.org/abs/2503.03480)|null|
|**2025-03-04**|**Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding**|Haoang Li Team|[2503.02310](http://arxiv.org/abs/2503.02310)|null|
|**2025-03-03**|**CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time Cognitive Task Solving and Reasoning in UAVs**|Dzmitry Tsetserukou Team|[2503.01378](http://arxiv.org/abs/2503.01378)|null|
|**2025-10-15**|**CoVLA: Comprehensive Vision-Language-Action Dataset for Autonomous Driving**|Issei Yamamoto Team|[2408.10845](http://arxiv.org/abs/2408.10845)|**[link](https://turingmotors.github.io/covla-ad/)**|
|**2024-07-23**|**Can VLMs be used on videos for action recognition? LLMs are Visual Reasoning Coordinators**|Harsh Lunia Team|[2407.14834](http://arxiv.org/abs/2407.14834)|null|
|**2024-03-15**|**3D-VLA: A 3D Vision-Language-Action Generative World Model**|Chuang Gan Team|[2403.09631](http://arxiv.org/abs/2403.09631)|**[link](https://vis-www.cs.umass.edu/3dvla/)**|
|**2022-07-19**|**Zero-Shot Temporal Action Detection via Vision-Language Prompting**|Tao Xiang Team|[2207.08184](http://arxiv.org/abs/2207.08184)|**[link](https://github.com/sauradip/STALE)**|
|**2022-06-01**|**ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts**|Xiaodan Liang Team|[2205.15509](http://arxiv.org/abs/2205.15509)|null|
|**2022-08-16**|**A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility**|Bryan A. Plummer Team|[2202.02312](http://arxiv.org/abs/2202.02312)|null|
|**2017-04-25**|**An Analysis of Action Recognition Datasets for Language and Vision Tasks**|Frank Keller Team|[1704.07129](http://arxiv.org/abs/1704.07129)|null|

## Humanoid

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**Task-oriented grasping for dexterous robots using postural synergies and reinforcement learning**|Plinio Moreno Team|[2602.20915](http://arxiv.org/abs/2602.20915)|null|
|**2026-02-24**|**Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**|Saurabh Gupta Team|[2602.16705](http://arxiv.org/abs/2602.16705)|**[link](https://hero-humanoid.github.io/)**|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-17**|**MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**|Yijie Guo Team|[2602.15733](http://arxiv.org/abs/2602.15733)|null|
|**2026-02-16**|**Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**|Kensuke Harada Team|[2602.14551](http://arxiv.org/abs/2602.14551)|null|
|**2026-02-16**|**AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**|Sehoon Ha Team|[2602.14363](http://arxiv.org/abs/2602.14363)|**[link](https://morganbyrd03.github.io/adaptmanip/)**|
|**2026-02-15**|**ProAct: A Dual-System Framework for Proactive Embodied Social Agents**|Libin Liu Team|[2602.14048](http://arxiv.org/abs/2602.14048)|**[link](https://proactrobot.github.io/)**|
|**2026-02-23**|**Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**|Alan Fern Team|[2602.13850](http://arxiv.org/abs/2602.13850)|**[link](https://osudrl.github.io/Humanoid_Hanoi/)**|
|**2026-02-22**|**Impact-Robust Posture Optimization for Aerial Manipulation**|Antonio Franchi Team|[2602.13762](http://arxiv.org/abs/2602.13762)|null|
|**2026-02-14**|**A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**|Xuesong Li Team|[2602.13656](http://arxiv.org/abs/2602.13656)|null|
|**2026-02-12**|**General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**|Zongqing Lu Team|[2602.11929](http://arxiv.org/abs/2602.11929)|null|
|**2026-02-12**|**HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**|Renjing Xu Team|[2602.11758](http://arxiv.org/abs/2602.11758)|**[link](https://haic-humanoid.github.io/)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Towards Learning a Generalizable 3D Scene Representation from 2D Observations**|Stefan Wermter Team|[2602.10943](http://arxiv.org/abs/2602.10943)|null|
|**2026-02-10**|**Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**|Lixiao Huang Team|[2602.10069](http://arxiv.org/abs/2602.10069)|null|
|**2026-02-10**|**TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**|Rongyun Cao Team|[2602.09628](http://arxiv.org/abs/2602.09628)|null|
|**2026-02-09**|**Learning Human-Like Badminton Skills for Humanoid Robots**|Peng Lu Team|[2602.08370](http://arxiv.org/abs/2602.08370)|null|
|**2026-02-14**|**VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**|Yang Zhang Team|[2602.07506](http://arxiv.org/abs/2602.07506)|null|
|**2026-02-07**|**TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**|Xuelong Li Team|[2602.07439](http://arxiv.org/abs/2602.07439)|**[link](https://text-op.github.io/)**|
|**2026-02-07**|**Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**|Miao Li Team|[2602.07434](http://arxiv.org/abs/2602.07434)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-05**|**A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion**|Simon F. G. Ehlers Team|[2602.05855](http://arxiv.org/abs/2602.05855)|null|
|**2026-02-05**|**Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**|Weinan Zhang Team|[2602.05791](http://arxiv.org/abs/2602.05791)|null|
|**2026-02-05**|**TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**|Jaeheung Park Team|[2602.05596](http://arxiv.org/abs/2602.05596)|null|
|**2026-02-05**|**Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework**|Xuelong Li Team|[2602.05310](http://arxiv.org/abs/2602.05310)|null|
|**2026-02-04**|**PDF-HR: Pose Distance Fields for Humanoid Robots**|Renjing Xu Team|[2602.04851](http://arxiv.org/abs/2602.04851)|**[link](https://gaoyukang33.github.io/PDF-HR/}{Project)**|
|**2026-02-04**|**EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**|Börje F. Karlsson Team|[2602.04515](http://arxiv.org/abs/2602.04515)|null|
|**2026-02-05**|**HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation**|Hong Jia Team|[2602.04412](http://arxiv.org/abs/2602.04412)|null|
|**2026-02-02**|**Flow Policy Gradients for Robot Control**|Angjoo Kanazawa Team|[2602.02481](http://arxiv.org/abs/2602.02481)|**[link](https://hongsukchoi.github.io/fpo-control)**|
|**2026-02-02**|**HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**|Ping Tan Team|[2602.02473](http://arxiv.org/abs/2602.02473)|null|
|**2026-02-02**|**TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour**|Hang Zhao Team|[2602.02331](http://arxiv.org/abs/2602.02331)|**[link](https://ttt-parkour.github.io/)**|
|**2026-02-02**|**A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**|Shreyas Kousik Team|[2602.01632](http://arxiv.org/abs/2602.01632)|**[link](https://sew-mimic.com/)**|
|**2026-02-02**|**RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots**|David Howard Team|[2602.01515](http://arxiv.org/abs/2602.01515)|null|
|**2026-02-01**|**T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation**|Xiaochun Mai Team|[2602.01352](http://arxiv.org/abs/2602.01352)|null|
|**2026-01-31**|**Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**|A. Postnikov Team|[2602.00919](http://arxiv.org/abs/2602.00919)|null|
|**2026-01-30**|**ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control**|Farbod Farshidian Team|[2602.00401](http://arxiv.org/abs/2602.00401)|null|
|**2026-01-30**|**Robust and Generalized Humanoid Motion Tracking**|Dongdong Zheng Team|[2601.23080](http://arxiv.org/abs/2601.23080)|null|
|**2026-01-30**|**RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing**|Weinan Zhang Team|[2601.22517](http://arxiv.org/abs/2601.22517)|null|
|**2026-01-26**|**HumanoidTurk: Expanding VR Haptics with Humanoids for Driving Simulations**|Jin-Hyuk Hong Team|[2601.18975](http://arxiv.org/abs/2601.18975)|null|
|**2026-01-26**|**Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot**|Josh Merel Team|[2601.18963](http://arxiv.org/abs/2601.18963)|null|
|**2026-01-24**|**MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions**|Tongtong Feng Team|[2601.17507](http://arxiv.org/abs/2601.17507)|null|
|**2026-01-24**|**PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**|Hesheng Wang Team|[2601.17440](http://arxiv.org/abs/2601.17440)|null|
|**2026-01-24**|**Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots**|Xihan Bian Team|[2601.17287](http://arxiv.org/abs/2601.17287)|null|
|**2026-01-21**|**Learning a Unified Latent Space for Cross-Embodiment Robot Control**|Dongheui Lee Team|[2601.15419](http://arxiv.org/abs/2601.15419)|null|
|**2026-01-21**|**Vision-Language Models on the Edge for Real-Time Robotic Perception**|Syed Ali Raza Zaidi Team|[2601.14921](http://arxiv.org/abs/2601.14921)|null|
|**2026-01-21**|**HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation**|Dzmitry Tsetserukou Team|[2601.14874](http://arxiv.org/abs/2601.14874)|null|
|**2026-01-19**|**FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions**|Xipeng Qiu Team|[2601.12799](http://arxiv.org/abs/2601.12799)|**[link](https://openmoss.github.io/FRoM-W1)**|
|**2026-01-19**|**FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation**|Yue Gao Team|[2601.12790](http://arxiv.org/abs/2601.12790)|null|
|**2026-01-20**|**ProjecTA: A Semi-Humanoid Robotic Teaching Assistant with In-Situ Projection for Guided Tours**|Pengcheng An Team|[2601.11328](http://arxiv.org/abs/2601.11328)|null|
|**2026-01-15**|**FastStair: Learning to Run Up Stairs with Humanoid Robots**|Jie Zhao Team|[2601.10365](http://arxiv.org/abs/2601.10365)|null|
|**2026-01-13**|**Heterogeneous computing platform for real-time robotics**|Steve Furber Team|[2601.09755](http://arxiv.org/abs/2601.09755)|null|
|**2026-01-14**|**Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations**|Wei-Shi Zheng Team|[2601.09518](http://arxiv.org/abs/2601.09518)|null|
|**2026-01-13**|**Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation**|Miao Li Team|[2601.09031](http://arxiv.org/abs/2601.09031)|null|
|**2026-01-12**|**WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots**|Jianfei Yang Team|[2601.07454](http://arxiv.org/abs/2601.07454)|null|
|**2026-01-12**|**AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers**|Zecui Zeng Team|[2601.07284](http://arxiv.org/abs/2601.07284)|null|
|**2026-01-11**|**RSLCPP -- Deterministic Simulations Using ROS 2**|Markus Lienkamp Team|[2601.07052](http://arxiv.org/abs/2601.07052)|null|
|**2026-01-09**|**Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds**|Aaron D. Ames Team|[2601.06286](http://arxiv.org/abs/2601.06286)|null|
|**2026-01-08**|**SKATER: Synthesized Kinematics for Advanced Traversing Efficiency on a Humanoid Robot via Roller Skate Swizzles**|Shiwu Zhang Team|[2601.04948](http://arxiv.org/abs/2601.04948)|null|
|**2026-01-07**|**Locomotion Beyond Feet**|C. Karen Liu Team|[2601.03607](http://arxiv.org/abs/2601.03607)|**[link](https://locomotion-beyond-feet.github.io/)**|
|**2026-01-06**|**Soft Responsive Materials Enhance Humanoid Safety**|Fan Shi Team|[2601.02857](http://arxiv.org/abs/2601.02857)|null|
|**2026-01-05**|**Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot**|Maoqing Yao Team|[2601.02078](http://arxiv.org/abs/2601.02078)|null|
|**2025-12-31**|**Coordinated Humanoid Manipulation with Choice Policies**|Jitendra Malik Team|[2512.25072](http://arxiv.org/abs/2512.25072)|**[link](https://choice-policy.github.io/)**|
|**2025-12-31**|**Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids**|David Hyunchul Shim Team|[2512.24657](http://arxiv.org/abs/2512.24657)|null|
|**2025-12-30**|**UniAct: Unified Motion Generation and Action Streaming for Humanoid Robots**|Siyuan Huang Team|[2512.24321](http://arxiv.org/abs/2512.24321)|**[link](https://jnnan.github.io/uniact/)**|
|**2026-01-04**|**Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control**|Shanghang Zhang Team|[2512.23650](http://arxiv.org/abs/2512.23650)|null|
|**2025-12-25**|**World-Coordinate Human Motion Retargeting via SAM 3D Body**|Yukun Zheng Team|[2512.21573](http://arxiv.org/abs/2512.21573)|null|
|**2025-12-24**|**Wireless Center of Pressure Feedback System for Humanoid Robot Balance Control using ESP32-C3**|Atar Fuady Babgei Team|[2512.21219](http://arxiv.org/abs/2512.21219)|null|
|**2025-12-19**|**Semantic Co-Speech Gesture Synthesis and Real-Time Control for Humanoid Robots**|Gang Zhang Team|[2512.17183](http://arxiv.org/abs/2512.17183)|null|
|**2025-12-18**|**PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence**|Kai Chen Team|[2512.16793](http://arxiv.org/abs/2512.16793)|null|
|**2025-12-16**|**CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation**|Yuke Zhu Team|[2512.14689](http://arxiv.org/abs/2512.14689)|**[link](https://nvlabs.github.io/CHIP/)**|
|**2025-12-15**|**Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories**|Christian Ott Team|[2512.13304](http://arxiv.org/abs/2512.13304)|**[link](https://youtu.be/HlAg2nbNct4)**|
|**2025-12-15**|**PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations**|Wenjun Zeng Team|[2512.13093](http://arxiv.org/abs/2512.13093)|null|
|**2025-12-13**|**Sim2Real Reinforcement Learning for Soccer skills**|Jonathan Spraggett Team|[2512.12437](http://arxiv.org/abs/2512.12437)|null|
|**2025-12-13**|**Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy**|Jonathan Spraggett Team|[2512.12230](http://arxiv.org/abs/2512.12230)|null|
|**2025-12-13**|**A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction**|Bishakh Bhattacharya Team|[2512.12208](http://arxiv.org/abs/2512.12208)|null|
|**2025-12-15**|**WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control**|Hongyang Li Team|[2512.11047](http://arxiv.org/abs/2512.11047)|null|
|**2026-01-01**|**Symphony: A Heuristic Normalized Calibrated Advantage Actor and Critic Algorithm in application for Humanoid Robots**|József Dombi Team|[2512.10477](http://arxiv.org/abs/2512.10477)|**[link](https://github.com/SuspensionRailway/symphony)**|
|**2025-12-10**|**A Hierarchical, Model-Based System for High-Performance Humanoid Soccer**|Dennis W. Hong Team|[2512.09431](http://arxiv.org/abs/2512.09431)|null|
|**2025-11-28**|**Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis**|Longbing Cao Team|[2512.08952](http://arxiv.org/abs/2512.08952)|null|
|**2025-12-10**|**SensHRPS: Sensing Comfortable Human-Robot Proxemics and Personal Space With Eye-Tracking**|Karsten Berns Team|[2512.08518](http://arxiv.org/abs/2512.08518)|null|
|**2025-12-08**|**Efficient and Compliant Control Framework for Versatile Human-Humanoid Collaborative Transportation**|Panagiotis Artemiadis Team|[2512.07819](http://arxiv.org/abs/2512.07819)|null|
|**2025-12-08**|**Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction**|Houqiang Li Team|[2512.07464](http://arxiv.org/abs/2512.07464)|null|
|**2025-12-07**|**CERNet: Class-Embedding Predictive-Coding RNN for Unified Robot Motion, Recognition, and Confidence Estimation**|Mathias Quoy Team|[2512.07041](http://arxiv.org/abs/2512.07041)|null|
|**2025-12-10**|**Learning Agile Striker Skills for Humanoid Soccer Robots from Noisy Sensory Input**|Peter Stone Team|[2512.06571](http://arxiv.org/abs/2512.06571)|null|
|**2025-12-11**|**From Generated Human Videos to Physically Plausible Robot Trajectories**|Roei Herzig Team|[2512.05094](http://arxiv.org/abs/2512.05094)|**[link](https://genmimic.github.io)**|
|**2025-12-04**|**Preliminary Analysis and Simulation of a Compact Variable Stiffness Wrist**|Giorgio Grioli Team|[2512.04973](http://arxiv.org/abs/2512.04973)|**[link](https://doi.org/10.1007/978-3-031-64057-5_9)**|
|**2025-12-04**|**X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale**|Mike Zheng Shou Team|[2512.04537](http://arxiv.org/abs/2512.04537)|null|
|**2025-12-02**|**RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning**|Haoqian Wang Team|[2512.02729](http://arxiv.org/abs/2512.02729)|null|
|**2025-12-01**|**Learning Sim-to-Real Humanoid Locomotion in 15 Minutes**|Pieter Abbeel Team|[2512.01996](http://arxiv.org/abs/2512.01996)|**[link](https://younggyo.me/fastsac-humanoid)**|
|**2025-12-01**|**Modality-Augmented Fine-Tuning of Foundation Robot Policies for Cross-Embodiment Manipulation on GR1 and G1**|Songhwai Oh Team|[2512.01358](http://arxiv.org/abs/2512.01358)|null|
|**2025-12-01**|**Discovering Self-Protective Falling Policy for Humanoid Robot via Deep Reinforcement Learning**|Donglin Wang Team|[2512.01336](http://arxiv.org/abs/2512.01336)|null|
|**2025-11-30**|**Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer**|Yuke Zhu Team|[2512.01061](http://arxiv.org/abs/2512.01061)|**[link](https://doorman-humanoid.github.io/)**|
|**2025-11-30**|**CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding**|Wei-Shi Zheng Team|[2512.01022](http://arxiv.org/abs/2512.01022)|**[link](https://isee-laboratory.github.io/OmniDexGrasp/)**|
|**2025-11-30**|**H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer**|Weinan Zhang Team|[2512.00971](http://arxiv.org/abs/2512.00971)|null|
|**2025-12-02**|**Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment**|Libo Wang Team|[2512.00783](http://arxiv.org/abs/2512.00783)|**[link](https://huggingface.co/Veltraxor/Sigma)**|
|**2025-11-25**|**A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs**|Bowen Zhi Team|[2512.00077](http://arxiv.org/abs/2512.00077)|null|
|**2025-11-28**|**SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot**|Dzmitry Tsetserukou Team|[2511.23300](http://arxiv.org/abs/2511.23300)|null|
|**2025-11-28**|**Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary**|Jingya Wang Team|[2511.22963](http://arxiv.org/abs/2511.22963)|**[link](https://humanoidlla.github.io/)**|
|**2025-11-26**|**Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation**|Qijun Chen Team|[2511.21169](http://arxiv.org/abs/2511.21169)|null|
|**2025-12-04**|**HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments**|Bin He Team|[2511.20275](http://arxiv.org/abs/2511.20275)|null|
|**2025-11-24**|**SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control**|Zongqing Lu Team|[2511.19236](http://arxiv.org/abs/2511.19236)|null|
|**2025-11-24**|**Reference-Free Sampling-Based Model Predictive Control**|Justin Carpentier Team|[2511.19204](http://arxiv.org/abs/2511.19204)|null|
|**2025-11-24**|**AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion**|Mingguo Zhao Team|[2511.18857](http://arxiv.org/abs/2511.18857)|null|
|**2025-11-23**|**SafeFall: Learning Protective Control for Humanoid Robots**|Siyuan Huang Team|[2511.18509](http://arxiv.org/abs/2511.18509)|null|
|**2025-11-22**|**Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game**|Tianyu Li Team|[2511.17925](http://arxiv.org/abs/2511.17925)|null|
|**2025-11-18**|**Translating Cultural Choreography from Humanoid Forms to Robotic Arm**|Aven-Le Zhou Team|[2511.17603](http://arxiv.org/abs/2511.17603)|null|
|**2025-11-24**|**Agility Meets Stability: Versatile Humanoid Control with Heterogeneous Data**|Hongyang Li Team|[2511.17373](http://arxiv.org/abs/2511.17373)|null|
|**2025-11-20**|**InEKFormer: A Hybrid State Estimator for Humanoid Robots**|Frank Kirchner Team|[2511.16306](http://arxiv.org/abs/2511.16306)|null|
|**2025-11-27**|**VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation**|Yuke Zhu Team|[2511.15200](http://arxiv.org/abs/2511.15200)|**[link](https://viral-humanoid.github.io/)**|
|**2025-11-18**|**HMC: Learning Heterogeneous Meta-Control for Contact-Rich Loco-Manipulation**|Xiaolong Wang Team|[2511.14756](http://arxiv.org/abs/2511.14756)|null|
|**2025-11-15**|**Learning Adaptive Neural Teleoperation for Humanoid Robots: From Inverse Kinematics to End-to-End Control**|Sanjar Atamuradov Team|[2511.12390](http://arxiv.org/abs/2511.12390)|null|
|**2025-11-14**|**Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning**|Xiaoyu Ren Team|[2511.11218](http://arxiv.org/abs/2511.11218)|null|
|**2025-11-13**|**DecARt Leg: Design and Evaluation of a Novel Humanoid Robot Leg with Decoupled Actuation for Agile Locomotion**|Roman Gorbachev Team|[2511.10021](http://arxiv.org/abs/2511.10021)|null|
|**2025-11-12**|**SPIDER: Scalable Physics-Informed Dexterous Retargeting**|Francois Hogan Team|[2511.09484](http://arxiv.org/abs/2511.09484)|**[link](https://jc-bao.github.io/spider-project/)**|
|**2025-11-12**|**Unveiling the Impact of Data and Model Scaling on High-Level Control for Humanoid Robots**|Siheng Chen Team|[2511.09241](http://arxiv.org/abs/2511.09241)|null|
|**2025-11-12**|**RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation**|Miao Li Team|[2511.09141](http://arxiv.org/abs/2511.09141)|null|
|**2025-11-10**|**Unified Humanoid Fall-Safety Policy from a Few Demonstrations**|Stella X. Yu Team|[2511.07407](http://arxiv.org/abs/2511.07407)|null|
|**2025-11-10**|**Human-Level Actuation for Humanoids**|MD-Nazmus Sunbeam Team|[2511.06796](http://arxiv.org/abs/2511.06796)|null|
|**2025-11-11**|**Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning**|Chenjia Bai Team|[2511.06371](http://arxiv.org/abs/2511.06371)|null|
|**2025-11-08**|**Towards Human-AI-Robot Collaboration and AI-Agent based Digital Twins for Parkinson's Disease Management: Review and Outlook**|Tareq Y. Al-Naffouri Team|[2511.06036](http://arxiv.org/abs/2511.06036)|null|
|**2025-11-06**|**ScheduleStream: Temporal Planning with Samplers for GPU-Accelerated Multi-Arm Task and Motion Planning & Scheduling**|Fabio Ramos Team|[2511.04758](http://arxiv.org/abs/2511.04758)|**[link](https://schedulestream.github.io)**|
|**2025-11-06**|**GentleHumanoid: Learning Upper-body Compliance for Contact-rich Human and Object Interaction**|C. Karen Liu Team|[2511.04679](http://arxiv.org/abs/2511.04679)|**[link](https://gentle-humanoid.axell.top)**|
|**2025-11-06**|**BFM-Zero: A Promptable Behavioral Foundation Model for Humanoid Control Using Unsupervised Reinforcement Learning**|Guanya Shi Team|[2511.04131](http://arxiv.org/abs/2511.04131)|null|
|**2025-11-06**|**Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots**|Mingguo Zhao Team|[2511.03996](http://arxiv.org/abs/2511.03996)|**[link](https://humanoid-kick.github.io)**|
|**2025-11-05**|**OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single Panoramic Camera**|Kaiwei Wang Team|[2511.03571](http://arxiv.org/abs/2511.03571)|**[link](https://github.com/MasterHow/OneOcc)**|
|**2025-11-04**|**TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System**|C. Karen Liu Team|[2511.02832](http://arxiv.org/abs/2511.02832)|**[link](https://yanjieze.com/TWIST2)**|
|**2025-11-02**|**Heuristic Step Planning for Learning Dynamic Bipedal Locomotion: A Comparative Study of Model-Based and Model-Free Approaches**|Roman Gorbachev Team|[2511.00840](http://arxiv.org/abs/2511.00840)|null|
|**2025-10-31**|**EgoMI: Learning Active Vision and Whole-Body Manipulation from Egocentric Human Demonstrations**|Philipp Wu Team|[2511.00153](http://arxiv.org/abs/2511.00153)|null|
|**2025-10-31**|**Towards a Multi-Embodied Grasping Agent**|Gerhard Neumann Team|[2510.27420](http://arxiv.org/abs/2510.27420)|null|
|**2025-10-30**|**Cooperative Task Spaces for Multi-Arm Manipulation Control based on Similarity Transformations**|Sylvain Calinon Team|[2510.26362](http://arxiv.org/abs/2510.26362)|null|
|**2025-11-05**|**Thor: Towards Human-Level Whole-Body Reactions for Intense Contact-Rich Environments**|Shaqi Luo Team|[2510.26280](http://arxiv.org/abs/2510.26280)|null|
|**2025-11-01**|**Beyond the Uncanny Valley: A Mixed-Method Investigation of Anthropomorphism in Protective Responses to Robot Abuse**|Renkai Ma Team|[2510.26082](http://arxiv.org/abs/2510.26082)|null|
|**2025-10-28**|**A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation**|Kyung-Joong Kim Team|[2510.25725](http://arxiv.org/abs/2510.25725)|null|
|**2025-10-27**|**Awakening Facial Emotional Expressions in Human-Robot**|Jianwei Zhang Team|[2510.23059](http://arxiv.org/abs/2510.23059)|null|
|**2025-11-05**|**Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and Morphology for Fall Recovery**|Guiliang Liu Team|[2510.22336](http://arxiv.org/abs/2510.22336)|null|
|**2025-10-21**|**SLICE: SLO-Driven Scheduling for LLM Inference on Edge Computing Devices**|Yueyue Dai Team|[2510.18544](http://arxiv.org/abs/2510.18544)|null|
|**2025-10-20**|**Humanoid Goalkeeper: Learning from Position Conditioned Task-Motion Constraints**|Jiangmiao Pang Team|[2510.18002](http://arxiv.org/abs/2510.18002)|null|
|**2025-10-20**|**SoftMimic: Learning Compliant Whole-body Control from Examples**|Pulkit Agrawal Team|[2510.17792](http://arxiv.org/abs/2510.17792)|**[link](https://gmargo11.github.io/softmimic/)**|
|**2025-10-19**|**CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions**|Aaron D. Ames Team|[2510.14959](http://arxiv.org/abs/2510.14959)|null|
|**2025-10-17**|**From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance**|Chang Xu Team|[2510.14952](http://arxiv.org/abs/2510.14952)|null|
|**2025-10-16**|**Towards Adaptable Humanoid Control via Adaptive Motion Tracking**|Jiangmiao Pang Team|[2510.14454](http://arxiv.org/abs/2510.14454)|null|
|**2025-10-15**|**A Modular Object Detection System for Humanoid Robots Using YOLO**|Meng Cheng Lau Team|[2510.13625](http://arxiv.org/abs/2510.13625)|null|
|**2025-10-15**|**Development of an Intuitive GUI for Non-Expert Teleoperation of Humanoid Robots**|Meng Cheng Lau Team|[2510.13594](http://arxiv.org/abs/2510.13594)|null|
|**2025-10-14**|**PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing**|Yucong Wu Team|[2510.12346](http://arxiv.org/abs/2510.12346)|null|
|**2025-10-13**|**Ego-Vision World Model for Humanoid Contact Planning**|Koushil Sreenath Team|[2510.11682](http://arxiv.org/abs/2510.11682)|null|
|**2025-10-13**|**Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization**|Xiaobin Xiong Team|[2510.11539](http://arxiv.org/abs/2510.11539)|null|
|**2025-10-13**|**Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots**|Yao Su Team|[2510.11401](http://arxiv.org/abs/2510.11401)|null|
|**2025-10-13**|**DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation**|Zongqing Lu Team|[2510.11258](http://arxiv.org/abs/2510.11258)|null|
|**2025-10-13**|**PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System**|Jiangmiao Pang Team|[2510.11072](http://arxiv.org/abs/2510.11072)|**[link](https://why618188.github.io/physhsi/)**|
|**2025-10-12**|**Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion**|Mingguo Zhao Team|[2510.10851](http://arxiv.org/abs/2510.10851)|null|
|**2025-10-11**|**It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots**|Siheng Chen Team|[2510.10206](http://arxiv.org/abs/2510.10206)|null|
|**2025-10-10**|**Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks**|Zhicheng He Team|[2510.09786](http://arxiv.org/abs/2510.09786)|null|
|**2025-10-09**|**Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation**|Yue Wang Team|[2510.08807](http://arxiv.org/abs/2510.08807)|null|
|**2025-10-09**|**DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos**|Tsung-Wei Ke Team|[2510.08475](http://arxiv.org/abs/2510.08475)|**[link](https://embodiedai-ntu.github.io/dexman/index.html)**|
|**2025-10-09**|**Reliability of Single-Level Equality-Constrained Inverse Optimal Control**|Vincent Bonnet Team|[2510.08406](http://arxiv.org/abs/2510.08406)|null|
|**2025-10-15**|**Towards Proprioception-Aware Embodied Planning for Dual-Arm Humanoid Robots**|Zongqing Lu Team|[2510.07882](http://arxiv.org/abs/2510.07882)|null|
|**2025-10-10**|**DPL: Depth-only Perceptive Humanoid Locomotion via Realistic Depth Synthesis and Cross-Attention Terrain Reconstruction**|Qiang Zhang Team|[2510.07152](http://arxiv.org/abs/2510.07152)|null|
|**2025-10-07**|**A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling**|Shishir Kolathaya Team|[2510.05923](http://arxiv.org/abs/2510.05923)|null|
|**2025-10-06**|**Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot**|Abhishek Warrier Team|[2510.05001](http://arxiv.org/abs/2510.05001)|null|
|**2025-10-05**|**Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation**|Robert Griffin Team|[2510.04353](http://arxiv.org/abs/2510.04353)|null|
|**2025-10-03**|**LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy**|Michael C. Yip Team|[2510.03529](http://arxiv.org/abs/2510.03529)|null|
|**2025-10-03**|**Embracing Evolution: A Call for Body-Control Co-Design in Embodied Humanoid Robot**|Kui Jia Team|[2510.03081](http://arxiv.org/abs/2510.03081)|null|
|**2025-10-03**|**HumanoidExo: Scalable Whole-Body Humanoid Manipulation via Wearable Exoskeleton**|Yi Xu Team|[2510.03022](http://arxiv.org/abs/2510.03022)|null|
|**2025-10-02**|**Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking**|C. Karen Liu Team|[2510.02252](http://arxiv.org/abs/2510.02252)|null|
|**2025-10-02**|**Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control**|Tim Laue Team|[2510.02129](http://arxiv.org/abs/2510.02129)|null|
|**2025-10-02**|**Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots**|Peng Lu Team|[2510.01843](http://arxiv.org/abs/2510.01843)|null|
|**2025-09-30**|**Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning**|Ludovic Righetti Team|[2510.00329](http://arxiv.org/abs/2510.00329)|null|
|**2025-10-08**|**OmniRetarget: Interaction-Preserving Data Generation for Humanoid Whole-Body Loco-Manipulation and Scene Interaction**|Guanya Shi Team|[2509.26633](http://arxiv.org/abs/2509.26633)|**[link](https://omniretarget.github.io)**|
|**2025-09-30**|**ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm**|Katherine J. Kuchenbecker Team|[2509.26236](http://arxiv.org/abs/2509.26236)|null|
|**2025-09-30**|**Evolutionary Continuous Adaptive RL-Powered Co-Design for Humanoid Chin-Up Performance**|Frank Kirchner Team|[2509.26082](http://arxiv.org/abs/2509.26082)|null|
|**2025-10-06**|**CoTaP: Compliant Task Pipeline and Reinforcement Learning of Its Controller with Compliance Modulation**|Yoshihiko Nakamura Team|[2509.25443](http://arxiv.org/abs/2509.25443)|null|
|**2025-09-29**|**Stabilizing Humanoid Robot Trajectory Generation via Physics-Informed Learning and Control-Informed Steering**|Daniele Pucci Team|[2509.24697](http://arxiv.org/abs/2509.24697)|null|
|**2025-09-29**|**Game Theory to Study Cooperation in Human-Robot Mixed Groups: Exploring the Potential of the Public Good Game**|Alessandra Sciutti Team|[2509.24530](http://arxiv.org/abs/2509.24530)|null|
|**2025-09-29**|**Preference-Based Long-Horizon Robotic Stacking with Multimodal Large Language Models**|Sethu Vijayakumar Team|[2509.24163](http://arxiv.org/abs/2509.24163)|null|
|**2025-09-28**|**SIG-Chat: Spatial Intent-Guided Conversational Gesture Generation Involving How, When and Where**|Chuanchen Luo Team|[2509.23852](http://arxiv.org/abs/2509.23852)|null|
|**2025-09-25**|**SEEC: Stable End-Effector Control with Model-Enhanced Residual Learning for Humanoid Loco-Manipulation**|Ye Zhao Team|[2509.21231](http://arxiv.org/abs/2509.21231)|null|
|**2025-09-25**|**RuN: Residual Policy for Natural Humanoid Locomotion**|Yong Liu Team|[2509.20696](http://arxiv.org/abs/2509.20696)|null|
|**2025-09-24**|**Large Pre-Trained Models for Bimanual Manipulation in 3D**|David Meger Team|[2509.20579](http://arxiv.org/abs/2509.20579)|null|
|**2025-09-24**|**VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation**|Jiajun Wu Team|[2509.20322](http://arxiv.org/abs/2509.20322)|**[link](https://visualmimic.github.io)**|
|**2025-09-25**|**HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms**|Houde Liu Team|[2509.20263](http://arxiv.org/abs/2509.20263)|null|
|**2025-09-23**|**Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning**|Aaron D. Ames Team|[2509.19573](http://arxiv.org/abs/2509.19573)|null|
|**2025-09-23**|**RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots**|Aaron D. Ames Team|[2509.19545](http://arxiv.org/abs/2509.19545)|null|
|**2025-09-25**|**Residual Off-Policy RL for Finetuning Behavior Cloning Policies**|Anusha Nagabandi Team|[2509.19301](http://arxiv.org/abs/2509.19301)|**[link](https://residual-offpolicy-rl.github.io)**|
|**2025-09-27**|**HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos**|Guanya Shi Team|[2509.16757](http://arxiv.org/abs/2509.16757)|null|
|**2025-09-20**|**KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control**|Chenjia Bai Team|[2509.16638](http://arxiv.org/abs/2509.16638)|null|
|**2025-09-19**|**A Framework for Optimal Ankle Design of Humanoid Robots**|Daniele Pucci Team|[2509.16469](http://arxiv.org/abs/2509.16469)|null|
|**2025-09-19**|**A Matter of Height: The Impact of a Robotic Object on Human Compliance**|Hadas Erel Team|[2509.16032](http://arxiv.org/abs/2509.16032)|null|
|**2025-09-18**|**Implicit Kinodynamic Motion Retargeting for Human-to-humanoid Imitation Learning**|Haodong Zhang Team|[2509.15443](http://arxiv.org/abs/2509.15443)|null|
|**2025-09-18**|**CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids**|Daniele Pucci Team|[2509.14935](http://arxiv.org/abs/2509.14935)|null|
|**2025-09-18**|**RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI**|Tao Shen Team|[2509.14687](http://arxiv.org/abs/2509.14687)|null|
|**2025-09-23**|**Cybersecurity AI: Humanoid Robots as Attack Vectors**|Kevin Finisterre Team|[2509.14139](http://arxiv.org/abs/2509.14139)|null|
|**2025-09-17**|**The Cybersecurity of a Humanoid Robot**|Víctor Mayoral-Vilches Team|[2509.14096](http://arxiv.org/abs/2509.14096)|null|
|**2025-09-17**|**Behavior Foundation Model for Humanoid Robots**|Jiangmiao Pang Team|[2509.13780](http://arxiv.org/abs/2509.13780)|null|
|**2025-09-17**|**FSR-VLN: Fast and Slow Reasoning for Vision-Language Navigation with Hierarchical Multi-modal Scene Graph**|Zhizhong Su Team|[2509.13733](http://arxiv.org/abs/2509.13733)|null|
|**2025-09-16**|**Embracing Bulky Objects with Humanoid Robots: Whole-Body Manipulation with Reinforcement Learning**|Jun Ma Team|[2509.13534](http://arxiv.org/abs/2509.13534)|null|
|**2025-09-18**|**StageACT: Stage-Conditioned Imitation for Robust Humanoid Door Opening**|Shayegan Omidshafiei Team|[2509.13200](http://arxiv.org/abs/2509.13200)|null|
|**2025-09-14**|**Quantum deep reinforcement learning for humanoid robot navigation task**|Ahmed Biyabani Team|[2509.11388](http://arxiv.org/abs/2509.11388)|null|
|**2025-09-16**|**FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers**|Zhigong Song Team|[2509.11109](http://arxiv.org/abs/2509.11109)|null|
|**2025-09-16**|**Data-fused Model Predictive Control with Guarantees: Application to Flying Humanoid Robots**|Daniele Pucci Team|[2509.10353](http://arxiv.org/abs/2509.10353)|null|
|**2025-09-11**|**MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos**|Yuke Zhu Team|[2509.09769](http://arxiv.org/abs/2509.09769)|null|
|**2025-09-11**|**AGILOped: Agile Open-Source Humanoid Robot for Research**|Sven Behnke Team|[2509.09364](http://arxiv.org/abs/2509.09364)|null|
|**2025-09-09**|**Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning**|Changhyun Choi Team|[2509.08126](http://arxiv.org/abs/2509.08126)|null|
|**2025-09-09**|**Interactive Shaping of Granular Media Using Reinforcement Learning**|Maren Bennewitz Team|[2509.06469](http://arxiv.org/abs/2509.06469)|null|
|**2025-09-06**|**Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids**|Dennis W. Hong Team|[2509.05581](http://arxiv.org/abs/2509.05581)|null|
|**2025-09-08**|**Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots**|Aaron D. Ames Team|[2509.04722](http://arxiv.org/abs/2509.04722)|null|
|**2025-09-03**|**The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile Manipulation**|Georgia Chalvatzaki Team|[2509.03222](http://arxiv.org/abs/2509.03222)|null|
|**2025-09-01**|**ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training**|Dieter Fox Team|[2509.01819](http://arxiv.org/abs/2509.01819)|null|
|**2025-09-04**|**HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and Learning**|S. Shankar Sastry Team|[2508.21043](http://arxiv.org/abs/2508.21043)|null|
|**2025-09-16**|**Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework for Humanoid Beam Walking**|Shiwu Zhang Team|[2508.20661](http://arxiv.org/abs/2508.20661)|**[link](https://huangtc233.github.io/Traversing-the-Narrow-Path/)**|
|**2025-08-26**|**HuBE: Cross-Embodiment Human-like Behavior Execution for Humanoid Robots**|Guodong Guo Team|[2508.19002](http://arxiv.org/abs/2508.19002)|null|
|**2025-08-21**|**PriorFormer: A Transformer for Real-time Monocular 3D Human Pose Estimation with Versatile Geometric Priors**|Vincent Bonnet Team|[2508.18238](http://arxiv.org/abs/2508.18238)|null|
|**2025-09-01**|**SoK: Cybersecurity Assessment of Humanoid Ecosystem**|Yuval Elovici Team|[2508.17481](http://arxiv.org/abs/2508.17481)|null|
|**2025-08-20**|**LookOut: Real-World Humanoid Egocentric Navigation**|Leonidas J. Guibas Team|[2508.14466](http://arxiv.org/abs/2508.14466)|null|
|**2025-08-18**|**Scaling Whole-body Multi-contact Manipulation with Contact Optimization**|Sethu Vijayakumar Team|[2508.12980](http://arxiv.org/abs/2508.12980)|null|
|**2025-08-18**|**Foundation Model for Skeleton-Based Human Action Understanding**|Liang Wang Team|[2508.12586](http://arxiv.org/abs/2508.12586)|**[link](https://github.com/wengwanjiang/FoundSkelModel)**|
|**2025-08-27**|**Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids**|Shuran Song Team|[2508.12252](http://arxiv.org/abs/2508.12252)|null|
|**2025-08-17**|**Humanoid Motion Scripting with Postural Synergies**|Oussama Khatib Team|[2508.12184](http://arxiv.org/abs/2508.12184)|null|
|**2025-08-16**|**Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System**|Yanan Sui Team|[2508.11885](http://arxiv.org/abs/2508.11885)|null|
|**2025-08-16**|**From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics**|Dennis W. Hong Team|[2508.11884](http://arxiv.org/abs/2508.11884)|null|
|**2025-08-15**|**Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots**|Robert Griffin Team|[2508.11802](http://arxiv.org/abs/2508.11802)|null|
|**2025-08-15**|**A Comparative Study of Floating-Base Space Parameterizations for Agile Whole-Body Motion Planning**|Konstantinos Chatzilygeroudis Team|[2508.11520](http://arxiv.org/abs/2508.11520)|null|
|**2025-08-15**|**Learning Differentiable Reachability Maps for Optimization-based Humanoid Motion Generation**|Fumio Kanehiro Team|[2508.11275](http://arxiv.org/abs/2508.11275)|null|
|**2025-08-15**|**Geometry-Aware Predictive Safety Filters on Humanoids: From Poisson Safety Functions to CBF Constrained MPC**|Aaron D. Ames Team|[2508.11129](http://arxiv.org/abs/2508.11129)|null|
|**2025-08-14**|**MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion**|Yanjie Li Team|[2508.10423](http://arxiv.org/abs/2508.10423)|null|
|**2025-08-13**|**GBC: Generalized Behavior-Cloning Framework for Whole-Body Humanoid Imitation**|Jun-Guo Lu Team|[2508.09960](http://arxiv.org/abs/2508.09960)|null|
|**2025-08-11**|**PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF**|Lorenzo Natale Team|[2508.07945](http://arxiv.org/abs/2508.07945)|null|
|**2025-08-11**|**End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy**|Junwei Liang Team|[2508.07611](http://arxiv.org/abs/2508.07611)|null|
|**2025-08-09**|**Learning a Vision-Based Footstep Planner for Hierarchical Walking Control**|Michael Posa Team|[2508.06779](http://arxiv.org/abs/2508.06779)|null|
|**2025-08-07**|**Examining the legibility of humanoid robot arm movements in a pointing task**|Igor Farkaš Team|[2508.05104](http://arxiv.org/abs/2508.05104)|null|
|**2025-08-06**|**INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM**|Nikos Tsagarakis Team|[2508.04931](http://arxiv.org/abs/2508.04931)|**[link](https://robo-intention.github.io)**|
|**2025-08-06**|**On the causality between affective impact and coordinated human-robot reactions**|Kasper Støy Team|[2508.04834](http://arxiv.org/abs/2508.04834)|null|
|**2025-08-06**|**Binaural Sound Event Localization and Detection Neural Network based on HRTF Localization Cues for Humanoid Robots**|Gyeong-Tae Lee Team|[2508.04333](http://arxiv.org/abs/2508.04333)|null|
|**2025-08-08**|**Would you let a humanoid play storytelling with your child? A usability study on LLM-powered narrative Human-Robot Interaction**|Agnieszka Wykowska Team|[2508.02505](http://arxiv.org/abs/2508.02505)|null|
|**2025-08-04**|**Towards Immersive Human-X Interaction: A Real-Time Framework for Physically Plausible Motion Synthesis**|Jingya Wang Team|[2508.02106](http://arxiv.org/abs/2508.02106)|null|
|**2025-08-02**|**Coordinated Humanoid Robot Locomotion with Symmetry Equivariant Reinforcement Learning Policy**|Yue Gao Team|[2508.01247](http://arxiv.org/abs/2508.01247)|null|
|**2025-08-01**|**A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot**|Rong Xiong Team|[2508.00362](http://arxiv.org/abs/2508.00362)|null|
|**2025-08-01**|**TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots**|Rong Xiong Team|[2508.00355](http://arxiv.org/abs/2508.00355)|null|
|**2025-07-31**|**CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System**|Joohyung Kim Team|[2508.00162](http://arxiv.org/abs/2508.00162)|null|
|**2025-07-31**|**The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking**|Taihú Pire Team|[2508.00088](http://arxiv.org/abs/2508.00088)|null|
|**2025-07-28**|**Binaural Sound Event Localization and Detection based on HRTF Cues for Humanoid Robots**|Yong-Hwa Park Team|[2507.20530](http://arxiv.org/abs/2507.20530)|null|
|**2025-07-28**|**LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models**|Yusuke Iwasawa Team|[2507.20509](http://arxiv.org/abs/2507.20509)|null|
|**2025-07-29**|**Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots**|Qiang Zhang Team|[2507.20217](http://arxiv.org/abs/2507.20217)|null|
|**2025-07-27**|**LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks**|Xiaoshuang Shi Team|[2507.20174](http://arxiv.org/abs/2507.20174)|null|
|**2025-07-25**|**How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version**|Giuseppe Palestra Team|[2507.19335](http://arxiv.org/abs/2507.19335)|null|
|**2025-07-24**|**Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces**|Christian Ott Team|[2507.18502](http://arxiv.org/abs/2507.18502)|**[link](https://youtu.be/Nfm50ycz-FU)**|
|**2025-07-22**|**Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors and a Single Plane**|Florent Lamiraux Team|[2507.16369](http://arxiv.org/abs/2507.16369)|null|
|**2025-07-20**|**Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture**|Lisa Dargasz Team|[2507.15895](http://arxiv.org/abs/2507.15895)|null|
|**2025-07-21**|**EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation**|Rong Xiong Team|[2507.15649](http://arxiv.org/abs/2507.15649)|null|
|**2025-07-16**|**Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming**|Loris Roveda Team|[2507.11498](http://arxiv.org/abs/2507.11498)|null|
|**2025-07-15**|**From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League**|Shohei Yasuda Team|[2507.11402](http://arxiv.org/abs/2507.11402)|null|
|**2025-07-14**|**Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots**|Daniele Pucci Team|[2507.10105](http://arxiv.org/abs/2507.10105)|null|
|**2025-07-11**|**Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots**|Yue Gao Team|[2507.08303](http://arxiv.org/abs/2507.08303)|null|
|**2025-07-10**|**UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots**|Weinan Zhang Team|[2507.07356](http://arxiv.org/abs/2507.07356)|null|
|**2025-07-09**|**ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation**|Zongwu Xie Team|[2507.06905](http://arxiv.org/abs/2507.06905)|null|
|**2025-07-08**|**Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction**|Alessio Del Bue Team|[2507.06404](http://arxiv.org/abs/2507.06404)|null|
|**2025-07-05**|**Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning**|Sangbae Kim Team|[2507.04140](http://arxiv.org/abs/2507.04140)|null|
|**2025-07-01**|**HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning**|Chenjia Bai Team|[2507.00833](http://arxiv.org/abs/2507.00833)|null|
|**2025-06-30**|**Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation**|Dennis Hong Team|[2507.00273](http://arxiv.org/abs/2507.00273)|null|
|**2025-07-02**|**DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover**|Yuexin Ma Team|[2506.23152](http://arxiv.org/abs/2506.23152)|null|
|**2025-06-29**|**Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots**|Yue Gao Team|[2506.23125](http://arxiv.org/abs/2506.23125)|null|
|**2025-07-10**|**Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation**|Navid Azizan Team|[2506.22827](http://arxiv.org/abs/2506.22827)|null|
|**2025-06-20**|**Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity**|Matej Hoffmann Team|[2506.22473](http://arxiv.org/abs/2506.22473)|null|
|**2025-07-14**|**Ark: An Open-source Python-based Framework for Robot Learning**|Haitham Bou-Ammar Team|[2506.21628](http://arxiv.org/abs/2506.21628)|null|
|**2025-07-18**|**A Survey of Behavior Foundation Model: Next-Generation Whole-Body Control System of Humanoid Robots**|Wenjun Zeng Team|[2506.20487](http://arxiv.org/abs/2506.20487)|null|
|**2025-06-19**|**DualTHOR: A Dual-Arm Humanoid Simulation Platform for Contingency-Aware Planning**|Zongqing Lu Team|[2506.16012](http://arxiv.org/abs/2506.16012)|**[link](https://github.com/ds199895/dualthor)**|
|**2025-06-18**|**TACT: Humanoid Whole-body Contact Manipulation through Deep Imitation Learning with Tactile Modality**|Eiichi Yoshida Team|[2506.15146](http://arxiv.org/abs/2506.15146)|null|
|**2025-06-18**|**Booster Gym: An End-to-End Reinforcement Learning Framework for Humanoid Robot Locomotion**|Mingguo Zhao Team|[2506.15132](http://arxiv.org/abs/2506.15132)|**[link](https://github.com/BoosterRobotics/booster_gym)**|
|**2025-06-17**|**GMT: General Motion Tracking for Humanoid Whole-Body Control**|Xiaolong Wang Team|[2506.14770](http://arxiv.org/abs/2506.14770)|null|
|**2025-06-17**|**Whole-Body Control Framework for Humanoid Robots with Heavy Limbs: A Model-Based Approach**|Yun-Hui Liu Team|[2506.14278](http://arxiv.org/abs/2506.14278)|null|
|**2025-06-15**|**KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills**|Xuelong Li Team|[2506.12851](http://arxiv.org/abs/2506.12851)|null|
|**2025-06-19**|**From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots**|Zongqing Lu Team|[2506.12779](http://arxiv.org/abs/2506.12779)|null|
|**2025-06-15**|**RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control**|Zongqing Lu Team|[2506.12769](http://arxiv.org/abs/2506.12769)|null|
|**2025-06-14**|**Explosive Output to Enhance Jumping Ability: A Variable Reduction Ratio Design Paradigm for Humanoid Robots Knee Joint**|Qiang Huang Team|[2506.12314](http://arxiv.org/abs/2506.12314)|null|
|**2025-06-13**|**mimic-one: a Scalable Model Recipe for General Purpose Robot Dexterity**|Robert K. Katzschmann Team|[2506.11916](http://arxiv.org/abs/2506.11916)|null|
|**2025-06-11**|**Exploring EEG Responses during Observation of Actions Performed by Human Actor and Humanoid Robot**|Michelle J. Johnson Team|[2506.10170](http://arxiv.org/abs/2506.10170)|null|
|**2025-06-11**|**Locomotion on Constrained Footholds via Layered Architectures and Model Predictive Control**|Aaron D. Ames Team|[2506.09979](http://arxiv.org/abs/2506.09979)|null|
|**2025-06-11**|**Attention-Based Map Encoding for Learning Generalized Legged Locomotion**|Marco Hutter Team|[2506.09588](http://arxiv.org/abs/2506.09588)|null|
|**2025-06-11**|**Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations**|Yanan Sui Team|[2506.09383](http://arxiv.org/abs/2506.09383)|null|
|**2025-06-11**|**SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation via Skill Blending**|Yue Wang Team|[2506.09366](http://arxiv.org/abs/2506.09366)|**[link](https://github.com/Humanoid-SkillBlender/SkillBlender)**|
|**2025-06-10**|**Fast Estimation of Globally Optimal Independent Contact Regions for Robust Grasping and Manipulation**|Nancy S. Pollard Team|[2506.08856](http://arxiv.org/abs/2506.08856)|null|
|**2025-06-12**|**MoRE: Mixture of Residual Experts for Humanoid Lifelike Gaits Learning on Complex Terrains**|Xuelong Li Team|[2506.08840](http://arxiv.org/abs/2506.08840)|null|
|**2025-06-10**|**Periodic Bipedal Gait Learning Using Reward Composition Based on a Novel Gait Planner for Humanoid Robots**|Lijun Zhu Team|[2506.08416](http://arxiv.org/abs/2506.08416)|null|
|**2025-06-05**|**Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline**|Qijun Chen Team|[2506.05117](http://arxiv.org/abs/2506.05117)|**[link](https://github.com/southwestCat/text2motion-nao)**|
|**2025-06-04**|**Phase-based Nonlinear Model Predictive Control for Humanoid Walking Stabilization with Single and Double Support Time Adjustments**|Jaeheung Park Team|[2506.03856](http://arxiv.org/abs/2506.03856)|null|
|**2025-06-03**|**AURA: Agentic Upskilling via Reinforced Abstractions**|Dennis Hong Team|[2506.02507](http://arxiv.org/abs/2506.02507)|null|
|**2025-06-02**|**Reinforcement Learning with Data Bootstrapping for Dynamic Subgoal Pursuit in Humanoid Robot Navigation**|Ayonga Hereid Team|[2506.02206](http://arxiv.org/abs/2506.02206)|null|
|**2025-06-02**|**Learning with pyCub: A New Simulation and Exercise Framework for Humanoid Robotics**|Matej Hoffmann Team|[2506.01756](http://arxiv.org/abs/2506.01756)|null|
|**2025-06-05**|**Hierarchical Intention-Aware Expressive Motion Generation for Humanoid Robots**|Chengxu Zhou Team|[2506.01563](http://arxiv.org/abs/2506.01563)|null|
|**2025-06-01**|**Humanoid World Models: Open World Foundation Models for Humanoid Robotics**|Mohammad Al-Sharman Team|[2506.01182](http://arxiv.org/abs/2506.01182)|null|
|**2025-06-01**|**iRonCub 3: The Jet-Powered Flying Humanoid Robot**|Daniele Pucci Team|[2506.01125](http://arxiv.org/abs/2506.01125)|null|
|**2025-05-30**|**Learning Aerodynamics for the Control of Flying Humanoid Robots**|Daniele Pucci Team|[2506.00305](http://arxiv.org/abs/2506.00305)|null|
|**2025-05-30**|**Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey**|Rania Rayyes Team|[2506.00098](http://arxiv.org/abs/2506.00098)|null|
|**2025-06-05**|**SignBot: Learning Human-to-Humanoid Sign Language Interaction**|Guiliang Liu Team|[2505.24266](http://arxiv.org/abs/2505.24266)|null|
|**2025-05-30**|**Humanoid Loco-Manipulations Pattern Generation and Stabilization Control**|Abderrahmane Kheddar Team|[2505.24116](http://arxiv.org/abs/2505.24116)|null|
|**2025-05-29**|**Humanoid Loco-manipulation Planning based on Graph Search and Reachability Maps**|Abderrahmane Kheddar Team|[2505.23505](http://arxiv.org/abs/2505.23505)|null|
|**2025-05-29**|**Centroidal Trajectory Generation and Stabilization based on Preview Control for Humanoid Multi-contact Motion**|Fumio Kanehiro Team|[2505.23499](http://arxiv.org/abs/2505.23499)|**[link](https://github.com/isri-aist/MultiContactController)**|
|**2025-06-01**|**FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control**|Pieter Abbeel Team|[2505.22642](http://arxiv.org/abs/2505.22642)|null|
|**2025-05-27**|**Learning Unified Force and Position Control for Legged Loco-Manipulation**|Siyuan Huang Team|[2505.20829](http://arxiv.org/abs/2505.20829)|null|
|**2025-05-27**|**Gait-Conditioned Reinforcement Learning with Multi-Phase Curriculum for Humanoid Locomotion**|CHengxu Zhou Team|[2505.20619](http://arxiv.org/abs/2505.20619)|null|
|**2025-05-26**|**Integrating emotional intelligence, memory architecture, and gestures to achieve empathetic humanoid robot interaction in an educational setting**|Paul Craig Team|[2505.19803](http://arxiv.org/abs/2505.19803)|null|
|**2025-05-26**|**Extremum Flow Matching for Offline Goal Conditioned Reinforcement Learning**|Jean-Baptiste Mouret Team|[2505.19717](http://arxiv.org/abs/2505.19717)|null|
|**2025-05-26**|**Whole-body Multi-contact Motion Control for Humanoid Robots Based on Distributed Tactile Sensors**|Eiichi Yoshida Team|[2505.19580](http://arxiv.org/abs/2505.19580)|**[link](https://github.com/isri-aist/MultiContactController)**|
|**2025-05-26**|**Heavy lifting tasks via haptic teleoperation of a wheeled humanoid**|Joao Ramos Team|[2505.19530](http://arxiv.org/abs/2505.19530)|null|
|**2025-05-26**|**SMAP: Self-supervised Motion Adaptation for Physically Plausible Humanoid Whole-body Control**|Junting Dong Team|[2505.19463](http://arxiv.org/abs/2505.19463)|null|
|**2025-05-25**|**Towards Humanoid Robot Autonomy: A Dynamic Architecture Integrating Continuous thought Machines (CTM) and Model Context Protocol (MCP)**|Libo Wang Team|[2505.19339](http://arxiv.org/abs/2505.19339)|**[link](https://github.com/brucewang123456789/GeniusTrail)**|
|**2025-05-25**|**Staircase Recognition and Location Based on Polarization Vision**|Zhiying Tan Team|[2505.19026](http://arxiv.org/abs/2505.19026)|null|
|**2025-05-23**|**DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation**|Ruqi Huang Team|[2505.18078](http://arxiv.org/abs/2505.18078)|null|
|**2025-08-11**|**Unified Multi-Rate Model Predictive Control for a Jet-Powered Humanoid Robot**|Daniele Pucci Team|[2505.16478](http://arxiv.org/abs/2505.16478)|null|
|**2025-05-19**|**TD-GRPC: Temporal Difference Learning with Group Relative Policy Constraint for Humanoid Locomotion**|Minh Nhat Vu Team|[2505.13549](http://arxiv.org/abs/2505.13549)|null|
|**2025-05-19**|**DreamGen: Unlocking Generalization in Robot Learning through Neural Trajectories**|Linxi Fan Team|[2505.12705](http://arxiv.org/abs/2505.12705)|null|
|**2025-05-19**|**Dribble Master: Learning Agile Humanoid Dribbling Through Legged Locomotion**|Qi Wu Team|[2505.12679](http://arxiv.org/abs/2505.12679)|null|
|**2025-05-16**|**Bracing for Impact: Robust Humanoid Push Recovery and Locomotion with Reduced Order Models**|Aaron D. Ames Team|[2505.11495](http://arxiv.org/abs/2505.11495)|null|
|**2025-05-16**|**X2C: A Dataset Featuring Nuanced Facial Expressions for Realistic Humanoid Imitation**|Xiaohan Yu Team|[2505.11146](http://arxiv.org/abs/2505.11146)|**[link](https://github.com/lipzh5/x2cnet)**|
|**2025-05-15**|**NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance**|Jiangmiao Pang Team|[2505.08712](http://arxiv.org/abs/2505.08712)|null|
|**2025-05-13**|**Rethink Repeatable Measures of Robot Performance with Statistical Query**|Dylan Khor Team|[2505.08216](http://arxiv.org/abs/2505.08216)|null|
|**2025-05-14**|**Neural Brain: A Neuroscience-inspired Framework for Embodied Agents**|Lin Wang Team|[2505.07634](http://arxiv.org/abs/2505.07634)|**[link](https://github.com/CNJianLiu/Neural-Brain-for-Embodied-Agents)**|
|**2025-05-12**|**HuB: Learning Extreme Humanoid Balance**|Yang Gao Team|[2505.07294](http://arxiv.org/abs/2505.07294)|null|
|**2025-05-11**|**Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson's Equation**|Aaron D. Ames Team|[2505.06794](http://arxiv.org/abs/2505.06794)|null|
|**2025-05-10**|**JAEGER: Dual-Level Humanoid Whole-Body Controller**|Zongqing Lu Team|[2505.06584](http://arxiv.org/abs/2505.06584)|null|
|**2025-05-09**|**Let Humanoids Hike! Integrative Skill Development on Complex Trails**|Stella X. Yu Team|[2505.06218](http://arxiv.org/abs/2505.06218)|null|
|**2025-05-09**|**Safe-EF: Error Feedback for Nonsmooth Constrained Optimization**|Ilyas Fatkhullin Team|[2505.06053](http://arxiv.org/abs/2505.06053)|null|
|**2025-05-09**|**Human-Robot Collaboration for the Remote Control of Mobile Humanoid Robots with Torso-Arm Coordination**|Zhi Li Team|[2505.05773](http://arxiv.org/abs/2505.05773)|null|
|**2025-05-07**|**Vision-Language-Action Models: Concepts, Progress, Applications and Challenges**|Manoj Karkee Team|[2505.04769](http://arxiv.org/abs/2505.04769)|null|
|**2025-05-06**|**AMO: Adaptive Motion Optimization for Hyper-Dexterous Humanoid Whole-Body Control**|Xiaolong Wang Team|[2505.03738](http://arxiv.org/abs/2505.03738)|null|
|**2025-05-13**|**Visual Imitation Enables Contextual Humanoid Control**|Angjoo Kanazawa Team|[2505.03729](http://arxiv.org/abs/2505.03729)|null|
|**2025-05-05**|**TWIST: Teleoperated Whole-Body Imitation System**|C. Karen Liu Team|[2505.02833](http://arxiv.org/abs/2505.02833)|null|
|**2025-04-30**|**LangWBC: Language-directed Humanoid Whole-Body Control via End-to-end Learning**|Koushil Sreenath Team|[2504.21738](http://arxiv.org/abs/2504.21738)|null|
|**2025-04-29**|**SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings**|Jianwei Zhang Team|[2504.20808](http://arxiv.org/abs/2504.20808)|null|
|**2025-04-27**|**Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems**|Jairaj Singh Shaktawat Team|[2504.20109](http://arxiv.org/abs/2504.20109)|null|
|**2025-04-24**|**Demonstrating Berkeley Humanoid Lite: An Open-source, Accessible, and Customizable 3D-printed Humanoid Robot**|Koushil Sreenath Team|[2504.17249](http://arxiv.org/abs/2504.17249)|null|
|**2025-04-20**|**ExFace: Expressive Facial Control for Humanoid Robots with Diffusion Transformers and Bootstrap Training**|Jiahao Chen Team|[2504.14477](http://arxiv.org/abs/2504.14477)|null|
|**2025-04-19**|**Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning**|Xuelong Li Team|[2504.14305](http://arxiv.org/abs/2504.14305)|null|
|**2025-04-18**|**Robust Humanoid Walking on Compliant and Uneven Terrain with Deep Reinforcement Learning**|Fumio Kanehiro Team|[2504.13619](http://arxiv.org/abs/2504.13619)|**[link](https://github.com/rohanpsingh/learninghumanoidwalking)**|
|**2025-04-16**|**EmoACT: a Framework to Embed Emotions into Artificial Agents Based on Affect Control Theory**|Carmine Tommaso Recchiuto Team|[2504.12125](http://arxiv.org/abs/2504.12125)|null|
|**2025-04-14**|**Teacher Motion Priors: Enhancing Robot Locomotion over Challenging Terrain**|Zhengtao Zhang Team|[2504.10390](http://arxiv.org/abs/2504.10390)|null|
|**2025-04-14**|**PreCi: Pretraining and Continual Improvement of Humanoid Locomotion via Model-Assumption-Based Regularization**|Sehoon Ha Team|[2504.09833](http://arxiv.org/abs/2504.09833)|null|
|**2025-04-13**|**Embodied Chain of Action Reasoning with Multi-Modal Foundation Model for Humanoid Loco-manipulation**|Yi Fang Team|[2504.09532](http://arxiv.org/abs/2504.09532)|null|
|**2025-04-11**|**Spectral Normalization for Lipschitz-Constrained Policies on Learning Humanoid Locomotion**|Jaeheung Park Team|[2504.08246](http://arxiv.org/abs/2504.08246)|null|
|**2025-04-07**|**MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond**|Xun Cao Team|[2504.05046](http://arxiv.org/abs/2504.05046)|null|
|**2025-04-07**|**A High-Force Gripper with Embedded Multimodal Sensing for Powerful and Perception Driven Grasping**|Nikos G. Tsagarakis Team|[2504.04970](http://arxiv.org/abs/2504.04970)|null|
|**2025-04-06**|**Public speech recognition transcripts as a configuring parameter**|Christian Licoppe Team|[2504.04488](http://arxiv.org/abs/2504.04488)|null|
|**2025-04-02**|**The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction**|Matthew K. X. J Pan Team|[2504.01260](http://arxiv.org/abs/2504.01260)|null|
|**2025-04-01**|**Extended Hybrid Zero Dynamics for Bipedal Walking of the Knee-less Robot SLIDER**|Petar Kormushev Team|[2504.01165](http://arxiv.org/abs/2504.01165)|null|
|**2025-04-11**|**Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs**|Masaya Kinoshita Team|[2504.00614](http://arxiv.org/abs/2504.00614)|null|
|**2025-03-30**|**Exploring GPT-4 for Robotic Agent Strategy with Real-Time State Feedback and a Reactive Behaviour Framework**|Ysobel Sims Team|[2503.23601](http://arxiv.org/abs/2503.23601)|null|
|**2025-03-28**|**Control of Humanoid Robots with Parallel Mechanisms using Kinematic Actuation Models**|Nicolas Mansard Team|[2503.22459](http://arxiv.org/abs/2503.22459)|null|
|**2025-03-28**|**FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation**|Debin Zhao Team|[2503.22249](http://arxiv.org/abs/2503.22249)|null|
|**2025-03-27**|**OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation**|Wanting Li Team|[2503.21257](http://arxiv.org/abs/2503.21257)|null|
|**2025-03-26**|**Anti Robot Speciesism**|Miklos Sarvary Team|[2503.20842](http://arxiv.org/abs/2503.20842)|null|
|**2025-03-25**|**Can Vision-Language Models Answer Face to Face Questions in the Real-World?**|Roland Memisevic Team|[2503.19356](http://arxiv.org/abs/2503.19356)|null|
|**2025-03-19**|**StyleLoco: Generative Adversarial Distillation for Natural Humanoid Robot Locomotion**|Siyuan Huang Team|[2503.15082](http://arxiv.org/abs/2503.15082)|null|
|**2025-03-27**|**GR00T N1: An Open Foundation Model for Generalist Humanoid Robots**|Yuke Zhu Team|[2503.14734](http://arxiv.org/abs/2503.14734)|null|
|**2025-03-24**|**Humanoid Policy ~ Human Policy**|Xiaolong Wang Team|[2503.13441](http://arxiv.org/abs/2503.13441)|null|
|**2025-03-17**|**Humanoids in Hospitals: A Technical Study of Humanoid Surrogates for Dexterous Medical Interventions**|Michael Yip Team|[2503.12725](http://arxiv.org/abs/2503.12725)|null|
|**2025-03-16**|**Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills**|Zongqing Lu Team|[2503.12533](http://arxiv.org/abs/2503.12533)|null|
|**2025-03-14**|**Fast and Robust Localization for Humanoid Soccer Robot via Iterative Landmark Matching**|Dennis W. Hong Team|[2503.11020](http://arxiv.org/abs/2503.11020)|null|
|**2025-03-13**|**NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models**|Michael Black Team|[2503.10626](http://arxiv.org/abs/2503.10626)|null|
|**2025-03-13**|**NuExo: A Wearable Exoskeleton Covering all Upper Limb ROM for Outdoor Data Collection and Teleoperation of Humanoid Robots**|Huimin Lu Team|[2503.10554](http://arxiv.org/abs/2503.10554)|null|
|**2025-03-12**|**Natural Humanoid Robot Locomotion with Generative Motion Prior**|Rong Xiong Team|[2503.09015](http://arxiv.org/abs/2503.09015)|null|
|**2025-03-13**|**HumanoidPano: Hybrid Spherical Panoramic-LiDAR Cross-Modal Perception for Humanoid Robots**|Renjing Xu Team|[2503.09010](http://arxiv.org/abs/2503.09010)|null|
|**2025-03-11**|**LiPS: Large-Scale Humanoid Robot Reinforcement Learning with Parallel-Series Structures**|Renjing Xu Team|[2503.08349](http://arxiv.org/abs/2503.08349)|null|
|**2025-04-29**|**Learning Getting-Up Policies for Real-World Humanoid Robots**|Saurabh Gupta Team|[2502.12152](http://arxiv.org/abs/2502.12152)|**[link](https://humanoid-getup.github.io/)**|
|**2024-10-17**|**Harmon: Whole-Body Motion Generation of Humanoid Robots from Language Descriptions**|Yuke Zhu Team|[2410.12773](http://arxiv.org/abs/2410.12773)|**[link](https://ut-austin-rpl.github.io/Harmon/)**|
|**2023-12-29**|**How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots**|Hannes Hartenstein Team|[2312.08820](http://arxiv.org/abs/2312.08820)|**[link](https://dl.acm.org/doi/abs/10.1145/3589608.3595078)**|
|**2022-11-28**|**Optimization of Humanoid Robot Designs for Human-Robot Ergonomic Payload Lifting**|Daniele Pucci Team|[2211.13503](http://arxiv.org/abs/2211.13503)|null|
|**2022-10-20**|**Dialogue system with humanoid robot**|Naoki Igo Team|[2210.10151](http://arxiv.org/abs/2210.10151)|null|
|**2021-04-20**|**The MIT Humanoid Robot: Design, Motion Planning, and Control For Acrobatic Behaviors**|Sangbae Kim Team|[2104.09025](http://arxiv.org/abs/2104.09025)|null|
|**2019-09-24**|**Whole-Body Geometric Retargeting for Humanoid Robots**|Daniele Pucci Team|[1909.10080](http://arxiv.org/abs/1909.10080)|null|
|**2019-09-06**|**NimbRo Robots Winning RoboCup 2018 Humanoid AdultSize Soccer Competitions**|Sven Behnke Team|[1909.02385](http://arxiv.org/abs/1909.02385)|null|
|**2018-10-22**|**NimbRo-OP2X: Adult-sized Open-source 3D Printed Humanoid Robot**|Sven Behnke Team|[1810.08395](http://arxiv.org/abs/1810.08395)|null|
|**2018-10-22**|**Online Balanced Motion Generation for Humanoid Robots**|Sven Behnke Team|[1810.08388](http://arxiv.org/abs/1810.08388)|null|
|**2018-10-01**|**NimbRo-OP2: Grown-up 3D Printed Open Humanoid Platform for Research**|Sven Behnke Team|[1809.11144](http://arxiv.org/abs/1809.11144)|null|
|**2018-10-01**|**A ROS-based Software Framework for the NimbRo-OP Humanoid Open Platform**|Sven Behnke Team|[1809.11051](http://arxiv.org/abs/1809.11051)|null|
|**2017-01-11**|**Automatic Gain Tuning of a Momentum Based Balancing Controller for Humanoid Robots**|Francesco Nori Team|[1610.02849](http://arxiv.org/abs/1610.02849)|null|
|**2017-07-18**|**Walking of the iCub humanoid robot in different scenarios: implementation and performance analysis**|Katja Mombaur Team|[1607.08525](http://arxiv.org/abs/1607.08525)|null|
|**2017-01-16**|**Walking on Partial Footholds Including Line Contacts with the Humanoid Robot Atlas**|Jerry Pratt Team|[1607.08089](http://arxiv.org/abs/1607.08089)|null|
|**2016-07-19**|**Design and implementation of computational platform for social-humanoid robot Lumen as an exhibition guide in Electrical Engineering Days 2015**|Ary Setijadi Prihatmanto Team|[1607.04763](http://arxiv.org/abs/1607.04763)|null|
|**2016-11-18**|**Gaze Stabilization for Humanoid Robots: a Comprehensive Framework**|Lorenzo Natale Team|[1411.3525](http://arxiv.org/abs/1411.3525)|null|

## Dexterous

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks**|Roland Memisevic Team|[2602.21013](http://arxiv.org/abs/2602.21013)|null|
|**2026-02-24**|**Grasp to Act: Dexterous Grasping for Tool Use in Dynamic Settings**|Wenzhen Yuan Team|[2602.20466](http://arxiv.org/abs/2602.20466)|**[link](https://grasp2act.github.io/)**|
|**2026-02-21**|**RoboCurate: Harnessing Diversity with Action-Verified Neural Trajectory for Robot Learning**|Jinwoo Shin Team|[2602.18742](http://arxiv.org/abs/2602.18742)|**[link](https://seungkukim.github.io/robocurate/)**|
|**2026-02-20**|**Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control**|Gordon Wetzstein Team|[2602.18422](http://arxiv.org/abs/2602.18422)|**[link](https://codeysun.github.io/generated-reality)**|
|**2026-02-20**|**WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection**|Dennis W. Hong Team|[2602.17908](http://arxiv.org/abs/2602.17908)|null|
|**2026-02-18**|**One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation**|Mingyu Ding Team|[2602.16712](http://arxiv.org/abs/2602.16712)|**[link](https://zhenyuwei2003.github.io/OHRA/)**|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|null|
|**2026-02-17**|**Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation**|Shubham Tulsiani Team|[2602.15828](http://arxiv.org/abs/2602.15828)|**[link](https://dex4d.github.io/)**|
|**2026-02-15**|**Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects**|Gangshan Jing Team|[2602.14104](http://arxiv.org/abs/2602.14104)|**[link](https://www.youtube.com/watch?v=kcf9dVW0Dpo)**|
|**2026-02-13**|**FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**|Xingxing Zuo Team|[2602.13444](http://arxiv.org/abs/2602.13444)|**[link](https://huajian-zeng.github.io/projects/flowhoi/)**|
|**2026-02-10**|**DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos**|Jiangmiao Pang Team|[2602.10105](http://arxiv.org/abs/2602.10105)|null|
|**2026-02-10**|**AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception**|Di Hu Team|[2602.09617](http://arxiv.org/abs/2602.09617)|null|
|**2026-02-10**|**Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows**|Robert K. Katzschmann Team|[2602.09580](http://arxiv.org/abs/2602.09580)|null|
|**2026-02-10**|**Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes**|Glen Chou Team|[2602.09368](http://arxiv.org/abs/2602.09368)|null|
|**2026-02-11**|**Dexterous Manipulation Policies from RGB Human Videos via 3D Hand-Object Trajectory Reconstruction**|Jeffrey Ichnowski Team|[2602.09013](http://arxiv.org/abs/2602.09013)|null|
|**2026-02-09**|**Mimic Intent, Not Just Trajectories**|Panpan Cai Team|[2602.08602](http://arxiv.org/abs/2602.08602)|null|
|**2026-02-09**|**DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer**|Renjing Xu Team|[2602.08278](http://arxiv.org/abs/2602.08278)|null|
|**2026-02-10**|**Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity**|Harish Ravichandar Team|[2602.07413](http://arxiv.org/abs/2602.07413)|**[link](https://github.com/GT-STAR-Lab/K-UBM/)**|
|**2026-02-05**|**DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter**|Zhenguo Sun Team|[2602.05513](http://arxiv.org/abs/2602.05513)|null|
|**2026-02-05**|**TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**|Shigeki Sugano Team|[2602.05468](http://arxiv.org/abs/2602.05468)|null|
|**2026-02-07**|**RoboPaint: From Human Demonstration to Any Robot and Any View**|Zhengxue Cheng Team|[2602.05325](http://arxiv.org/abs/2602.05325)|null|
|**2026-02-05**|**MobileManiBench: Simplifying Model Verification for Mobile Manipulation**|Baining Guo Team|[2602.05233](http://arxiv.org/abs/2602.05233)|null|
|**2026-02-04**|**AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**|Chunhua Shen Team|[2602.04672](http://arxiv.org/abs/2602.04672)|null|
|**2026-02-03**|**CRL-VLA: Continual Vision-Language-Action Learning**|Chao Huang Team|[2602.03445](http://arxiv.org/abs/2602.03445)|null|
|**2026-02-02**|**FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**|Haiyue Zhu Team|[2602.02142](http://arxiv.org/abs/2602.02142)|null|
|**2026-02-05**|**RFS: Reinforcement Learning with Residual Flow Steering for Dexterous Manipulation**|Abhishek Gupta Team|[2602.01789](http://arxiv.org/abs/2602.01789)|null|
|**2026-02-01**|**A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**|Jose Barreiros Team|[2602.01067](http://arxiv.org/abs/2602.01067)|null|
|**2026-01-29**|**DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching**|Shuo Wang Team|[2601.21474](http://arxiv.org/abs/2601.21474)|null|
|**2026-01-26**|**Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization**|Jongwoo Lim Team|[2601.18121](http://arxiv.org/abs/2601.18121)|null|
|**2026-01-24**|**PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**|Hesheng Wang Team|[2601.17440](http://arxiv.org/abs/2601.17440)|null|
|**2026-01-31**|**CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes**|Hao Dong Team|[2601.15039](http://arxiv.org/abs/2601.15039)|null|
|**2026-01-23**|**Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation**|Wanxin Jin Team|[2601.10930](http://arxiv.org/abs/2601.10930)|null|
|**2026-01-13**|**FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models**|Wenzhao Lian Team|[2601.08246](http://arxiv.org/abs/2601.08246)|null|
|**2026-01-12**|**Stable In-hand Manipulation for a Lightweight Four-motor Prosthetic Hand**|Masashi Hamaya Team|[2601.07559](http://arxiv.org/abs/2601.07559)|null|
|**2026-01-09**|**DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation**|Libin Liu Team|[2601.05844](http://arxiv.org/abs/2601.05844)|null|
|**2026-02-02**|**LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model**|Shanghang Zhang Team|[2601.05248](http://arxiv.org/abs/2601.05248)|null|
|**2026-01-08**|**UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation**|Peng Zhou Team|[2601.04629](http://arxiv.org/abs/2601.04629)|null|
|**2026-01-09**|**Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation**|Zhibin Li Team|[2601.02778](http://arxiv.org/abs/2601.02778)|null|
|**2026-01-04**|**DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos**|Robert B. Fisher Team|[2601.01651](http://arxiv.org/abs/2601.01651)|null|
|**2025-12-31**|**ShowUI- $π$ : Flow-based Generative Models as GUI Dexterous Hands**|Mike Zheng Shou Team|[2512.24965](http://arxiv.org/abs/2512.24965)|null|
|**2025-12-31**|**Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids**|David Hyunchul Shim Team|[2512.24657](http://arxiv.org/abs/2512.24657)|null|
|**2026-01-01**|**World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild**|Wenchao Ding Team|[2512.24310](http://arxiv.org/abs/2512.24310)|null|
|**2026-01-09**|**GR-Dexter Technical Report**|Hang Li Team|[2512.24210](http://arxiv.org/abs/2512.24210)|null|
|**2025-12-29**|**UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer**|Zongqing Lu Team|[2512.21233](http://arxiv.org/abs/2512.21233)|null|
|**2025-12-22**|**Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations**|Ping Tan Team|[2512.19583](http://arxiv.org/abs/2512.19583)|null|
|**2025-12-19**|**SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning**|Axel Krieger Team|[2512.18068](http://arxiv.org/abs/2512.18068)|null|
|**2025-12-17**|**ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision**|Jie Mei Team|[2512.15020](http://arxiv.org/abs/2512.15020)|null|
|**2025-12-15**|**World Models Can Leverage Human Videos for Dexterous Manipulation**|Yann LeCun Team|[2512.13644](http://arxiv.org/abs/2512.13644)|null|
|**2025-12-15**|**WholeBodyVLA: Towards Unified Latent VLA for Whole-Body Loco-Manipulation Control**|Hongyang Li Team|[2512.11047](http://arxiv.org/abs/2512.11047)|null|
|**2025-12-11**|**Design and Validation of an Under-actuated Robotic Finger with Synchronous Tendon Routing**|Weibang Bai Team|[2512.10349](http://arxiv.org/abs/2512.10349)|null|
|**2025-12-06**|**Vision-Guided Grasp Planning for Prosthetic Hands in Unstructured Environments**|Simon Bøgh Team|[2512.06517](http://arxiv.org/abs/2512.06517)|null|
|**2025-12-04**|**Vision-Language-Action Models for Selective Robotic Disassembly: A Case Study on Critical Component Extraction from Desktops**|Minghui Zheng Team|[2512.04446](http://arxiv.org/abs/2512.04446)|null|
|**2025-12-04**|**Development of a 15-Degree-of-Freedom Bionic Hand with Cable-Driven Transmission and Distributed Actuation**|Hesheng Wang Team|[2512.04399](http://arxiv.org/abs/2512.04399)|null|
|**2025-12-03**|**ResponsibleRobotBench: Benchmarking Responsible Robot Manipulation using Multi-modal Large Language Models**|Jianwei Zhang Team|[2512.04308](http://arxiv.org/abs/2512.04308)|**[link](https://sites.google.com/view/responsible-robotbench)**|
|**2025-12-30**|**Cross-embodied Co-design for Dexterous Hands**|Xiaolong Wang Team|[2512.03743](http://arxiv.org/abs/2512.03743)|null|
|**2025-12-02**|**Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger**|Nilanjan Chakraborty Team|[2512.02951](http://arxiv.org/abs/2512.02951)|null|
|**2025-12-02**|**RoboWheel: A Data Engine from Real-World Human Demonstrations for Cross-Embodiment Robotic Learning**|Haoqian Wang Team|[2512.02729](http://arxiv.org/abs/2512.02729)|null|
|**2025-12-01**|**Learning Dexterous Manipulation Skills from Imperfect Simulations**|Haozhi Qi Team|[2512.02011](http://arxiv.org/abs/2512.02011)|null|
|**2025-12-23**|**GR-RL: Going Dexterous and Precise for Long-Horizon Robotic Manipulation**|Yonghui Wu Team|[2512.01801](http://arxiv.org/abs/2512.01801)|null|
|**2025-11-30**|**Tactile Robotics: Past and Future**|Nathan F. Lepora Team|[2512.01106](http://arxiv.org/abs/2512.01106)|null|
|**2025-11-30**|**CycleManip: Enabling Cyclic Task Manipulation via Effective Historical Perception and Understanding**|Wei-Shi Zheng Team|[2512.01022](http://arxiv.org/abs/2512.01022)|**[link](https://isee-laboratory.github.io/OmniDexGrasp/)**|
|**2025-12-19**|**MILE: A Mechanically Isomorphic Exoskeleton Data Collection System with Fingertip Visuotactile Sensing for Dexterous Manipulation**|Xiangyang Zhu Team|[2512.00324](http://arxiv.org/abs/2512.00324)|null|
|**2025-11-27**|**Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation**|Yaonan Wang Team|[2511.22100](http://arxiv.org/abs/2511.22100)|null|
|**2025-11-26**|**Kinematics-Aware Multi-Policy Reinforcement Learning for Force-Capable Humanoid Loco-Manipulation**|Qijun Chen Team|[2511.21169](http://arxiv.org/abs/2511.21169)|null|
|**2025-11-25**|**ACE-F: A Cross Embodiment Foldable System with Force Feedback for Dexterous Teleoperation**|Xiaolong Wang Team|[2511.20887](http://arxiv.org/abs/2511.20887)|null|
|**2025-11-21**|**RoboCOIN: An Open-Sourced Bimanual Robotic Data COllection for INtegrated Manipulation**|Guocai Yao Team|[2511.17441](http://arxiv.org/abs/2511.17441)|null|
|**2025-11-21**|**METIS: Multi-Source Egocentric Training for Integrated Dexterous Vision-Language-Action Model**|Shanghang Zhang Team|[2511.17366](http://arxiv.org/abs/2511.17366)|null|
|**2025-11-20**|**Dexterity from Smart Lenses: Multi-Fingered Robot Manipulation with In-the-Wild Human Demonstrations**|Homanga Bharadhwaj Team|[2511.16661](http://arxiv.org/abs/2511.16661)|null|
|**2025-11-20**|**InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy**|Jiangmiao Pang Team|[2511.16651](http://arxiv.org/abs/2511.16651)|null|
|**2025-11-27**|**VIRAL: Visual Sim-to-Real at Scale for Humanoid Loco-Manipulation**|Yuke Zhu Team|[2511.15200](http://arxiv.org/abs/2511.15200)|**[link](https://viral-humanoid.github.io/)**|
|**2025-11-18**|**Toward Robust and Harmonious Adaptation for Cross-modal Retrieval**|Xi Peng Team|[2511.14416](http://arxiv.org/abs/2511.14416)|null|
|**2025-11-17**|**From Power to Precision: Learning Fine-grained Dexterity for Multi-fingered Robotic Hands**|Xiaolong Wang Team|[2511.13710](http://arxiv.org/abs/2511.13710)|**[link](https://jianglongye.com/power-to-precision)**|
|**2025-11-17**|**ZeroDexGrasp: Zero-Shot Task-Oriented Dexterous Grasp Synthesis with Prompt-Based Multi-Stage Semantic Reasoning**|Ruizhen Hu Team|[2511.13327](http://arxiv.org/abs/2511.13327)|null|
|**2025-11-14**|**Dexterous Manipulation Transfer via Progressive Kinematic-Dynamic Alignment**|Yi Sun Team|[2511.10987](http://arxiv.org/abs/2511.10987)|null|
|**2025-11-13**|**Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning**|Xiaocong Li Team|[2511.10087](http://arxiv.org/abs/2511.10087)|null|
|**2025-11-12**|**ScaleADFG: Affordance-based Dexterous Functional Grasping via Scalable Dataset**|Peng Wang Team|[2511.09602](http://arxiv.org/abs/2511.09602)|null|
|**2025-11-12**|**IFG: Internet-Scale Guidance for Functional Grasping Generation**|Deepak Pathak Team|[2511.09558](http://arxiv.org/abs/2511.09558)|**[link](https://ifgrasping.github.io/)**|
|**2025-11-12**|**SPIDER: Scalable Physics-Informed Dexterous Retargeting**|Francois Hogan Team|[2511.09484](http://arxiv.org/abs/2511.09484)|**[link](https://jc-bao.github.io/spider-project/)**|
|**2025-11-12**|**RGMP: Recurrent Geometric-prior Multimodal Policy for Generalizable Humanoid Robot Manipulation**|Miao Li Team|[2511.09141](http://arxiv.org/abs/2511.09141)|null|
|**2025-11-12**|**MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror**|Tao Shen Team|[2511.08865](http://arxiv.org/abs/2511.08865)|null|
|**2025-11-10**|**Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields**|Pieter Abbeel Team|[2511.07418](http://arxiv.org/abs/2511.07418)|**[link](https://github.com/zhaohengyin/lightning-grasp)**|
|**2025-11-09**|**Robust Differentiable Collision Detection for General Objects**|He Wang Team|[2511.06267](http://arxiv.org/abs/2511.06267)|null|
|**2025-11-08**|**Adversarial Game-Theoretic Algorithm for Dexterous Grasp Synthesis**|Jeffrey Ichnowski Team|[2511.05809](http://arxiv.org/abs/2511.05809)|null|
|**2025-11-06**|**Isaac Lab: A GPU-Accelerated Simulation Framework for Multi-Modal Robot Learning**|Gavriel State Team|[2511.04831](http://arxiv.org/abs/2511.04831)|**[link](https://github.com/isaac-sim/IsaacLab)**|
|**2025-11-05**|**Dexterous Intramyocardial Needle Ablation (d-INA): Design, Fabrication, and In-Vivo Validation**|Yue Chen Team|[2511.03763](http://arxiv.org/abs/2511.03763)|null|
|**2025-11-09**|**Development of the Bioinspired Tendon-Driven DexHand 021 with Proprioceptive Compliance Control**|Sheng Yi Team|[2511.03481](http://arxiv.org/abs/2511.03481)|null|
|**2025-11-10**|**3D Cal: An Open-Source Software Library for Calibrating Tactile Sensors**|Gregory Reardon Team|[2511.03078](http://arxiv.org/abs/2511.03078)|null|
|**2025-11-04**|**TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System**|C. Karen Liu Team|[2511.02832](http://arxiv.org/abs/2511.02832)|**[link](https://yanjieze.com/TWIST2)**|
|**2025-11-04**|**Dexterous Robotic Piano Playing at Scale**|Dieter Büchler Team|[2511.02504](http://arxiv.org/abs/2511.02504)|null|
|**2025-11-10**|**Whole-body motion planning and safety-critical control for aerial manipulation**|Jeonghyun Byun Team|[2511.02342](http://arxiv.org/abs/2511.02342)|null|
|**2025-11-03**|**GenDexHand: Generative Simulation for Dexterous Hands**|Yi Ma Team|[2511.01791](http://arxiv.org/abs/2511.01791)|null|
|**2025-11-09**|**Scaling Cross-Embodiment World Models for Dexterous Manipulation**|Hao Su Team|[2511.01177](http://arxiv.org/abs/2511.01177)|null|
|**2025-10-31**|**End-to-End Dexterous Arm-Hand VLA Policies via Shared Autonomy: VR Teleoperation Augmented by Autonomous Hand VLA Policy for Efficient Data Collection**|Zhibin Li Team|[2511.00139](http://arxiv.org/abs/2511.00139)|null|
|**2025-10-31**|**Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping**|Xiaonan Huang Team|[2510.27666](http://arxiv.org/abs/2510.27666)|null|
|**2025-10-30**|**SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation**|Matei Ciocarlie Team|[2510.27048](http://arxiv.org/abs/2510.27048)|null|
|**2025-10-28**|**A Humanoid Visual-Tactile-Action Dataset for Contact-Rich Manipulation**|Kyung-Joong Kim Team|[2510.25725](http://arxiv.org/abs/2510.25725)|null|
|**2025-10-27**|**OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback**|Wei-Shi Zheng Team|[2510.23119](http://arxiv.org/abs/2510.23119)|**[link](https://isee-laboratory.github.io/OmniDexGrasp/)**|
|**2025-10-24**|**Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos**|Baining Guo Team|[2510.21571](http://arxiv.org/abs/2510.21571)|**[link](https://microsoft.github.io/VITRA/)**|
|**2025-10-23**|**SutureBot: A Precision Framework & Benchmark For Autonomous End-to-End Suturing**|Axel Krieger Team|[2510.20965](http://arxiv.org/abs/2510.20965)|null|
|**2025-10-21**|**RAPID Hand Prototype: Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation**|Hui Cheng Team|[2510.16931](http://arxiv.org/abs/2510.16931)|null|
|**2025-10-23**|**DexCanvas: Bridging Human Demonstrations and Robot Learning for Dexterous Manipulation**|Yiwen Lu Team|[2510.15786](http://arxiv.org/abs/2510.15786)|null|
|**2025-10-16**|**Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation**|Shan An Team|[2510.14771](http://arxiv.org/abs/2510.14771)|null|
|**2025-10-16**|**Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery**|Dmitry Berenson Team|[2510.14768](http://arxiv.org/abs/2510.14768)|null|
|**2025-10-16**|**Spatially anchored Tactile Awareness for Robust Dexterous Manipulation**|Kaifeng Zhang Team|[2510.14647](http://arxiv.org/abs/2510.14647)|null|
|**2025-10-16**|**Restoring Noisy Demonstration for Imitation Learning With Diffusion Models**|Shao-Hua Sun Team|[2510.14467](http://arxiv.org/abs/2510.14467)|null|
|**2025-10-14**|**Learning to Grasp Anything by Playing with Random Toys**|Roei Herzig Team|[2510.12866](http://arxiv.org/abs/2510.12866)|null|
|**2025-10-14**|**T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping**|Lin Shao Team|[2510.12724](http://arxiv.org/abs/2510.12724)|null|
|**2025-10-10**|**Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench Feedback Teleoperation System**|Pai Zheng Team|[2510.09229](http://arxiv.org/abs/2510.09229)|null|
|**2025-10-10**|**PLEXUS Hand: Lightweight Four-Motor Prosthetic Hand Enabling Precision-Lateral Dexterous Manipulation**|Kazutoshi Tanaka Team|[2510.09209](http://arxiv.org/abs/2510.09209)|null|
|**2025-10-09**|**DexNDM: Closing the Reality Gap for Dexterous In-Hand Rotation via Joint-Wise Neural Dynamics Model**|Li Yi Team|[2510.08556](http://arxiv.org/abs/2510.08556)|**[link](https://meowuu7.github.io/DexNDM/)**|
|**2025-10-09**|**DexMan: Learning Bimanual Dexterous Manipulation from Human and Generated Videos**|Tsung-Wei Ke Team|[2510.08475](http://arxiv.org/abs/2510.08475)|**[link](https://embodiedai-ntu.github.io/dexman/index.html)**|
|**2025-10-08**|**AVO: Amortized Value Optimization for Contact Mode Switching in Multi-Finger Manipulation**|Dmitry Berenson Team|[2510.07548](http://arxiv.org/abs/2510.07548)|null|
|**2025-10-07**|**Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning**|Yan Wu Team|[2510.06068](http://arxiv.org/abs/2510.06068)|null|
|**2025-10-06**|**A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation**|Zeynep Temel Team|[2510.05382](http://arxiv.org/abs/2510.05382)|null|
|**2025-10-01**|**ISyHand: A Dexterous Multi-finger Robot Hand with an Articulated Palm**|Katherine J. Kuchenbecker Team|[2509.26236](http://arxiv.org/abs/2509.26236)|null|
|**2025-09-28**|**DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation**|Yuanpei Chen Team|[2509.23829](http://arxiv.org/abs/2509.23829)|null|
|**2025-09-26**|**DemoGrasp: Universal Dexterous Grasping from a Single Demonstration**|Zongqing Lu Team|[2509.22149](http://arxiv.org/abs/2509.22149)|null|
|**2025-09-25**|**Residual Off-Policy RL for Finetuning Behavior Cloning Policies**|Anusha Nagabandi Team|[2509.19301](http://arxiv.org/abs/2509.19301)|**[link](https://residual-offpolicy-rl.github.io)**|
|**2025-09-23**|**Lang2Morph: Language-Driven Morphological Design of Robotic Hands**|Josie Hughes Team|[2509.18937](http://arxiv.org/abs/2509.18937)|null|
|**2025-10-07**|**Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands**|Daniel Seita Team|[2509.18455](http://arxiv.org/abs/2509.18455)|null|
|**2025-09-23**|**Learning Dexterous Manipulation with Quantized Hand State**|Cewu Lu Team|[2509.17450](http://arxiv.org/abs/2509.17450)|null|
|**2025-09-18**|**A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects**|Yongduan Song Team|[2509.14939](http://arxiv.org/abs/2509.14939)|null|
|**2025-09-18**|**Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking**|Chen Peng Team|[2509.14530](http://arxiv.org/abs/2509.14530)|null|
|**2025-09-17**|**LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation**|Han Liu Team|[2509.14349](http://arxiv.org/abs/2509.14349)|null|
|**2025-09-16**|**\textsc{Gen2Real}: Towards Demo-Free Dexterous Manipulation by Harnessing Generated Video**|Rui Huang Team|[2509.14178](http://arxiv.org/abs/2509.14178)|null|
|**2025-09-17**|**Whole-body Motion Control of an Omnidirectional Wheel-Legged Mobile Manipulator via Contact-Aware Dynamic Optimization**|Yiqun Li Team|[2509.14010](http://arxiv.org/abs/2509.14010)|null|
|**2025-10-03**|**Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of Freedom by Fusing the Abduction of Digits Four and Five**|Robert K. Katzschmann Team|[2509.13074](http://arxiv.org/abs/2509.13074)|null|
|**2025-09-16**|**MoiréTac: A Dual-Mode Visuotactile Sensor for Multidimensional Perception Using Moiré Pattern Amplification**|Wenbo Ding Team|[2509.12714](http://arxiv.org/abs/2509.12714)|null|
|**2025-09-11**|**Dexplore: Scalable Neural Control for Dexterous Manipulation from Reference-Scoped Exploration**|Wei Yang Team|[2509.09671](http://arxiv.org/abs/2509.09671)|null|
|**2025-09-10**|**Grasp Like Humans: Learning Generalizable Multi-Fingered Grasping from Human Proprioceptive Sensorimotor Integration**|Huimin Lu Team|[2509.08354](http://arxiv.org/abs/2509.08354)|null|
|**2025-09-09**|**Text2Touch: Tactile In-Hand Manipulation with LLM-Designed Reward Functions**|Nathan F. Lepora Team|[2509.07445](http://arxiv.org/abs/2509.07445)|null|
|**2025-09-05**|**OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation**|Yu Xiang Team|[2509.05513](http://arxiv.org/abs/2509.05513)|null|
|**2025-09-08**|**DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation**|Pulkit Agrawal Team|[2509.04441](http://arxiv.org/abs/2509.04441)|**[link](https://dex-op.github.io)**|
|**2025-09-09**|**EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control**|Dong Wang Team|[2508.21112](http://arxiv.org/abs/2508.21112)|null|
|**2025-08-31**|**HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation**|Huazhe Xu Team|[2508.20085](http://arxiv.org/abs/2508.20085)|null|
|**2025-08-24**|**LodeStar: Long-horizon Dexterity via Synthetic Data Augmentation from Human Demonstrations**|Hao Su Team|[2508.17547](http://arxiv.org/abs/2508.17547)|null|
|**2025-08-21**|**Exploiting Policy Idling for Dexterous Manipulation**|Dushyant Rao Team|[2508.15669](http://arxiv.org/abs/2508.15669)|null|
|**2025-08-20**|**GraspQP: Differentiable Optimization of Force Closure for Diverse and Robust Dexterous Grasping**|Marco Hutter Team|[2508.15002](http://arxiv.org/abs/2508.15002)|null|
|**2025-08-20**|**FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy**|Cewu Lu Team|[2508.14441](http://arxiv.org/abs/2508.14441)|null|
|**2025-08-17**|**Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation**|Nancy S. Pollard Team|[2508.12439](http://arxiv.org/abs/2508.12439)|null|
|**2025-08-15**|**Towards Affordance-Aware Robotic Dexterous Grasping with Human-like Priors**|Hua Zou Team|[2508.08896](http://arxiv.org/abs/2508.08896)|null|
|**2025-08-22**|**OmniVTLA: Vision-Tactile-Language-Action Model with Semantic-Aligned Tactile Sensing**|Hengdi Zhang Team|[2508.08706](http://arxiv.org/abs/2508.08706)|**[link](https://readerek.github.io/Objtac.github.io)**|
|**2025-08-11**|**PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF**|Lorenzo Natale Team|[2508.07945](http://arxiv.org/abs/2508.07945)|null|
|**2025-08-29**|**DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit**|Monroe Kennedy III Team|[2508.07118](http://arxiv.org/abs/2508.07118)|null|
|**2025-08-05**|**UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands**|Yaonan Wang Team|[2508.03339](http://arxiv.org/abs/2508.03339)|**[link](https://haochen611.github.io/UFG)**|
|**2025-08-03**|**DexReMoE:In-hand Reorientation of General Object via Mixtures of Experts**|Yunlong Dong Team|[2508.01695](http://arxiv.org/abs/2508.01695)|null|
|**2025-08-01**|**Video Generators are Robot Policies**|Carl Vondrick Team|[2508.00795](http://arxiv.org/abs/2508.00795)|null|
|**2025-07-31**|**XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation**|Ning Yang Team|[2508.00097](http://arxiv.org/abs/2508.00097)|**[link](https://github.com/XR-Robotics)**|
|**2025-09-11**|**villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models**|Jiang Bian Team|[2507.23682](http://arxiv.org/abs/2507.23682)|**[link](https://aka.ms/villa-x)**|
|**2025-07-19**|**A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0**|Erbao Dong Team|[2507.14538](http://arxiv.org/abs/2507.14538)|null|
|**2025-07-18**|**Improving Low-Cost Teleoperation: Augmenting GELLO with Force**|Kai Arulkumaran Team|[2507.13602](http://arxiv.org/abs/2507.13602)|null|
|**2025-07-16**|**The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey**|Jiming Chen Team|[2507.11840](http://arxiv.org/abs/2507.11840)|null|
|**2025-07-14**|**Demonstrating the Octopi-1.5 Visual-Tactile-Language Model**|Harold Soh Team|[2507.09985](http://arxiv.org/abs/2507.09985)|null|
|**2025-07-09**|**Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand**|Xinjun Sheng Team|[2507.06822](http://arxiv.org/abs/2507.06822)|null|
|**2025-07-07**|**A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation**|Russ Tedrake Team|[2507.05331](http://arxiv.org/abs/2507.05331)|null|
|**2025-07-06**|**SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training**|Hao Dong Team|[2507.04452](http://arxiv.org/abs/2507.04452)|null|
|**2025-07-03**|**DexVLG: Dexterous Vision-Language-Grasp Model at Scale**|He Wang Team|[2507.02747](http://arxiv.org/abs/2507.02747)|null|
|**2025-07-03**|**TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types**|Wei-Shi Zheng Team|[2507.01857](http://arxiv.org/abs/2507.01857)|**[link](https://isee-laboratory.github.io/TypeTele)**|
|**2025-07-01**|**HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning**|Chenjia Bai Team|[2507.00833](http://arxiv.org/abs/2507.00833)|**[link](https://openhumanoidgen.github.io)**|
|**2025-06-26**|**Lightweight Fingernail Haptic Device: Unobstructed Fingerpad Force and Vibration Feedback for Enhanced Virtual Dexterous Manipulation**|Shoichi Hasegawa Team|[2506.21417](http://arxiv.org/abs/2506.21417)|null|
|**2025-06-24**|**Scaffolding Dexterous Manipulation with Vision-Language Models**|Dorsa Sadigh Team|[2506.19212](http://arxiv.org/abs/2506.19212)|null|
|**2025-06-24**|**The MOTIF Hand: A Robotic Hand for Multimodal Observations with Thermal, Inertial, and Force Sensors**|Daniel Seita Team|[2506.19201](http://arxiv.org/abs/2506.19201)|null|
|**2025-06-21**|**VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models**|Lin Shao Team|[2506.17561](http://arxiv.org/abs/2506.17561)|null|
|**2025-06-20**|**Dex1B: Learning with 1B Demonstrations for Dexterous Manipulation**|Xiaolong Wang Team|[2506.17198](http://arxiv.org/abs/2506.17198)|**[link](https://jianglongye.com/dex1b)**|
|**2025-06-19**|**ViTacFormer: Learning Cross-Modal Representation for Visuo-Tactile Dexterous Manipulation**|Jitendra Malik Team|[2506.15953](http://arxiv.org/abs/2506.15953)|null|
|**2025-06-17**|**Tactile Beyond Pixels: Multisensory Touch Representations for Robot Manipulation**|Mustafa Mukadam Team|[2506.14754](http://arxiv.org/abs/2506.14754)|null|
|**2025-06-16**|**CEED-VLA: Consistency Vision-Language-Action Model with Early-Exit Decoding**|Haoang Li Team|[2506.13725](http://arxiv.org/abs/2506.13725)|null|
|**2025-06-13**|**ViTaSCOPE: Visuo-tactile Implicit Representation for In-hand Pose and Extrinsic Contact Estimation**|Nima Fazeli Team|[2506.12239](http://arxiv.org/abs/2506.12239)|null|
|**2025-06-13**|**ExoStart: Efficient learning for dexterous manipulation with sensorized exoskeleton demonstrations**|Maria Bauza Villalonga Team|[2506.11775](http://arxiv.org/abs/2506.11775)|null|
|**2025-06-30**|**Adaptive event-triggered robust tracking control of soft robots**|Marios M. Polycarpou Team|[2506.09523](http://arxiv.org/abs/2506.09523)|null|
|**2025-06-11**|**Analyzing Key Objectives in Human-to-Robot Retargeting for Dexterous Manipulation**|Xiang Li Team|[2506.09384](http://arxiv.org/abs/2506.09384)|null|
|**2025-06-09**|**TensorTouch: Calibration of Tactile Sensors for High Resolution Stress Tensor and Deformation for Dexterous Manipulation**|Monroe Kennedy III Team|[2506.08291](http://arxiv.org/abs/2506.08291)|null|
|**2025-06-09**|**RAPID Hand: A Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platform for Generalist Robot Autonomy**|Hui Cheng Team|[2506.07490](http://arxiv.org/abs/2506.07490)|null|
|**2025-06-05**|**GEX: Democratizing Dexterity with Fully-Actuated Dexterous Hand and Exoskeleton Glove**|Zelin Deng Team|[2506.04982](http://arxiv.org/abs/2506.04982)|**[link](https://github.com/democratizing-dexterous/libgex)**|
|**2025-06-06**|**ArtVIP: Articulated Digital Assets of Visual Realism, Modular Interaction, and Physical Fidelity for Robot Learning**|Jian Tang Team|[2506.04941](http://arxiv.org/abs/2506.04941)|null|
|**2025-06-03**|**Reachability Weighted Offline Goal-conditioned Resampling**|Joni Pajarinen Team|[2506.02577](http://arxiv.org/abs/2506.02577)|null|
|**2025-05-30**|**Interactive Imitation Learning for Dexterous Robotic Manipulation: Challenges and Perspectives -- A Survey**|Rania Rayyes Team|[2506.00098](http://arxiv.org/abs/2506.00098)|null|
|**2025-05-30**|**DexMachina: Functional Retargeting for Bimanual Dexterous Manipulation**|Shuran Song Team|[2505.24853](http://arxiv.org/abs/2505.24853)|null|
|**2025-05-28**|**ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation**|Wenqiang Zhang Team|[2505.22159](http://arxiv.org/abs/2505.22159)|null|
|**2025-10-03**|**DexUMI: Using Human Hand as the Universal Manipulation Interface for Dexterous Manipulation**|Shuran Song Team|[2505.21864](http://arxiv.org/abs/2505.21864)|null|
|**2025-05-27**|**Learning Generalizable Robot Policy with Human Demonstration Video as a Prompt**|Jianyu Chen Team|[2505.20795](http://arxiv.org/abs/2505.20795)|null|
|**2025-05-25**|**MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation**|Xue Bin Peng Team|[2505.19086](http://arxiv.org/abs/2505.19086)|null|
|**2025-05-24**|**Beyond Domain Randomization: Event-Inspired Perception for Visually Robust Adversarial Imitation from Videos**|Mario Bijelic Team|[2505.18899](http://arxiv.org/abs/2505.18899)|**[link](https://github.com/vittoriogiammarino/eb-laifo)**|
|**2025-05-24**|**DiffusionRL: Efficient Training of Diffusion Policies for Robotic Grasping Using RL-Adapted Large-Scale Datasets**|Dzmitry Tsetserukou Team|[2505.18876](http://arxiv.org/abs/2505.18876)|null|
|**2025-05-27**|**GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning**|Ye Shi Team|[2505.18763](http://arxiv.org/abs/2505.18763)|null|
|**2025-05-22**|**TacCompress: A Benchmark for Multi-Point Tactile Data Compression in Dexterous Manipulation**|Hengdi Zhang Team|[2505.16289](http://arxiv.org/abs/2505.16289)|null|
|**2025-05-21**|**Object-Focus Actor for Data-efficient Robot Generalization Dexterous Manipulation**|Xiaodong He Team|[2505.15098](http://arxiv.org/abs/2505.15098)|null|
|**2025-05-20**|**Adaptive Visuo-Tactile Fusion with Predictive Force Attention for Dexterous Manipulation**|Hao Dong Team|[2505.13982](http://arxiv.org/abs/2505.13982)|null|
|**2025-05-19**|**Approximating Global Contact-Implicit MPC via Sampling and Local Complementarity**|Michael Posa Team|[2505.13350](http://arxiv.org/abs/2505.13350)|null|
|**2025-05-19**|**TeleOpBench: A Simulator-Centric Benchmark for Dual-Arm Dexterous Teleoperation**|Jiangmiao Pang Team|[2505.12748](http://arxiv.org/abs/2505.12748)|null|
|**2025-05-18**|**PartDexTOG: Generating Dexterous Task-Oriented Grasping via Language-driven Part Analysis**|Zhipong Cai Team|[2505.12294](http://arxiv.org/abs/2505.12294)|null|
|**2025-05-17**|**OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning**|Yang Gao Team|[2505.11917](http://arxiv.org/abs/2505.11917)|null|
|**2025-05-16**|**EgoDex: Learning Dexterous Manipulation from Large-Scale Egocentric Video**|Jian Zhang Team|[2505.11709](http://arxiv.org/abs/2505.11709)|null|
|**2025-05-16**|**Self-supervised perception for tactile skin covered dexterous hands**|Mustafa Mukadam Team|[2505.11420](http://arxiv.org/abs/2505.11420)|null|
|**2025-05-16**|**Learning Multimodal AI Algorithms for Amplifying Limited User Input into High-dimensional Control Space**|Reza Abiri Team|[2505.11366](http://arxiv.org/abs/2505.11366)|**[link](https://github.com/abirilab/aras)**|
|**2025-05-16**|**Estimating Deformable-Rigid Contact Interactions for a Deformable Tool via Learning and Model-Based Optimization**|Nima Fazeli Team|[2505.10884](http://arxiv.org/abs/2505.10884)|null|
|**2025-05-15**|**SRT-H: A Hierarchical Framework for Autonomous Surgery via Language Conditioned Imitation Learning**|Axel Krieger Team|[2505.10251](http://arxiv.org/abs/2505.10251)|null|
|**2025-05-13**|**HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands**|Yunhui Liu Team|[2505.08213](http://arxiv.org/abs/2505.08213)|null|
|**2025-05-12**|**DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies**|Deepak Pathak Team|[2505.07813](http://arxiv.org/abs/2505.07813)|null|
|**2025-05-08**|**Morphologically Symmetric Reinforcement Learning for Ambidextrous Bimanual Manipulation**|Georgia Chalvatzaki Team|[2505.05287](http://arxiv.org/abs/2505.05287)|null|
|**2025-05-04**|**Prompt-responsive Object Retrieval with Memory-augmented Student-Teacher Learning**|Sven Behnke Team|[2505.02232](http://arxiv.org/abs/2505.02232)|null|
|**2025-05-04**|**KineDex: Learning Tactile-Informed Visuomotor Policies via Kinesthetic Teaching for Dexterous Manipulation**|Yang Gao Team|[2505.01974](http://arxiv.org/abs/2505.01974)|null|
|**2025-05-02**|**DexFlow: A Unified Approach for Dexterous Hand Pose Retargeting and Interaction**|Miao Li Team|[2505.01083](http://arxiv.org/abs/2505.01083)|null|
|**2025-05-02**|**DexCtrl: Towards Sim-to-Real Dexterity with Adaptive Controller Learning**|Masayoshi Tomizuka Team|[2505.00991](http://arxiv.org/abs/2505.00991)|null|
|**2025-05-01**|**Multi-Goal Dexterous Hand Manipulation using Probabilistic Model-based Reinforcement Learning**|Yunduan Cui Team|[2504.21585](http://arxiv.org/abs/2504.21585)|null|
|**2025-04-27**|**PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation Using Tactile-Diffusion Policies**|Edward Adelson Team|[2504.19341](http://arxiv.org/abs/2504.19341)|null|
|**2025-04-23**|**PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands**|Ziyuan Jiao Team|[2504.16649](http://arxiv.org/abs/2504.16649)|null|
|**2025-04-22**|**$π_{0.5}$ : a Vision-Language-Action Model with Open-World Generalization**|Ury Zhilinsky Team|[2504.16054](http://arxiv.org/abs/2504.16054)|null|
|**2025-04-21**|**LAPP: Large Language Model Feedback for Preference-Driven Reinforcement Learning**|Boyuan Chen Team|[2504.15472](http://arxiv.org/abs/2504.15472)|null|
|**2025-04-21**|**SuFIA-BC: Generating High Quality Demonstration Data for Visuomotor Policy Learning in Surgical Subtasks**|Animesh Garg Team|[2504.14857](http://arxiv.org/abs/2504.14857)|null|
|**2025-04-20**|**BiDexHand: Design and Evaluation of an Open-Source 16-DoF Biomimetic Dexterous Hand**|Zhengyang Kris Weng Team|[2504.14712](http://arxiv.org/abs/2504.14712)|null|
|**2025-04-18**|**On the Importance of Tactile Sensing for Imitation Learning: A Case Study on Robotic Match Lighting**|Jan Peters Team|[2504.13618](http://arxiv.org/abs/2504.13618)|null|
|**2025-04-17**|**RUKA: Rethinking the Design of Humanoid Hands with Learning**|Lerrel Pinto Team|[2504.13165](http://arxiv.org/abs/2504.13165)|null|
|**2025-04-17**|**Adaptive Task Space Non-Singular Terminal Super-Twisting Sliding Mode Control of a 7-DOF Robotic Manipulator**|E. Witrant Team|[2504.13056](http://arxiv.org/abs/2504.13056)|null|
|**2025-04-17**|**Krysalis Hand: A Lightweight, High-Payload, 18-DoF Anthropomorphic End-Effector for Robotic Learning and Dexterous Manipulation**|Iman Soltani Team|[2504.12967](http://arxiv.org/abs/2504.12967)|null|
|**2025-04-22**|**Crossing the Human-Robot Embodiment Gap with Sim-to-Real RL using One Human Demonstration**|Jeannette Bohg Team|[2504.12609](http://arxiv.org/abs/2504.12609)|null|
|**2025-04-14**|**Look-to-Touch: A Vision-Enhanced Proximity and Tactile Sensor for Distance and Geometry Perception in Robotic Manipulation**|Guoying Gu Team|[2504.10280](http://arxiv.org/abs/2504.10280)|null|
|**2025-04-08**|**Functionally graded keratin facilitates tactile sensing in elephant whiskers**|Katherine J. Kuchenbecker Team|[2504.07143](http://arxiv.org/abs/2504.07143)|null|
|**2025-04-08**|**ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile Manipulation Interface**|Rui Chen Team|[2504.06156](http://arxiv.org/abs/2504.06156)|null|
|**2025-04-06**|**DexTOG: Learning Task-Oriented Dexterous Grasp with Language**|Cewu Lu Team|[2504.04573](http://arxiv.org/abs/2504.04573)|null|
|**2025-04-06**|**DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation and Grasping in Cluttered Environments**|Lin Shao Team|[2504.04516](http://arxiv.org/abs/2504.04516)|null|
|**2025-04-05**|**ORCA: An Open-Source, Reliable, Cost-Effective, Anthropomorphic Robotic Hand for Uninterrupted Dexterous Task Learning**|Robert K. Katzschmann Team|[2504.04259](http://arxiv.org/abs/2504.04259)|null|
|**2025-09-11**|**Dexterous Manipulation through Imitation Learning: A Survey**|Hong Zhang Team|[2504.03515](http://arxiv.org/abs/2504.03515)|null|
|**2025-03-29**|**Dexterous Non-Prehensile Manipulation for Ungraspable Object via Extrinsic Dexterity**|Yuanpei Chen Team|[2503.23120](http://arxiv.org/abs/2503.23120)|null|
|**2025-03-27**|**ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning**|Siyuan Huang Team|[2503.21860](http://arxiv.org/abs/2503.21860)|null|
|**2025-03-25**|**G-DexGrasp: Generalizable Dexterous Grasping Synthesis Via Part-Aware Prior Retrieval and Prior-Assisted Generation**|Ruizhen Hu Team|[2503.19457](http://arxiv.org/abs/2503.19457)|null|
|**2025-03-16**|**Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills**|Zongqing Lu Team|[2503.12533](http://arxiv.org/abs/2503.12533)|null|
|**2025-03-14**|**Is Your Imitation Learning Policy Better than Mine? Policy Comparison with Near-Optimal Stopping**|Haruki Nishimura Team|[2503.10966](http://arxiv.org/abs/2503.10966)|null|
|**2025-03-12**|**Sequential Multi-Object Grasping with One Dexterous Hand**|Daniel Seita Team|[2503.09078](http://arxiv.org/abs/2503.09078)|null|
|**2025-03-16**|**DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness**|Yuexin Ma Team|[2503.08257](http://arxiv.org/abs/2503.08257)|**[link](https://github.com/4DVLab/DexGrasp-Anything)**|
|**2025-03-13**|**AgiBot World Colosseo: A Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems**|Jianchao Zhu Team|[2503.06669](http://arxiv.org/abs/2503.06669)|**[link](https://github.com/opendrivelab/agibot-world)**|
|**2025-03-08**|**ReJSHand: Efficient Real-Time Hand Pose Estimation and Mesh Reconstruction Using Refined Joint and Skeleton Features**|Hong Zhang Team|[2503.05995](http://arxiv.org/abs/2503.05995)|**[link](https://github.com/daishipeng/rejshand)**|
|**2025-03-07**|**Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction**|Bin He Team|[2503.05231](http://arxiv.org/abs/2503.05231)|null|
|**2025-03-06**|**Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online Reinforcement Learning**|Xiaodong He Team|[2503.04014](http://arxiv.org/abs/2503.04014)|null|
|**2025-03-05**|**LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation**|Alois Knoll Team|[2503.03890](http://arxiv.org/abs/2503.03890)|null|
|**2025-03-05**|**Selective Tweezing and Immobilization of Colloids for Dexterous Manipulation of Biological Materials**|Kimani C. Toussaint Jr Team|[2503.03102](http://arxiv.org/abs/2503.03102)|null|
|**2025-03-03**|**TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot Skill Transfer**|Mark R. Cutkosky Team|[2503.01789](http://arxiv.org/abs/2503.01789)|null|
|**2025-03-03**|**RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion Control for Dexterous Robot Manipulation**|Jun Ma Team|[2503.01616](http://arxiv.org/abs/2503.01616)|null|
|**2025-03-03**|**Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic Feedback for Efficient Dexterous Skill Learning**|Wenbo Ding Team|[2503.01543](http://arxiv.org/abs/2503.01543)|null|
|**2025-03-03**|**KineSoft: Learning Proprioceptive Manipulation Policies with Soft Robot Hands**|Jeffrey Ichnowski Team|[2503.01078](http://arxiv.org/abs/2503.01078)|null|
|**2025-02-27**|**Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids**|Yuke Zhu Team|[2502.20396](http://arxiv.org/abs/2502.20396)|null|
|**2025-02-28**|**ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration**|Feifei Feng Team|[2502.19250](http://arxiv.org/abs/2502.19250)|null|
|**2025-02-26**|**Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand**|Yuanpei Chen Team|[2502.18423](http://arxiv.org/abs/2502.18423)|null|
|**2025-02-07**|**Dexterous Cable Manipulation: Taxonomy, Multi-Fingered Hand Design, and Long-Horizon Manipulation**|Robert B. Fisher Team|[2502.00396](http://arxiv.org/abs/2502.00396)|**[link](https://sites.google.com/view/dexterous-cable-manipulation/home)**|
|**2024-12-23**|**Dexterous Manipulation Based on Prior Dexterous Grasp Pose Knowledge**|Cewu Lu Team|[2412.15587](http://arxiv.org/abs/2412.15587)|null|
|**2024-08-22**|**Tilde: Teleoperation for Dexterous In-Hand Manipulation Learning with a DeltaHand**|Oliver Kroemer Team|[2405.18804](http://arxiv.org/abs/2405.18804)|null|
|**2023-12-13**|**DEFT: Dexterous Fine-Tuning for Real-World Hand Policies**|Deepak Pathak Team|[2310.19797](http://arxiv.org/abs/2310.19797)|**[link](https://dexterous-finetuning.github.io/)**|
|**2023-12-27**|**DELTAHANDS: A Synergistic Dexterous Hand Framework Based on Delta Robots**|F. Zeynep Temel Team|[2310.05266](http://arxiv.org/abs/2310.05266)|null|
|**2023-10-17**|**Sequential Dexterity: Chaining Dexterous Policies for Long-Horizon Manipulation**|C. Karen Liu Team|[2309.00987](http://arxiv.org/abs/2309.00987)|null|
|**2023-08-23**|**Dexterous Soft Hands Linearize Feedback-Control for In-Hand Manipulation**|Oliver Brock Team|[2308.10691](http://arxiv.org/abs/2308.10691)|null|
|**2023-04-20**|**Progressive Transfer Learning for Dexterous In-Hand Manipulation with Multi-Fingered Anthropomorphic Hand**|Jia Sun Team|[2304.09526](http://arxiv.org/abs/2304.09526)|null|
|**2022-03-25**|**Dexterous Imitation Made Easy: A Learning-Based Framework for Efficient Dexterous Manipulation**|Lerrel Pinto Team|[2203.13251](http://arxiv.org/abs/2203.13251)|null|
|**2022-05-11**|**RBO Hand 3 -- A Platform for Soft Dexterous Manipulation**|Oliver Brock Team|[2201.10883](http://arxiv.org/abs/2201.10883)|null|
|**2019-01-23**|**Learning Dexterous In-Hand Manipulation**|Wojciech Zaremba Team|[1808.00177](http://arxiv.org/abs/1808.00177)|null|
|**2018-06-27**|**Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations**|Sergey Levine Team|[1709.10087](http://arxiv.org/abs/1709.10087)|**[link](https://sites.google.com/view/deeprl-dexterous-manipulation)**|
|**2017-03-21**|**Learning Dexterous Manipulation for a Soft Robotic Hand from Human Demonstration**|Pieter Abbeel Team|[1603.06348](http://arxiv.org/abs/1603.06348)|null|

## Humanoid-Locomotion

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-24**|**KCFRC: Kinematic Collision-Aware Foothold Reachability Criteria for Legged Locomotion**|Liang Ding Team|[2602.20850](http://arxiv.org/abs/2602.20850)|null|
|**2026-02-24**|**Scout-Rover cooperation: online terrain strength mapping and traversal risk estimation for planetary-analog explorations**|Feifei Qian Team|[2602.18688](http://arxiv.org/abs/2602.18688)|null|
|**2026-02-24**|**Soft Surfaced Vision-Based Tactile Sensing for Bipedal Robot Applications**|Yu She Team|[2602.18638](http://arxiv.org/abs/2602.18638)|**[link](https://youtu.be/ceJiy9q_2Aw)**|
|**2026-02-18**|**Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped**|Madhu Vadali Team|[2602.16371](http://arxiv.org/abs/2602.16371)|null|
|**2026-02-17**|**Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**|C. Karen Liu Team|[2602.15827](http://arxiv.org/abs/2602.15827)|null|
|**2026-02-24**|**PMG: Parameterized Motion Generator for Human-like Locomotion Control**|Houde Liu Team|[2602.12656](http://arxiv.org/abs/2602.12656)|**[link](https://pmg-icra26.github.io/)**|
|**2026-02-11**|**APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**|Ding Zhao Team|[2602.11143](http://arxiv.org/abs/2602.11143)|**[link](https://apex-humanoid.github.io/)**|
|**2026-02-11**|**Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning**|Peng Lu Team|[2602.10514](http://arxiv.org/abs/2602.10514)|null|
|**2026-02-11**|**LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies**|Hyun Myung Team|[2602.10399](http://arxiv.org/abs/2602.10399)|**[link](https://locovlm.github.io)**|
|**2026-02-09**|**Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality**|Baxi Chong Team|[2602.09123](http://arxiv.org/abs/2602.09123)|null|
|**2026-02-06**|**ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**|Yao Su Team|[2602.06445](http://arxiv.org/abs/2602.06445)|null|
|**2026-02-06**|**Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**|Zongwu Xie Team|[2602.06382](http://arxiv.org/abs/2602.06382)|null|
|**2026-02-06**|**HiWET: Hierarchical World-Frame End-Effector Tracking for Long-Horizon Humanoid Loco-Manipulation**|Yue Gao Team|[2602.06341](http://arxiv.org/abs/2602.06341)|null|
|**2026-02-05**|**Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**|Weinan Zhang Team|[2602.05791](http://arxiv.org/abs/2602.05791)|null|
|**2026-02-05**|**TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**|Jaeheung Park Team|[2602.05596](http://arxiv.org/abs/2602.05596)|null|
|**2026-02-03**|**CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains**|Chao Huang Team|[2602.03511](http://arxiv.org/abs/2602.03511)|null|
|**2026-02-09**|**Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms**|Sung-Eui Yoon Team|[2602.03397](http://arxiv.org/abs/2602.03397)|**[link](https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/)**|
|**2026-02-02**|**Flow Policy Gradients for Robot Control**|Angjoo Kanazawa Team|[2602.02481](http://arxiv.org/abs/2602.02481)|**[link](https://hongsukchoi.github.io/fpo-control)**|
|**2026-02-21**|**Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control**|Jingwen Zhang Team|[2601.21363](http://arxiv.org/abs/2601.21363)|null|
|**2026-01-28**|**GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control**|Guillaume Sartoretti Team|[2601.20668](http://arxiv.org/abs/2601.20668)|null|
|**2026-01-22**|**Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision**|Dongheui Lee Team|[2601.16109](http://arxiv.org/abs/2601.16109)|null|
|**2026-01-13**|**AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding**|Marco Hutter Team|[2601.08485](http://arxiv.org/abs/2601.08485)|null|
|**2026-01-09**|**Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds**|Aaron D. Ames Team|[2601.06286](http://arxiv.org/abs/2601.06286)|null|
|**2026-01-07**|**Locomotion Beyond Feet**|C. Karen Liu Team|[2601.03607](http://arxiv.org/abs/2601.03607)|**[link](https://locomotion-beyond-feet.github.io/)**|
|**2025-12-31**|**Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer**|Hae-Won Park Team|[2512.24698](http://arxiv.org/abs/2512.24698)|null|
|**2026-01-04**|**Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control**|Shanghang Zhang Team|[2512.23650](http://arxiv.org/abs/2512.23650)|null|
|**2026-01-04**|**RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion**|Shanghang Zhang Team|[2512.23649](http://arxiv.org/abs/2512.23649)|null|
|**2025-12-23**|**Pneumatic bladder links with wide range of motion joints for articulated inflatable robots**|Ryuma Niiyama Team|[2512.20322](http://arxiv.org/abs/2512.20322)|null|
|**2025-12-18**|**E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion**|Dimitrios Kanoulas Team|[2512.16446](http://arxiv.org/abs/2512.16446)|null|
|**2025-12-15**|**Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations**|Ayonga Hereid Team|[2512.12993](http://arxiv.org/abs/2512.12993)|null|
|**2025-12-08**|**Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction**|Houqiang Li Team|[2512.07464](http://arxiv.org/abs/2512.07464)|null|
|**2025-12-03**|**Variable-Impedance Muscle Coordination under Slow-Rate Control Frequencies and Limited Observation Conditions Evaluated through Legged Locomotion**|Jun Morimoto Team|[2512.03459](http://arxiv.org/abs/2512.03459)|null|
|**2025-12-01**|**Learning Sim-to-Real Humanoid Locomotion in 15 Minutes**|Pieter Abbeel Team|[2512.01996](http://arxiv.org/abs/2512.01996)|**[link](https://younggyo.me/fastsac-humanoid)**|
|**2025-11-30**|**H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer**|Weinan Zhang Team|[2512.00971](http://arxiv.org/abs/2512.00971)|null|
|**2025-11-25**|**A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs**|Bowen Zhi Team|[2512.00077](http://arxiv.org/abs/2512.00077)|null|
|**2025-11-28**|**Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary**|Jingya Wang Team|[2511.22963](http://arxiv.org/abs/2511.22963)|**[link](https://humanoidlla.github.io/)**|
|**2025-11-27**|**Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion**|Wael Suleiman Team|[2511.22744](http://arxiv.org/abs/2511.22744)|**[link](https://anonymous.4open.science/r/multiview-parkour-6FB8)**|
|**2026-01-29**|**HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments**|Bin He Team|[2511.20275](http://arxiv.org/abs/2511.20275)|null|

## VLN-Navigation

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-20**|**CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation**|Jon Froehlich Team|[2602.18424](http://arxiv.org/abs/2602.18424)|null|
|**2026-01-26**|**ReasonNavi: Human-Inspired Global Map Reasoning for Zero-Shot Embodied Navigation**|Chi-Keung Tang Team|[2602.15864](http://arxiv.org/abs/2602.15864)|**[link](https://reasonnavi.github.io/)**|
|**2026-02-16**|**pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI**|Haibing Guan Team|[2602.14401](http://arxiv.org/abs/2602.14401)|null|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|**[link](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**|
|**2026-01-30**|**Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering**|Deyu Zhang Team|[2602.11183](http://arxiv.org/abs/2602.11183)|null|
|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|null|
|**2026-02-09**|**When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning**|Mohit Bansal Team|[2602.08236](http://arxiv.org/abs/2602.08236)|**[link](https://adaptive-visual-tts.github.io/)**|
|**2026-02-10**|**LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation**|Soumik Sarkar Team|[2602.07629](http://arxiv.org/abs/2602.07629)|null|
|**2026-02-06**|**Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters**|Mu Xu Team|[2602.06427](http://arxiv.org/abs/2602.06427)|null|
|**2026-02-06**|**Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation**|Weiying Xie Team|[2602.06356](http://arxiv.org/abs/2602.06356)|null|
|**2026-02-05**|**Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation**|Hongyang Li Team|[2602.05827](http://arxiv.org/abs/2602.05827)|null|
|**2026-02-05**|**Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**|Weiming Zhang Team|[2602.05789](http://arxiv.org/abs/2602.05789)|null|
|**2026-02-02**|**LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation**|Anton van den Hengel Team|[2602.02220](http://arxiv.org/abs/2602.02220)|null|
|**2026-02-03**|**MapDream: Task-Driven Map Learning for Vision-Language Navigation**|Zhaoxin Fan Team|[2602.00222](http://arxiv.org/abs/2602.00222)|null|
|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**|Xiaoming Wang Team|[2601.21751](http://arxiv.org/abs/2601.21751)|null|
|**2026-01-26**|**\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation**|Feng Zheng Team|[2601.18188](http://arxiv.org/abs/2601.18188)|null|
|**2026-01-23**|**FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation**|Yonggang Qi Team|[2601.13976](http://arxiv.org/abs/2601.13976)|null|
|**2026-01-14**|**Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning**|Yahong Han Team|[2601.09111](http://arxiv.org/abs/2601.09111)|null|
|**2026-01-11**|**Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**|Bin Ren Team|[2601.08868](http://arxiv.org/abs/2601.08868)|null|
|**2026-01-13**|**VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**|Junzhi Yu Team|[2601.08665](http://arxiv.org/abs/2601.08665)|**[link](https://wsakobe.github.io/VLingNav-web/)**|
|**2026-01-07**|**AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions**|Renxin Zhong Team|[2601.03707](http://arxiv.org/abs/2601.03707)|null|
|**2026-01-19**|**LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality**|Ge Lin Kan Team|[2601.02167](http://arxiv.org/abs/2601.02167)|null|
|**2026-01-05**|**CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios**|Xueqian Wang Team|[2601.01872](http://arxiv.org/abs/2601.01872)|null|
|**2026-01-06**|**VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents**|Qi Wu Team|[2512.24851](http://arxiv.org/abs/2512.24851)|null|
|**2026-01-23**|**VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs**|Jiangmiao Pang Team|[2512.22342](http://arxiv.org/abs/2512.22342)|null|
|**2025-12-25**|**AstraNav-World: World Model for Foresight Control and Consistency**|Shanghang Zhang Team|[2512.21714](http://arxiv.org/abs/2512.21714)|null|
|**2025-12-25**|**AstraNav-Memory: Contexts Compression for Long Memory**|Mu Xu Team|[2512.21627](http://arxiv.org/abs/2512.21627)|null|
|**2025-12-24**|**ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments**|Yue Wang Team|[2512.20940](http://arxiv.org/abs/2512.20940)|null|
|**2025-12-23**|**TongSIM: A General Platform for Simulating Intelligent Machines**|Zhenliang Zhang Team|[2512.20206](http://arxiv.org/abs/2512.20206)|null|
|**2025-12-22**|**IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments**|Zhouhui Lian Team|[2512.19024](http://arxiv.org/abs/2512.19024)|null|
|**2025-12-22**|**VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation**|Qi Wu Team|[2512.19021](http://arxiv.org/abs/2512.19021)|null|
|**2025-12-19**|**Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation**|Eric Sax Team|[2512.18028](http://arxiv.org/abs/2512.18028)|null|
|**2026-01-08**|**ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**|Changyin Sun Team|[2512.17435](http://arxiv.org/abs/2512.17435)|null|
|**2025-12-17**|**HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles**|Renjing Xu Team|[2512.15047](http://arxiv.org/abs/2512.15047)|null|

## VLA-Navigation

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-12**|**LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation**|Maani Ghaffari Team|[2602.12351](http://arxiv.org/abs/2602.12351)|null|
|**2025-08-14**|**CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model**|Hao Dong Team|[2508.10416](http://arxiv.org/abs/2508.10416)|null|
|**2024-07-12**|**Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**|Jie Tan Team|[2407.07775](http://arxiv.org/abs/2407.07775)|null|

## Semantic-SLAM

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2026-02-21**|**IRIS-SLAM: Unified Geo-Instance Representations for Robust Semantic Localization and Mapping**|Zhizhong Su Team|[2602.18709](http://arxiv.org/abs/2602.18709)|null|
|**2026-02-19**|**SceneVGGT: VGGT-based online 3D semantic SLAM for indoor scene understanding and navigation**|Kristóf Karacs Team|[2602.15899](http://arxiv.org/abs/2602.15899)|null|
|**2026-01-09**|**FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time**|Simon Hadfield Team|[2601.05738](http://arxiv.org/abs/2601.05738)|null|
|**2025-12-01**|**KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM**|Sergey Kolyubin Team|[2512.01889](http://arxiv.org/abs/2512.01889)|null|
|**2025-11-28**|**Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM**|Zhenhong Jia Team|[2511.22968](http://arxiv.org/abs/2511.22968)|null|
|**2025-11-27**|**Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM**|Anna Gelencsér-Horváth Team|[2511.16282](http://arxiv.org/abs/2511.16282)|null|
|**2025-10-01**|**Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions**|Nak Young Chong Team|[2510.00783](http://arxiv.org/abs/2510.00783)|null|
|**2025-09-18**|**Human Interaction for Collaborative Semantic SLAM using Extended Reality**|Jose Luis Sanchez-Lopez Team|[2509.14949](http://arxiv.org/abs/2509.14949)|null|
|**2025-07-16**|**Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards**|Gert Kootstra Team|[2507.12093](http://arxiv.org/abs/2507.12093)|null|
|**2025-12-03**|**GS4: Generalizable Sparse Splatting Semantic SLAM**|Li Fuxin Team|[2506.06517](http://arxiv.org/abs/2506.06517)|null|
|**2025-06-03**|**GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal**|Yingchun Fan Team|[2506.02736](http://arxiv.org/abs/2506.02736)|null|
|**2025-05-18**|**Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**|François Goulette Team|[2505.12384](http://arxiv.org/abs/2505.12384)|null|
|**2025-05-16**|**GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field**|Changyin Sun Team|[2504.19409](http://arxiv.org/abs/2504.19409)|null|
|**2025-04-01**|**Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments**|Haoyi Xiong Team|[2504.01997](http://arxiv.org/abs/2504.01997)|null|
|**2025-03-03**|**OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding**|Mengyin Fu Team|[2503.01646](http://arxiv.org/abs/2503.01646)|null|
|**2025-07-09**|**Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting**|Hamid Rezatofighi Team|[2502.14931](http://arxiv.org/abs/2502.14931)|null|
|**2024-12-31**|**PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM**|Tongliang Liu Team|[2501.00352](http://arxiv.org/abs/2501.00352)|null|
|**2025-07-11**|**Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem**|Hesheng Wang Team|[2410.12169](http://arxiv.org/abs/2410.12169)|null|
|**2025-03-10**|**Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting**|Hamid Rezatofighi Team|[2409.12518](http://arxiv.org/abs/2409.12518)|**[link](https://github.com/LeeBY68/Hier-SLAM)**|
|**2024-09-02**|**Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration**|Giovanni Beltrame Team|[2408.14726](http://arxiv.org/abs/2408.14726)|null|
|**2025-10-03**|**SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation**|Vijay Kumar Team|[2406.17249](http://arxiv.org/abs/2406.17249)|null|
|**2024-06-09**|**MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps**|Iro Armeni Team|[2406.05849](http://arxiv.org/abs/2406.05849)|null|

