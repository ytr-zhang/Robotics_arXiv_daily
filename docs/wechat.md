> Updated on 2026.02.06
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#manipulation>Manipulation</a></li>
    <li><a href=#vlm>VLM</a></li>
    <li><a href=#vla>VLA</a></li>
    <li><a href=#humanoid>Humanoid</a></li>
    <li><a href=#humanoid-locomotion>Humanoid-Locomotion</a></li>
    <li><a href=#vln-navigation>VLN-Navigation</a></li>
    <li><a href=#vla-navigation>VLA-Navigation</a></li>
    <li><a href=#dexterous>Dexterous</a></li>
    <li><a href=#semantic-slam>Semantic-SLAM</a></li>
  </ol>
</details>

## Manipulation

- 2026-02-05, **TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**, Shigeki Sugano Team, Paper: [http://arxiv.org/abs/2602.05468](http://arxiv.org/abs/2602.05468)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **CoWTracker: Tracking by Warping instead of Correlation**, Andrea Vedaldi Team, Paper: [http://arxiv.org/abs/2602.04877](http://arxiv.org/abs/2602.04877)
- 2026-02-04, **A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction**, Riddhiman Laha Team, Paper: [http://arxiv.org/abs/2602.04522](http://arxiv.org/abs/2602.04522)
- 2026-02-04, **Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2602.04243](http://arxiv.org/abs/2602.04243)
- 2026-02-04, **GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning**, Hongliang Ren Team, Paper: [http://arxiv.org/abs/2602.04231](http://arxiv.org/abs/2602.04231)
- 2026-02-04, **Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**, Badong Chen Team, Paper: [http://arxiv.org/abs/2602.04228](http://arxiv.org/abs/2602.04228)
- 2026-02-04, **OAT: Ordered Action Tokenization**, Yilun Du Team, Paper: [http://arxiv.org/abs/2602.04215](http://arxiv.org/abs/2602.04215)
- 2026-02-04, **InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons**, Reid Simmons Team, Paper: [http://arxiv.org/abs/2602.04213](http://arxiv.org/abs/2602.04213)
- 2026-02-04, **A brief review of evolutionary game dynamics in the reinforcement learning paradigm**, Li Chen Team, Paper: [http://arxiv.org/abs/2602.04150](http://arxiv.org/abs/2602.04150)
- 2026-02-03, **Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study**, Farshid Alambeigi Team, Paper: [http://arxiv.org/abs/2602.04076](http://arxiv.org/abs/2602.04076)
- 2026-02-03, **VLS: Steering Pretrained Robot Policies via Vision-Language Models**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.03973](http://arxiv.org/abs/2602.03973), Code: **[https://vision-language-steering.github.io/webpage/](https://vision-language-steering.github.io/webpage/)**
- 2026-02-03, **MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**, Jungwoo Lee Team, Paper: [http://arxiv.org/abs/2602.03668](http://arxiv.org/abs/2602.03668)
- 2026-02-03, **Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives**, Sho Sakaino Team, Paper: [http://arxiv.org/abs/2602.03188](http://arxiv.org/abs/2602.03188)
- 2026-02-02, **Language Movement Primitives: Grounding Language Models in Robot Motion**, Simon Stepputtis Team, Paper: [http://arxiv.org/abs/2602.02839](http://arxiv.org/abs/2602.02839)
- 2026-02-02, **On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning**, Sébastien Lachapelle Team, Paper: [http://arxiv.org/abs/2602.02762](http://arxiv.org/abs/2602.02762)
- 2026-02-02, **HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**, Ping Tan Team, Paper: [http://arxiv.org/abs/2602.02473](http://arxiv.org/abs/2602.02473)
- 2026-02-02, **TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**, Jiaqi Ma Team, Paper: [http://arxiv.org/abs/2602.02459](http://arxiv.org/abs/2602.02459)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning**, Alan Ritter Team, Paper: [http://arxiv.org/abs/2602.02405](http://arxiv.org/abs/2602.02405)
- 2026-02-02, **SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.02402](http://arxiv.org/abs/2602.02402), Code: **[https://city-super.github.io/SoMA/](https://city-super.github.io/SoMA/)**
- 2026-02-02, **PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning**, Alexander Schperberg Team, Paper: [http://arxiv.org/abs/2602.02396](http://arxiv.org/abs/2602.02396)
- 2026-02-02, **Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy**, Qiang Nie Team, Paper: [http://arxiv.org/abs/2602.01939](http://arxiv.org/abs/2602.01939)
- 2026-02-02, **ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning**, Sifa Zheng Team, Paper: [http://arxiv.org/abs/2602.01916](http://arxiv.org/abs/2602.01916)
- 2026-02-02, **BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01870](http://arxiv.org/abs/2602.01870)
- 2026-02-03, **RFS: Reinforcement learning with Residual flow steering for dexterous manipulation**, Abhishek Gupta Team, Paper: [http://arxiv.org/abs/2602.01789](http://arxiv.org/abs/2602.01789)
- 2026-02-02, **AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**, Yu She Team, Paper: [http://arxiv.org/abs/2602.01662](http://arxiv.org/abs/2602.01662)
- 2026-02-02, **A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**, Shreyas Kousik Team, Paper: [http://arxiv.org/abs/2602.01632](http://arxiv.org/abs/2602.01632), Code: **[https://sew-mimic.com/](https://sew-mimic.com/)**
- 2026-02-01, **Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning**, Weitong Zhang Team, Paper: [http://arxiv.org/abs/2602.01357](http://arxiv.org/abs/2602.01357)
- 2026-02-01, **Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.01166](http://arxiv.org/abs/2602.01166)
- 2026-02-01, **Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01158](http://arxiv.org/abs/2602.01158)
- 2026-02-01, **UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors**, Shan Luo Team, Paper: [http://arxiv.org/abs/2602.01153](http://arxiv.org/abs/2602.01153)
- 2026-02-01, **KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV**, Ziyang Wang Team, Paper: [http://arxiv.org/abs/2602.01115](http://arxiv.org/abs/2602.01115)
- 2026-02-01, **StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**, Lu Fang Team, Paper: [http://arxiv.org/abs/2602.01100](http://arxiv.org/abs/2602.01100)
- 2026-02-01, **Estimating Force Interactions of Deformable Linear Objects from their Shapes**, Quang-Cuong Pham Team, Paper: [http://arxiv.org/abs/2602.01085](http://arxiv.org/abs/2602.01085)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-30, **Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**, Liu Hong Team, Paper: [http://arxiv.org/abs/2601.23087](http://arxiv.org/abs/2601.23087)
- 2026-01-30, **Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**, Guang Chen Team, Paper: [http://arxiv.org/abs/2601.22988](http://arxiv.org/abs/2601.22988)
- 2026-01-30, **Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation**, Wuyue Zhao Team, Paper: [http://arxiv.org/abs/2601.22965](http://arxiv.org/abs/2601.22965)
- 2026-01-29, **PoSafeNet: Safe Learning with Poset-Structured Neural Nets**, Daniela Rus Team, Paper: [http://arxiv.org/abs/2601.22356](http://arxiv.org/abs/2601.22356)
- 2026-01-29, **Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data**, Bowen Weng Team, Paper: [http://arxiv.org/abs/2601.22242](http://arxiv.org/abs/2601.22242)
- 2026-01-29, **Causal Imitation Learning Under Measurement Error and Distribution Shift**, AmirEmad Ghassami Team, Paper: [http://arxiv.org/abs/2601.22206](http://arxiv.org/abs/2601.22206)
- 2026-01-29, **mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning**, Pieter Abbeel Team, Paper: [http://arxiv.org/abs/2601.22074](http://arxiv.org/abs/2601.22074), Code: **[https://github.com/mujocolab/mjlab](https://github.com/mujocolab/mjlab)**
- 2026-01-30, **PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**, Jie Mei Team, Paper: [http://arxiv.org/abs/2601.22018](http://arxiv.org/abs/2601.22018)
- 2026-01-29, **Causal World Modeling for Robot Control**, Yinghao Xu Team, Paper: [http://arxiv.org/abs/2601.21998](http://arxiv.org/abs/2601.21998), Code: **[https://technology.robbyant.com/lingbot-va](https://technology.robbyant.com/lingbot-va)**
- 2026-01-29, **MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**, Stefanie Speidel Team, Paper: [http://arxiv.org/abs/2601.21971](http://arxiv.org/abs/2601.21971)
- 2026-01-29, **Information Filtering via Variational Regularization for Robot Manipulation**, Jie Me Team, Paper: [http://arxiv.org/abs/2601.21926](http://arxiv.org/abs/2601.21926)
- 2026-01-29, **When does predictive inverse dynamics outperform behavior cloning?**, Sergio Valcarcel Macua Team, Paper: [http://arxiv.org/abs/2601.21718](http://arxiv.org/abs/2601.21718)
- 2026-01-29, **Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation**, Liming Chen Team, Paper: [http://arxiv.org/abs/2601.21416](http://arxiv.org/abs/2601.21416)
- 2026-01-29, **Towards Space-Based Environmentally-Adaptive Grasping**, Aleksandr Artemov Team, Paper: [http://arxiv.org/abs/2601.21394](http://arxiv.org/abs/2601.21394)
- 2026-01-29, **Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies**, Harold Soh Team, Paper: [http://arxiv.org/abs/2601.21251](http://arxiv.org/abs/2601.21251)
- 2026-01-28, **Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy**, Christos Bergeles Team, Paper: [http://arxiv.org/abs/2601.20776](http://arxiv.org/abs/2601.20776)
- 2026-01-28, **Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands**, Nicolás Navarro-Guerrero Team, Paper: [http://arxiv.org/abs/2601.20555](http://arxiv.org/abs/2601.20555)
- 2026-01-28, **Advancing Open-source World Models**, Hao Ouyang Team, Paper: [http://arxiv.org/abs/2601.20540](http://arxiv.org/abs/2601.20540), Code: **[https://technology.robbyant.com/lingbot-world](https://technology.robbyant.com/lingbot-world)**
- 2026-01-28, **STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation**, Liming Chen Team, Paper: [http://arxiv.org/abs/2601.20381](http://arxiv.org/abs/2601.20381)
- 2026-01-28, **Demonstration-Free Robotic Control via LLM Agents**, Tiffany J. Hwu Team, Paper: [http://arxiv.org/abs/2601.20334](http://arxiv.org/abs/2601.20334)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## VLM

- 2026-02-05, **Can vision language models learn intuitive physics from interaction?**, Eric Schulz Team, Paper: [http://arxiv.org/abs/2602.06033](http://arxiv.org/abs/2602.06033)
- 2026-02-05, **GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?**, Jiaqi Wang Team, Paper: [http://arxiv.org/abs/2602.06013](http://arxiv.org/abs/2602.06013), Code: **[https://genarena.github.io/](https://genarena.github.io/)**
- 2026-02-05, **Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning**, Xianming Liu Team, Paper: [http://arxiv.org/abs/2602.05809](http://arxiv.org/abs/2602.05809)
- 2026-02-05, **Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**, Weiming Zhang Team, Paper: [http://arxiv.org/abs/2602.05789](http://arxiv.org/abs/2602.05789)
- 2026-02-05, **Ethology of Latent Spaces**, Philippe Boisnard Team, Paper: [http://arxiv.org/abs/2602.05710](http://arxiv.org/abs/2602.05710), Code: **[https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf](https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf)**
- 2026-02-05, **LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation**, Yiguo Qiao Team, Paper: [http://arxiv.org/abs/2602.05578](http://arxiv.org/abs/2602.05578)
- 2026-02-05, **TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?**, Cheston Tan Team, Paper: [http://arxiv.org/abs/2602.05570](http://arxiv.org/abs/2602.05570)
- 2026-02-05, **VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator**, Miguel Cazorla Team, Paper: [http://arxiv.org/abs/2602.05552](http://arxiv.org/abs/2602.05552)
- 2026-02-05, **Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification**, Liping Jing Team, Paper: [http://arxiv.org/abs/2602.05535](http://arxiv.org/abs/2602.05535), Code: **[https://github.com/HT86159/EUQ](https://github.com/HT86159/EUQ)**
- 2026-02-05, **Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models**, Nadir Durrani Team, Paper: [http://arxiv.org/abs/2602.05437](http://arxiv.org/abs/2602.05437)
- 2026-02-05, **Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting**, Can Huang Team, Paper: [http://arxiv.org/abs/2602.05384](http://arxiv.org/abs/2602.05384)
- 2026-02-05, **VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs**, Konstantinos Psounis Team, Paper: [http://arxiv.org/abs/2602.05382](http://arxiv.org/abs/2602.05382)
- 2026-02-05, **Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions**, Tao Zhang Team, Paper: [http://arxiv.org/abs/2602.05273](http://arxiv.org/abs/2602.05273)
- 2026-02-05, **Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR**, Haibo Qiu Team, Paper: [http://arxiv.org/abs/2602.05261](http://arxiv.org/abs/2602.05261)
- 2026-02-05, **GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling**, Tong Zhang Team, Paper: [http://arxiv.org/abs/2602.05202](http://arxiv.org/abs/2602.05202)
- 2026-02-04, **ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation**, Yapeng Tian Team, Paper: [http://arxiv.org/abs/2602.05132](http://arxiv.org/abs/2602.05132)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?**, Alan Ritter Team, Paper: [http://arxiv.org/abs/2602.05023](http://arxiv.org/abs/2602.05023)
- 2026-02-04, **When LLaVA Meets Objects: Token Composition for Vision-Language-Models**, Hilde Kuehne Team, Paper: [http://arxiv.org/abs/2602.04864](http://arxiv.org/abs/2602.04864)
- 2026-02-04, **El Agente Estructural: An Artificially Intelligent Molecular Editor**, Varinia Bernales Team, Paper: [http://arxiv.org/abs/2602.04849](http://arxiv.org/abs/2602.04849)
- 2026-02-04, **VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?**, Huchuan Lu Team, Paper: [http://arxiv.org/abs/2602.04802](http://arxiv.org/abs/2602.04802)
- 2026-02-04, **Annotation Free Spacecraft Detection and Segmentation using Vision Language Models**, Djamila Aouada Team, Paper: [http://arxiv.org/abs/2602.04699](http://arxiv.org/abs/2602.04699)
- 2026-02-04, **AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04672](http://arxiv.org/abs/2602.04672)
- 2026-02-04, **PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04657](http://arxiv.org/abs/2602.04657)
- 2026-02-04, **Relational Scene Graphs for Object Grounding of Natural Language Commands**, Ville Kyrki Team, Paper: [http://arxiv.org/abs/2602.04635](http://arxiv.org/abs/2602.04635)
- 2026-02-04, **LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation**, Yan Song Team, Paper: [http://arxiv.org/abs/2602.04617](http://arxiv.org/abs/2602.04617)
- 2026-02-04, **VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration**, Kunwoo Park Team, Paper: [http://arxiv.org/abs/2602.04587](http://arxiv.org/abs/2602.04587)
- 2026-02-04, **Understanding Degradation with Vision Language Model**, Xuelong Li Team, Paper: [http://arxiv.org/abs/2602.04565](http://arxiv.org/abs/2602.04565)
- 2026-02-04, **EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**, Börje F. Karlsson Team, Paper: [http://arxiv.org/abs/2602.04515](http://arxiv.org/abs/2602.04515)
- 2026-02-04, **When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models**, Se-Young Yun Team, Paper: [http://arxiv.org/abs/2602.04356](http://arxiv.org/abs/2602.04356)
- 2026-02-04, **Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models**, Deyu Zhou Team, Paper: [http://arxiv.org/abs/2602.04355](http://arxiv.org/abs/2602.04355)
- 2026-02-04, **Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning**, Shu-Tao Xia Team, Paper: [http://arxiv.org/abs/2602.04340](http://arxiv.org/abs/2602.04340)
- 2026-02-04, **Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner**, Shu-Tao Xia Team, Paper: [http://arxiv.org/abs/2602.04337](http://arxiv.org/abs/2602.04337)
- 2026-02-04, **Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement**, Lin Gui Team, Paper: [http://arxiv.org/abs/2602.04304](http://arxiv.org/abs/2602.04304)
- 2026-02-04, **AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models**, Yunjiang Lou Team, Paper: [http://arxiv.org/abs/2602.04256](http://arxiv.org/abs/2602.04256)
- 2026-02-04, **VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding**, Weining Shen Team, Paper: [http://arxiv.org/abs/2602.04094](http://arxiv.org/abs/2602.04094)
- 2026-02-03, **Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**, Rex Ying Team, Paper: [http://arxiv.org/abs/2602.03983](http://arxiv.org/abs/2602.03983)
- 2026-02-03, **VLS: Steering Pretrained Robot Policies via Vision-Language Models**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.03973](http://arxiv.org/abs/2602.03973), Code: **[https://vision-language-steering.github.io/webpage/](https://vision-language-steering.github.io/webpage/)**
- 2026-02-03, **They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References**, Usman Naseem Team, Paper: [http://arxiv.org/abs/2602.03822](http://arxiv.org/abs/2602.03822)
- 2026-02-03, **Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives**, Katherine D. Van Schaik Team, Paper: [http://arxiv.org/abs/2602.03750](http://arxiv.org/abs/2602.03750)
- 2026-02-03, **Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment**, Mahdi Abdelguerfi Team, Paper: [http://arxiv.org/abs/2602.03742](http://arxiv.org/abs/2602.03742)
- 2026-02-03, **RegionReasoner: Region-Grounded Multi-Round Visual Reasoning**, Cees G. M. Snoek Team, Paper: [http://arxiv.org/abs/2602.03733](http://arxiv.org/abs/2602.03733)
- 2026-02-03, **MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment**, Gunhee Kim Team, Paper: [http://arxiv.org/abs/2602.03665](http://arxiv.org/abs/2602.03665)
- 2026-02-03, **KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs**, Jianyuan Guo Team, Paper: [http://arxiv.org/abs/2602.03615](http://arxiv.org/abs/2602.03615)
- 2026-02-03, **TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection**, Mohammad Sabokrou Team, Paper: [http://arxiv.org/abs/2602.03594](http://arxiv.org/abs/2602.03594)
- 2026-02-03, **Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning**, Jianxiong Wang Team, Paper: [http://arxiv.org/abs/2602.03530](http://arxiv.org/abs/2602.03530)
- 2026-02-03, **Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance**, Min Zhang Team, Paper: [http://arxiv.org/abs/2602.03491](http://arxiv.org/abs/2602.03491)
- 2026-02-03, **Contextualized Visual Personalization in Vision-Language Models**, Sungroh Yoon Team, Paper: [http://arxiv.org/abs/2602.03454](http://arxiv.org/abs/2602.03454), Code: **[https://github.com/oyt9306/CoViP](https://github.com/oyt9306/CoViP)**
- 2026-02-03, **AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation**, Jimmy Xiangji Huang Team, Paper: [http://arxiv.org/abs/2602.03416](http://arxiv.org/abs/2602.03416)
- 2026-02-03, **Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility**, Ming Li Team, Paper: [http://arxiv.org/abs/2602.03402](http://arxiv.org/abs/2602.03402)
- 2026-02-03, **POP: Prefill-Only Pruning for Efficient Large Model Inference**, Qingan Li Team, Paper: [http://arxiv.org/abs/2602.03295](http://arxiv.org/abs/2602.03295)
- 2026-02-03, **LaVPR: Benchmarking Language and Vision for Place Recognition**, Yoli Shavit Team, Paper: [http://arxiv.org/abs/2602.03253](http://arxiv.org/abs/2602.03253)
- 2026-02-03, **Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration**, Haixia Bi Team, Paper: [http://arxiv.org/abs/2602.03151](http://arxiv.org/abs/2602.03151)
- 2026-02-03, **SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass**, Xin Miao Team, Paper: [http://arxiv.org/abs/2602.03134](http://arxiv.org/abs/2602.03134)
- 2026-02-03, **FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation**, Rongjunchen Zhang Team, Paper: [http://arxiv.org/abs/2602.03130](http://arxiv.org/abs/2602.03130)
- 2026-02-03, **Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors**, Wenbo Ding Team, Paper: [http://arxiv.org/abs/2602.03119](http://arxiv.org/abs/2602.03119)
- 2026-02-03, **IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning**, Yongchao Xu Team, Paper: [http://arxiv.org/abs/2602.03060](http://arxiv.org/abs/2602.03060)
- 2026-02-03, **Bongards at the Boundary of Perception and Reasoning: Programs or Language?**, Kevin Ellis Team, Paper: [http://arxiv.org/abs/2602.03038](http://arxiv.org/abs/2602.03038)
- 2026-02-02, **Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning**, Kostas Alexis Team, Paper: [http://arxiv.org/abs/2602.02456](http://arxiv.org/abs/2602.02456)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **ReasonEdit: Editing Vision-Language Models using Human Reasoning**, Thomas Hartvigsen Team, Paper: [http://arxiv.org/abs/2602.02408](http://arxiv.org/abs/2602.02408)
- 2026-02-02, **LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization**, Limin Wang Team, Paper: [http://arxiv.org/abs/2602.02341](http://arxiv.org/abs/2602.02341)
- 2026-02-02, **See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers**, Takeo Igarashi Team, Paper: [http://arxiv.org/abs/2602.02063](http://arxiv.org/abs/2602.02063)
- 2026-02-02, **Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models**, Toshihiko Yamasaki Team, Paper: [http://arxiv.org/abs/2602.02043](http://arxiv.org/abs/2602.02043)
- 2026-02-02, **Rethinking Genomic Modeling Through Optical Character Recognition**, Xiangxiang Zeng Team, Paper: [http://arxiv.org/abs/2602.02014](http://arxiv.org/abs/2602.02014)
- 2026-02-02, **Enhancing Multi-Image Understanding through Delimiter Token Scaling**, Junsuk Choe Team, Paper: [http://arxiv.org/abs/2602.01984](http://arxiv.org/abs/2602.01984)
- 2026-02-02, **VLM-Guided Experience Replay**, Shie Mannor Team, Paper: [http://arxiv.org/abs/2602.01915](http://arxiv.org/abs/2602.01915)
- 2026-02-02, **Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery**, J. Marius Zöllner Team, Paper: [http://arxiv.org/abs/2602.01836](http://arxiv.org/abs/2602.01836)
- 2026-02-02, **CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding**, Xiaodong Gu Team, Paper: [http://arxiv.org/abs/2602.01785](http://arxiv.org/abs/2602.01785), Code: **[https://github.com/YerbaPage/CodeOCR](https://github.com/YerbaPage/CodeOCR)**
- 2026-02-02, **Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models**, Bin Li Team, Paper: [http://arxiv.org/abs/2602.01738](http://arxiv.org/abs/2602.01738)
- 2026-02-02, **AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**, Yu She Team, Paper: [http://arxiv.org/abs/2602.01662](http://arxiv.org/abs/2602.01662)
- 2026-02-02, **ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval**, Tat-Seng Chua Team, Paper: [http://arxiv.org/abs/2602.01639](http://arxiv.org/abs/2602.01639)
- 2026-02-02, **PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards**, Mei Chen Team, Paper: [http://arxiv.org/abs/2602.01624](http://arxiv.org/abs/2602.01624)
- 2026-02-02, **Generative Visual Code Mobile World Models**, Jamin Shin Team, Paper: [http://arxiv.org/abs/2602.01576](http://arxiv.org/abs/2602.01576)
- 2026-02-02, **SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models**, Xiaochun Cao Team, Paper: [http://arxiv.org/abs/2602.01574](http://arxiv.org/abs/2602.01574)
- 2026-02-02, **Preserving Localized Patch Semantics in VLMs**, Longin Jan Latecki Team, Paper: [http://arxiv.org/abs/2602.01530](http://arxiv.org/abs/2602.01530)
- 2026-02-02, **Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition**, Brian Keith-Norambuena Team, Paper: [http://arxiv.org/abs/2602.01527](http://arxiv.org/abs/2602.01527)
- 2026-02-01, **Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles**, Jiachen Bian Team, Paper: [http://arxiv.org/abs/2602.01452](http://arxiv.org/abs/2602.01452)
- 2026-01-30, **User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**, Maria Gorlatova Team, Paper: [http://arxiv.org/abs/2601.23281](http://arxiv.org/abs/2601.23281)
- 2026-01-30, **Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**, Liang-Jie Zhang Team, Paper: [http://arxiv.org/abs/2601.23253](http://arxiv.org/abs/2601.23253)
- 2026-01-30, **Structured Over Scale: Learning Spatial Reasoning from Educational Video**, Sarah Ostadabbas Team, Paper: [http://arxiv.org/abs/2601.23251](http://arxiv.org/abs/2601.23251)
- 2026-01-30, **Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**, Lijie Hu Team, Paper: [http://arxiv.org/abs/2601.23149](http://arxiv.org/abs/2601.23149)
- 2026-01-30, **One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**, Dong Liu Team, Paper: [http://arxiv.org/abs/2601.23041](http://arxiv.org/abs/2601.23041)
- 2026-01-30, **Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2601.22959](http://arxiv.org/abs/2601.22959)
- 2026-01-30, **Alignment among Language, Vision and Action Representations**, Stefano Nolfi Team, Paper: [http://arxiv.org/abs/2601.22948](http://arxiv.org/abs/2601.22948)
- 2026-01-30, **A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions**, Arno Eichberger Team, Paper: [http://arxiv.org/abs/2601.22830](http://arxiv.org/abs/2601.22830)
- 2026-01-30, **Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA**, Yinghuan Shi Team, Paper: [http://arxiv.org/abs/2601.22828](http://arxiv.org/abs/2601.22828)
- 2026-01-30, **HeatMat: Simulation of City Material Impact on Urban Heat Island Effect**, Rosalie Martin Team, Paper: [http://arxiv.org/abs/2601.22796](http://arxiv.org/abs/2601.22796)
- 2026-01-30, **Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models**, Christos Emmanouilidis Team, Paper: [http://arxiv.org/abs/2601.22754](http://arxiv.org/abs/2601.22754)
- 2026-01-30, **StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing**, Roy Ka-Wei Lee Team, Paper: [http://arxiv.org/abs/2601.22738](http://arxiv.org/abs/2601.22738)
- 2026-01-30, **Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models**, Tat-Seng Chua Team, Paper: [http://arxiv.org/abs/2601.22737](http://arxiv.org/abs/2601.22737)
- 2026-01-30, **Vision-Language Models Unlock Task-Centric Latent Actions**, Vladislav Kurenkov Team, Paper: [http://arxiv.org/abs/2601.22714](http://arxiv.org/abs/2601.22714)
- 2026-01-30, **Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs**, Yawei Li Team, Paper: [http://arxiv.org/abs/2601.22709](http://arxiv.org/abs/2601.22709)
- 2026-01-30, **Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**, Kai Yuan Team, Paper: [http://arxiv.org/abs/2601.22701](http://arxiv.org/abs/2601.22701)
- 2026-01-30, **Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding**, Hyun Gyu Lee Team, Paper: [http://arxiv.org/abs/2601.22696](http://arxiv.org/abs/2601.22696)
- 2026-01-30, **Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction**, Nuno Vasconcelos Team, Paper: [http://arxiv.org/abs/2601.22570](http://arxiv.org/abs/2601.22570)
- 2026-01-30, **Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**, Jinsong Su Team, Paper: [http://arxiv.org/abs/2601.22451](http://arxiv.org/abs/2601.22451), Code: **[https://github.com/Liushiyu-0709/SelfVal](https://github.com/Liushiyu-0709/SelfVal)**
- 2026-01-29, **Jailbreaks on Vision Language Model via Multimodal Reasoning**, Yuguang Yao Team, Paper: [http://arxiv.org/abs/2601.22398](http://arxiv.org/abs/2601.22398)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## VLA

- 2026-02-05, **RL-VLA $^3$ : Reinforcement Learning VLA Accelerating via Full Asynchronism**, Junwu Xiong Team, Paper: [http://arxiv.org/abs/2602.05765](http://arxiv.org/abs/2602.05765)
- 2026-02-05, **Benchmarking Affordance Generalization with BusyBox**, Galen Mullins Team, Paper: [http://arxiv.org/abs/2602.05441](http://arxiv.org/abs/2602.05441)
- 2026-02-05, **RoboPaint: From Human Demonstration to Any Robot and Any View**, Zhengxue Cheng Team, Paper: [http://arxiv.org/abs/2602.05325](http://arxiv.org/abs/2602.05325)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2602.04600](http://arxiv.org/abs/2602.04600)
- 2026-02-04, **GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning**, Hao Tang Team, Paper: [http://arxiv.org/abs/2602.04315](http://arxiv.org/abs/2602.04315)
- 2026-02-04, **Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**, Badong Chen Team, Paper: [http://arxiv.org/abs/2602.04228](http://arxiv.org/abs/2602.04228)
- 2026-02-04, **SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models**, Jonghyun Choi Team, Paper: [http://arxiv.org/abs/2602.04208](http://arxiv.org/abs/2602.04208)
- 2026-02-04, **Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models**, Ross Greer Team, Paper: [http://arxiv.org/abs/2602.04184](http://arxiv.org/abs/2602.04184)
- 2026-02-03, **Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**, Rex Ying Team, Paper: [http://arxiv.org/abs/2602.03983](http://arxiv.org/abs/2602.03983)
- 2026-02-03, **QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization**, Zhipeng Zhang Team, Paper: [http://arxiv.org/abs/2602.03782](http://arxiv.org/abs/2602.03782)
- 2026-02-03, **MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**, Jungwoo Lee Team, Paper: [http://arxiv.org/abs/2602.03668](http://arxiv.org/abs/2602.03668)
- 2026-02-03, **CRL-VLA: Continual Vision-Language-Action Learning**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03445](http://arxiv.org/abs/2602.03445)
- 2026-02-03, **RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization**, Jun Zhu Team, Paper: [http://arxiv.org/abs/2602.03310](http://arxiv.org/abs/2602.03310)
- 2026-02-03, **When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens**, Miao Li Team, Paper: [http://arxiv.org/abs/2602.03153](http://arxiv.org/abs/2602.03153)
- 2026-02-02, **Accelerating Structured Chain-of-Thought in Autonomous Vehicles**, Marco Pavone Team, Paper: [http://arxiv.org/abs/2602.02864](http://arxiv.org/abs/2602.02864)
- 2026-02-02, **TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**, Jiaqi Ma Team, Paper: [http://arxiv.org/abs/2602.02459](http://arxiv.org/abs/2602.02459)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models**, Lemiao Qiu Team, Paper: [http://arxiv.org/abs/2602.02212](http://arxiv.org/abs/2602.02212)
- 2026-02-02, **FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**, Haiyue Zhu Team, Paper: [http://arxiv.org/abs/2602.02142](http://arxiv.org/abs/2602.02142)
- 2026-02-02, **Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models**, Di Wang Team, Paper: [http://arxiv.org/abs/2602.01834](http://arxiv.org/abs/2602.01834)
- 2026-02-02, **From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2602.01811](http://arxiv.org/abs/2602.01811)
- 2026-02-01, **Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.01166](http://arxiv.org/abs/2602.01166)
- 2026-02-01, **Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01158](http://arxiv.org/abs/2602.01158)
- 2026-02-01, **StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**, Lu Fang Team, Paper: [http://arxiv.org/abs/2602.01100](http://arxiv.org/abs/2602.01100)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-31, **Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**, A. Postnikov Team, Paper: [http://arxiv.org/abs/2602.00919](http://arxiv.org/abs/2602.00919)
- 2026-01-31, **Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds**, Hengshuang Zhao Team, Paper: [http://arxiv.org/abs/2602.00807](http://arxiv.org/abs/2602.00807)
- 2026-01-31, **Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models**, Yanyong Zhang Team, Paper: [http://arxiv.org/abs/2602.00780](http://arxiv.org/abs/2602.00780)
- 2026-01-31, **SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning**, Ivor Tsang Team, Paper: [http://arxiv.org/abs/2602.00743](http://arxiv.org/abs/2602.00743)
- 2026-01-31, **Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00686](http://arxiv.org/abs/2602.00686)
- 2026-01-31, **ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00557](http://arxiv.org/abs/2602.00557)
- 2026-01-31, **Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00500](http://arxiv.org/abs/2602.00500)
- 2026-01-30, **Vision-Language Models Unlock Task-Centric Latent Actions**, Vladislav Kurenkov Team, Paper: [http://arxiv.org/abs/2601.22714](http://arxiv.org/abs/2601.22714)
- 2026-01-30, **CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2601.22467](http://arxiv.org/abs/2601.22467)
- 2026-01-29, **DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**, Ziwei Liu Team, Paper: [http://arxiv.org/abs/2601.22153](http://arxiv.org/abs/2601.22153), Code: **[https://www.infinitescript.com/project/dynamic-vla/](https://www.infinitescript.com/project/dynamic-vla/)**
- 2026-01-29, **MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**, Stefanie Speidel Team, Paper: [http://arxiv.org/abs/2601.21971](http://arxiv.org/abs/2601.21971)
- 2026-01-29, **CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**, Yaohua Liu Team, Paper: [http://arxiv.org/abs/2601.21712](http://arxiv.org/abs/2601.21712)
- 2026-01-29, **AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**, Yonglin Tian Team, Paper: [http://arxiv.org/abs/2601.21602](http://arxiv.org/abs/2601.21602)
- 2026-01-29, **IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation**, Jeonggil Ko Team, Paper: [http://arxiv.org/abs/2601.21506](http://arxiv.org/abs/2601.21506)
- 2026-01-28, **Demonstration-Free Robotic Control via LLM Agents**, Tiffany J. Hwu Team, Paper: [http://arxiv.org/abs/2601.20334](http://arxiv.org/abs/2601.20334)
- 2026-01-30, **TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation**, Ziyuan Jiao Team, Paper: [http://arxiv.org/abs/2601.20321](http://arxiv.org/abs/2601.20321)
- 2026-01-28, **Shallow-π: Knowledge Distillation for Flow-based VLAs**, Taehan Kim Team, Paper: [http://arxiv.org/abs/2601.20262](http://arxiv.org/abs/2601.20262)
- 2026-01-27, **AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation**, Lei Zhu Team, Paper: [http://arxiv.org/abs/2601.19634](http://arxiv.org/abs/2601.19634)
- 2026-01-26, **Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods**, Hong Liu Team, Paper: [http://arxiv.org/abs/2601.18723](http://arxiv.org/abs/2601.18723)
- 2026-01-26, **A Pragmatic VLA Foundation Model**, Kecheng Zheng Team, Paper: [http://arxiv.org/abs/2601.18692](http://arxiv.org/abs/2601.18692), Code: **[https://technology.robbyant.com/lingbot-vla/](https://technology.robbyant.com/lingbot-vla/)**
- 2026-01-26, **TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion**, Jian Tang Team, Paper: [http://arxiv.org/abs/2601.18323](http://arxiv.org/abs/2601.18323)
- 2026-01-25, **PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation**, Xun Cao Team, Paper: [http://arxiv.org/abs/2601.17885](http://arxiv.org/abs/2601.17885)
- 2026-01-25, **SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation**, Andrew Jaeyong Choi Team, Paper: [http://arxiv.org/abs/2601.17657](http://arxiv.org/abs/2601.17657)
- 2026-01-23, **ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance**, Wei-Shi Zheng Team, Paper: [http://arxiv.org/abs/2601.16667](http://arxiv.org/abs/2601.16667)
- 2026-01-23, **Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)**, Walid G. Aref Team, Paper: [http://arxiv.org/abs/2601.16409](http://arxiv.org/abs/2601.16409)
- 2026-01-22, **IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance**, Michael S Ryoo Team, Paper: [http://arxiv.org/abs/2601.16207](http://arxiv.org/abs/2601.16207)
- 2026-01-22, **Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning**, Jinwei Gu Team, Paper: [http://arxiv.org/abs/2601.16163](http://arxiv.org/abs/2601.16163)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## Humanoid

- 2026-02-05, **A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion**, Simon F. G. Ehlers Team, Paper: [http://arxiv.org/abs/2602.05855](http://arxiv.org/abs/2602.05855)
- 2026-02-05, **Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2602.05791](http://arxiv.org/abs/2602.05791)
- 2026-02-05, **TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**, Jaeheung Park Team, Paper: [http://arxiv.org/abs/2602.05596](http://arxiv.org/abs/2602.05596)
- 2026-02-05, **Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework**, Xuelong Li Team, Paper: [http://arxiv.org/abs/2602.05310](http://arxiv.org/abs/2602.05310)
- 2026-02-04, **PDF-HR: Pose Distance Fields for Humanoid Robots**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2602.04851](http://arxiv.org/abs/2602.04851), Code: **[https://gaoyukang33.github.io/PDF-HR/}{Project](https://gaoyukang33.github.io/PDF-HR/}{Project)**
- 2026-02-04, **EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**, Börje F. Karlsson Team, Paper: [http://arxiv.org/abs/2602.04515](http://arxiv.org/abs/2602.04515)
- 2026-02-05, **HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation**, Hong Jia Team, Paper: [http://arxiv.org/abs/2602.04412](http://arxiv.org/abs/2602.04412)
- 2026-02-02, **Flow Policy Gradients for Robot Control**, Angjoo Kanazawa Team, Paper: [http://arxiv.org/abs/2602.02481](http://arxiv.org/abs/2602.02481), Code: **[https://hongsukchoi.github.io/fpo-control](https://hongsukchoi.github.io/fpo-control)**
- 2026-02-02, **HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**, Ping Tan Team, Paper: [http://arxiv.org/abs/2602.02473](http://arxiv.org/abs/2602.02473)
- 2026-02-02, **TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour**, Hang Zhao Team, Paper: [http://arxiv.org/abs/2602.02331](http://arxiv.org/abs/2602.02331), Code: **[https://ttt-parkour.github.io/](https://ttt-parkour.github.io/)**
- 2026-02-02, **A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**, Shreyas Kousik Team, Paper: [http://arxiv.org/abs/2602.01632](http://arxiv.org/abs/2602.01632), Code: **[https://sew-mimic.com/](https://sew-mimic.com/)**
- 2026-02-02, **RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots**, David Howard Team, Paper: [http://arxiv.org/abs/2602.01515](http://arxiv.org/abs/2602.01515)
- 2026-02-01, **T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation**, Xiaochun Mai Team, Paper: [http://arxiv.org/abs/2602.01352](http://arxiv.org/abs/2602.01352)
- 2026-01-31, **Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**, A. Postnikov Team, Paper: [http://arxiv.org/abs/2602.00919](http://arxiv.org/abs/2602.00919)
- 2026-01-30, **ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control**, Farbod Farshidian Team, Paper: [http://arxiv.org/abs/2602.00401](http://arxiv.org/abs/2602.00401)
- 2026-01-30, **Robust and Generalized Humanoid Motion Tracking**, Dongdong Zheng Team, Paper: [http://arxiv.org/abs/2601.23080](http://arxiv.org/abs/2601.23080)
- 2026-01-30, **RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2601.22517](http://arxiv.org/abs/2601.22517)
- 2026-01-26, **HumanoidTurk: Expanding VR Haptics with Humanoids for Driving Simulations**, Jin-Hyuk Hong Team, Paper: [http://arxiv.org/abs/2601.18975](http://arxiv.org/abs/2601.18975)
- 2026-01-26, **Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot**, Josh Merel Team, Paper: [http://arxiv.org/abs/2601.18963](http://arxiv.org/abs/2601.18963)
- 2026-01-24, **MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions**, Tongtong Feng Team, Paper: [http://arxiv.org/abs/2601.17507](http://arxiv.org/abs/2601.17507)
- 2026-01-24, **PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2601.17440](http://arxiv.org/abs/2601.17440)
- 2026-01-24, **Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots**, Xihan Bian Team, Paper: [http://arxiv.org/abs/2601.17287](http://arxiv.org/abs/2601.17287)
- 2026-01-21, **Learning a Unified Latent Space for Cross-Embodiment Robot Control**, Dongheui Lee Team, Paper: [http://arxiv.org/abs/2601.15419](http://arxiv.org/abs/2601.15419)
- 2026-01-21, **Vision-Language Models on the Edge for Real-Time Robotic Perception**, Syed Ali Raza Zaidi Team, Paper: [http://arxiv.org/abs/2601.14921](http://arxiv.org/abs/2601.14921)
- 2026-01-21, **HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation**, Dzmitry Tsetserukou Team, Paper: [http://arxiv.org/abs/2601.14874](http://arxiv.org/abs/2601.14874)
- 2026-01-19, **FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions**, Xipeng Qiu Team, Paper: [http://arxiv.org/abs/2601.12799](http://arxiv.org/abs/2601.12799), Code: **[https://openmoss.github.io/FRoM-W1](https://openmoss.github.io/FRoM-W1)**
- 2026-01-19, **FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation**, Yue Gao Team, Paper: [http://arxiv.org/abs/2601.12790](http://arxiv.org/abs/2601.12790)
- 2026-01-20, **ProjecTA: A Semi-Humanoid Robotic Teaching Assistant with In-Situ Projection for Guided Tours**, Pengcheng An Team, Paper: [http://arxiv.org/abs/2601.11328](http://arxiv.org/abs/2601.11328)
- 2026-01-15, **FastStair: Learning to Run Up Stairs with Humanoid Robots**, Jie Zhao Team, Paper: [http://arxiv.org/abs/2601.10365](http://arxiv.org/abs/2601.10365)
- 2026-01-13, **Heterogeneous computing platform for real-time robotics**, Steve Furber Team, Paper: [http://arxiv.org/abs/2601.09755](http://arxiv.org/abs/2601.09755)
- 2026-01-14, **Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations**, Wei-Shi Zheng Team, Paper: [http://arxiv.org/abs/2601.09518](http://arxiv.org/abs/2601.09518)
- 2026-01-13, **Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation**, Miao Li Team, Paper: [http://arxiv.org/abs/2601.09031](http://arxiv.org/abs/2601.09031)
- 2026-01-12, **WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots**, Jianfei Yang Team, Paper: [http://arxiv.org/abs/2601.07454](http://arxiv.org/abs/2601.07454)
- 2026-01-12, **AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers**, Zecui Zeng Team, Paper: [http://arxiv.org/abs/2601.07284](http://arxiv.org/abs/2601.07284)
- 2026-01-11, **RSLCPP -- Deterministic Simulations Using ROS 2**, Markus Lienkamp Team, Paper: [http://arxiv.org/abs/2601.07052](http://arxiv.org/abs/2601.07052)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## Humanoid-Locomotion

- 2026-02-05, **Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2602.05791](http://arxiv.org/abs/2602.05791)
- 2026-02-05, **TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**, Jaeheung Park Team, Paper: [http://arxiv.org/abs/2602.05596](http://arxiv.org/abs/2602.05596)
- 2026-02-03, **CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03511](http://arxiv.org/abs/2602.03511)
- 2026-02-03, **Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms**, Sung-Eui Yoon Team, Paper: [http://arxiv.org/abs/2602.03397](http://arxiv.org/abs/2602.03397), Code: **[https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/"](https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/")**
- 2026-02-02, **Flow Policy Gradients for Robot Control**, Angjoo Kanazawa Team, Paper: [http://arxiv.org/abs/2602.02481](http://arxiv.org/abs/2602.02481), Code: **[https://hongsukchoi.github.io/fpo-control](https://hongsukchoi.github.io/fpo-control)**
- 2026-01-29, **Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control**, Jingwen Zhang Team, Paper: [http://arxiv.org/abs/2601.21363](http://arxiv.org/abs/2601.21363)
- 2026-01-28, **GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control**, Guillaume Sartoretti Team, Paper: [http://arxiv.org/abs/2601.20668](http://arxiv.org/abs/2601.20668)
- 2026-01-22, **Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision**, Dongheui Lee Team, Paper: [http://arxiv.org/abs/2601.16109](http://arxiv.org/abs/2601.16109)
- 2026-01-13, **AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding**, Marco Hutter Team, Paper: [http://arxiv.org/abs/2601.08485](http://arxiv.org/abs/2601.08485)
- 2026-01-09, **Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds**, Aaron D. Ames Team, Paper: [http://arxiv.org/abs/2601.06286](http://arxiv.org/abs/2601.06286)
- 2026-01-07, **Locomotion Beyond Feet**, C. Karen Liu Team, Paper: [http://arxiv.org/abs/2601.03607](http://arxiv.org/abs/2601.03607), Code: **[https://locomotion-beyond-feet.github.io/](https://locomotion-beyond-feet.github.io/)**
- 2025-12-31, **Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer**, Hae-Won Park Team, Paper: [http://arxiv.org/abs/2512.24698](http://arxiv.org/abs/2512.24698)
- 2026-01-04, **Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.23650](http://arxiv.org/abs/2512.23650)
- 2026-01-04, **RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.23649](http://arxiv.org/abs/2512.23649)
- 2025-12-23, **Pneumatic bladder links with wide range of motion joints for articulated inflatable robots**, Ryuma Niiyama Team, Paper: [http://arxiv.org/abs/2512.20322](http://arxiv.org/abs/2512.20322)
- 2025-12-18, **E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion**, Dimitrios Kanoulas Team, Paper: [http://arxiv.org/abs/2512.16446](http://arxiv.org/abs/2512.16446)
- 2025-12-15, **Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations**, Ayonga Hereid Team, Paper: [http://arxiv.org/abs/2512.12993](http://arxiv.org/abs/2512.12993)
- 2025-12-08, **Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction**, Houqiang Li Team, Paper: [http://arxiv.org/abs/2512.07464](http://arxiv.org/abs/2512.07464)
- 2025-12-03, **Variable-Impedance Muscle Coordination under Slow-Rate Control Frequencies and Limited Observation Conditions Evaluated through Legged Locomotion**, Jun Morimoto Team, Paper: [http://arxiv.org/abs/2512.03459](http://arxiv.org/abs/2512.03459)
- 2025-12-01, **Learning Sim-to-Real Humanoid Locomotion in 15 Minutes**, Pieter Abbeel Team, Paper: [http://arxiv.org/abs/2512.01996](http://arxiv.org/abs/2512.01996), Code: **[https://younggyo.me/fastsac-humanoid](https://younggyo.me/fastsac-humanoid)**
- 2025-11-30, **H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2512.00971](http://arxiv.org/abs/2512.00971)
- 2025-11-25, **A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs**, Bowen Zhi Team, Paper: [http://arxiv.org/abs/2512.00077](http://arxiv.org/abs/2512.00077)
- 2025-11-28, **Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary**, Jingya Wang Team, Paper: [http://arxiv.org/abs/2511.22963](http://arxiv.org/abs/2511.22963), Code: **[https://humanoidlla.github.io/](https://humanoidlla.github.io/)**
- 2025-11-27, **Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion**, Wael Suleiman Team, Paper: [http://arxiv.org/abs/2511.22744](http://arxiv.org/abs/2511.22744), Code: **[https://anonymous.4open.science/r/multiview-parkour-6FB8](https://anonymous.4open.science/r/multiview-parkour-6FB8)**
- 2026-01-29, **HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments**, Bin He Team, Paper: [http://arxiv.org/abs/2511.20275](http://arxiv.org/abs/2511.20275)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## VLN-Navigation

- 2026-02-05, **Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation**, Hongyang Li Team, Paper: [http://arxiv.org/abs/2602.05827](http://arxiv.org/abs/2602.05827)
- 2026-02-05, **Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**, Weiming Zhang Team, Paper: [http://arxiv.org/abs/2602.05789](http://arxiv.org/abs/2602.05789)
- 2026-02-02, **LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation**, Anton van den Hengel Team, Paper: [http://arxiv.org/abs/2602.02220](http://arxiv.org/abs/2602.02220)
- 2026-02-03, **MapDream: Task-Driven Map Learning for Vision-Language Navigation**, Zhaoxin Fan Team, Paper: [http://arxiv.org/abs/2602.00222](http://arxiv.org/abs/2602.00222)
- 2026-01-29, **Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**, Xiaoming Wang Team, Paper: [http://arxiv.org/abs/2601.21751](http://arxiv.org/abs/2601.21751)
- 2026-01-26, **\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation**, Feng Zheng Team, Paper: [http://arxiv.org/abs/2601.18188](http://arxiv.org/abs/2601.18188)
- 2026-01-23, **FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation**, Yonggang Qi Team, Paper: [http://arxiv.org/abs/2601.13976](http://arxiv.org/abs/2601.13976)
- 2026-01-14, **Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning**, Yahong Han Team, Paper: [http://arxiv.org/abs/2601.09111](http://arxiv.org/abs/2601.09111)
- 2026-01-11, **Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**, Bin Ren Team, Paper: [http://arxiv.org/abs/2601.08868](http://arxiv.org/abs/2601.08868)
- 2026-01-13, **VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**, Junzhi Yu Team, Paper: [http://arxiv.org/abs/2601.08665](http://arxiv.org/abs/2601.08665), Code: **[https://wsakobe.github.io/VLingNav-web/](https://wsakobe.github.io/VLingNav-web/)**
- 2026-01-07, **AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions**, Renxin Zhong Team, Paper: [http://arxiv.org/abs/2601.03707](http://arxiv.org/abs/2601.03707)
- 2026-01-19, **LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality**, Ge Lin Kan Team, Paper: [http://arxiv.org/abs/2601.02167](http://arxiv.org/abs/2601.02167)
- 2026-01-05, **CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios**, Xueqian Wang Team, Paper: [http://arxiv.org/abs/2601.01872](http://arxiv.org/abs/2601.01872)
- 2026-01-06, **VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents**, Qi Wu Team, Paper: [http://arxiv.org/abs/2512.24851](http://arxiv.org/abs/2512.24851)
- 2026-01-23, **VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2512.22342](http://arxiv.org/abs/2512.22342)
- 2025-12-25, **AstraNav-World: World Model for Foresight Control and Consistency**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.21714](http://arxiv.org/abs/2512.21714)
- 2025-12-25, **AstraNav-Memory: Contexts Compression for Long Memory**, Mu Xu Team, Paper: [http://arxiv.org/abs/2512.21627](http://arxiv.org/abs/2512.21627)
- 2025-12-24, **ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments**, Yue Wang Team, Paper: [http://arxiv.org/abs/2512.20940](http://arxiv.org/abs/2512.20940)
- 2025-12-23, **TongSIM: A General Platform for Simulating Intelligent Machines**, Zhenliang Zhang Team, Paper: [http://arxiv.org/abs/2512.20206](http://arxiv.org/abs/2512.20206)
- 2025-12-22, **IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments**, Zhouhui Lian Team, Paper: [http://arxiv.org/abs/2512.19024](http://arxiv.org/abs/2512.19024)
- 2025-12-22, **VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation**, Qi Wu Team, Paper: [http://arxiv.org/abs/2512.19021](http://arxiv.org/abs/2512.19021)
- 2025-12-19, **Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation**, Eric Sax Team, Paper: [http://arxiv.org/abs/2512.18028](http://arxiv.org/abs/2512.18028)
- 2026-01-08, **ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**, Changyin Sun Team, Paper: [http://arxiv.org/abs/2512.17435](http://arxiv.org/abs/2512.17435)
- 2025-12-17, **HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2512.15047](http://arxiv.org/abs/2512.15047)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## VLA-Navigation

- 2025-08-14, **CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model**, Hao Dong Team, Paper: [http://arxiv.org/abs/2508.10416](http://arxiv.org/abs/2508.10416)
- 2024-07-12, **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**, Jie Tan Team, Paper: [http://arxiv.org/abs/2407.07775](http://arxiv.org/abs/2407.07775)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## Dexterous

- 2026-02-05, **DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter**, Zhenguo Sun Team, Paper: [http://arxiv.org/abs/2602.05513](http://arxiv.org/abs/2602.05513)
- 2026-02-05, **TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**, Shigeki Sugano Team, Paper: [http://arxiv.org/abs/2602.05468](http://arxiv.org/abs/2602.05468)
- 2026-02-05, **RoboPaint: From Human Demonstration to Any Robot and Any View**, Zhengxue Cheng Team, Paper: [http://arxiv.org/abs/2602.05325](http://arxiv.org/abs/2602.05325)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04672](http://arxiv.org/abs/2602.04672)
- 2026-02-03, **CRL-VLA: Continual Vision-Language-Action Learning**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03445](http://arxiv.org/abs/2602.03445)
- 2026-02-02, **FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**, Haiyue Zhu Team, Paper: [http://arxiv.org/abs/2602.02142](http://arxiv.org/abs/2602.02142)
- 2026-02-05, **RFS: Reinforcement Learning with Residual Flow Steering for Dexterous Manipulation**, Abhishek Gupta Team, Paper: [http://arxiv.org/abs/2602.01789](http://arxiv.org/abs/2602.01789)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-29, **DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching**, Shuo Wang Team, Paper: [http://arxiv.org/abs/2601.21474](http://arxiv.org/abs/2601.21474)
- 2026-01-26, **Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization**, Jongwoo Lim Team, Paper: [http://arxiv.org/abs/2601.18121](http://arxiv.org/abs/2601.18121)
- 2026-01-24, **PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2601.17440](http://arxiv.org/abs/2601.17440)
- 2026-01-31, **CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes**, Hao Dong Team, Paper: [http://arxiv.org/abs/2601.15039](http://arxiv.org/abs/2601.15039)
- 2026-01-23, **Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation**, Wanxin Jin Team, Paper: [http://arxiv.org/abs/2601.10930](http://arxiv.org/abs/2601.10930)
- 2026-01-13, **FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2601.08246](http://arxiv.org/abs/2601.08246)
- 2026-01-12, **Stable In-hand Manipulation for a Lightweight Four-motor Prosthetic Hand**, Masashi Hamaya Team, Paper: [http://arxiv.org/abs/2601.07559](http://arxiv.org/abs/2601.07559)
- 2026-01-09, **DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation**, Libin Liu Team, Paper: [http://arxiv.org/abs/2601.05844](http://arxiv.org/abs/2601.05844)
- 2026-02-02, **LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2601.05248](http://arxiv.org/abs/2601.05248)
- 2026-01-08, **UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation**, Peng Zhou Team, Paper: [http://arxiv.org/abs/2601.04629](http://arxiv.org/abs/2601.04629)
- 2026-01-09, **Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation**, Zhibin Li Team, Paper: [http://arxiv.org/abs/2601.02778](http://arxiv.org/abs/2601.02778)
- 2026-01-04, **DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos**, Robert B. Fisher Team, Paper: [http://arxiv.org/abs/2601.01651](http://arxiv.org/abs/2601.01651)
- 2025-12-31, **ShowUI- $π$ : Flow-based Generative Models as GUI Dexterous Hands**, Mike Zheng Shou Team, Paper: [http://arxiv.org/abs/2512.24965](http://arxiv.org/abs/2512.24965)
- 2025-12-31, **Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids**, David Hyunchul Shim Team, Paper: [http://arxiv.org/abs/2512.24657](http://arxiv.org/abs/2512.24657)
- 2026-01-01, **World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild**, Wenchao Ding Team, Paper: [http://arxiv.org/abs/2512.24310](http://arxiv.org/abs/2512.24310)
- 2026-01-09, **GR-Dexter Technical Report**, Hang Li Team, Paper: [http://arxiv.org/abs/2512.24210](http://arxiv.org/abs/2512.24210)
- 2025-12-29, **UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer**, Zongqing Lu Team, Paper: [http://arxiv.org/abs/2512.21233](http://arxiv.org/abs/2512.21233)
- 2025-12-22, **Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations**, Ping Tan Team, Paper: [http://arxiv.org/abs/2512.19583](http://arxiv.org/abs/2512.19583)
- 2025-12-19, **SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning**, Axel Krieger Team, Paper: [http://arxiv.org/abs/2512.18068](http://arxiv.org/abs/2512.18068)
- 2025-12-17, **ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision**, Jie Mei Team, Paper: [http://arxiv.org/abs/2512.15020](http://arxiv.org/abs/2512.15020)
- 2025-12-15, **World Models Can Leverage Human Videos for Dexterous Manipulation**, Yann LeCun Team, Paper: [http://arxiv.org/abs/2512.13644](http://arxiv.org/abs/2512.13644)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

## Semantic-SLAM

- 2026-01-09, **FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time**, Simon Hadfield Team, Paper: [http://arxiv.org/abs/2601.05738](http://arxiv.org/abs/2601.05738)
- 2025-12-01, **KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM**, Sergey Kolyubin Team, Paper: [http://arxiv.org/abs/2512.01889](http://arxiv.org/abs/2512.01889)
- 2025-11-28, **Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM**, Zhenhong Jia Team, Paper: [http://arxiv.org/abs/2511.22968](http://arxiv.org/abs/2511.22968)
- 2025-11-27, **Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM**, Anna Gelencsér-Horváth Team, Paper: [http://arxiv.org/abs/2511.16282](http://arxiv.org/abs/2511.16282)
- 2025-10-01, **Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions**, Nak Young Chong Team, Paper: [http://arxiv.org/abs/2510.00783](http://arxiv.org/abs/2510.00783)
- 2025-09-18, **Human Interaction for Collaborative Semantic SLAM using Extended Reality**, Jose Luis Sanchez-Lopez Team, Paper: [http://arxiv.org/abs/2509.14949](http://arxiv.org/abs/2509.14949)
- 2025-07-16, **Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards**, Gert Kootstra Team, Paper: [http://arxiv.org/abs/2507.12093](http://arxiv.org/abs/2507.12093)
- 2025-12-03, **GS4: Generalizable Sparse Splatting Semantic SLAM**, Li Fuxin Team, Paper: [http://arxiv.org/abs/2506.06517](http://arxiv.org/abs/2506.06517)
- 2025-06-03, **GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal**, Yingchun Fan Team, Paper: [http://arxiv.org/abs/2506.02736](http://arxiv.org/abs/2506.02736)
- 2025-05-18, **Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**, François Goulette Team, Paper: [http://arxiv.org/abs/2505.12384](http://arxiv.org/abs/2505.12384)
- 2025-05-16, **GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field**, Changyin Sun Team, Paper: [http://arxiv.org/abs/2504.19409](http://arxiv.org/abs/2504.19409)
- 2025-04-01, **Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments**, Haoyi Xiong Team, Paper: [http://arxiv.org/abs/2504.01997](http://arxiv.org/abs/2504.01997)
- 2025-03-03, **OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding**, Mengyin Fu Team, Paper: [http://arxiv.org/abs/2503.01646](http://arxiv.org/abs/2503.01646)
- 2025-07-09, **Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting**, Hamid Rezatofighi Team, Paper: [http://arxiv.org/abs/2502.14931](http://arxiv.org/abs/2502.14931)
- 2024-12-31, **PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM**, Tongliang Liu Team, Paper: [http://arxiv.org/abs/2501.00352](http://arxiv.org/abs/2501.00352)
- 2025-07-11, **Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2410.12169](http://arxiv.org/abs/2410.12169)
- 2025-03-10, **Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting**, Hamid Rezatofighi Team, Paper: [http://arxiv.org/abs/2409.12518](http://arxiv.org/abs/2409.12518), Code: **[https://github.com/LeeBY68/Hier-SLAM](https://github.com/LeeBY68/Hier-SLAM)**
- 2024-09-02, **Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration**, Giovanni Beltrame Team, Paper: [http://arxiv.org/abs/2408.14726](http://arxiv.org/abs/2408.14726)
- 2025-10-03, **SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation**, Vijay Kumar Team, Paper: [http://arxiv.org/abs/2406.17249](http://arxiv.org/abs/2406.17249)
- 2024-06-09, **MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps**, Iro Armeni Team, Paper: [http://arxiv.org/abs/2406.05849](http://arxiv.org/abs/2406.05849)

<p align=right>(<a href=#updated-on-20260206>back to top</a>)</p>

