> Updated on 2026.02.20
> Usage instructions: [here](./docs/README.md#usage)

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#manipulation>Manipulation</a></li>
    <li><a href=#vlm>VLM</a></li>
    <li><a href=#vla>VLA</a></li>
    <li><a href=#humanoid>Humanoid</a></li>
    <li><a href=#humanoid-locomotion>Humanoid-Locomotion</a></li>
    <li><a href=#vln-navigation>VLN-Navigation</a></li>
    <li><a href=#vla-navigation>VLA-Navigation</a></li>
    <li><a href=#dexterous>Dexterous</a></li>
    <li><a href=#semantic-slam>Semantic-SLAM</a></li>
  </ol>
</details>

## Manipulation

- 2026-02-18, **Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**, Saurabh Gupta Team, Paper: [http://arxiv.org/abs/2602.16705](http://arxiv.org/abs/2602.16705), Code: **[https://hero-humanoid.github.io/](https://hero-humanoid.github.io/)**
- 2026-02-18, **VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall Safety**, Stella X. Yu Team, Paper: [http://arxiv.org/abs/2602.16511](http://arxiv.org/abs/2602.16511)
- 2026-02-18, **RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation**, Jian Tang Team, Paper: [http://arxiv.org/abs/2602.16444](http://arxiv.org/abs/2602.16444)
- 2026-02-17, **Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation**, Lina Yao Team, Paper: [http://arxiv.org/abs/2602.15724](http://arxiv.org/abs/2602.15724)
- 2026-02-17, **Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**, Soo-Chul Lim Team, Paper: [http://arxiv.org/abs/2602.15543](http://arxiv.org/abs/2602.15543)
- 2026-02-17, **Feasibility-aware Imitation Learning from Observation with Multimodal Feedback**, Takamitsu Matsubara Team, Paper: [http://arxiv.org/abs/2602.15351](http://arxiv.org/abs/2602.15351)
- 2026-02-18, **BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**, Aviral Kumar Team, Paper: [http://arxiv.org/abs/2602.15010](http://arxiv.org/abs/2602.15010)
- 2026-02-16, **PhyScensis: Physics-Augmented LLM Agents for Complex Physical Scene Arrangement**, Chuang Gan Team, Paper: [http://arxiv.org/abs/2602.14968](http://arxiv.org/abs/2602.14968)
- 2026-02-16, **Affordance Transfer Across Object Instances via Semantically Anchored Functional Map**, Weiming Zhi Team, Paper: [http://arxiv.org/abs/2602.14874](http://arxiv.org/abs/2602.14874)
- 2026-02-16, **DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**, Yan Wang Team, Paper: [http://arxiv.org/abs/2602.14577](http://arxiv.org/abs/2602.14577)
- 2026-02-16, **RoboSolver: A Multi-Agent Large Language Model Framework for Solving Robotic Arm Problems**, Alireza Taheri Team, Paper: [http://arxiv.org/abs/2602.14438](http://arxiv.org/abs/2602.14438)
- 2026-02-16, **A Soft Wrist with Anisotropic and Selectable Stiffness for Robust Robot Learning in Contact-rich Manipulation**, Masashi Hamaya Team, Paper: [http://arxiv.org/abs/2602.14434](http://arxiv.org/abs/2602.14434)
- 2026-02-16, **AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**, Sehoon Ha Team, Paper: [http://arxiv.org/abs/2602.14363](http://arxiv.org/abs/2602.14363), Code: **[https://morganbyrd03.github.io/adaptmanip/](https://morganbyrd03.github.io/adaptmanip/)**
- 2026-02-15, **GRAIL: Goal Recognition Alignment through Imitation Learning**, Reuth Mirsky Team, Paper: [http://arxiv.org/abs/2602.14252](http://arxiv.org/abs/2602.14252)
- 2026-02-15, **Learning Part-Aware Dense 3D Feature Field for Generalizable Articulated Object Manipulation**, Hao Dong Team, Paper: [http://arxiv.org/abs/2602.14193](http://arxiv.org/abs/2602.14193), Code: **[https://pa3ff.github.io](https://pa3ff.github.io)**
- 2026-02-15, **RoboAug: One Annotation to Hundreds of Scenes via Region-Contrastive Data Augmentation for Robotic Manipulation**, Jian Tang Team, Paper: [http://arxiv.org/abs/2602.14032](http://arxiv.org/abs/2602.14032)
- 2026-02-15, **WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**, Dongbin Zhao Team, Paper: [http://arxiv.org/abs/2602.13977](http://arxiv.org/abs/2602.13977)
- 2026-02-14, **Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay**, Gabriel de Oliveira Ramos Team, Paper: [http://arxiv.org/abs/2602.13865](http://arxiv.org/abs/2602.13865)
- 2026-02-14, **Mean Flow Policy with Instantaneous Velocity Constraint for One-step Action Generation**, Shengbo Eben Li Team, Paper: [http://arxiv.org/abs/2602.13810](http://arxiv.org/abs/2602.13810)
- 2026-02-14, **Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos**, Lei Sun Team, Paper: [http://arxiv.org/abs/2602.13806](http://arxiv.org/abs/2602.13806)
- 2026-02-14, **MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**, Heng Tao Shen Team, Paper: [http://arxiv.org/abs/2602.13764](http://arxiv.org/abs/2602.13764)
- 2026-02-14, **HybridFlow: A Two-Step Generative Policy for Robotic Manipulation**, Yide Liu Team, Paper: [http://arxiv.org/abs/2602.13718](http://arxiv.org/abs/2602.13718)
- 2026-02-14, **Symmetry-Aware Fusion of Vision and Tactile Sensing via Bilateral Force Priors for Robotic Manipulation**, Tao Yu Team, Paper: [http://arxiv.org/abs/2602.13689](http://arxiv.org/abs/2602.13689)
- 2026-02-14, **Hierarchical Audio-Visual-Proprioceptive Fusion for Precise Robotic Manipulation**, Peng Liu Team, Paper: [http://arxiv.org/abs/2602.13640](http://arxiv.org/abs/2602.13640)
- 2026-02-13, **FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**, Xingxing Zuo Team, Paper: [http://arxiv.org/abs/2602.13444](http://arxiv.org/abs/2602.13444), Code: **[https://huajian-zeng.github.io/projects/flowhoi/](https://huajian-zeng.github.io/projects/flowhoi/)**
- 2026-02-13, **Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos**, Wei-Chiu Ma Team, Paper: [http://arxiv.org/abs/2602.13197](http://arxiv.org/abs/2602.13197)
- 2026-02-13, **UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**, Ziwei Wang Team, Paper: [http://arxiv.org/abs/2602.13086](http://arxiv.org/abs/2602.13086), Code: **[https://henryhcliu.github.io/unimanip](https://henryhcliu.github.io/unimanip)**
- 2026-02-13, **How Swarms Differ: Challenges in Collective Behaviour Comparison**, Jonas Kuckling Team, Paper: [http://arxiv.org/abs/2602.13016](http://arxiv.org/abs/2602.13016)
- 2026-02-13, **SafeFlowMPC: Predictive and Safe Trajectory Planning for Robot Manipulators with Learning-based Policies**, Andreas Kugi Team, Paper: [http://arxiv.org/abs/2602.12794](http://arxiv.org/abs/2602.12794)
- 2026-02-13, **Scaling Single Human Demonstrations for Imitation Learning using Generative Foundational Models**, Abhinav Valada Team, Paper: [http://arxiv.org/abs/2602.12734](http://arxiv.org/abs/2602.12734)
- 2026-02-13, **$\mathcal{X}$ -KD: General Experiential Knowledge Distillation for Large Language Models**, Yuyu Yuan Team, Paper: [http://arxiv.org/abs/2602.12674](http://arxiv.org/abs/2602.12674)
- 2026-02-13, **PMG: Parameterized Motion Generator for Human-like Locomotion Control**, Houde Liu Team, Paper: [http://arxiv.org/abs/2602.12656](http://arxiv.org/abs/2602.12656)
- 2026-02-13, **Real-to-Sim for Highly Cluttered Environments via Physics-Consistent Inter-Object Reasoning**, Jun Ma Team, Paper: [http://arxiv.org/abs/2602.12633](http://arxiv.org/abs/2602.12633), Code: **[https://physics-constrained-real2sim.github.io](https://physics-constrained-real2sim.github.io)**
- 2026-02-12, **Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**, Yesh Dattatreya Team, Paper: [http://arxiv.org/abs/2602.12405](http://arxiv.org/abs/2602.12405)
- 2026-02-12, **FAIL: Flow Matching Adversarial Imitation Learning for Image Generation**, Weidi Xie Team, Paper: [http://arxiv.org/abs/2602.12155](http://arxiv.org/abs/2602.12155)
- 2026-02-12, **GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**, Zheng Zhu Team, Paper: [http://arxiv.org/abs/2602.12099](http://arxiv.org/abs/2602.12099), Code: **[https://gigabrain05m.github.io/](https://gigabrain05m.github.io/)**
- 2026-02-12, **Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**, Changshui Zhang Team, Paper: [http://arxiv.org/abs/2602.12065](http://arxiv.org/abs/2602.12065)
- 2026-02-12, **HoloBrain-0 Technical Report**, Zhizhong Su Team, Paper: [http://arxiv.org/abs/2602.12062](http://arxiv.org/abs/2602.12062)
- 2026-02-12, **When would Vision-Proprioception Policies Fail in Robotic Manipulation?**, Di Hu Team, Paper: [http://arxiv.org/abs/2602.12032](http://arxiv.org/abs/2602.12032)
- 2026-02-12, **Accelerating Robotic Reinforcement Learning with Agent Guidance**, Yaodong Yang Team, Paper: [http://arxiv.org/abs/2602.11978](http://arxiv.org/abs/2602.11978)
- 2026-02-12, **Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control**, Georgia Chalvatzaki Team, Paper: [http://arxiv.org/abs/2602.11934](http://arxiv.org/abs/2602.11934)
- 2026-02-12, **JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**, Mingsheng Long Team, Paper: [http://arxiv.org/abs/2602.11832](http://arxiv.org/abs/2602.11832)
- 2026-02-12, **Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes**, Ayoung Kim Team, Paper: [http://arxiv.org/abs/2602.11660](http://arxiv.org/abs/2602.11660)
- 2026-02-12, **ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning**, Huazhe Xu Team, Paper: [http://arxiv.org/abs/2602.11643](http://arxiv.org/abs/2602.11643)
- 2026-02-12, **EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos**, Qin Jin Team, Paper: [http://arxiv.org/abs/2602.11464](http://arxiv.org/abs/2602.11464)
- 2026-02-11, **Human Preference Modeling Using Visual Motion Prediction Improves Robot Skill Learning from Egocentric Human Video**, Christopher G. Atkeson Team, Paper: [http://arxiv.org/abs/2602.11393](http://arxiv.org/abs/2602.11393)
- 2026-02-11, **MolmoSpaces: A Large-Scale Open Ecosystem for Robot Navigation and Manipulation**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.11337](http://arxiv.org/abs/2602.11337)
- 2026-02-11, **ABot-M0: VLA Foundation Model for Robotic Manipulation with Action Manifold Learning**, Mu Xu Team, Paper: [http://arxiv.org/abs/2602.11236](http://arxiv.org/abs/2602.11236), Code: **[https://amap-cvlab.github.io/ABot-Manipulation/](https://amap-cvlab.github.io/ABot-Manipulation/)**
- 2026-02-11, **YOR: Your Own Mobile Manipulator for Generalizable Robotics**, Zichen Jeff Cui Team, Paper: [http://arxiv.org/abs/2602.11150](http://arxiv.org/abs/2602.11150)
- 2026-02-11, **OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories**, Balaraman Ravindran Team, Paper: [http://arxiv.org/abs/2602.11018](http://arxiv.org/abs/2602.11018)
- 2026-02-12, **Scaling World Model for Hierarchical Manipulation Policies**, Xinghang Li Team, Paper: [http://arxiv.org/abs/2602.10983](http://arxiv.org/abs/2602.10983)
- 2026-02-11, **Towards Learning a Generalizable 3D Scene Representation from 2D Observations**, Stefan Wermter Team, Paper: [http://arxiv.org/abs/2602.10943](http://arxiv.org/abs/2602.10943)
- 2026-02-11, **Semi-Supervised Cross-Domain Imitation Learning**, Ping-Chun Hsieh Team, Paper: [http://arxiv.org/abs/2602.10793](http://arxiv.org/abs/2602.10793)
- 2026-02-11, **Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**, Yanwei Fu Team, Paper: [http://arxiv.org/abs/2602.10717](http://arxiv.org/abs/2602.10717)
- 2026-02-11, **From Interaction to Demonstration Quality in Virtual Reality: Effects of Interaction Modality and Visual Representation on Everyday Tasks**, Anna-Lisa Vollmer Team, Paper: [http://arxiv.org/abs/2602.10618](http://arxiv.org/abs/2602.10618)
- 2026-02-11, **Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning**, Penny Sweetser Team, Paper: [http://arxiv.org/abs/2602.10594](http://arxiv.org/abs/2602.10594)
- 2026-02-10, **Adaptive Time Step Flow Matching for Autonomous Driving Motion Planning**, Faizan M. Tariq Team, Paper: [http://arxiv.org/abs/2602.10285](http://arxiv.org/abs/2602.10285)
- 2026-02-10, **ST4VLA: Spatially Guided Training for Vision-Language-Action Models**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10109](http://arxiv.org/abs/2602.10109)
- 2026-02-10, **DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10105](http://arxiv.org/abs/2602.10105)
- 2026-02-10, **Robo3R: Enhancing Robotic Manipulation with Accurate Feed-Forward 3D Reconstruction**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10101](http://arxiv.org/abs/2602.10101)
- 2026-02-10, **UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking**, Yao Mu Team, Paper: [http://arxiv.org/abs/2602.10093](http://arxiv.org/abs/2602.10093), Code: **[https://univtac.github.io/](https://univtac.github.io/)**
- 2026-02-11, **RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments**, Laxmidhar Behera Team, Paper: [http://arxiv.org/abs/2602.10015](http://arxiv.org/abs/2602.10015)
- 2026-02-10, **Learning Force-Regulated Manipulation with a Low-Cost Tactile-Force-Controlled Gripper**, Yen-Ling Kuo Team, Paper: [http://arxiv.org/abs/2602.10013](http://arxiv.org/abs/2602.10013)
- 2026-02-10, **RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.09973](http://arxiv.org/abs/2602.09973)
- 2026-02-10, **Instruct2Act: From Human Instruction to Actions Sequencing and Execution via Robot Action Network for Robotic Manipulation**, Laxmidhar Behera Team, Paper: [http://arxiv.org/abs/2602.09940](http://arxiv.org/abs/2602.09940)
- 2026-02-10, **TriPilot-FF: Coordinated Whole-Body Teleoperation with Force Feedback**, Weiming Zhi Team, Paper: [http://arxiv.org/abs/2602.09888](http://arxiv.org/abs/2602.09888)
- 2026-02-10, **MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation**, Xiangyu Yue Team, Paper: [http://arxiv.org/abs/2602.09878](http://arxiv.org/abs/2602.09878)
- 2026-02-10, **Diverse Skill Discovery for Quadruped Robots via Unsupervised Learning**, Wei Li Team, Paper: [http://arxiv.org/abs/2602.09767](http://arxiv.org/abs/2602.09767)
- 2026-02-10, **VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model**, Hui Xiong Team, Paper: [http://arxiv.org/abs/2602.09638](http://arxiv.org/abs/2602.09638)
- 2026-02-10, **Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation**, Danica Kragic Team, Paper: [http://arxiv.org/abs/2602.09583](http://arxiv.org/abs/2602.09583)
- 2026-02-09, **SceneSmith: Agentic Generation of Simulation-Ready Indoor Scenes**, Russ Tedrake Team, Paper: [http://arxiv.org/abs/2602.09153](http://arxiv.org/abs/2602.09153), Code: **[https://scenesmith.github.io/](https://scenesmith.github.io/)**
- 2026-02-09, **TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.09023](http://arxiv.org/abs/2602.09023)
- 2026-02-09, **$χ_{0}$ : Resource-Aware Robust Manipulation via Taming Distributional Inconsistencies**, Yibo Yuan Team, Paper: [http://arxiv.org/abs/2602.09021](http://arxiv.org/abs/2602.09021)
- 2026-02-09, **Contact-Anchored Policies: Contact Conditioning Creates Strong Robot Utility Models**, Nur Muhammad Mahi Shafiullah Team, Paper: [http://arxiv.org/abs/2602.09017](http://arxiv.org/abs/2602.09017)
- 2026-02-09, **Learning Potentials for Dynamic Matching and Application to Heart Transplantation**, Tuomas Sandholm Team, Paper: [http://arxiv.org/abs/2602.08878](http://arxiv.org/abs/2602.08878)
- 2026-02-09, **Mimic Intent, Not Just Trajectories**, Panpan Cai Team, Paper: [http://arxiv.org/abs/2602.08602](http://arxiv.org/abs/2602.08602)
- 2026-02-09, **GISA: A Benchmark for General Information-Seeking Assistant**, Zhicheng Dou Team, Paper: [http://arxiv.org/abs/2602.08543](http://arxiv.org/abs/2602.08543)
- 2026-02-09, **Informative Object-centric Next Best View for Object-aware 3D Gaussian Splatting in Cluttered Scenes**, Ayoung Kim Team, Paper: [http://arxiv.org/abs/2602.08266](http://arxiv.org/abs/2602.08266)
- 2026-02-09, **STEP: Warm-Started Visuomotor Policies with Spatiotemporal Consistency Prediction**, Guohao Dai Team, Paper: [http://arxiv.org/abs/2602.08245](http://arxiv.org/abs/2602.08245)
- 2026-02-07, **Trace-Focused Diffusion Policy for Multi-Modal Action Disambiguation in Long-Horizon Robotic Manipulation**, Jianfei Yang Team, Paper: [http://arxiv.org/abs/2602.07388](http://arxiv.org/abs/2602.07388)
- 2026-02-07, **Scalable Dexterous Robot Learning with AR-based Remote Human-Robot Interactions**, Zhuo Zou Team, Paper: [http://arxiv.org/abs/2602.07341](http://arxiv.org/abs/2602.07341)
- 2026-02-07, **RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving**, Ganesh Krishnasamy Team, Paper: [http://arxiv.org/abs/2602.07339](http://arxiv.org/abs/2602.07339)
- 2026-02-07, **Why Look at It at All?: Vision-Free Multifingered Blind Grasping Using Uniaxial Fingertip Force Sensing**, Seokhwan Jeong Team, Paper: [http://arxiv.org/abs/2602.07326](http://arxiv.org/abs/2602.07326)
- 2026-02-06, **MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation**, Wei Gao Team, Paper: [http://arxiv.org/abs/2602.07082](http://arxiv.org/abs/2602.07082)
- 2026-02-06, **SURE: Safe Uncertainty-Aware Robot-Environment Interaction using Trajectory Optimization**, Majid Khadiv Team, Paper: [http://arxiv.org/abs/2602.06864](http://arxiv.org/abs/2602.06864)
- 2026-02-06, **RAPID: Reconfigurable, Adaptive Platform for Iterative Design**, Jia Liu Team, Paper: [http://arxiv.org/abs/2602.06653](http://arxiv.org/abs/2602.06653)
- 2026-02-06, **Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique**, Toshiaki Tsuji Team, Paper: [http://arxiv.org/abs/2602.06620](http://arxiv.org/abs/2602.06620)
- 2026-02-06, **The Law of Task-Achieving Body Motion: Axiomatizing Success of Robot Manipulation Actions**, Michael Beetz Team, Paper: [http://arxiv.org/abs/2602.06572](http://arxiv.org/abs/2602.06572)
- 2026-02-06, **Beyond the Majority: Long-tail Imitation Learning for Robotic Manipulation**, Heng Tao Shen Team, Paper: [http://arxiv.org/abs/2602.06512](http://arxiv.org/abs/2602.06512)
- 2026-02-06, **World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy**, Mike Zheng Shou Team, Paper: [http://arxiv.org/abs/2602.06508](http://arxiv.org/abs/2602.06508)
- 2026-02-06, **Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation**, Weiying Xie Team, Paper: [http://arxiv.org/abs/2602.06356](http://arxiv.org/abs/2602.06356)
- 2026-02-06, **A High-Fidelity Robotic Manipulator Teleoperation Framework for Human-Centered Augmented Reality Evaluation**, Tian Guo Team, Paper: [http://arxiv.org/abs/2602.06273](http://arxiv.org/abs/2602.06273)
- 2026-02-05, **TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**, Shigeki Sugano Team, Paper: [http://arxiv.org/abs/2602.05468](http://arxiv.org/abs/2602.05468)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **CoWTracker: Tracking by Warping instead of Correlation**, Andrea Vedaldi Team, Paper: [http://arxiv.org/abs/2602.04877](http://arxiv.org/abs/2602.04877)
- 2026-02-04, **A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction**, Riddhiman Laha Team, Paper: [http://arxiv.org/abs/2602.04522](http://arxiv.org/abs/2602.04522)
- 2026-02-04, **Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2602.04243](http://arxiv.org/abs/2602.04243)
- 2026-02-04, **GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning**, Hongliang Ren Team, Paper: [http://arxiv.org/abs/2602.04231](http://arxiv.org/abs/2602.04231)
- 2026-02-04, **Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**, Badong Chen Team, Paper: [http://arxiv.org/abs/2602.04228](http://arxiv.org/abs/2602.04228)
- 2026-02-04, **OAT: Ordered Action Tokenization**, Yilun Du Team, Paper: [http://arxiv.org/abs/2602.04215](http://arxiv.org/abs/2602.04215)
- 2026-02-04, **InterPReT: Interactive Policy Restructuring and Training Enable Effective Imitation Learning from Laypersons**, Reid Simmons Team, Paper: [http://arxiv.org/abs/2602.04213](http://arxiv.org/abs/2602.04213)
- 2026-02-04, **A brief review of evolutionary game dynamics in the reinforcement learning paradigm**, Li Chen Team, Paper: [http://arxiv.org/abs/2602.04150](http://arxiv.org/abs/2602.04150)
- 2026-02-03, **Comparative Analysis of Autonomous Robotic and Manual Techniques for Ultrasonic Sacral Osteotomy: A Preliminary Study**, Farshid Alambeigi Team, Paper: [http://arxiv.org/abs/2602.04076](http://arxiv.org/abs/2602.04076)
- 2026-02-03, **VLS: Steering Pretrained Robot Policies via Vision-Language Models**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.03973](http://arxiv.org/abs/2602.03973), Code: **[https://vision-language-steering.github.io/webpage/](https://vision-language-steering.github.io/webpage/)**
- 2026-02-03, **MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**, Jungwoo Lee Team, Paper: [http://arxiv.org/abs/2602.03668](http://arxiv.org/abs/2602.03668)
- 2026-02-03, **Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives**, Sho Sakaino Team, Paper: [http://arxiv.org/abs/2602.03188](http://arxiv.org/abs/2602.03188)
- 2026-02-02, **Language Movement Primitives: Grounding Language Models in Robot Motion**, Simon Stepputtis Team, Paper: [http://arxiv.org/abs/2602.02839](http://arxiv.org/abs/2602.02839)
- 2026-02-02, **On the Sample Efficiency of Inverse Dynamics Models for Semi-Supervised Imitation Learning**, Sébastien Lachapelle Team, Paper: [http://arxiv.org/abs/2602.02762](http://arxiv.org/abs/2602.02762)
- 2026-02-02, **HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**, Ping Tan Team, Paper: [http://arxiv.org/abs/2602.02473](http://arxiv.org/abs/2602.02473)
- 2026-02-02, **TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**, Jiaqi Ma Team, Paper: [http://arxiv.org/abs/2602.02459](http://arxiv.org/abs/2602.02459)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning**, Alan Ritter Team, Paper: [http://arxiv.org/abs/2602.02405](http://arxiv.org/abs/2602.02405)
- 2026-02-02, **SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.02402](http://arxiv.org/abs/2602.02402), Code: **[https://city-super.github.io/SoMA/](https://city-super.github.io/SoMA/)**
- 2026-02-02, **PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning**, Alexander Schperberg Team, Paper: [http://arxiv.org/abs/2602.02396](http://arxiv.org/abs/2602.02396)
- 2026-02-02, **Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy**, Qiang Nie Team, Paper: [http://arxiv.org/abs/2602.01939](http://arxiv.org/abs/2602.01939)
- 2026-02-02, **ForSim: Stepwise Forward Simulation for Traffic Policy Fine-Tuning**, Sifa Zheng Team, Paper: [http://arxiv.org/abs/2602.01916](http://arxiv.org/abs/2602.01916)
- 2026-02-02, **BTGenBot-2: Efficient Behavior Tree Generation with Small Language Models**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01870](http://arxiv.org/abs/2602.01870)
- 2026-02-03, **RFS: Reinforcement learning with Residual flow steering for dexterous manipulation**, Abhishek Gupta Team, Paper: [http://arxiv.org/abs/2602.01789](http://arxiv.org/abs/2602.01789)
- 2026-02-02, **AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**, Yu She Team, Paper: [http://arxiv.org/abs/2602.01662](http://arxiv.org/abs/2602.01662)
- 2026-02-02, **A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**, Shreyas Kousik Team, Paper: [http://arxiv.org/abs/2602.01632](http://arxiv.org/abs/2602.01632), Code: **[https://sew-mimic.com/](https://sew-mimic.com/)**
- 2026-02-01, **Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning**, Weitong Zhang Team, Paper: [http://arxiv.org/abs/2602.01357](http://arxiv.org/abs/2602.01357)
- 2026-02-01, **Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.01166](http://arxiv.org/abs/2602.01166)
- 2026-02-01, **Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01158](http://arxiv.org/abs/2602.01158)
- 2026-02-01, **UniForce: A Unified Latent Force Model for Robot Manipulation with Diverse Tactile Sensors**, Shan Luo Team, Paper: [http://arxiv.org/abs/2602.01153](http://arxiv.org/abs/2602.01153)
- 2026-02-01, **KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV**, Ziyang Wang Team, Paper: [http://arxiv.org/abs/2602.01115](http://arxiv.org/abs/2602.01115)
- 2026-02-01, **StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**, Lu Fang Team, Paper: [http://arxiv.org/abs/2602.01100](http://arxiv.org/abs/2602.01100)
- 2026-02-01, **Estimating Force Interactions of Deformable Linear Objects from their Shapes**, Quang-Cuong Pham Team, Paper: [http://arxiv.org/abs/2602.01085](http://arxiv.org/abs/2602.01085)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-30, **Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**, Liu Hong Team, Paper: [http://arxiv.org/abs/2601.23087](http://arxiv.org/abs/2601.23087)
- 2026-01-30, **Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**, Guang Chen Team, Paper: [http://arxiv.org/abs/2601.22988](http://arxiv.org/abs/2601.22988)
- 2026-01-30, **Self-Imitated Diffusion Policy for Efficient and Robust Visual Navigation**, Wuyue Zhao Team, Paper: [http://arxiv.org/abs/2601.22965](http://arxiv.org/abs/2601.22965)
- 2026-01-29, **PoSafeNet: Safe Learning with Poset-Structured Neural Nets**, Daniela Rus Team, Paper: [http://arxiv.org/abs/2601.22356](http://arxiv.org/abs/2601.22356)
- 2026-01-29, **Aligning Microscopic Vehicle and Macroscopic Traffic Statistics: Reconstructing Driving Behavior from Partial Data**, Bowen Weng Team, Paper: [http://arxiv.org/abs/2601.22242](http://arxiv.org/abs/2601.22242)
- 2026-01-29, **Causal Imitation Learning Under Measurement Error and Distribution Shift**, AmirEmad Ghassami Team, Paper: [http://arxiv.org/abs/2601.22206](http://arxiv.org/abs/2601.22206)
- 2026-01-29, **mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning**, Pieter Abbeel Team, Paper: [http://arxiv.org/abs/2601.22074](http://arxiv.org/abs/2601.22074), Code: **[https://github.com/mujocolab/mjlab](https://github.com/mujocolab/mjlab)**
- 2026-01-30, **PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**, Jie Mei Team, Paper: [http://arxiv.org/abs/2601.22018](http://arxiv.org/abs/2601.22018)
- 2026-01-29, **Causal World Modeling for Robot Control**, Yinghao Xu Team, Paper: [http://arxiv.org/abs/2601.21998](http://arxiv.org/abs/2601.21998), Code: **[https://technology.robbyant.com/lingbot-va](https://technology.robbyant.com/lingbot-va)**
- 2026-01-29, **MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**, Stefanie Speidel Team, Paper: [http://arxiv.org/abs/2601.21971](http://arxiv.org/abs/2601.21971)
- 2026-01-29, **Information Filtering via Variational Regularization for Robot Manipulation**, Jie Me Team, Paper: [http://arxiv.org/abs/2601.21926](http://arxiv.org/abs/2601.21926)
- 2026-01-29, **When does predictive inverse dynamics outperform behavior cloning?**, Sergio Valcarcel Macua Team, Paper: [http://arxiv.org/abs/2601.21718](http://arxiv.org/abs/2601.21718)
- 2026-01-29, **Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation**, Liming Chen Team, Paper: [http://arxiv.org/abs/2601.21416](http://arxiv.org/abs/2601.21416)
- 2026-01-29, **Towards Space-Based Environmentally-Adaptive Grasping**, Aleksandr Artemov Team, Paper: [http://arxiv.org/abs/2601.21394](http://arxiv.org/abs/2601.21394)
- 2026-01-29, **Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies**, Harold Soh Team, Paper: [http://arxiv.org/abs/2601.21251](http://arxiv.org/abs/2601.21251)
- 2026-01-28, **Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy**, Christos Bergeles Team, Paper: [http://arxiv.org/abs/2601.20776](http://arxiv.org/abs/2601.20776)
- 2026-01-28, **Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands**, Nicolás Navarro-Guerrero Team, Paper: [http://arxiv.org/abs/2601.20555](http://arxiv.org/abs/2601.20555)
- 2026-01-28, **Advancing Open-source World Models**, Hao Ouyang Team, Paper: [http://arxiv.org/abs/2601.20540](http://arxiv.org/abs/2601.20540), Code: **[https://technology.robbyant.com/lingbot-world](https://technology.robbyant.com/lingbot-world)**
- 2026-01-28, **STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation**, Liming Chen Team, Paper: [http://arxiv.org/abs/2601.20381](http://arxiv.org/abs/2601.20381)
- 2026-01-28, **Demonstration-Free Robotic Control via LLM Agents**, Tiffany J. Hwu Team, Paper: [http://arxiv.org/abs/2601.20334](http://arxiv.org/abs/2601.20334)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLM

- 2026-02-18, **Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning**, Jundong Li Team, Paper: [http://arxiv.org/abs/2602.16702](http://arxiv.org/abs/2602.16702)
- 2026-02-18, **A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification**, James Haworth Team, Paper: [http://arxiv.org/abs/2602.16590](http://arxiv.org/abs/2602.16590)
- 2026-02-18, **DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images**, Chenfanfu Jiang Team, Paper: [http://arxiv.org/abs/2602.16502](http://arxiv.org/abs/2602.16502)
- 2026-02-18, **Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing**, Dahua Lin Team, Paper: [http://arxiv.org/abs/2602.16455](http://arxiv.org/abs/2602.16455)
- 2026-02-18, **Designing Production-Scale OCR for India: Multilingual and Domain-Specific Systems**, Shubham Agarwal Team, Paper: [http://arxiv.org/abs/2602.16430](http://arxiv.org/abs/2602.16430)
- 2026-02-18, **Peeking Ahead of the Field Study: Exploring VLM Personas as Support Tools for Embodied Studies in HCI**, Takeo Igarashi Team, Paper: [http://arxiv.org/abs/2602.16157](http://arxiv.org/abs/2602.16157)
- 2026-02-18, **Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing**, Jean Oh Team, Paper: [http://arxiv.org/abs/2602.16149](http://arxiv.org/abs/2602.16149)
- 2026-02-18, **IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models**, Miguel P. Eckstein Team, Paper: [http://arxiv.org/abs/2602.16138](http://arxiv.org/abs/2602.16138)
- 2026-02-18, **OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis**, Beng Chin Ooi Team, Paper: [http://arxiv.org/abs/2602.16110](http://arxiv.org/abs/2602.16110)
- 2026-02-17, **MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval**, Gongbo Liang Team, Paper: [http://arxiv.org/abs/2602.16019](http://arxiv.org/abs/2602.16019)
- 2026-02-17, **BTReport: A Framework for Brain Tumor Radiology Report Generation with Clinically Relevant Features**, Mehmet Kurt Team, Paper: [http://arxiv.org/abs/2602.16006](http://arxiv.org/abs/2602.16006)
- 2026-02-17, **Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families**, Yuval Levental Team, Paper: [http://arxiv.org/abs/2602.15950](http://arxiv.org/abs/2602.15950)
- 2026-02-17, **Visual Memory Injection Attacks for Multi-Turn Conversations**, Matthias Hein Team, Paper: [http://arxiv.org/abs/2602.15927](http://arxiv.org/abs/2602.15927)
- 2026-02-17, **Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation**, Valerio Guarrasi Team, Paper: [http://arxiv.org/abs/2602.15650](http://arxiv.org/abs/2602.15650)
- 2026-02-17, **CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving**, Arkady Zgonnikov Team, Paper: [http://arxiv.org/abs/2602.15645](http://arxiv.org/abs/2602.15645)
- 2026-02-17, **Req2Road: A GenAI Pipeline for SDV Test Artifact Generation and On-Vehicle Execution**, Alois Knoll Team, Paper: [http://arxiv.org/abs/2602.15591](http://arxiv.org/abs/2602.15591)
- 2026-02-17, **VLM-DEWM: Dynamic External World Model for Verifiable and Resilient Vision-Language Planning in Manufacturing**, Ning Ji Team, Paper: [http://arxiv.org/abs/2602.15549](http://arxiv.org/abs/2602.15549)
- 2026-02-17, **Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**, Soo-Chul Lim Team, Paper: [http://arxiv.org/abs/2602.15543](http://arxiv.org/abs/2602.15543)
- 2026-02-17, **Semantic-Guided 3D Gaussian Splatting for Transient Object Removal**, Priyesh Shukla Team, Paper: [http://arxiv.org/abs/2602.15516](http://arxiv.org/abs/2602.15516)
- 2026-02-17, **On the Out-of-Distribution Generalization of Reasoning in Multimodal LLMs for Simple Visual Planning Tasks**, Francesco Croce Team, Paper: [http://arxiv.org/abs/2602.15460](http://arxiv.org/abs/2602.15460)
- 2026-02-17, **ActionCodec: What Makes for Good Action Tokenizers**, Jianye Hao Team, Paper: [http://arxiv.org/abs/2602.15397](http://arxiv.org/abs/2602.15397)
- 2026-02-17, **The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems**, Jing Gao Team, Paper: [http://arxiv.org/abs/2602.15382](http://arxiv.org/abs/2602.15382)
- 2026-02-17, **GMAIL: Generative Modality Alignment for generated Image Learning**, Sukmin Yun Team, Paper: [http://arxiv.org/abs/2602.15368](http://arxiv.org/abs/2602.15368)
- 2026-02-17, **Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs**, Dongsheng Li Team, Paper: [http://arxiv.org/abs/2602.15318](http://arxiv.org/abs/2602.15318)
- 2026-02-17, **Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models**, Jaehyun Ahn Team, Paper: [http://arxiv.org/abs/2602.15315](http://arxiv.org/abs/2602.15315)
- 2026-02-17, **EAA: Automating materials characterization with vision language model agents**, Mathew J. Cherukara Team, Paper: [http://arxiv.org/abs/2602.15294](http://arxiv.org/abs/2602.15294)
- 2026-02-17, **Visual Persuasion: What Influences Decisions of Vision-Language Models?**, Nikhil Singh Team, Paper: [http://arxiv.org/abs/2602.15278](http://arxiv.org/abs/2602.15278)
- 2026-02-16, **How to Train Your Long-Context Visual Document Model**, Austin Veselka Team, Paper: [http://arxiv.org/abs/2602.15257](http://arxiv.org/abs/2602.15257)
- 2026-02-16, **Ground-Truth Depth in Vision Language Models: Spatial Context Understanding in Conversational AI for XR-Robotic Support in Emergency First Response**, Manfred Tscheligi Team, Paper: [http://arxiv.org/abs/2602.15237](http://arxiv.org/abs/2602.15237)
- 2026-02-16, **Seeing to Generalize: How Visual Data Corrects Binding Shortcuts**, Rodrigo Toro Icarte Team, Paper: [http://arxiv.org/abs/2602.15183](http://arxiv.org/abs/2602.15183)
- 2026-02-16, **Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition**, Jinhui Tang Team, Paper: [http://arxiv.org/abs/2602.15124](http://arxiv.org/abs/2602.15124)
- 2026-02-16, **BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames**, Aviral Kumar Team, Paper: [http://arxiv.org/abs/2602.15010](http://arxiv.org/abs/2602.15010)
- 2026-02-16, **ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery**, Nipun Batra Team, Paper: [http://arxiv.org/abs/2602.14989](http://arxiv.org/abs/2602.14989)
- 2026-02-16, **DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**, Tiancai Wang Team, Paper: [http://arxiv.org/abs/2602.14974](http://arxiv.org/abs/2602.14974), Code: **[https://github.com/Dexmal/dexbotic](https://github.com/Dexmal/dexbotic)**
- 2026-02-16, **MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs**, Giuseppe Riccardi Team, Paper: [http://arxiv.org/abs/2602.14589](http://arxiv.org/abs/2602.14589)
- 2026-02-16, **Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**, Kensuke Harada Team, Paper: [http://arxiv.org/abs/2602.14551](http://arxiv.org/abs/2602.14551)
- 2026-02-16, **Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model**, Mikko Tolonen Team, Paper: [http://arxiv.org/abs/2602.14524](http://arxiv.org/abs/2602.14524)
- 2026-02-16, **S2D: Selective Spectral Decay for Quantization-Friendly Conditioning of Neural Activations**, Deepak Gupta Team, Paper: [http://arxiv.org/abs/2602.14432](http://arxiv.org/abs/2602.14432)
- 2026-02-16, **Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models**, Yiliao Song Team, Paper: [http://arxiv.org/abs/2602.14399](http://arxiv.org/abs/2602.14399)
- 2026-02-15, **Moving Beyond Sparse Grounding with Complete Screen Parsing Supervision**, Peter Staar Team, Paper: [http://arxiv.org/abs/2602.14276](http://arxiv.org/abs/2602.14276)
- 2026-02-15, **Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models**, Priyesh Shukla Team, Paper: [http://arxiv.org/abs/2602.14236](http://arxiv.org/abs/2602.14236)
- 2026-02-15, **Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering**, Tao Xu Team, Paper: [http://arxiv.org/abs/2602.14162](http://arxiv.org/abs/2602.14162)
- 2026-02-15, **Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework**, Wojciech Kusa Team, Paper: [http://arxiv.org/abs/2602.14073](http://arxiv.org/abs/2602.14073)
- 2026-02-15, **MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars**, Hongxin Wei Team, Paper: [http://arxiv.org/abs/2602.13961](http://arxiv.org/abs/2602.13961)
- 2026-02-14, **RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction**, Yu Hong Team, Paper: [http://arxiv.org/abs/2602.13748](http://arxiv.org/abs/2602.13748)
- 2026-02-14, **Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images**, Nouar AlDahoul Team, Paper: [http://arxiv.org/abs/2602.13712](http://arxiv.org/abs/2602.13712)
- 2026-02-14, **LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases**, Luyl-Da Quach Team, Paper: [http://arxiv.org/abs/2602.13662](http://arxiv.org/abs/2602.13662)
- 2026-02-14, **KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination**, Edward Choi Team, Paper: [http://arxiv.org/abs/2602.13650](http://arxiv.org/abs/2602.13650)
- 2026-02-14, **Towards Sparse Video Understanding and Reasoning**, Han Liu Team, Paper: [http://arxiv.org/abs/2602.13602](http://arxiv.org/abs/2602.13602)
- 2026-02-14, **AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting**, Tianyu Pang Team, Paper: [http://arxiv.org/abs/2602.13600](http://arxiv.org/abs/2602.13600)
- 2026-02-14, **OpAgent: Operator Agent for Web Navigation**, Peng Di Team, Paper: [http://arxiv.org/abs/2602.13559](http://arxiv.org/abs/2602.13559)
- 2026-02-13, **Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**, Sergey Levine Team, Paper: [http://arxiv.org/abs/2602.13193](http://arxiv.org/abs/2602.13193)
- 2026-02-13, **Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images**, Jiangpeng He Team, Paper: [http://arxiv.org/abs/2602.13041](http://arxiv.org/abs/2602.13041), Code: **[https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data](https://www.kaggle.com/competitions/3d-reconstruction-from-monocular-multi-food-images/data)**
- 2026-02-13, **Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding**, Lianwen Jin Team, Paper: [http://arxiv.org/abs/2602.12957](http://arxiv.org/abs/2602.12957)
- 2026-02-13, **HoRAMA: Holistic Reconstruction with Automated Material Assignment for Ray Tracing using NYURay**, Theodore S. Rappaport Team, Paper: [http://arxiv.org/abs/2602.12942](http://arxiv.org/abs/2602.12942)
- 2026-02-13, **RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads**, Jyothikamalesh S Team, Paper: [http://arxiv.org/abs/2602.12877](http://arxiv.org/abs/2602.12877)
- 2026-02-13, **Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation**, Wei Shen Team, Paper: [http://arxiv.org/abs/2602.12843](http://arxiv.org/abs/2602.12843)
- 2026-02-13, **X-SYS: A Reference Architecture for Interactive Explanation Systems**, Sebastian Lapuschkin Team, Paper: [http://arxiv.org/abs/2602.12748](http://arxiv.org/abs/2602.12748)
- 2026-02-13, **SignScene: Visual Sign Grounding for Mapless Navigation**, David Hsu Team, Paper: [http://arxiv.org/abs/2602.12686](http://arxiv.org/abs/2602.12686)
- 2026-02-13, **IndicFairFace: Balanced Indian Face Dataset for Auditing and Mitigating Geographical Bias in Vision-Language Models**, Jiechao Gao Team, Paper: [http://arxiv.org/abs/2602.12659](http://arxiv.org/abs/2602.12659)
- 2026-02-13, **PISHYAR: A Socially Intelligent Smart Cane for Indoor Social Navigation and Multimodal Human-Robot Interaction for Visually Impaired People**, Alireza Taheri Team, Paper: [http://arxiv.org/abs/2602.12597](http://arxiv.org/abs/2602.12597)
- 2026-02-13, **On Robustness and Chain-of-Thought Consistency of RL-Finetuned VLMs**, Arnab Mondal Team, Paper: [http://arxiv.org/abs/2602.12506](http://arxiv.org/abs/2602.12506)
- 2026-02-13, **Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models**, Rahmatollah Beheshti Team, Paper: [http://arxiv.org/abs/2602.12498](http://arxiv.org/abs/2602.12498)
- 2026-02-12, **Self-Refining Vision Language Model for Robotic Failure Detection and Reasoning**, Yesh Dattatreya Team, Paper: [http://arxiv.org/abs/2602.12405](http://arxiv.org/abs/2602.12405)
- 2026-02-12, **What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis**, Tianyi Zhou Team, Paper: [http://arxiv.org/abs/2602.12395](http://arxiv.org/abs/2602.12395)
- 2026-02-12, **Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues**, Michael Graber Team, Paper: [http://arxiv.org/abs/2602.12381](http://arxiv.org/abs/2602.12381)
- 2026-02-12, **ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**, Song Han Team, Paper: [http://arxiv.org/abs/2602.12322](http://arxiv.org/abs/2602.12322)
- 2026-02-12, **LatentAM: Real-Time, Large-Scale Latent Gaussian Attention Mapping via Online Dictionary Learning**, Yulun Tian Team, Paper: [http://arxiv.org/abs/2602.12314](http://arxiv.org/abs/2602.12314)
- 2026-02-12, **Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**, Marco Pavone Team, Paper: [http://arxiv.org/abs/2602.12281](http://arxiv.org/abs/2602.12281)
- 2026-02-12, **ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images**, Manuela Veloso Team, Paper: [http://arxiv.org/abs/2602.12203](http://arxiv.org/abs/2602.12203)
- 2026-02-12, **3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting**, Xinyi Yu Team, Paper: [http://arxiv.org/abs/2602.12159](http://arxiv.org/abs/2602.12159)
- 2026-02-12, **Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning**, Changshui Zhang Team, Paper: [http://arxiv.org/abs/2602.12065](http://arxiv.org/abs/2602.12065)
- 2026-02-12, **Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation**, Øyvind Meinich-Bache Team, Paper: [http://arxiv.org/abs/2602.12002](http://arxiv.org/abs/2602.12002)
- 2026-02-12, **Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion**, Nicolas Mery Team, Paper: [http://arxiv.org/abs/2602.11960](http://arxiv.org/abs/2602.11960)
- 2026-02-12, **Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization**, Anubhav Girdhar Team, Paper: [http://arxiv.org/abs/2602.11957](http://arxiv.org/abs/2602.11957)
- 2026-02-12, **LAMP: Implicit Language Map for Robot Navigation**, Sunwook Choi Team, Paper: [http://arxiv.org/abs/2602.11862](http://arxiv.org/abs/2602.11862), Code: **[https://lab-of-ai-and-robotics.github.io/LAMP/](https://lab-of-ai-and-robotics.github.io/LAMP/)**
- 2026-02-12, **JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**, Mingsheng Long Team, Paper: [http://arxiv.org/abs/2602.11832](http://arxiv.org/abs/2602.11832)
- 2026-02-12, **Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models**, Zhou Yang Team, Paper: [http://arxiv.org/abs/2602.11824](http://arxiv.org/abs/2602.11824)
- 2026-02-12, **Adaptive Debiasing Tsallis Entropy for Test-Time Adaptation**, Jianfeng Lu Team, Paper: [http://arxiv.org/abs/2602.11743](http://arxiv.org/abs/2602.11743)
- 2026-02-12, **Adapting Vision-Language Models for E-commerce Understanding at Scale**, Shahram Khadivi Team, Paper: [http://arxiv.org/abs/2602.11733](http://arxiv.org/abs/2602.11733)
- 2026-02-12, **STVG-R1: Incentivizing Instance-Level Reasoning and Grounding in Videos via Reinforcement Learning**, Qing Li Team, Paper: [http://arxiv.org/abs/2602.11730](http://arxiv.org/abs/2602.11730)
- 2026-02-12, **ScalSelect: Scalable Training-Free Multimodal Data Selection for Efficient Visual Instruction Tuning**, Kai Chen Team, Paper: [http://arxiv.org/abs/2602.11636](http://arxiv.org/abs/2602.11636), Code: **[https://github.com/ChangtiWu/ScalSelect}{ScalSelect}](https://github.com/ChangtiWu/ScalSelect}{ScalSelect})**
- 2026-02-12, **SkillRater: Untangling Capabilities in Multimodal Data**, Akshat Shrivastava Team, Paper: [http://arxiv.org/abs/2602.11615](http://arxiv.org/abs/2602.11615)
- 2026-02-11, **Hierarchical Concept Embedding & Pursuit for Interpretable Image Classification**, René Vidal Team, Paper: [http://arxiv.org/abs/2602.11448](http://arxiv.org/abs/2602.11448)
- 2026-02-11, **Active Zero: Self-Evolving Vision-Language Models through Active Environment Exploration**, Tat-Seng Chua Team, Paper: [http://arxiv.org/abs/2602.11241](http://arxiv.org/abs/2602.11241)
- 2026-02-11, **Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling**, Wenhan Luo Team, Paper: [http://arxiv.org/abs/2602.11146](http://arxiv.org/abs/2602.11146), Code: **[https://github.com/HKUST-C4G/diffusion-rm](https://github.com/HKUST-C4G/diffusion-rm)**
- 2026-02-12, **Chatting with Images for Introspective Visual Thinking**, Tieniu Tan Team, Paper: [http://arxiv.org/abs/2602.11073](http://arxiv.org/abs/2602.11073)
- 2026-02-11, **Safe mobility support system using crowd mapping and avoidance route planning using VLM**, Koichi Ozaki Team, Paper: [http://arxiv.org/abs/2602.10910](http://arxiv.org/abs/2602.10910)
- 2026-02-11, **Chart Specification: Structural Representations for Incentivizing VLM Reasoning in Chart-to-Code Generation**, Yuya Ieiri Team, Paper: [http://arxiv.org/abs/2602.10880](http://arxiv.org/abs/2602.10880)
- 2026-02-11, **SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios**, Haonan Li Team, Paper: [http://arxiv.org/abs/2602.10840](http://arxiv.org/abs/2602.10840)
- 2026-02-11, **Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training**, Yanan Sun Team, Paper: [http://arxiv.org/abs/2602.10815](http://arxiv.org/abs/2602.10815)
- 2026-02-11, **DeepImageSearch: Benchmarking Multimodal Agents for Context-Aware Image Retrieval in Visual Histories**, Zhicheng Dou Team, Paper: [http://arxiv.org/abs/2602.10809](http://arxiv.org/abs/2602.10809)
- 2026-02-11, **From Steering to Pedalling: Do Autonomous Driving VLMs Generalize to Cyclist-Assistive Spatial Perception and Planning?**, Vedasri Nakka Team, Paper: [http://arxiv.org/abs/2602.10771](http://arxiv.org/abs/2602.10771)
- 2026-02-11, **Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs**, Edith C. H. Ngai Team, Paper: [http://arxiv.org/abs/2602.10740](http://arxiv.org/abs/2602.10740)
- 2026-02-11, **Say, Dream, and Act: Learning Video World Models for Instruction-Driven Robot Manipulation**, Yanwei Fu Team, Paper: [http://arxiv.org/abs/2602.10717](http://arxiv.org/abs/2602.10717)
- 2026-02-11, **Assessing Vision-Language Models for Perception in Autonomous Underwater Robotic Software**, Shuai Wang Team, Paper: [http://arxiv.org/abs/2602.10655](http://arxiv.org/abs/2602.10655)
- 2026-02-11, **LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**, Anirudha Majumdar Team, Paper: [http://arxiv.org/abs/2602.10556](http://arxiv.org/abs/2602.10556), Code: **[https://lap-vla.github.io](https://lap-vla.github.io)**
- 2026-02-11, **MapVerse: A Benchmark for Geospatial Question Answering on Diverse Real-World Maps**, Vivek Gupta Team, Paper: [http://arxiv.org/abs/2602.10518](http://arxiv.org/abs/2602.10518)
- 2026-02-11, **Found-RL: foundation model-enhanced reinforcement learning for autonomous driving**, Sikai Chen Team, Paper: [http://arxiv.org/abs/2602.10458](http://arxiv.org/abs/2602.10458)
- 2026-02-11, **HII-DPO: Eliminate Hallucination via Accurate Hallucination-Inducing Counterfactual Images**, Chengming Zhang Team, Paper: [http://arxiv.org/abs/2602.10425](http://arxiv.org/abs/2602.10425)
- 2026-02-11, **LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies**, Hyun Myung Team, Paper: [http://arxiv.org/abs/2602.10399](http://arxiv.org/abs/2602.10399), Code: **[https://locovlm.github.io](https://locovlm.github.io)**
- 2026-02-11, **When Tables Go Crazy: Evaluating Multimodal Models on French Financial Documents**, Djamé Seddah Team, Paper: [http://arxiv.org/abs/2602.10384](http://arxiv.org/abs/2602.10384)
- 2026-02-10, **ST4VLA: Spatially Guided Training for Vision-Language-Action Models**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10109](http://arxiv.org/abs/2602.10109)
- 2026-02-11, **Fake-HR1: Rethinking Reasoning of Vision Language Model for Synthetic Image Detection**, Wei Lu Team, Paper: [http://arxiv.org/abs/2602.10042](http://arxiv.org/abs/2602.10042)
- 2026-02-10, **RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.09973](http://arxiv.org/abs/2602.09973)
- 2026-02-10, **Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning**, Yiming Gan Team, Paper: [http://arxiv.org/abs/2602.09972](http://arxiv.org/abs/2602.09972)
- 2026-02-10, **Kelix Technique Report**, Ziqi Wang Team, Paper: [http://arxiv.org/abs/2602.09843](http://arxiv.org/abs/2602.09843)
- 2026-02-10, **SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding**, Xudong Jiang Team, Paper: [http://arxiv.org/abs/2602.09825](http://arxiv.org/abs/2602.09825)
- 2026-02-10, **GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation**, Uma Mahesh Team, Paper: [http://arxiv.org/abs/2602.09701](http://arxiv.org/abs/2602.09701)
- 2026-02-10, **AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models**, Linlin Wang Team, Paper: [http://arxiv.org/abs/2602.09611](http://arxiv.org/abs/2602.09611)
- 2026-02-10, **Delving into Spectral Clustering with Vision-Language Representations**, Zhen Fang Team, Paper: [http://arxiv.org/abs/2602.09586](http://arxiv.org/abs/2602.09586)
- 2026-02-10, **Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination**, Koichi Shirahata Team, Paper: [http://arxiv.org/abs/2602.09541](http://arxiv.org/abs/2602.09541)
- 2026-02-10, **DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment**, Runze Hu Team, Paper: [http://arxiv.org/abs/2602.09531](http://arxiv.org/abs/2602.09531)
- 2026-02-10, **Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs**, Rujie Liu Team, Paper: [http://arxiv.org/abs/2602.09521](http://arxiv.org/abs/2602.09521)
- 2026-02-10, **SpotAgent: Grounding Visual Geo-localization in Large Vision-Language Models through Agentic Reasoning**, Yu Liu Team, Paper: [http://arxiv.org/abs/2602.09463](http://arxiv.org/abs/2602.09463)
- 2026-02-10, **P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads**, Ganqu Cui Team, Paper: [http://arxiv.org/abs/2602.09443](http://arxiv.org/abs/2602.09443)
- 2026-02-10, **Understanding and Enhancing Encoder-based Adversarial Transferability against Large Vision-Language Models**, Haibo Hu Team, Paper: [http://arxiv.org/abs/2602.09431](http://arxiv.org/abs/2602.09431)
- 2026-02-10, **Bridging the Modality Gap in Roadside LiDAR: A Training-Free Vision-Language Model Framework for Vehicle Classification**, Jie Wei Team, Paper: [http://arxiv.org/abs/2602.09425](http://arxiv.org/abs/2602.09425)
- 2026-02-10, **K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge**, Kurt Keutzer Team, Paper: [http://arxiv.org/abs/2602.09411](http://arxiv.org/abs/2602.09411), Code: **[https://github.com/zkkli/K-Sort-Eval](https://github.com/zkkli/K-Sort-Eval)**
- 2026-02-09, **VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models**, Tianyu Luan Team, Paper: [http://arxiv.org/abs/2602.09252](http://arxiv.org/abs/2602.09252)
- 2026-02-09, **Barycentric alignment for instance-level comparison of neural representations**, Meenakshi Khosla Team, Paper: [http://arxiv.org/abs/2602.09225](http://arxiv.org/abs/2602.09225)
- 2026-02-09, **VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models**, Wenchao Li Team, Paper: [http://arxiv.org/abs/2602.09214](http://arxiv.org/abs/2602.09214)
- 2026-02-09, **From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection**, Gim Hee Lee Team, Paper: [http://arxiv.org/abs/2602.09002](http://arxiv.org/abs/2602.09002)
- 2026-02-09, **Learning Self-Correction in Vision-Language Models via Rollout Augmentation**, Ruqi Zhang Team, Paper: [http://arxiv.org/abs/2602.08503](http://arxiv.org/abs/2602.08503)
- 2026-02-09, **SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios**, Chelsea Finn Team, Paper: [http://arxiv.org/abs/2602.08440](http://arxiv.org/abs/2602.08440)
- 2026-02-09, **What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning**, Sirui Han Team, Paper: [http://arxiv.org/abs/2602.08346](http://arxiv.org/abs/2602.08346)
- 2026-02-09, **CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT**, Luxin Xu Team, Paper: [http://arxiv.org/abs/2602.08339](http://arxiv.org/abs/2602.08339)
- 2026-02-09, **Moral Sycophancy in Vision Language Models**, Irfan Ahmad Team, Paper: [http://arxiv.org/abs/2602.08311](http://arxiv.org/abs/2602.08311)
- 2026-02-08, **Robustness of Vision Language Models Against Split-Image Harmful Input Attacks**, Shagufta Mehnaz Team, Paper: [http://arxiv.org/abs/2602.08136](http://arxiv.org/abs/2602.08136)
- 2026-02-08, **Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models**, Peng Wang Team, Paper: [http://arxiv.org/abs/2602.07899](http://arxiv.org/abs/2602.07899)
- 2026-02-08, **Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds**, Jiansheng Fan Team, Paper: [http://arxiv.org/abs/2602.07864](http://arxiv.org/abs/2602.07864)
- 2026-02-08, **LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge**, Tao Gu Team, Paper: [http://arxiv.org/abs/2602.07849](http://arxiv.org/abs/2602.07849)
- 2026-02-08, **Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures**, Simiao Ren Team, Paper: [http://arxiv.org/abs/2602.07815](http://arxiv.org/abs/2602.07815)
- 2026-02-08, **MaD-Mix: Multi-Modal Data Mixtures via Latent Space Coupling for Vision-Language Model Training**, Volkan Cevher Team, Paper: [http://arxiv.org/abs/2602.07790](http://arxiv.org/abs/2602.07790)
- 2026-02-08, **PAND: Prompt-Aware Neighborhood Distillation for Lightweight Fine-Grained Visual Classification**, Chang Kong Team, Paper: [http://arxiv.org/abs/2602.07768](http://arxiv.org/abs/2602.07768)
- 2026-02-07, **Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning**, Mohan Trivedi Team, Paper: [http://arxiv.org/abs/2602.07680](http://arxiv.org/abs/2602.07680)
- 2026-02-07, **From Dead Pixels to Editable Slides: Infographic Reconstruction into Native Google Slides via Vision-Language Region Understanding**, Leonardo Gonzalez Team, Paper: [http://arxiv.org/abs/2602.07645](http://arxiv.org/abs/2602.07645)
- 2026-02-07, **LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation**, Soumik Sarkar Team, Paper: [http://arxiv.org/abs/2602.07629](http://arxiv.org/abs/2602.07629)
- 2026-02-07, **HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology**, M. Khalid Khan Niazi Team, Paper: [http://arxiv.org/abs/2602.07608](http://arxiv.org/abs/2602.07608)
- 2026-02-07, **From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection**, Usman Naseem Team, Paper: [http://arxiv.org/abs/2602.07497](http://arxiv.org/abs/2602.07497)
- 2026-02-07, **Toward Vision-Language Assistants for Radio Astronomical Source Analysis**, S. Riggi Team, Paper: [http://arxiv.org/abs/2602.07469](http://arxiv.org/abs/2602.07469)
- 2026-02-07, **Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**, Miao Li Team, Paper: [http://arxiv.org/abs/2602.07434](http://arxiv.org/abs/2602.07434)
- 2026-02-06, **POP: Online Structural Pruning Enables Efficient Inference of Large Foundation Models**, Joo-Young Kim Team, Paper: [http://arxiv.org/abs/2602.06822](http://arxiv.org/abs/2602.06822)
- 2026-02-06, **Same Answer, Different Representations: Hidden instability in VLMs**, Pasquale Minervini Team, Paper: [http://arxiv.org/abs/2602.06652](http://arxiv.org/abs/2602.06652)
- 2026-02-06, **CauCLIP: Bridging the Sim-to-Real Gap in Surgical Video Understanding via Causality-Inspired Vision-Language Modeling**, Cheng Xue Team, Paper: [http://arxiv.org/abs/2602.06619](http://arxiv.org/abs/2602.06619)
- 2026-02-06, **SPARC: Separating Perception And Reasoning Circuits for Test-time Scaling of VLMs**, Mattia Rigotti Team, Paper: [http://arxiv.org/abs/2602.06566](http://arxiv.org/abs/2602.06566)
- 2026-02-06, **Universal Anti-forensics Attack against Image Forgery Detection via Multi-modal Guidance**, Anastasia Antsiferova Team, Paper: [http://arxiv.org/abs/2602.06530](http://arxiv.org/abs/2602.06530)
- 2026-02-06, **FloorplanVLM: A Vision-Language Model for Floorplan Vectorization**, Yue Yang Team, Paper: [http://arxiv.org/abs/2602.06507](http://arxiv.org/abs/2602.06507)
- 2026-02-06, **MeDocVL: A Visual Language Model for Medical Document Understanding and Parsing**, Hong Li Team, Paper: [http://arxiv.org/abs/2602.06402](http://arxiv.org/abs/2602.06402)
- 2026-02-06, **POINTS-GUI-G: GUI-Grounding Journey**, Jie Zhou Team, Paper: [http://arxiv.org/abs/2602.06391](http://arxiv.org/abs/2602.06391)
- 2026-02-05, **Cross-Modal Redundancy and the Geometry of Vision-Language Embeddings**, Agustin Picard Team, Paper: [http://arxiv.org/abs/2602.06218](http://arxiv.org/abs/2602.06218)
- 2026-02-05, **DeDPO: Debiased Direct Preference Optimization for Diffusion Models**, Ramin Zabih Team, Paper: [http://arxiv.org/abs/2602.06195](http://arxiv.org/abs/2602.06195)
- 2026-02-05, **PhenoLIP: Integrating Phenotype Ontology Knowledge into Medical Vision-Language Pretraining**, Weidi Xie Team, Paper: [http://arxiv.org/abs/2602.06184](http://arxiv.org/abs/2602.06184)
- 2026-02-05, **Compressing LLMs with MoP: Mixture of Pruners**, Artur Jordao Team, Paper: [http://arxiv.org/abs/2602.06127](http://arxiv.org/abs/2602.06127), Code: **[https://github.com/c2d-usp/Efficient-LLMs-with-MoP](https://github.com/c2d-usp/Efficient-LLMs-with-MoP)**
- 2026-02-05, **Can vision language models learn intuitive physics from interaction?**, Eric Schulz Team, Paper: [http://arxiv.org/abs/2602.06033](http://arxiv.org/abs/2602.06033)
- 2026-02-05, **GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?**, Jiaqi Wang Team, Paper: [http://arxiv.org/abs/2602.06013](http://arxiv.org/abs/2602.06013), Code: **[https://genarena.github.io/](https://genarena.github.io/)**
- 2026-02-05, **Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning**, Xianming Liu Team, Paper: [http://arxiv.org/abs/2602.05809](http://arxiv.org/abs/2602.05809)
- 2026-02-05, **Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**, Weiming Zhang Team, Paper: [http://arxiv.org/abs/2602.05789](http://arxiv.org/abs/2602.05789)
- 2026-02-05, **Ethology of Latent Spaces**, Philippe Boisnard Team, Paper: [http://arxiv.org/abs/2602.05710](http://arxiv.org/abs/2602.05710), Code: **[https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf](https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf)**
- 2026-02-05, **LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation**, Yiguo Qiao Team, Paper: [http://arxiv.org/abs/2602.05578](http://arxiv.org/abs/2602.05578)
- 2026-02-05, **TangramSR: Can Vision-Language Models Reason in Continuous Geometric Space?**, Cheston Tan Team, Paper: [http://arxiv.org/abs/2602.05570](http://arxiv.org/abs/2602.05570)
- 2026-02-05, **VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator**, Miguel Cazorla Team, Paper: [http://arxiv.org/abs/2602.05552](http://arxiv.org/abs/2602.05552)
- 2026-02-05, **Detecting Misbehaviors of Large Vision-Language Models by Evidential Uncertainty Quantification**, Liping Jing Team, Paper: [http://arxiv.org/abs/2602.05535](http://arxiv.org/abs/2602.05535), Code: **[https://github.com/HT86159/EUQ](https://github.com/HT86159/EUQ)**
- 2026-02-05, **Once Correct, Still Wrong: Counterfactual Hallucination in Multilingual Vision-Language Models**, Nadir Durrani Team, Paper: [http://arxiv.org/abs/2602.05437](http://arxiv.org/abs/2602.05437)
- 2026-02-05, **Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting**, Can Huang Team, Paper: [http://arxiv.org/abs/2602.05384](http://arxiv.org/abs/2602.05384)
- 2026-02-05, **VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs**, Konstantinos Psounis Team, Paper: [http://arxiv.org/abs/2602.05382](http://arxiv.org/abs/2602.05382)
- 2026-02-05, **Affordance-Aware Interactive Decision-Making and Execution for Ambiguous Instructions**, Tao Zhang Team, Paper: [http://arxiv.org/abs/2602.05273](http://arxiv.org/abs/2602.05273)
- 2026-02-05, **Length-Unbiased Sequence Policy Optimization: Revealing and Controlling Response Length Variation in RLVR**, Haibo Qiu Team, Paper: [http://arxiv.org/abs/2602.05261](http://arxiv.org/abs/2602.05261)
- 2026-02-05, **GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling**, Tong Zhang Team, Paper: [http://arxiv.org/abs/2602.05202](http://arxiv.org/abs/2602.05202)
- 2026-02-04, **ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation**, Yapeng Tian Team, Paper: [http://arxiv.org/abs/2602.05132](http://arxiv.org/abs/2602.05132)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **Do Vision-Language Models Respect Contextual Integrity in Location Disclosure?**, Alan Ritter Team, Paper: [http://arxiv.org/abs/2602.05023](http://arxiv.org/abs/2602.05023)
- 2026-02-04, **When LLaVA Meets Objects: Token Composition for Vision-Language-Models**, Hilde Kuehne Team, Paper: [http://arxiv.org/abs/2602.04864](http://arxiv.org/abs/2602.04864)
- 2026-02-04, **El Agente Estructural: An Artificially Intelligent Molecular Editor**, Varinia Bernales Team, Paper: [http://arxiv.org/abs/2602.04849](http://arxiv.org/abs/2602.04849)
- 2026-02-04, **VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?**, Huchuan Lu Team, Paper: [http://arxiv.org/abs/2602.04802](http://arxiv.org/abs/2602.04802)
- 2026-02-04, **Annotation Free Spacecraft Detection and Segmentation using Vision Language Models**, Djamila Aouada Team, Paper: [http://arxiv.org/abs/2602.04699](http://arxiv.org/abs/2602.04699)
- 2026-02-04, **AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04672](http://arxiv.org/abs/2602.04672)
- 2026-02-04, **PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04657](http://arxiv.org/abs/2602.04657)
- 2026-02-04, **Relational Scene Graphs for Object Grounding of Natural Language Commands**, Ville Kyrki Team, Paper: [http://arxiv.org/abs/2602.04635](http://arxiv.org/abs/2602.04635)
- 2026-02-04, **LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation**, Yan Song Team, Paper: [http://arxiv.org/abs/2602.04617](http://arxiv.org/abs/2602.04617)
- 2026-02-04, **VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration**, Kunwoo Park Team, Paper: [http://arxiv.org/abs/2602.04587](http://arxiv.org/abs/2602.04587)
- 2026-02-04, **Understanding Degradation with Vision Language Model**, Xuelong Li Team, Paper: [http://arxiv.org/abs/2602.04565](http://arxiv.org/abs/2602.04565)
- 2026-02-04, **EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**, Börje F. Karlsson Team, Paper: [http://arxiv.org/abs/2602.04515](http://arxiv.org/abs/2602.04515)
- 2026-02-04, **When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models**, Se-Young Yun Team, Paper: [http://arxiv.org/abs/2602.04356](http://arxiv.org/abs/2602.04356)
- 2026-02-04, **Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models**, Deyu Zhou Team, Paper: [http://arxiv.org/abs/2602.04355](http://arxiv.org/abs/2602.04355)
- 2026-02-04, **Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning**, Shu-Tao Xia Team, Paper: [http://arxiv.org/abs/2602.04340](http://arxiv.org/abs/2602.04340)
- 2026-02-04, **Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner**, Shu-Tao Xia Team, Paper: [http://arxiv.org/abs/2602.04337](http://arxiv.org/abs/2602.04337)
- 2026-02-04, **Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement**, Lin Gui Team, Paper: [http://arxiv.org/abs/2602.04304](http://arxiv.org/abs/2602.04304)
- 2026-02-04, **AppleVLM: End-to-end Autonomous Driving with Advanced Perception and Planning-Enhanced Vision-Language Models**, Yunjiang Lou Team, Paper: [http://arxiv.org/abs/2602.04256](http://arxiv.org/abs/2602.04256)
- 2026-02-04, **VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding**, Weining Shen Team, Paper: [http://arxiv.org/abs/2602.04094](http://arxiv.org/abs/2602.04094)
- 2026-02-03, **Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**, Rex Ying Team, Paper: [http://arxiv.org/abs/2602.03983](http://arxiv.org/abs/2602.03983)
- 2026-02-03, **VLS: Steering Pretrained Robot Policies via Vision-Language Models**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.03973](http://arxiv.org/abs/2602.03973), Code: **[https://vision-language-steering.github.io/webpage/](https://vision-language-steering.github.io/webpage/)**
- 2026-02-03, **They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References**, Usman Naseem Team, Paper: [http://arxiv.org/abs/2602.03822](http://arxiv.org/abs/2602.03822)
- 2026-02-03, **Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives**, Katherine D. Van Schaik Team, Paper: [http://arxiv.org/abs/2602.03750](http://arxiv.org/abs/2602.03750)
- 2026-02-03, **Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment**, Mahdi Abdelguerfi Team, Paper: [http://arxiv.org/abs/2602.03742](http://arxiv.org/abs/2602.03742)
- 2026-02-03, **RegionReasoner: Region-Grounded Multi-Round Visual Reasoning**, Cees G. M. Snoek Team, Paper: [http://arxiv.org/abs/2602.03733](http://arxiv.org/abs/2602.03733)
- 2026-02-03, **MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment**, Gunhee Kim Team, Paper: [http://arxiv.org/abs/2602.03665](http://arxiv.org/abs/2602.03665)
- 2026-02-03, **KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs**, Jianyuan Guo Team, Paper: [http://arxiv.org/abs/2602.03615](http://arxiv.org/abs/2602.03615)
- 2026-02-03, **TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection**, Mohammad Sabokrou Team, Paper: [http://arxiv.org/abs/2602.03594](http://arxiv.org/abs/2602.03594)
- 2026-02-03, **Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning**, Jianxiong Wang Team, Paper: [http://arxiv.org/abs/2602.03530](http://arxiv.org/abs/2602.03530)
- 2026-02-03, **Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance**, Min Zhang Team, Paper: [http://arxiv.org/abs/2602.03491](http://arxiv.org/abs/2602.03491)
- 2026-02-03, **Contextualized Visual Personalization in Vision-Language Models**, Sungroh Yoon Team, Paper: [http://arxiv.org/abs/2602.03454](http://arxiv.org/abs/2602.03454), Code: **[https://github.com/oyt9306/CoViP](https://github.com/oyt9306/CoViP)**
- 2026-02-03, **AesRec: A Dataset for Aesthetics-Aligned Clothing Outfit Recommendation**, Jimmy Xiangji Huang Team, Paper: [http://arxiv.org/abs/2602.03416](http://arxiv.org/abs/2602.03416)
- 2026-02-03, **Risk Awareness Injection: Calibrating Vision-Language Models for Safety without Compromising Utility**, Ming Li Team, Paper: [http://arxiv.org/abs/2602.03402](http://arxiv.org/abs/2602.03402)
- 2026-02-03, **POP: Prefill-Only Pruning for Efficient Large Model Inference**, Qingan Li Team, Paper: [http://arxiv.org/abs/2602.03295](http://arxiv.org/abs/2602.03295)
- 2026-02-03, **LaVPR: Benchmarking Language and Vision for Place Recognition**, Yoli Shavit Team, Paper: [http://arxiv.org/abs/2602.03253](http://arxiv.org/abs/2602.03253)
- 2026-02-03, **Enhancing Foundation VLM Robustness to Missing Modality: Scalable Diffusion for Bi-directional Feature Restoration**, Haixia Bi Team, Paper: [http://arxiv.org/abs/2602.03151](http://arxiv.org/abs/2602.03151)
- 2026-02-03, **SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass**, Xin Miao Team, Paper: [http://arxiv.org/abs/2602.03134](http://arxiv.org/abs/2602.03134)
- 2026-02-03, **FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation**, Rongjunchen Zhang Team, Paper: [http://arxiv.org/abs/2602.03130](http://arxiv.org/abs/2602.03130)
- 2026-02-03, **Function-Space Empirical Bayes Regularisation with Large Vision-Language Model Priors**, Wenbo Ding Team, Paper: [http://arxiv.org/abs/2602.03119](http://arxiv.org/abs/2602.03119)
- 2026-02-03, **IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning**, Yongchao Xu Team, Paper: [http://arxiv.org/abs/2602.03060](http://arxiv.org/abs/2602.03060)
- 2026-02-03, **Bongards at the Boundary of Perception and Reasoning: Programs or Language?**, Kevin Ellis Team, Paper: [http://arxiv.org/abs/2602.03038](http://arxiv.org/abs/2602.03038)
- 2026-02-02, **Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning**, Kostas Alexis Team, Paper: [http://arxiv.org/abs/2602.02456](http://arxiv.org/abs/2602.02456)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **ReasonEdit: Editing Vision-Language Models using Human Reasoning**, Thomas Hartvigsen Team, Paper: [http://arxiv.org/abs/2602.02408](http://arxiv.org/abs/2602.02408)
- 2026-02-02, **LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization**, Limin Wang Team, Paper: [http://arxiv.org/abs/2602.02341](http://arxiv.org/abs/2602.02341)
- 2026-02-02, **See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers**, Takeo Igarashi Team, Paper: [http://arxiv.org/abs/2602.02063](http://arxiv.org/abs/2602.02063)
- 2026-02-02, **Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models**, Toshihiko Yamasaki Team, Paper: [http://arxiv.org/abs/2602.02043](http://arxiv.org/abs/2602.02043)
- 2026-02-02, **Rethinking Genomic Modeling Through Optical Character Recognition**, Xiangxiang Zeng Team, Paper: [http://arxiv.org/abs/2602.02014](http://arxiv.org/abs/2602.02014)
- 2026-02-02, **Enhancing Multi-Image Understanding through Delimiter Token Scaling**, Junsuk Choe Team, Paper: [http://arxiv.org/abs/2602.01984](http://arxiv.org/abs/2602.01984)
- 2026-02-02, **VLM-Guided Experience Replay**, Shie Mannor Team, Paper: [http://arxiv.org/abs/2602.01915](http://arxiv.org/abs/2602.01915)
- 2026-02-02, **Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery**, J. Marius Zöllner Team, Paper: [http://arxiv.org/abs/2602.01836](http://arxiv.org/abs/2602.01836)
- 2026-02-02, **CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding**, Xiaodong Gu Team, Paper: [http://arxiv.org/abs/2602.01785](http://arxiv.org/abs/2602.01785), Code: **[https://github.com/YerbaPage/CodeOCR](https://github.com/YerbaPage/CodeOCR)**
- 2026-02-02, **Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models**, Bin Li Team, Paper: [http://arxiv.org/abs/2602.01738](http://arxiv.org/abs/2602.01738)
- 2026-02-02, **AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act**, Yu She Team, Paper: [http://arxiv.org/abs/2602.01662](http://arxiv.org/abs/2602.01662)
- 2026-02-02, **ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval**, Tat-Seng Chua Team, Paper: [http://arxiv.org/abs/2602.01639](http://arxiv.org/abs/2602.01639)
- 2026-02-02, **PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards**, Mei Chen Team, Paper: [http://arxiv.org/abs/2602.01624](http://arxiv.org/abs/2602.01624)
- 2026-02-02, **Generative Visual Code Mobile World Models**, Jamin Shin Team, Paper: [http://arxiv.org/abs/2602.01576](http://arxiv.org/abs/2602.01576)
- 2026-02-02, **SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models**, Xiaochun Cao Team, Paper: [http://arxiv.org/abs/2602.01574](http://arxiv.org/abs/2602.01574)
- 2026-02-02, **Preserving Localized Patch Semantics in VLMs**, Longin Jan Latecki Team, Paper: [http://arxiv.org/abs/2602.01530](http://arxiv.org/abs/2602.01530)
- 2026-02-02, **Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition**, Brian Keith-Norambuena Team, Paper: [http://arxiv.org/abs/2602.01527](http://arxiv.org/abs/2602.01527)
- 2026-02-01, **Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles**, Jiachen Bian Team, Paper: [http://arxiv.org/abs/2602.01452](http://arxiv.org/abs/2602.01452)
- 2026-01-30, **User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**, Maria Gorlatova Team, Paper: [http://arxiv.org/abs/2601.23281](http://arxiv.org/abs/2601.23281)
- 2026-01-30, **Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**, Liang-Jie Zhang Team, Paper: [http://arxiv.org/abs/2601.23253](http://arxiv.org/abs/2601.23253)
- 2026-01-30, **Structured Over Scale: Learning Spatial Reasoning from Educational Video**, Sarah Ostadabbas Team, Paper: [http://arxiv.org/abs/2601.23251](http://arxiv.org/abs/2601.23251)
- 2026-01-30, **Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**, Lijie Hu Team, Paper: [http://arxiv.org/abs/2601.23149](http://arxiv.org/abs/2601.23149)
- 2026-01-30, **One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**, Dong Liu Team, Paper: [http://arxiv.org/abs/2601.23041](http://arxiv.org/abs/2601.23041)
- 2026-01-30, **Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2601.22959](http://arxiv.org/abs/2601.22959)
- 2026-01-30, **Alignment among Language, Vision and Action Representations**, Stefano Nolfi Team, Paper: [http://arxiv.org/abs/2601.22948](http://arxiv.org/abs/2601.22948)
- 2026-01-30, **A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions**, Arno Eichberger Team, Paper: [http://arxiv.org/abs/2601.22830](http://arxiv.org/abs/2601.22830)
- 2026-01-30, **Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA**, Yinghuan Shi Team, Paper: [http://arxiv.org/abs/2601.22828](http://arxiv.org/abs/2601.22828)
- 2026-01-30, **HeatMat: Simulation of City Material Impact on Urban Heat Island Effect**, Rosalie Martin Team, Paper: [http://arxiv.org/abs/2601.22796](http://arxiv.org/abs/2601.22796)
- 2026-01-30, **Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models**, Christos Emmanouilidis Team, Paper: [http://arxiv.org/abs/2601.22754](http://arxiv.org/abs/2601.22754)
- 2026-01-30, **StreamSense: Streaming Social Task Detection with Selective Vision-Language Model Routing**, Roy Ka-Wei Lee Team, Paper: [http://arxiv.org/abs/2601.22738](http://arxiv.org/abs/2601.22738)
- 2026-01-30, **Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models**, Tat-Seng Chua Team, Paper: [http://arxiv.org/abs/2601.22737](http://arxiv.org/abs/2601.22737)
- 2026-01-30, **Vision-Language Models Unlock Task-Centric Latent Actions**, Vladislav Kurenkov Team, Paper: [http://arxiv.org/abs/2601.22714](http://arxiv.org/abs/2601.22714)
- 2026-01-30, **Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs**, Yawei Li Team, Paper: [http://arxiv.org/abs/2601.22709](http://arxiv.org/abs/2601.22709)
- 2026-01-30, **Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**, Kai Yuan Team, Paper: [http://arxiv.org/abs/2601.22701](http://arxiv.org/abs/2601.22701)
- 2026-01-30, **Bi-MCQ: Reformulating Vision-Language Alignment for Negation Understanding**, Hyun Gyu Lee Team, Paper: [http://arxiv.org/abs/2601.22696](http://arxiv.org/abs/2601.22696)
- 2026-01-30, **Leveraging Data to Say No: Memory Augmented Plug-and-Play Selective Prediction**, Nuno Vasconcelos Team, Paper: [http://arxiv.org/abs/2601.22570](http://arxiv.org/abs/2601.22570)
- 2026-01-30, **Countering the Over-Reliance Trap: Mitigating Object Hallucination for LVLMs via a Self-Validation Framework**, Jinsong Su Team, Paper: [http://arxiv.org/abs/2601.22451](http://arxiv.org/abs/2601.22451), Code: **[https://github.com/Liushiyu-0709/SelfVal](https://github.com/Liushiyu-0709/SelfVal)**
- 2026-01-29, **Jailbreaks on Vision Language Model via Multimodal Reasoning**, Yuguang Yao Team, Paper: [http://arxiv.org/abs/2601.22398](http://arxiv.org/abs/2601.22398)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLA

- 2026-02-18, **EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**, Linxi Fan Team, Paper: [http://arxiv.org/abs/2602.16710](http://arxiv.org/abs/2602.16710)
- 2026-02-17, **World Action Models are Zero-shot Policies**, Joel Jang Team, Paper: [http://arxiv.org/abs/2602.15922](http://arxiv.org/abs/2602.15922), Code: **[https://dreamzero0.github.io/](https://dreamzero0.github.io/)**
- 2026-02-17, **Selective Perception for Robot: Task-Aware Attention in Multimodal VLA**, Soo-Chul Lim Team, Paper: [http://arxiv.org/abs/2602.15543](http://arxiv.org/abs/2602.15543)
- 2026-02-17, **ActionCodec: What Makes for Good Action Tokenizers**, Jianye Hao Team, Paper: [http://arxiv.org/abs/2602.15397](http://arxiv.org/abs/2602.15397)
- 2026-02-16, **DM0: An Embodied-Native Vision-Language-Action Model towards Physical AI**, Tiancai Wang Team, Paper: [http://arxiv.org/abs/2602.14974](http://arxiv.org/abs/2602.14974), Code: **[https://github.com/Dexmal/dexbotic](https://github.com/Dexmal/dexbotic)**
- 2026-02-16, **DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving**, Yan Wang Team, Paper: [http://arxiv.org/abs/2602.14577](http://arxiv.org/abs/2602.14577)
- 2026-02-15, **WoVR: World Models as Reliable Simulators for Post-Training VLA Policies with RL**, Dongbin Zhao Team, Paper: [http://arxiv.org/abs/2602.13977](http://arxiv.org/abs/2602.13977)
- 2026-02-14, **Semantic-Contact Fields for Category-Level Generalizable Tactile Tool Manipulation**, Yan Wu Team, Paper: [http://arxiv.org/abs/2602.13833](http://arxiv.org/abs/2602.13833)
- 2026-02-14, **MOTIF: Learning Action Motifs for Few-shot Cross-Embodiment Transfer**, Heng Tao Shen Team, Paper: [http://arxiv.org/abs/2602.13764](http://arxiv.org/abs/2602.13764)
- 2026-02-14, **HBVLA: Pushing 1-Bit Post-Training Quantization for Vision-Language-Action Models**, Ivor Tsang Team, Paper: [http://arxiv.org/abs/2602.13710](http://arxiv.org/abs/2602.13710)
- 2026-02-13, **FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**, Xingxing Zuo Team, Paper: [http://arxiv.org/abs/2602.13444](http://arxiv.org/abs/2602.13444), Code: **[https://huajian-zeng.github.io/projects/flowhoi/](https://huajian-zeng.github.io/projects/flowhoi/)**
- 2026-02-13, **Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control**, Sergey Levine Team, Paper: [http://arxiv.org/abs/2602.13193](http://arxiv.org/abs/2602.13193)
- 2026-02-13, **UniManip: General-Purpose Zero-Shot Robotic Manipulation with Agentic Operational Graph**, Ziwei Wang Team, Paper: [http://arxiv.org/abs/2602.13086](http://arxiv.org/abs/2602.13086), Code: **[https://henryhcliu.github.io/unimanip](https://henryhcliu.github.io/unimanip)**
- 2026-02-13, **Learning Native Continuation for Action Chunking Flow Policies**, Yang Gao Team, Paper: [http://arxiv.org/abs/2602.12978](http://arxiv.org/abs/2602.12978), Code: **[https://lyfeng001.github.io/Legato/](https://lyfeng001.github.io/Legato/)**
- 2026-02-13, **ALOE: Action-Level Off-Policy Evaluation for Vision-Language-Action Model Post-Training**, Maoqing Yao Team, Paper: [http://arxiv.org/abs/2602.12691](http://arxiv.org/abs/2602.12691)
- 2026-02-13, **Xiaomi-Robotics-0: An Open-Sourced Vision-Language-Action Model with Real-Time Execution**, Quanyun Zhou Team, Paper: [http://arxiv.org/abs/2602.12684](http://arxiv.org/abs/2602.12684), Code: **[https://xiaomi-robotics-0.github.io](https://xiaomi-robotics-0.github.io)**
- 2026-02-16, **Beyond Imitation: Reinforcement Learning-Based Sim-Real Co-Training for VLA Models**, Yu Wang Team, Paper: [http://arxiv.org/abs/2602.12628](http://arxiv.org/abs/2602.12628)
- 2026-02-13, **CRAFT: Adapting VLA Models to Contact-rich Manipulation via Force-aware Curriculum Fine-tuning**, Jingtao Sun Team, Paper: [http://arxiv.org/abs/2602.12532](http://arxiv.org/abs/2602.12532)
- 2026-02-12, **ForeAct: Steering Your VLA with Efficient Visual Foresight Planning**, Song Han Team, Paper: [http://arxiv.org/abs/2602.12322](http://arxiv.org/abs/2602.12322)
- 2026-02-18, **Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment**, Marco Pavone Team, Paper: [http://arxiv.org/abs/2602.12281](http://arxiv.org/abs/2602.12281)
- 2026-02-12, **GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning**, Zheng Zhu Team, Paper: [http://arxiv.org/abs/2602.12099](http://arxiv.org/abs/2602.12099), Code: **[https://gigabrain05m.github.io/](https://gigabrain05m.github.io/)**
- 2026-02-15, **VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model**, Chelsea Finn Team, Paper: [http://arxiv.org/abs/2602.12063](http://arxiv.org/abs/2602.12063), Code: **[https://sites.google.com/view/vlaw-arxiv](https://sites.google.com/view/vlaw-arxiv)**
- 2026-02-12, **HoloBrain-0 Technical Report**, Zhizhong Su Team, Paper: [http://arxiv.org/abs/2602.12062](http://arxiv.org/abs/2602.12062)
- 2026-02-12, **When would Vision-Proprioception Policies Fail in Robotic Manipulation?**, Di Hu Team, Paper: [http://arxiv.org/abs/2602.12032](http://arxiv.org/abs/2602.12032)
- 2026-02-12, **JEPA-VLA: Video Predictive Embedding is Needed for VLA Models**, Mingsheng Long Team, Paper: [http://arxiv.org/abs/2602.11832](http://arxiv.org/abs/2602.11832)
- 2026-02-12, **ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**, Mu Xu Team, Paper: [http://arxiv.org/abs/2602.11598](http://arxiv.org/abs/2602.11598), Code: **[https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**
- 2026-02-11, **H-WM: Robotic Task and Motion Planning Guided by Hierarchical World Model**, Yingxue Zhang Team, Paper: [http://arxiv.org/abs/2602.11291](http://arxiv.org/abs/2602.11291)
- 2026-02-11, **RISE: Self-Improving Robot Policy with Compositional World Model**, Hongyang Li Team, Paper: [http://arxiv.org/abs/2602.11075](http://arxiv.org/abs/2602.11075), Code: **[https://opendrivelab.com/kai0-rl/](https://opendrivelab.com/kai0-rl/)**
- 2026-02-12, **Scaling World Model for Hierarchical Manipulation Policies**, Xinghang Li Team, Paper: [http://arxiv.org/abs/2602.10983](http://arxiv.org/abs/2602.10983)
- 2026-02-11, **RADAR: Benchmarking Vision-Language-Action Generalization via Real-World Dynamics, Spatial-Physical Intelligence, and Autonomous Evaluation**, Guangrun Wang Team, Paper: [http://arxiv.org/abs/2602.10980](http://arxiv.org/abs/2602.10980)
- 2026-02-11, **From Representational Complementarity to Dual Systems: Synergizing VLM and Vision-Only Backbones for End-to-End Driving**, Yan Wang Team, Paper: [http://arxiv.org/abs/2602.10719](http://arxiv.org/abs/2602.10719)
- 2026-02-11, **AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models**, F. Richard Yu Team, Paper: [http://arxiv.org/abs/2602.10698](http://arxiv.org/abs/2602.10698)
- 2026-02-11, **LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer**, Anirudha Majumdar Team, Paper: [http://arxiv.org/abs/2602.10556](http://arxiv.org/abs/2602.10556), Code: **[https://lap-vla.github.io](https://lap-vla.github.io)**
- 2026-02-10, **Hardware Co-Design Scaling Laws via Roofline Modelling for On-Device LLMs**, Cheng Deng Team, Paper: [http://arxiv.org/abs/2602.10377](http://arxiv.org/abs/2602.10377)
- 2026-02-10, **ST4VLA: Spatially Guided Training for Vision-Language-Action Models**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10109](http://arxiv.org/abs/2602.10109)
- 2026-02-10, **EgoHumanoid: Unlocking In-the-Wild Loco-Manipulation with Robot-Free Egocentric Demonstration**, Li Chen Team, Paper: [http://arxiv.org/abs/2602.10106](http://arxiv.org/abs/2602.10106), Code: **[https://opendrivelab.com/EgoHumanoid](https://opendrivelab.com/EgoHumanoid)**
- 2026-02-10, **VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model**, Zhibo Chen Team, Paper: [http://arxiv.org/abs/2602.10098](http://arxiv.org/abs/2602.10098)
- 2026-02-10, **UniVTAC: A Unified Simulation Platform for Visuo-Tactile Manipulation Data Generation, Learning, and Benchmarking**, Yao Mu Team, Paper: [http://arxiv.org/abs/2602.10093](http://arxiv.org/abs/2602.10093), Code: **[https://univtac.github.io/](https://univtac.github.io/)**
- 2026-02-10, **RoboInter: A Holistic Intermediate Representation Suite Towards Robotic Manipulation**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.09973](http://arxiv.org/abs/2602.09973)
- 2026-02-11, **BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation**, Jianyu Chen Team, Paper: [http://arxiv.org/abs/2602.09849](http://arxiv.org/abs/2602.09849)
- 2026-02-10, **NavDreamer: Video Models as Zero-Shot 3D Navigators**, Fei Gao Team, Paper: [http://arxiv.org/abs/2602.09765](http://arxiv.org/abs/2602.09765)
- 2026-02-10, **Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization**, Qin Jin Team, Paper: [http://arxiv.org/abs/2602.09722](http://arxiv.org/abs/2602.09722)
- 2026-02-10, **AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild**, Hui Xiong Team, Paper: [http://arxiv.org/abs/2602.09657](http://arxiv.org/abs/2602.09657)
- 2026-02-10, **Sci-VLA: Agentic VLA Inference Plugin for Long-Horizon Tasks in Scientific Experiments**, Shimin Di Team, Paper: [http://arxiv.org/abs/2602.09430](http://arxiv.org/abs/2602.09430)
- 2026-02-10, **CAPER: Constrained and Procedural Reasoning for Robotic Scientific Experiments**, Yifan Wu Team, Paper: [http://arxiv.org/abs/2602.09367](http://arxiv.org/abs/2602.09367)
- 2026-02-09, **TwinRL-VLA: Digital Twin-Driven Reinforcement Learning for Real-World Robotic Manipulation**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.09023](http://arxiv.org/abs/2602.09023)
- 2026-02-09, **Mimic Intent, Not Just Trajectories**, Panpan Cai Team, Paper: [http://arxiv.org/abs/2602.08602](http://arxiv.org/abs/2602.08602)
- 2026-02-09, **SteerVLA: Steering Vision-Language-Action Models in Long-Tail Driving Scenarios**, Chelsea Finn Team, Paper: [http://arxiv.org/abs/2602.08440](http://arxiv.org/abs/2602.08440)
- 2026-02-09, **Self-Supervised Bootstrapping of Action-Predictive Embodied Reasoning**, Marco Pavone Team, Paper: [http://arxiv.org/abs/2602.08167](http://arxiv.org/abs/2602.08167)
- 2026-02-08, **Recurrent-Depth VLA: Implicit Test-Time Compute Scaling of Vision-Language-Action Models via Latent Iterative Reasoning**, Ranjay Krishna Team, Paper: [http://arxiv.org/abs/2602.07845](http://arxiv.org/abs/2602.07845), Code: **[https://rd-vla.github.io/](https://rd-vla.github.io/)**
- 2026-02-10, **RLinf-USER: A Unified and Extensible System for Real-World Online Policy Learning in Embodied AI**, Yu Wang Team, Paper: [http://arxiv.org/abs/2602.07837](http://arxiv.org/abs/2602.07837)
- 2026-02-07, **VISOR: VIsual Spatial Object Reasoning for Language-driven Object Navigation**, Angel X Chang Team, Paper: [http://arxiv.org/abs/2602.07555](http://arxiv.org/abs/2602.07555)
- 2026-02-07, **Differentiate-and-Inject: Enhancing VLAs via Functional Differentiation Induced by In-Parameter Structural Reasoning**, Wei He Team, Paper: [http://arxiv.org/abs/2602.07541](http://arxiv.org/abs/2602.07541)
- 2026-02-07, **VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation**, En Yu Team, Paper: [http://arxiv.org/abs/2602.07399](http://arxiv.org/abs/2602.07399)
- 2026-02-06, **Force Generative Imitation Learning: Bridging Position Trajectory and Force Commands through Control Technique**, Toshiaki Tsuji Team, Paper: [http://arxiv.org/abs/2602.06620](http://arxiv.org/abs/2602.06620)
- 2026-02-06, **Think Proprioceptively: Embodied Visual Reasoning for VLA Manipulation**, Guodong Guo Team, Paper: [http://arxiv.org/abs/2602.06575](http://arxiv.org/abs/2602.06575)
- 2026-02-06, **LIBERO-X: Robustness Litmus for Vision-Language-Action Models**, Xinmin Liu Team, Paper: [http://arxiv.org/abs/2602.06556](http://arxiv.org/abs/2602.06556)
- 2026-02-06, **DriveWorld-VLA: Unified Latent-Space World Modeling with Vision-Language-Action for Autonomous Driving**, Long Chen Team, Paper: [http://arxiv.org/abs/2602.06521](http://arxiv.org/abs/2602.06521)
- 2026-02-06, **World-VLA-Loop: Closed-Loop Learning of Video World Model and VLA Policy**, Mike Zheng Shou Team, Paper: [http://arxiv.org/abs/2602.06508](http://arxiv.org/abs/2602.06508)
- 2026-02-06, **Action Hallucination in Generative Visual-Language-Action Models**, Eugene Lim Team, Paper: [http://arxiv.org/abs/2602.06339](http://arxiv.org/abs/2602.06339)
- 2026-02-05, **RL-VLA $^3$ : Reinforcement Learning VLA Accelerating via Full Asynchronism**, Junwu Xiong Team, Paper: [http://arxiv.org/abs/2602.05765](http://arxiv.org/abs/2602.05765)
- 2026-02-05, **Benchmarking Affordance Generalization with BusyBox**, Galen Mullins Team, Paper: [http://arxiv.org/abs/2602.05441](http://arxiv.org/abs/2602.05441)
- 2026-02-07, **RoboPaint: From Human Demonstration to Any Robot and Any View**, Zhengxue Cheng Team, Paper: [http://arxiv.org/abs/2602.05325](http://arxiv.org/abs/2602.05325)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models**, Dongdong Chen Team, Paper: [http://arxiv.org/abs/2602.05049](http://arxiv.org/abs/2602.05049), Code: **[https://vista-vla.github.io/](https://vista-vla.github.io/)**
- 2026-02-04, **Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2602.04600](http://arxiv.org/abs/2602.04600)
- 2026-02-04, **GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning**, Hao Tang Team, Paper: [http://arxiv.org/abs/2602.04315](http://arxiv.org/abs/2602.04315)
- 2026-02-04, **Reshaping Action Error Distributions for Reliable Vision-Language-Action Models**, Badong Chen Team, Paper: [http://arxiv.org/abs/2602.04228](http://arxiv.org/abs/2602.04228)
- 2026-02-04, **SCALE: Self-uncertainty Conditioned Adaptive Looking and Execution for Vision-Language-Action Models**, Jonghyun Choi Team, Paper: [http://arxiv.org/abs/2602.04208](http://arxiv.org/abs/2602.04208)
- 2026-02-04, **Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models**, Ross Greer Team, Paper: [http://arxiv.org/abs/2602.04184](http://arxiv.org/abs/2602.04184)
- 2026-02-03, **Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement**, Rex Ying Team, Paper: [http://arxiv.org/abs/2602.03983](http://arxiv.org/abs/2602.03983)
- 2026-02-03, **QVLA: Not All Channels Are Equal in Vision-Language-Action Model's Quantization**, Zhipeng Zhang Team, Paper: [http://arxiv.org/abs/2602.03782](http://arxiv.org/abs/2602.03782)
- 2026-02-03, **MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction**, Jungwoo Lee Team, Paper: [http://arxiv.org/abs/2602.03668](http://arxiv.org/abs/2602.03668)
- 2026-02-03, **CRL-VLA: Continual Vision-Language-Action Learning**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03445](http://arxiv.org/abs/2602.03445)
- 2026-02-03, **RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization**, Jun Zhu Team, Paper: [http://arxiv.org/abs/2602.03310](http://arxiv.org/abs/2602.03310)
- 2026-02-03, **When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens**, Miao Li Team, Paper: [http://arxiv.org/abs/2602.03153](http://arxiv.org/abs/2602.03153)
- 2026-02-02, **Accelerating Structured Chain-of-Thought in Autonomous Vehicles**, Marco Pavone Team, Paper: [http://arxiv.org/abs/2602.02864](http://arxiv.org/abs/2602.02864)
- 2026-02-02, **TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments**, Jiaqi Ma Team, Paper: [http://arxiv.org/abs/2602.02459](http://arxiv.org/abs/2602.02459)
- 2026-02-02, **World-Gymnast: Training Robots with Reinforcement Learning in a World Model**, Sherry Yang Team, Paper: [http://arxiv.org/abs/2602.02454](http://arxiv.org/abs/2602.02454), Code: **[https://world-gymnast.github.io/](https://world-gymnast.github.io/)**
- 2026-02-02, **MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models**, Lemiao Qiu Team, Paper: [http://arxiv.org/abs/2602.02212](http://arxiv.org/abs/2602.02212)
- 2026-02-02, **FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**, Haiyue Zhu Team, Paper: [http://arxiv.org/abs/2602.02142](http://arxiv.org/abs/2602.02142)
- 2026-02-02, **Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models**, Di Wang Team, Paper: [http://arxiv.org/abs/2602.01834](http://arxiv.org/abs/2602.01834)
- 2026-02-02, **From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2602.01811](http://arxiv.org/abs/2602.01811)
- 2026-02-01, **Latent Reasoning VLA: Latent Thinking and Prediction for Vision-Language-Action Models**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2602.01166](http://arxiv.org/abs/2602.01166)
- 2026-02-01, **Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs**, Matteo Matteucci Team, Paper: [http://arxiv.org/abs/2602.01158](http://arxiv.org/abs/2602.01158)
- 2026-02-01, **StreamVLA: Breaking the Reason-Act Cycle via Completion-State Gating**, Lu Fang Team, Paper: [http://arxiv.org/abs/2602.01100](http://arxiv.org/abs/2602.01100)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-31, **Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**, A. Postnikov Team, Paper: [http://arxiv.org/abs/2602.00919](http://arxiv.org/abs/2602.00919)
- 2026-01-31, **Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds**, Hengshuang Zhao Team, Paper: [http://arxiv.org/abs/2602.00807](http://arxiv.org/abs/2602.00807)
- 2026-01-31, **Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models**, Yanyong Zhang Team, Paper: [http://arxiv.org/abs/2602.00780](http://arxiv.org/abs/2602.00780)
- 2026-01-31, **SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning**, Ivor Tsang Team, Paper: [http://arxiv.org/abs/2602.00743](http://arxiv.org/abs/2602.00743)
- 2026-01-31, **Learning to Accelerate Vision-Language-Action Models through Adaptive Visual Token Caching**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00686](http://arxiv.org/abs/2602.00686)
- 2026-01-31, **ConLA: Contrastive Latent Action Learning from Human Videos for Robotic Manipulation**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00557](http://arxiv.org/abs/2602.00557)
- 2026-01-31, **Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning**, Shuo Yang Team, Paper: [http://arxiv.org/abs/2602.00500](http://arxiv.org/abs/2602.00500)
- 2026-01-30, **Vision-Language Models Unlock Task-Centric Latent Actions**, Vladislav Kurenkov Team, Paper: [http://arxiv.org/abs/2601.22714](http://arxiv.org/abs/2601.22714)
- 2026-01-30, **CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**, Jianzong Wang Team, Paper: [http://arxiv.org/abs/2601.22467](http://arxiv.org/abs/2601.22467)
- 2026-01-29, **DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**, Ziwei Liu Team, Paper: [http://arxiv.org/abs/2601.22153](http://arxiv.org/abs/2601.22153), Code: **[https://www.infinitescript.com/project/dynamic-vla/](https://www.infinitescript.com/project/dynamic-vla/)**
- 2026-01-29, **MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**, Stefanie Speidel Team, Paper: [http://arxiv.org/abs/2601.21971](http://arxiv.org/abs/2601.21971)
- 2026-01-29, **CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**, Yaohua Liu Team, Paper: [http://arxiv.org/abs/2601.21712](http://arxiv.org/abs/2601.21712)
- 2026-01-29, **AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**, Yonglin Tian Team, Paper: [http://arxiv.org/abs/2601.21602](http://arxiv.org/abs/2601.21602)
- 2026-01-29, **IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation**, Jeonggil Ko Team, Paper: [http://arxiv.org/abs/2601.21506](http://arxiv.org/abs/2601.21506)
- 2026-01-28, **Demonstration-Free Robotic Control via LLM Agents**, Tiffany J. Hwu Team, Paper: [http://arxiv.org/abs/2601.20334](http://arxiv.org/abs/2601.20334)
- 2026-01-30, **TaF-VLA: Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation**, Ziyuan Jiao Team, Paper: [http://arxiv.org/abs/2601.20321](http://arxiv.org/abs/2601.20321)
- 2026-01-28, **Shallow-π: Knowledge Distillation for Flow-based VLAs**, Taehan Kim Team, Paper: [http://arxiv.org/abs/2601.20262](http://arxiv.org/abs/2601.20262)
- 2026-01-27, **AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation**, Lei Zhu Team, Paper: [http://arxiv.org/abs/2601.19634](http://arxiv.org/abs/2601.19634)
- 2026-01-26, **Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods**, Hong Liu Team, Paper: [http://arxiv.org/abs/2601.18723](http://arxiv.org/abs/2601.18723)
- 2026-01-26, **A Pragmatic VLA Foundation Model**, Kecheng Zheng Team, Paper: [http://arxiv.org/abs/2601.18692](http://arxiv.org/abs/2601.18692), Code: **[https://technology.robbyant.com/lingbot-vla/](https://technology.robbyant.com/lingbot-vla/)**
- 2026-01-26, **TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion**, Jian Tang Team, Paper: [http://arxiv.org/abs/2601.18323](http://arxiv.org/abs/2601.18323)
- 2026-01-25, **PEAfowl: Perception-Enhanced Multi-View Vision-Language-Action for Bimanual Manipulation**, Xun Cao Team, Paper: [http://arxiv.org/abs/2601.17885](http://arxiv.org/abs/2601.17885)
- 2026-01-25, **SPACE-CLIP: Spatial Perception via Adaptive CLIP Embeddings for Monocular Depth Estimation**, Andrew Jaeyong Choi Team, Paper: [http://arxiv.org/abs/2601.17657](http://arxiv.org/abs/2601.17657)
- 2026-01-23, **ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance**, Wei-Shi Zheng Team, Paper: [http://arxiv.org/abs/2601.16667](http://arxiv.org/abs/2601.16667)
- 2026-01-23, **Gen-DBA: Generative Database Agents (Towards a Move 37 for Databases)**, Walid G. Aref Team, Paper: [http://arxiv.org/abs/2601.16409](http://arxiv.org/abs/2601.16409)
- 2026-01-22, **IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance**, Michael S Ryoo Team, Paper: [http://arxiv.org/abs/2601.16207](http://arxiv.org/abs/2601.16207)
- 2026-01-22, **Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning**, Jinwei Gu Team, Paper: [http://arxiv.org/abs/2601.16163](http://arxiv.org/abs/2601.16163)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## Humanoid

- 2026-02-18, **Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation**, Saurabh Gupta Team, Paper: [http://arxiv.org/abs/2602.16705](http://arxiv.org/abs/2602.16705), Code: **[https://hero-humanoid.github.io/](https://hero-humanoid.github.io/)**
- 2026-02-17, **Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**, C. Karen Liu Team, Paper: [http://arxiv.org/abs/2602.15827](http://arxiv.org/abs/2602.15827)
- 2026-02-17, **MeshMimic: Geometry-Aware Humanoid Motion Learning through 3D Scene Reconstruction**, Yijie Guo Team, Paper: [http://arxiv.org/abs/2602.15733](http://arxiv.org/abs/2602.15733)
- 2026-02-16, **Replanning Human-Robot Collaborative Tasks with Vision-Language Models via Semantic and Physical Dual-Correction**, Kensuke Harada Team, Paper: [http://arxiv.org/abs/2602.14551](http://arxiv.org/abs/2602.14551)
- 2026-02-16, **AdaptManip: Learning Adaptive Whole-Body Object Lifting and Delivery with Online Recurrent State Estimation**, Sehoon Ha Team, Paper: [http://arxiv.org/abs/2602.14363](http://arxiv.org/abs/2602.14363), Code: **[https://morganbyrd03.github.io/adaptmanip/](https://morganbyrd03.github.io/adaptmanip/)**
- 2026-02-15, **ProAct: A Dual-System Framework for Proactive Embodied Social Agents**, Libin Liu Team, Paper: [http://arxiv.org/abs/2602.14048](http://arxiv.org/abs/2602.14048), Code: **[https://proactrobot.github.io/](https://proactrobot.github.io/)**
- 2026-02-14, **Humanoid Hanoi: Investigating Shared Whole-Body Control for Skill-Based Box Rearrangement**, Alan Fern Team, Paper: [http://arxiv.org/abs/2602.13850](http://arxiv.org/abs/2602.13850)
- 2026-02-14, **Impact-Robust Posture Optimization for Aerial Manipulation**, Antonio Franchi Team, Paper: [http://arxiv.org/abs/2602.13762](http://arxiv.org/abs/2602.13762)
- 2026-02-14, **A Kung Fu Athlete Bot That Can Do It All Day: Highly Dynamic, Balance-Challenging Motion Dataset and Autonomous Fall-Resilient Tracking**, Xuesong Li Team, Paper: [http://arxiv.org/abs/2602.13656](http://arxiv.org/abs/2602.13656)
- 2026-02-12, **General Humanoid Whole-Body Control via Pretraining and Fast Adaptation**, Zongqing Lu Team, Paper: [http://arxiv.org/abs/2602.11929](http://arxiv.org/abs/2602.11929)
- 2026-02-12, **HAIC: Humanoid Agile Object Interaction Control via Dynamics-Aware World Model**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2602.11758](http://arxiv.org/abs/2602.11758), Code: **[https://haic-humanoid.github.io/](https://haic-humanoid.github.io/)**
- 2026-02-11, **APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**, Ding Zhao Team, Paper: [http://arxiv.org/abs/2602.11143](http://arxiv.org/abs/2602.11143), Code: **[https://apex-humanoid.github.io/](https://apex-humanoid.github.io/)**
- 2026-02-11, **Towards Learning a Generalizable 3D Scene Representation from 2D Observations**, Stefan Wermter Team, Paper: [http://arxiv.org/abs/2602.10943](http://arxiv.org/abs/2602.10943)
- 2026-02-10, **Humanoid Factors: Design Principles for AI Humanoids in Human Worlds**, Lixiao Huang Team, Paper: [http://arxiv.org/abs/2602.10069](http://arxiv.org/abs/2602.10069)
- 2026-02-10, **TeleGate: Whole-Body Humanoid Teleoperation via Gated Expert Selection with Motion Prior**, Rongyun Cao Team, Paper: [http://arxiv.org/abs/2602.09628](http://arxiv.org/abs/2602.09628)
- 2026-02-09, **Learning Human-Like Badminton Skills for Humanoid Robots**, Peng Lu Team, Paper: [http://arxiv.org/abs/2602.08370](http://arxiv.org/abs/2602.08370)
- 2026-02-14, **VividFace: Real-Time and Realistic Facial Expression Shadowing for Humanoid Robots**, Yang Zhang Team, Paper: [http://arxiv.org/abs/2602.07506](http://arxiv.org/abs/2602.07506)
- 2026-02-07, **TextOp: Real-time Interactive Text-Driven Humanoid Robot Motion Generation and Control**, Xuelong Li Team, Paper: [http://arxiv.org/abs/2602.07439](http://arxiv.org/abs/2602.07439), Code: **[https://text-op.github.io/](https://text-op.github.io/)**
- 2026-02-07, **Bridging Speech, Emotion, and Motion: a VLM-based Multimodal Edge-deployable Framework for Humanoid Robots**, Miao Li Team, Paper: [http://arxiv.org/abs/2602.07434](http://arxiv.org/abs/2602.07434)
- 2026-02-06, **ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**, Yao Su Team, Paper: [http://arxiv.org/abs/2602.06445](http://arxiv.org/abs/2602.06445)
- 2026-02-05, **A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion**, Simon F. G. Ehlers Team, Paper: [http://arxiv.org/abs/2602.05855](http://arxiv.org/abs/2602.05855)
- 2026-02-05, **Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2602.05791](http://arxiv.org/abs/2602.05791)
- 2026-02-05, **TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**, Jaeheung Park Team, Paper: [http://arxiv.org/abs/2602.05596](http://arxiv.org/abs/2602.05596)
- 2026-02-05, **Learning Soccer Skills for Humanoid Robots: A Progressive Perception-Action Framework**, Xuelong Li Team, Paper: [http://arxiv.org/abs/2602.05310](http://arxiv.org/abs/2602.05310)
- 2026-02-04, **PDF-HR: Pose Distance Fields for Humanoid Robots**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2602.04851](http://arxiv.org/abs/2602.04851), Code: **[https://gaoyukang33.github.io/PDF-HR/}{Project](https://gaoyukang33.github.io/PDF-HR/}{Project)**
- 2026-02-04, **EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models**, Börje F. Karlsson Team, Paper: [http://arxiv.org/abs/2602.04515](http://arxiv.org/abs/2602.04515)
- 2026-02-05, **HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation**, Hong Jia Team, Paper: [http://arxiv.org/abs/2602.04412](http://arxiv.org/abs/2602.04412)
- 2026-02-02, **Flow Policy Gradients for Robot Control**, Angjoo Kanazawa Team, Paper: [http://arxiv.org/abs/2602.02481](http://arxiv.org/abs/2602.02481), Code: **[https://hongsukchoi.github.io/fpo-control](https://hongsukchoi.github.io/fpo-control)**
- 2026-02-02, **HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos**, Ping Tan Team, Paper: [http://arxiv.org/abs/2602.02473](http://arxiv.org/abs/2602.02473)
- 2026-02-02, **TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour**, Hang Zhao Team, Paper: [http://arxiv.org/abs/2602.02331](http://arxiv.org/abs/2602.02331), Code: **[https://ttt-parkour.github.io/](https://ttt-parkour.github.io/)**
- 2026-02-02, **A Closed-Form Geometric Retargeting Solver for Upper Body Humanoid Robot Teleoperation**, Shreyas Kousik Team, Paper: [http://arxiv.org/abs/2602.01632](http://arxiv.org/abs/2602.01632), Code: **[https://sew-mimic.com/](https://sew-mimic.com/)**
- 2026-02-02, **RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots**, David Howard Team, Paper: [http://arxiv.org/abs/2602.01515](http://arxiv.org/abs/2602.01515)
- 2026-02-01, **T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation**, Xiaochun Mai Team, Paper: [http://arxiv.org/abs/2602.01352](http://arxiv.org/abs/2602.01352)
- 2026-01-31, **Green-VLA: Staged Vision-Language-Action Model for Generalist Robots**, A. Postnikov Team, Paper: [http://arxiv.org/abs/2602.00919](http://arxiv.org/abs/2602.00919)
- 2026-01-30, **ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control**, Farbod Farshidian Team, Paper: [http://arxiv.org/abs/2602.00401](http://arxiv.org/abs/2602.00401)
- 2026-01-30, **Robust and Generalized Humanoid Motion Tracking**, Dongdong Zheng Team, Paper: [http://arxiv.org/abs/2601.23080](http://arxiv.org/abs/2601.23080)
- 2026-01-30, **RoboStriker: Hierarchical Decision-Making for Autonomous Humanoid Boxing**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2601.22517](http://arxiv.org/abs/2601.22517)
- 2026-01-26, **HumanoidTurk: Expanding VR Haptics with Humanoids for Driving Simulations**, Jin-Hyuk Hong Team, Paper: [http://arxiv.org/abs/2601.18975](http://arxiv.org/abs/2601.18975)
- 2026-01-26, **Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot**, Josh Merel Team, Paper: [http://arxiv.org/abs/2601.18963](http://arxiv.org/abs/2601.18963)
- 2026-01-24, **MetaWorld: Skill Transfer and Composition in a Hierarchical World Model for Grounding High-Level Instructions**, Tongtong Feng Team, Paper: [http://arxiv.org/abs/2601.17507](http://arxiv.org/abs/2601.17507)
- 2026-01-24, **PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2601.17440](http://arxiv.org/abs/2601.17440)
- 2026-01-24, **Real-Time Synchronized Interaction Framework for Emotion-Aware Humanoid Robots**, Xihan Bian Team, Paper: [http://arxiv.org/abs/2601.17287](http://arxiv.org/abs/2601.17287)
- 2026-01-21, **Learning a Unified Latent Space for Cross-Embodiment Robot Control**, Dongheui Lee Team, Paper: [http://arxiv.org/abs/2601.15419](http://arxiv.org/abs/2601.15419)
- 2026-01-21, **Vision-Language Models on the Edge for Real-Time Robotic Perception**, Syed Ali Raza Zaidi Team, Paper: [http://arxiv.org/abs/2601.14921](http://arxiv.org/abs/2601.14921)
- 2026-01-21, **HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation**, Dzmitry Tsetserukou Team, Paper: [http://arxiv.org/abs/2601.14874](http://arxiv.org/abs/2601.14874)
- 2026-01-19, **FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions**, Xipeng Qiu Team, Paper: [http://arxiv.org/abs/2601.12799](http://arxiv.org/abs/2601.12799), Code: **[https://openmoss.github.io/FRoM-W1](https://openmoss.github.io/FRoM-W1)**
- 2026-01-19, **FocusNav: Spatial Selective Attention with Waypoint Guidance for Humanoid Local Navigation**, Yue Gao Team, Paper: [http://arxiv.org/abs/2601.12790](http://arxiv.org/abs/2601.12790)
- 2026-01-20, **ProjecTA: A Semi-Humanoid Robotic Teaching Assistant with In-Situ Projection for Guided Tours**, Pengcheng An Team, Paper: [http://arxiv.org/abs/2601.11328](http://arxiv.org/abs/2601.11328)
- 2026-01-15, **FastStair: Learning to Run Up Stairs with Humanoid Robots**, Jie Zhao Team, Paper: [http://arxiv.org/abs/2601.10365](http://arxiv.org/abs/2601.10365)
- 2026-01-13, **Heterogeneous computing platform for real-time robotics**, Steve Furber Team, Paper: [http://arxiv.org/abs/2601.09755](http://arxiv.org/abs/2601.09755)
- 2026-01-14, **Learning Whole-Body Human-Humanoid Interaction from Human-Human Demonstrations**, Wei-Shi Zheng Team, Paper: [http://arxiv.org/abs/2601.09518](http://arxiv.org/abs/2601.09518)
- 2026-01-13, **Generalizable Geometric Prior and Recurrent Spiking Feature Learning for Humanoid Robot Manipulation**, Miao Li Team, Paper: [http://arxiv.org/abs/2601.09031](http://arxiv.org/abs/2601.09031)
- 2026-01-12, **WaveMan: mmWave-Based Room-Scale Human Interaction Perception for Humanoid Robots**, Jianfei Yang Team, Paper: [http://arxiv.org/abs/2601.07454](http://arxiv.org/abs/2601.07454)
- 2026-01-12, **AdaMorph: Unified Motion Retargeting via Embodiment-Aware Adaptive Transformers**, Zecui Zeng Team, Paper: [http://arxiv.org/abs/2601.07284](http://arxiv.org/abs/2601.07284)
- 2026-01-11, **RSLCPP -- Deterministic Simulations Using ROS 2**, Markus Lienkamp Team, Paper: [http://arxiv.org/abs/2601.07052](http://arxiv.org/abs/2601.07052)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## Humanoid-Locomotion

- 2026-02-18, **Dynamic Modeling and MPC for Locomotion of Tendon-Driven Soft Quadruped**, Madhu Vadali Team, Paper: [http://arxiv.org/abs/2602.16371](http://arxiv.org/abs/2602.16371)
- 2026-02-17, **Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching**, C. Karen Liu Team, Paper: [http://arxiv.org/abs/2602.15827](http://arxiv.org/abs/2602.15827)
- 2026-02-13, **PMG: Parameterized Motion Generator for Human-like Locomotion Control**, Houde Liu Team, Paper: [http://arxiv.org/abs/2602.12656](http://arxiv.org/abs/2602.12656)
- 2026-02-11, **APEX: Learning Adaptive High-Platform Traversal for Humanoid Robots**, Ding Zhao Team, Paper: [http://arxiv.org/abs/2602.11143](http://arxiv.org/abs/2602.11143), Code: **[https://apex-humanoid.github.io/](https://apex-humanoid.github.io/)**
- 2026-02-11, **Co-jump: Cooperative Jumping with Quadrupedal Robots via Multi-Agent Reinforcement Learning**, Peng Lu Team, Paper: [http://arxiv.org/abs/2602.10514](http://arxiv.org/abs/2602.10514)
- 2026-02-11, **LocoVLM: Grounding Vision and Language for Adapting Versatile Legged Locomotion Policies**, Hyun Myung Team, Paper: [http://arxiv.org/abs/2602.10399](http://arxiv.org/abs/2602.10399), Code: **[https://locovlm.github.io](https://locovlm.github.io)**
- 2026-02-09, **Agile asymmetric multi-legged locomotion: contact planning via geometric mechanics and spin model duality**, Baxi Chong Team, Paper: [http://arxiv.org/abs/2602.09123](http://arxiv.org/abs/2602.09123)
- 2026-02-06, **ECO: Energy-Constrained Optimization with Reinforcement Learning for Humanoid Walking**, Yao Su Team, Paper: [http://arxiv.org/abs/2602.06445](http://arxiv.org/abs/2602.06445)
- 2026-02-06, **Now You See That: Learning End-to-End Humanoid Locomotion from Raw Pixels**, Zongwu Xie Team, Paper: [http://arxiv.org/abs/2602.06382](http://arxiv.org/abs/2602.06382)
- 2026-02-06, **HiWET: Hierarchical World-Frame End-Effector Tracking for Long-Horizon Humanoid Loco-Manipulation**, Yue Gao Team, Paper: [http://arxiv.org/abs/2602.06341](http://arxiv.org/abs/2602.06341)
- 2026-02-05, **Scalable and General Whole-Body Control for Cross-Humanoid Locomotion**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2602.05791](http://arxiv.org/abs/2602.05791)
- 2026-02-05, **TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards**, Jaeheung Park Team, Paper: [http://arxiv.org/abs/2602.05596](http://arxiv.org/abs/2602.05596)
- 2026-02-03, **CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03511](http://arxiv.org/abs/2602.03511)
- 2026-02-09, **Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms**, Sung-Eui Yoon Team, Paper: [http://arxiv.org/abs/2602.03397](http://arxiv.org/abs/2602.03397), Code: **[https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/](https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA25/)**
- 2026-02-02, **Flow Policy Gradients for Robot Control**, Angjoo Kanazawa Team, Paper: [http://arxiv.org/abs/2602.02481](http://arxiv.org/abs/2602.02481), Code: **[https://hongsukchoi.github.io/fpo-control](https://hongsukchoi.github.io/fpo-control)**
- 2026-02-13, **Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control**, Jingwen Zhang Team, Paper: [http://arxiv.org/abs/2601.21363](http://arxiv.org/abs/2601.21363)
- 2026-01-28, **GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control**, Guillaume Sartoretti Team, Paper: [http://arxiv.org/abs/2601.20668](http://arxiv.org/abs/2601.20668)
- 2026-01-22, **Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision**, Dongheui Lee Team, Paper: [http://arxiv.org/abs/2601.16109](http://arxiv.org/abs/2601.16109)
- 2026-01-13, **AME-2: Agile and Generalized Legged Locomotion via Attention-Based Neural Map Encoding**, Marco Hutter Team, Paper: [http://arxiv.org/abs/2601.08485](http://arxiv.org/abs/2601.08485)
- 2026-01-09, **Walk the PLANC: Physics-Guided RL for Agile Humanoid Locomotion on Constrained Footholds**, Aaron D. Ames Team, Paper: [http://arxiv.org/abs/2601.06286](http://arxiv.org/abs/2601.06286)
- 2026-01-07, **Locomotion Beyond Feet**, C. Karen Liu Team, Paper: [http://arxiv.org/abs/2601.03607](http://arxiv.org/abs/2601.03607), Code: **[https://locomotion-beyond-feet.github.io/](https://locomotion-beyond-feet.github.io/)**
- 2025-12-31, **Dynamic Policy Learning for Legged Robot with Simplified Model Pretraining and Model Homotopy Transfer**, Hae-Won Park Team, Paper: [http://arxiv.org/abs/2512.24698](http://arxiv.org/abs/2512.24698)
- 2026-01-04, **Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.23650](http://arxiv.org/abs/2512.23650)
- 2026-01-04, **RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.23649](http://arxiv.org/abs/2512.23649)
- 2025-12-23, **Pneumatic bladder links with wide range of motion joints for articulated inflatable robots**, Ryuma Niiyama Team, Paper: [http://arxiv.org/abs/2512.20322](http://arxiv.org/abs/2512.20322)
- 2025-12-18, **E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion**, Dimitrios Kanoulas Team, Paper: [http://arxiv.org/abs/2512.16446](http://arxiv.org/abs/2512.16446)
- 2025-12-15, **Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations**, Ayonga Hereid Team, Paper: [http://arxiv.org/abs/2512.12993](http://arxiv.org/abs/2512.12993)
- 2025-12-08, **Gait-Adaptive Perceptive Humanoid Locomotion with Real-Time Under-Base Terrain Reconstruction**, Houqiang Li Team, Paper: [http://arxiv.org/abs/2512.07464](http://arxiv.org/abs/2512.07464)
- 2025-12-03, **Variable-Impedance Muscle Coordination under Slow-Rate Control Frequencies and Limited Observation Conditions Evaluated through Legged Locomotion**, Jun Morimoto Team, Paper: [http://arxiv.org/abs/2512.03459](http://arxiv.org/abs/2512.03459)
- 2025-12-01, **Learning Sim-to-Real Humanoid Locomotion in 15 Minutes**, Pieter Abbeel Team, Paper: [http://arxiv.org/abs/2512.01996](http://arxiv.org/abs/2512.01996), Code: **[https://younggyo.me/fastsac-humanoid](https://younggyo.me/fastsac-humanoid)**
- 2025-11-30, **H-Zero: Cross-Humanoid Locomotion Pretraining Enables Few-shot Novel Embodiment Transfer**, Weinan Zhang Team, Paper: [http://arxiv.org/abs/2512.00971](http://arxiv.org/abs/2512.00971)
- 2025-11-25, **A Hierarchical Framework for Humanoid Locomotion with Supernumerary Limbs**, Bowen Zhi Team, Paper: [http://arxiv.org/abs/2512.00077](http://arxiv.org/abs/2512.00077)
- 2025-11-28, **Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary**, Jingya Wang Team, Paper: [http://arxiv.org/abs/2511.22963](http://arxiv.org/abs/2511.22963), Code: **[https://humanoidlla.github.io/](https://humanoidlla.github.io/)**
- 2025-11-27, **Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion**, Wael Suleiman Team, Paper: [http://arxiv.org/abs/2511.22744](http://arxiv.org/abs/2511.22744), Code: **[https://anonymous.4open.science/r/multiview-parkour-6FB8](https://anonymous.4open.science/r/multiview-parkour-6FB8)**
- 2026-01-29, **HAFO: A Force-Adaptive Control Framework for Humanoid Robots in Intense Interaction Environments**, Bin He Team, Paper: [http://arxiv.org/abs/2511.20275](http://arxiv.org/abs/2511.20275)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLN-Navigation

- 2026-01-26, **ReasonNavi: Human-Inspired Global Map Reasoning for Zero-Shot Embodied Navigation**, Chi-Keung Tang Team, Paper: [http://arxiv.org/abs/2602.15864](http://arxiv.org/abs/2602.15864), Code: **[https://reasonnavi.github.io/](https://reasonnavi.github.io/)**
- 2026-02-16, **pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI**, Haibing Guan Team, Paper: [http://arxiv.org/abs/2602.14401](http://arxiv.org/abs/2602.14401)
- 2026-02-12, **ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation**, Mu Xu Team, Paper: [http://arxiv.org/abs/2602.11598](http://arxiv.org/abs/2602.11598), Code: **[https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/](https://amap-cvlab.github.io/ABot-Navigation/ABot-N0/)**
- 2026-01-30, **Mitigating Error Accumulation in Continuous Navigation via Memory-Augmented Kalman Filtering**, Deyu Zhang Team, Paper: [http://arxiv.org/abs/2602.11183](http://arxiv.org/abs/2602.11183)
- 2026-02-10, **AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild**, Hui Xiong Team, Paper: [http://arxiv.org/abs/2602.09657](http://arxiv.org/abs/2602.09657)
- 2026-02-09, **When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning**, Mohit Bansal Team, Paper: [http://arxiv.org/abs/2602.08236](http://arxiv.org/abs/2602.08236), Code: **[https://adaptive-visual-tts.github.io/](https://adaptive-visual-tts.github.io/)**
- 2026-02-10, **LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation**, Soumik Sarkar Team, Paper: [http://arxiv.org/abs/2602.07629](http://arxiv.org/abs/2602.07629)
- 2026-02-06, **Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters**, Mu Xu Team, Paper: [http://arxiv.org/abs/2602.06427](http://arxiv.org/abs/2602.06427)
- 2026-02-06, **Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation**, Weiying Xie Team, Paper: [http://arxiv.org/abs/2602.06356](http://arxiv.org/abs/2602.06356)
- 2026-02-05, **Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation**, Hongyang Li Team, Paper: [http://arxiv.org/abs/2602.05827](http://arxiv.org/abs/2602.05827)
- 2026-02-05, **Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation**, Weiming Zhang Team, Paper: [http://arxiv.org/abs/2602.05789](http://arxiv.org/abs/2602.05789)
- 2026-02-02, **LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation**, Anton van den Hengel Team, Paper: [http://arxiv.org/abs/2602.02220](http://arxiv.org/abs/2602.02220)
- 2026-02-03, **MapDream: Task-Driven Map Learning for Vision-Language Navigation**, Zhaoxin Fan Team, Paper: [http://arxiv.org/abs/2602.00222](http://arxiv.org/abs/2602.00222)
- 2026-01-29, **Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**, Xiaoming Wang Team, Paper: [http://arxiv.org/abs/2601.21751](http://arxiv.org/abs/2601.21751)
- 2026-01-26, **\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation**, Feng Zheng Team, Paper: [http://arxiv.org/abs/2601.18188](http://arxiv.org/abs/2601.18188)
- 2026-01-23, **FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation**, Yonggang Qi Team, Paper: [http://arxiv.org/abs/2601.13976](http://arxiv.org/abs/2601.13976)
- 2026-01-14, **Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning**, Yahong Han Team, Paper: [http://arxiv.org/abs/2601.09111](http://arxiv.org/abs/2601.09111)
- 2026-01-11, **Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**, Bin Ren Team, Paper: [http://arxiv.org/abs/2601.08868](http://arxiv.org/abs/2601.08868)
- 2026-01-13, **VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**, Junzhi Yu Team, Paper: [http://arxiv.org/abs/2601.08665](http://arxiv.org/abs/2601.08665), Code: **[https://wsakobe.github.io/VLingNav-web/](https://wsakobe.github.io/VLingNav-web/)**
- 2026-01-07, **AirNav: A Large-Scale Real-World UAV Vision-and-Language Navigation Dataset with Natural and Diverse Instructions**, Renxin Zhong Team, Paper: [http://arxiv.org/abs/2601.03707](http://arxiv.org/abs/2601.03707)
- 2026-01-19, **LocoScooter: Designing a Stationary Scooter-Based Locomotion System for Navigation in Virtual Reality**, Ge Lin Kan Team, Paper: [http://arxiv.org/abs/2601.02167](http://arxiv.org/abs/2601.02167)
- 2026-01-05, **CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios**, Xueqian Wang Team, Paper: [http://arxiv.org/abs/2601.01872](http://arxiv.org/abs/2601.01872)
- 2026-01-06, **VLN-MME: Diagnosing MLLMs as Language-guided Visual Navigation agents**, Qi Wu Team, Paper: [http://arxiv.org/abs/2512.24851](http://arxiv.org/abs/2512.24851)
- 2026-01-23, **VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2512.22342](http://arxiv.org/abs/2512.22342)
- 2025-12-25, **AstraNav-World: World Model for Foresight Control and Consistency**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2512.21714](http://arxiv.org/abs/2512.21714)
- 2025-12-25, **AstraNav-Memory: Contexts Compression for Long Memory**, Mu Xu Team, Paper: [http://arxiv.org/abs/2512.21627](http://arxiv.org/abs/2512.21627)
- 2025-12-24, **ETP-R1: Evolving Topological Planning with Reinforcement Fine-tuning for Vision-Language Navigation in Continuous Environments**, Yue Wang Team, Paper: [http://arxiv.org/abs/2512.20940](http://arxiv.org/abs/2512.20940)
- 2025-12-23, **TongSIM: A General Platform for Simulating Intelligent Machines**, Zhenliang Zhang Team, Paper: [http://arxiv.org/abs/2512.20206](http://arxiv.org/abs/2512.20206)
- 2025-12-22, **IndoorUAV: Benchmarking Vision-Language UAV Navigation in Continuous Indoor Environments**, Zhouhui Lian Team, Paper: [http://arxiv.org/abs/2512.19024](http://arxiv.org/abs/2512.19024)
- 2025-12-22, **VLNVerse: A Benchmark for Vision-Language Navigation with Versatile, Embodied, Realistic Simulation and Evaluation**, Qi Wu Team, Paper: [http://arxiv.org/abs/2512.19021](http://arxiv.org/abs/2512.19021)
- 2025-12-19, **Embodied4C: Measuring What Matters for Embodied Vision-Language Navigation**, Eric Sax Team, Paper: [http://arxiv.org/abs/2512.18028](http://arxiv.org/abs/2512.18028)
- 2026-01-08, **ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination**, Changyin Sun Team, Paper: [http://arxiv.org/abs/2512.17435](http://arxiv.org/abs/2512.17435)
- 2025-12-17, **HERO: Hierarchical Traversable 3D Scene Graphs for Embodied Navigation Among Movable Obstacles**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2512.15047](http://arxiv.org/abs/2512.15047)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## VLA-Navigation

- 2026-02-12, **LongNav-R1: Horizon-Adaptive Multi-Turn RL for Long-Horizon VLA Navigation**, Maani Ghaffari Team, Paper: [http://arxiv.org/abs/2602.12351](http://arxiv.org/abs/2602.12351)
- 2025-08-14, **CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model**, Hao Dong Team, Paper: [http://arxiv.org/abs/2508.10416](http://arxiv.org/abs/2508.10416)
- 2024-07-12, **Mobility VLA: Multimodal Instruction Navigation with Long-Context VLMs and Topological Graphs**, Jie Tan Team, Paper: [http://arxiv.org/abs/2407.07775](http://arxiv.org/abs/2407.07775)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## Dexterous

- 2026-02-18, **One Hand to Rule Them All: Canonical Representations for Unified Dexterous Manipulation**, Mingyu Ding Team, Paper: [http://arxiv.org/abs/2602.16712](http://arxiv.org/abs/2602.16712), Code: **[https://zhenyuwei2003.github.io/OHRA/](https://zhenyuwei2003.github.io/OHRA/)**
- 2026-02-18, **EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data**, Linxi Fan Team, Paper: [http://arxiv.org/abs/2602.16710](http://arxiv.org/abs/2602.16710)
- 2026-02-17, **Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation**, Shubham Tulsiani Team, Paper: [http://arxiv.org/abs/2602.15828](http://arxiv.org/abs/2602.15828), Code: **[https://dex4d.github.io/](https://dex4d.github.io/)**
- 2026-02-15, **Rigidity-Based Multi-Finger Coordination for Precise In-Hand Manipulation of Force-Sensitive Objects**, Gangshan Jing Team, Paper: [http://arxiv.org/abs/2602.14104](http://arxiv.org/abs/2602.14104), Code: **[https://www.youtube.com/watch?v=kcf9dVW0Dpo](https://www.youtube.com/watch?v=kcf9dVW0Dpo)**
- 2026-02-13, **FlowHOI: Flow-based Semantics-Grounded Generation of Hand-Object Interactions for Dexterous Robot Manipulation**, Xingxing Zuo Team, Paper: [http://arxiv.org/abs/2602.13444](http://arxiv.org/abs/2602.13444), Code: **[https://huajian-zeng.github.io/projects/flowhoi/](https://huajian-zeng.github.io/projects/flowhoi/)**
- 2026-02-10, **DexImit: Learning Bimanual Dexterous Manipulation from Monocular Human Videos**, Jiangmiao Pang Team, Paper: [http://arxiv.org/abs/2602.10105](http://arxiv.org/abs/2602.10105)
- 2026-02-10, **AnyTouch 2: General Optical Tactile Representation Learning For Dynamic Tactile Perception**, Di Hu Team, Paper: [http://arxiv.org/abs/2602.09617](http://arxiv.org/abs/2602.09617)
- 2026-02-10, **Sample-Efficient Real-World Dexterous Policy Fine-Tuning via Action-Chunked Critics and Normalizing Flows**, Robert K. Katzschmann Team, Paper: [http://arxiv.org/abs/2602.09580](http://arxiv.org/abs/2602.09580)
- 2026-02-10, **Certified Gradient-Based Contact-Rich Manipulation via Smoothing-Error Reachable Tubes**, Glen Chou Team, Paper: [http://arxiv.org/abs/2602.09368](http://arxiv.org/abs/2602.09368)
- 2026-02-11, **Dexterous Manipulation Policies from RGB Human Videos via 3D Hand-Object Trajectory Reconstruction**, Jeffrey Ichnowski Team, Paper: [http://arxiv.org/abs/2602.09013](http://arxiv.org/abs/2602.09013)
- 2026-02-09, **Mimic Intent, Not Just Trajectories**, Panpan Cai Team, Paper: [http://arxiv.org/abs/2602.08602](http://arxiv.org/abs/2602.08602)
- 2026-02-09, **DexFormer: Cross-Embodied Dexterous Manipulation via History-Conditioned Transformer**, Renjing Xu Team, Paper: [http://arxiv.org/abs/2602.08278](http://arxiv.org/abs/2602.08278)
- 2026-02-10, **Going with the Flow: Koopman Behavioral Models as Implicit Planners for Visuo-Motor Dexterity**, Harish Ravichandar Team, Paper: [http://arxiv.org/abs/2602.07413](http://arxiv.org/abs/2602.07413), Code: **[https://github.com/GT-STAR-Lab/K-UBM/](https://github.com/GT-STAR-Lab/K-UBM/)**
- 2026-02-05, **DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter**, Zhenguo Sun Team, Paper: [http://arxiv.org/abs/2602.05513](http://arxiv.org/abs/2602.05513)
- 2026-02-05, **TaSA: Two-Phased Deep Predictive Learning of Tactile Sensory Attenuation for Improving In-Grasp Manipulation**, Shigeki Sugano Team, Paper: [http://arxiv.org/abs/2602.05468](http://arxiv.org/abs/2602.05468)
- 2026-02-07, **RoboPaint: From Human Demonstration to Any Robot and Any View**, Zhengxue Cheng Team, Paper: [http://arxiv.org/abs/2602.05325](http://arxiv.org/abs/2602.05325)
- 2026-02-05, **MobileManiBench: Simplifying Model Verification for Mobile Manipulation**, Baining Guo Team, Paper: [http://arxiv.org/abs/2602.05233](http://arxiv.org/abs/2602.05233)
- 2026-02-04, **AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation**, Chunhua Shen Team, Paper: [http://arxiv.org/abs/2602.04672](http://arxiv.org/abs/2602.04672)
- 2026-02-03, **CRL-VLA: Continual Vision-Language-Action Learning**, Chao Huang Team, Paper: [http://arxiv.org/abs/2602.03445](http://arxiv.org/abs/2602.03445)
- 2026-02-02, **FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation**, Haiyue Zhu Team, Paper: [http://arxiv.org/abs/2602.02142](http://arxiv.org/abs/2602.02142)
- 2026-02-05, **RFS: Reinforcement Learning with Residual Flow Steering for Dexterous Manipulation**, Abhishek Gupta Team, Paper: [http://arxiv.org/abs/2602.01789](http://arxiv.org/abs/2602.01789)
- 2026-02-01, **A Systematic Study of Data Modalities and Strategies for Co-training Large Behavior Models for Robot Manipulation**, Jose Barreiros Team, Paper: [http://arxiv.org/abs/2602.01067](http://arxiv.org/abs/2602.01067)
- 2026-01-29, **DexTac: Learning Contact-aware Visuotactile Policies via Hand-by-hand Teaching**, Shuo Wang Team, Paper: [http://arxiv.org/abs/2601.21474](http://arxiv.org/abs/2601.21474)
- 2026-01-26, **Grasp-and-Lift: Executable 3D Hand-Object Interaction Reconstruction via Physics-in-the-Loop Optimization**, Jongwoo Lim Team, Paper: [http://arxiv.org/abs/2601.18121](http://arxiv.org/abs/2601.18121)
- 2026-01-24, **PILOT: A Perceptive Integrated Low-level Controller for Loco-manipulation over Unstructured Scenes**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2601.17440](http://arxiv.org/abs/2601.17440)
- 2026-01-31, **CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes**, Hao Dong Team, Paper: [http://arxiv.org/abs/2601.15039](http://arxiv.org/abs/2601.15039)
- 2026-01-23, **Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation**, Wanxin Jin Team, Paper: [http://arxiv.org/abs/2601.10930](http://arxiv.org/abs/2601.10930)
- 2026-01-13, **FSAG: Enhancing Human-to-Dexterous-Hand Finger-Specific Affordance Grounding via Diffusion Models**, Wenzhao Lian Team, Paper: [http://arxiv.org/abs/2601.08246](http://arxiv.org/abs/2601.08246)
- 2026-01-12, **Stable In-hand Manipulation for a Lightweight Four-motor Prosthetic Hand**, Masashi Hamaya Team, Paper: [http://arxiv.org/abs/2601.07559](http://arxiv.org/abs/2601.07559)
- 2026-01-09, **DexterCap: An Affordable and Automated System for Capturing Dexterous Hand-Object Manipulation**, Libin Liu Team, Paper: [http://arxiv.org/abs/2601.05844](http://arxiv.org/abs/2601.05844)
- 2026-02-02, **LaST $_{0}$ : Latent Spatio-Temporal Chain-of-Thought for Robotic Vision-Language-Action Model**, Shanghang Zhang Team, Paper: [http://arxiv.org/abs/2601.05248](http://arxiv.org/abs/2601.05248)
- 2026-01-08, **UniBiDex: A Unified Teleoperation Framework for Robotic Bimanual Dexterous Manipulation**, Peng Zhou Team, Paper: [http://arxiv.org/abs/2601.04629](http://arxiv.org/abs/2601.04629)
- 2026-01-09, **Closing the Reality Gap: Zero-Shot Sim-to-Real Deployment for Dexterous Force-Based Grasping and Manipulation**, Zhibin Li Team, Paper: [http://arxiv.org/abs/2601.02778](http://arxiv.org/abs/2601.02778)
- 2026-01-04, **DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos**, Robert B. Fisher Team, Paper: [http://arxiv.org/abs/2601.01651](http://arxiv.org/abs/2601.01651)
- 2025-12-31, **ShowUI- $π$ : Flow-based Generative Models as GUI Dexterous Hands**, Mike Zheng Shou Team, Paper: [http://arxiv.org/abs/2512.24965](http://arxiv.org/abs/2512.24965)
- 2025-12-31, **Antagonistic Bowden-Cable Actuation of a Lightweight Robotic Hand: Toward Dexterous Manipulation for Payload Constrained Humanoids**, David Hyunchul Shim Team, Paper: [http://arxiv.org/abs/2512.24657](http://arxiv.org/abs/2512.24657)
- 2026-01-01, **World In Your Hands: A Large-Scale and Open-source Ecosystem for Learning Human-centric Manipulation in the Wild**, Wenchao Ding Team, Paper: [http://arxiv.org/abs/2512.24310](http://arxiv.org/abs/2512.24310)
- 2026-01-09, **GR-Dexter Technical Report**, Hang Li Team, Paper: [http://arxiv.org/abs/2512.24210](http://arxiv.org/abs/2512.24210)
- 2025-12-29, **UniTacHand: Unified Spatio-Tactile Representation for Human to Robotic Hand Skill Transfer**, Zongqing Lu Team, Paper: [http://arxiv.org/abs/2512.21233](http://arxiv.org/abs/2512.21233)
- 2025-12-22, **Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations**, Ping Tan Team, Paper: [http://arxiv.org/abs/2512.19583](http://arxiv.org/abs/2512.19583)
- 2025-12-19, **SurgiPose: Estimating Surgical Tool Kinematics from Monocular Video for Surgical Robot Learning**, Axel Krieger Team, Paper: [http://arxiv.org/abs/2512.18068](http://arxiv.org/abs/2512.18068)
- 2025-12-17, **ISS Policy : Scalable Diffusion Policy with Implicit Scene Supervision**, Jie Mei Team, Paper: [http://arxiv.org/abs/2512.15020](http://arxiv.org/abs/2512.15020)
- 2025-12-15, **World Models Can Leverage Human Videos for Dexterous Manipulation**, Yann LeCun Team, Paper: [http://arxiv.org/abs/2512.13644](http://arxiv.org/abs/2512.13644)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

## Semantic-SLAM

- 2026-02-12, **VGGT-based online 3D semantic SLAM for indoor scene understanding and navigation**, Kristóf Karacs Team, Paper: [http://arxiv.org/abs/2602.15899](http://arxiv.org/abs/2602.15899)
- 2026-01-09, **FeatureSLAM: Feature-enriched 3D gaussian splatting SLAM in real time**, Simon Hadfield Team, Paper: [http://arxiv.org/abs/2601.05738](http://arxiv.org/abs/2601.05738)
- 2025-12-01, **KM-ViPE: Online Tightly Coupled Vision-Language-Geometry Fusion for Open-Vocabulary Semantic SLAM**, Sergey Kolyubin Team, Paper: [http://arxiv.org/abs/2512.01889](http://arxiv.org/abs/2512.01889)
- 2025-11-28, **Taming the Light: Illumination-Invariant Semantic 3DGS-SLAM**, Zhenhong Jia Team, Paper: [http://arxiv.org/abs/2511.22968](http://arxiv.org/abs/2511.22968)
- 2025-11-27, **Building temporally coherent 3D maps with VGGT for memory-efficient Semantic SLAM**, Anna Gelencsér-Horváth Team, Paper: [http://arxiv.org/abs/2511.16282](http://arxiv.org/abs/2511.16282)
- 2025-10-01, **Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions**, Nak Young Chong Team, Paper: [http://arxiv.org/abs/2510.00783](http://arxiv.org/abs/2510.00783)
- 2025-09-18, **Human Interaction for Collaborative Semantic SLAM using Extended Reality**, Jose Luis Sanchez-Lopez Team, Paper: [http://arxiv.org/abs/2509.14949](http://arxiv.org/abs/2509.14949)
- 2025-07-16, **Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards**, Gert Kootstra Team, Paper: [http://arxiv.org/abs/2507.12093](http://arxiv.org/abs/2507.12093)
- 2025-12-03, **GS4: Generalizable Sparse Splatting Semantic SLAM**, Li Fuxin Team, Paper: [http://arxiv.org/abs/2506.06517](http://arxiv.org/abs/2506.06517)
- 2025-06-03, **GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal**, Yingchun Fan Team, Paper: [http://arxiv.org/abs/2506.02736](http://arxiv.org/abs/2506.02736)
- 2025-05-18, **Is Semantic SLAM Ready for Embedded Systems ? A Comparative Survey**, François Goulette Team, Paper: [http://arxiv.org/abs/2505.12384](http://arxiv.org/abs/2505.12384)
- 2025-05-16, **GSFF-SLAM: 3D Semantic Gaussian Splatting SLAM via Feature Field**, Changyin Sun Team, Paper: [http://arxiv.org/abs/2504.19409](http://arxiv.org/abs/2504.19409)
- 2025-04-01, **Semantic SLAM with Rolling-Shutter Cameras and Low-Precision INS in Outdoor Environments**, Haoyi Xiong Team, Paper: [http://arxiv.org/abs/2504.01997](http://arxiv.org/abs/2504.01997)
- 2025-03-03, **OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding**, Mengyin Fu Team, Paper: [http://arxiv.org/abs/2503.01646](http://arxiv.org/abs/2503.01646)
- 2025-07-09, **Hier-SLAM++: Neuro-Symbolic Semantic SLAM with a Hierarchically Categorical Gaussian Splatting**, Hamid Rezatofighi Team, Paper: [http://arxiv.org/abs/2502.14931](http://arxiv.org/abs/2502.14931)
- 2024-12-31, **PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM**, Tongliang Liu Team, Paper: [http://arxiv.org/abs/2501.00352](http://arxiv.org/abs/2501.00352)
- 2025-07-11, **Towards Autonomous Indoor Parking: A Globally Consistent Semantic SLAM System and A Semantic Localization Subsystem**, Hesheng Wang Team, Paper: [http://arxiv.org/abs/2410.12169](http://arxiv.org/abs/2410.12169)
- 2025-03-10, **Hier-SLAM: Scaling-up Semantics in SLAM with a Hierarchically Categorical Gaussian Splatting**, Hamid Rezatofighi Team, Paper: [http://arxiv.org/abs/2409.12518](http://arxiv.org/abs/2409.12518), Code: **[https://github.com/LeeBY68/Hier-SLAM](https://github.com/LeeBY68/Hier-SLAM)**
- 2024-09-02, **Active Semantic Mapping and Pose Graph Spectral Analysis for Robot Exploration**, Giovanni Beltrame Team, Paper: [http://arxiv.org/abs/2408.14726](http://arxiv.org/abs/2408.14726)
- 2025-10-03, **SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation**, Vijay Kumar Team, Paper: [http://arxiv.org/abs/2406.17249](http://arxiv.org/abs/2406.17249)
- 2024-06-09, **MAP-ADAPT: Real-Time Quality-Adaptive Semantic 3D Maps**, Iro Armeni Team, Paper: [http://arxiv.org/abs/2406.05849](http://arxiv.org/abs/2406.05849)

<p align=right>(<a href=#updated-on-20260220>back to top</a>)</p>

